{"2024-08-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2405.18320v2","updated":"2024-08-01T17:43:19Z","published":"2024-05-28T16:11:11Z","title":"Self-Supervised Learning Based Handwriting Verification","summary":"  We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.\n","authors":["Mihir Chauhan","Mohammad Abuzar Hashemi","Abhishek Satbhai","Mir Basheer Ali","Bina Ramamurthy","Mingchen Gao","Siwei Lyu","Sargur Srihari"],"pdf_url":"https://arxiv.org/pdf/2405.18320v2.pdf","comment":"8 pages, 2 figures, 2 tables, Accepted at Irish Machine Vision and\n  Image Processing Conference 2024"},{"id":"http://arxiv.org/abs/2407.13692v2","updated":"2024-08-01T17:18:54Z","published":"2024-07-18T16:58:18Z","title":"Prover-Verifier Games improve legibility of LLM outputs","summary":"  One way to increase confidence in the outputs of Large Language Models (LLMs)\nis to support them with reasoning that is clear and easy to check -- a property\nwe call legibility. We study legibility in the context of solving grade-school\nmath problems and show that optimizing chain-of-thought solutions only for\nanswer correctness can make them less legible. To mitigate the loss in\nlegibility, we propose a training algorithm inspired by Prover-Verifier Game\nfrom Anil et al. (2021). Our algorithm iteratively trains small verifiers to\npredict solution correctness, \"helpful\" provers to produce correct solutions\nthat the verifier accepts, and \"sneaky\" provers to produce incorrect solutions\nthat fool the verifier. We find that the helpful prover's accuracy and the\nverifier's robustness to adversarial attacks increase over the course of\ntraining. Furthermore, we show that legibility training transfers to\ntime-constrained humans tasked with verifying solution correctness. Over course\nof LLM training human accuracy increases when checking the helpful prover's\nsolutions, and decreases when checking the sneaky prover's solutions. Hence,\ntraining for checkability by small verifiers is a plausible technique for\nincreasing output legibility. Our results suggest legibility training against\nsmall verifiers as a practical avenue for increasing legibility of large LLMs\nto humans, and thus could help with alignment of superhuman models.\n","authors":["Jan Hendrik Kirchner","Yining Chen","Harri Edwards","Jan Leike","Nat McAleese","Yuri Burda"],"pdf_url":"https://arxiv.org/pdf/2407.13692v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10251v3","updated":"2024-08-01T16:27:20Z","published":"2024-06-10T08:23:52Z","title":"The Impact of Quantization on Retrieval-Augmented Generation: An\n  Analysis of Small LLMs","summary":"  Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.\n","authors":["Mert Yazan","Suzan Verberne","Frederik Situmeang"],"pdf_url":"https://arxiv.org/pdf/2406.10251v3.pdf","comment":"Accepted to the IR-RAG Workshop at SIGIR 2024"},{"id":"http://arxiv.org/abs/2303.07865v4","updated":"2024-08-01T16:14:04Z","published":"2023-03-14T12:56:47Z","title":"Predicting the Geolocation of Tweets Using transformer models on\n  Customized Data","summary":"  This research is aimed to solve the tweet/user geolocation prediction task\nand provide a flexible methodology for the geotagging of textual big data. The\nsuggested approach implements neural networks for natural language processing\n(NLP) to estimate the location as coordinate pairs (longitude, latitude) and\ntwo-dimensional Gaussian Mixture Models (GMMs). The scope of proposed models\nhas been finetuned on a Twitter dataset using pretrained Bidirectional Encoder\nRepresentations from Transformers (BERT) as base models. Performance metrics\nshow a median error of fewer than 30 km on a worldwide-level, and fewer than 15\nkm on the US-level datasets for the models trained and evaluated on text\nfeatures of tweets' content and metadata context. Our source code and data are\navailable at https://github.com/K4TEL/geo-twitter.git\n","authors":["Kateryna Lutsai","Christoph H. Lampert"],"pdf_url":"https://arxiv.org/pdf/2303.07865v4.pdf","comment":"31 pages, 5 tables, 9 figures"},{"id":"http://arxiv.org/abs/2406.11911v2","updated":"2024-08-01T15:44:19Z","published":"2024-06-16T16:46:55Z","title":"A Notion of Complexity for Theory of Mind via Discrete World Models","summary":"  Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework to measure the complexity of ToM tasks. We quantify a problem's\ncomplexity as the number of states necessary to solve it correctly. Our\ncomplexity measure also accounts for spurious states of a ToM problem designed\nto make it apparently harder. We use our method to assess the complexity of\nfive widely adopted ToM benchmarks. On top of this framework, we design a\nprompting technique that augments the information available to a model with a\ndescription of how the environment changes with the agents' interactions. We\nname this technique Discrete World Models (DWM) and show how it elicits\nsuperior performance on ToM tasks.\n","authors":["X. Angelo Huang","Emanuele La Malfa","Samuele Marro","Andrea Asperti","Anthony Cohn","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2406.11911v2.pdf","comment":"https://flecart.github.io/complexity-tom-dwm"},{"id":"http://arxiv.org/abs/2403.14814v3","updated":"2024-08-01T15:15:34Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v3.pdf","comment":"15 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2407.11733v2","updated":"2024-08-01T15:09:12Z","published":"2024-07-16T14:04:35Z","title":"How Are LLMs Mitigating Stereotyping Harms? Learning from Search Engine\n  Studies","summary":"  With the widespread availability of LLMs since the release of ChatGPT and\nincreased public scrutiny, commercial model development appears to have focused\ntheir efforts on 'safety' training concerning legal liabilities at the expense\nof social impact evaluation. This mimics a similar trend which we could observe\nfor search engine autocompletion some years prior. We draw on scholarship from\nNLP and search engine auditing and present a novel evaluation task in the style\nof autocompletion prompts to assess stereotyping in LLMs. We assess LLMs by\nusing four metrics, namely refusal rates, toxicity, sentiment and regard, with\nand without safety system prompts. Our findings indicate an improvement to\nstereotyping outputs with the system prompt, but overall a lack of attention by\nLLMs under study to certain harms classified as toxic, particularly for prompts\nabout peoples/ethnicities and sexual orientation. Mentions of intersectional\nidentities trigger a disproportionate amount of stereotyping. Finally, we\ndiscuss the implications of these findings about stereotyping harms in light of\nthe coming intermingling of LLMs and search and the choice of stereotyping\nmitigation policy to adopt. We address model builders, academics, NLP\npractitioners and policy makers, calling for accountability and awareness\nconcerning stereotyping harms, be it for training data curation, leader board\ndesign and usage, or social impact measurement.\n","authors":["Alina Leidinger","Richard Rogers"],"pdf_url":"https://arxiv.org/pdf/2407.11733v2.pdf","comment":"Accepted at AAAI/ACM AI, Ethics, and Society"},{"id":"http://arxiv.org/abs/2312.07913v5","updated":"2024-08-01T14:35:52Z","published":"2023-12-13T06:11:42Z","title":"A Survey of Text Watermarking in the Era of Large Language Models","summary":"  Text watermarking algorithms are crucial for protecting the copyright of\ntextual content. Historically, their capabilities and application scenarios\nwere limited. However, recent advancements in large language models (LLMs) have\nrevolutionized these techniques. LLMs not only enhance text watermarking\nalgorithms with their advanced abilities but also create a need for employing\nthese algorithms to protect their own copyrights or prevent potential misuse.\nThis paper conducts a comprehensive survey of the current state of text\nwatermarking technology, covering four main aspects: (1) an overview and\ncomparison of different text watermarking techniques; (2) evaluation methods\nfor text watermarking algorithms, including their detectability, impact on text\nor LLM quality, robustness under target or untargeted attacks; (3) potential\napplication scenarios for text watermarking technology; (4) current challenges\nand future directions for text watermarking. This survey aims to provide\nresearchers with a thorough understanding of text watermarking technology in\nthe era of LLM, thereby promoting its further advancement.\n","authors":["Aiwei Liu","Leyi Pan","Yijian Lu","Jingjing Li","Xuming Hu","Xi Zhang","Lijie Wen","Irwin King","Hui Xiong","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2312.07913v5.pdf","comment":"35 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2406.16107v2","updated":"2024-08-01T13:55:54Z","published":"2024-06-23T13:50:08Z","title":"Decoder-only Architecture for Streaming End-to-end Speech Recognition","summary":"  Decoder-only language models (LMs) have been successfully adopted for\nspeech-processing tasks including automatic speech recognition (ASR). The LMs\nhave ample expressiveness and perform efficiently. This efficiency is a\nsuitable characteristic for streaming applications of ASR. In this work, we\npropose to use a decoder-only architecture for blockwise streaming ASR. In our\napproach, speech features are compressed using CTC output and context embedding\nusing blockwise speech subnetwork, and are sequentially provided as prompts to\nthe decoder. The decoder estimates the output tokens promptly at each block. To\nthis end, we also propose a novel training scheme using random-length prefix\nprompts to make the model robust to the truncated prompts caused by blockwise\nprocessing. An experimental comparison shows that our proposed decoder-only\nstreaming ASR achieves 8% relative word error rate reduction in the LibriSpeech\ntest-other set while being twice as fast as the baseline model.\n","authors":["Emiru Tsunoo","Hayato Futami","Yosuke Kashiwagi","Siddhant Arora","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2406.16107v2.pdf","comment":"Accepted for Interspeech 2024"},{"id":"http://arxiv.org/abs/2407.21139v2","updated":"2024-08-01T12:24:01Z","published":"2024-07-30T19:03:03Z","title":"Enhancing Semantic Similarity Understanding in Arabic NLP with Nested\n  Embedding Learning","summary":"  This work presents a novel framework for training Arabic nested embedding\nmodels through Matryoshka Embedding Learning, leveraging multilingual,\nArabic-specific, and English-based models, to highlight the power of nested\nembeddings models in various Arabic NLP downstream tasks. Our innovative\ncontribution includes the translation of various sentence similarity datasets\ninto Arabic, enabling a comprehensive evaluation framework to compare these\nmodels across different dimensions. We trained several nested embedding models\non the Arabic Natural Language Inference triplet dataset and assessed their\nperformance using multiple evaluation metrics, including Pearson and Spearman\ncorrelations for cosine similarity, Manhattan distance, Euclidean distance, and\ndot product similarity. The results demonstrate the superior performance of the\nMatryoshka embedding models, particularly in capturing semantic nuances unique\nto the Arabic language. Results demonstrated that Arabic Matryoshka embedding\nmodels have superior performance in capturing semantic nuances unique to the\nArabic language, significantly outperforming traditional models by up to\n20-25\\% across various similarity metrics. These results underscore the\neffectiveness of language-specific training and highlight the potential of\nMatryoshka models in enhancing semantic textual similarity tasks for Arabic\nNLP.\n","authors":["Omer Nacar","Anis Koubaa"],"pdf_url":"https://arxiv.org/pdf/2407.21139v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21659v2","updated":"2024-08-01T12:22:28Z","published":"2024-07-31T15:02:46Z","title":"Defending Jailbreak Attack in VLMs via Cross-modality Information\n  Detector","summary":"  Vision Language Models (VLMs) extend the capacity of LLMs to comprehensively\nunderstand vision information, achieving remarkable performance in many\nvision-centric tasks. Despite that, recent studies have shown that these models\nare susceptible to jailbreak attacks, which refer to an exploitative technique\nwhere malicious users can break the safety alignment of the target model and\ngenerate misleading and harmful answers. This potential threat is caused by\nboth the inherent vulnerabilities of LLM and the larger attack scope introduced\nby vision input. To enhance the security of VLMs against jailbreak attacks,\nresearchers have developed various defense techniques. However, these methods\neither require modifications to the model's internal structure or demand\nsignificant computational resources during the inference phase. Multimodal\ninformation is a double-edged sword. While it increases the risk of attacks, it\nalso provides additional data that can enhance safeguards. Inspired by this, we\npropose $\\underline{\\textbf{C}}$ross-modality\n$\\underline{\\textbf{I}}$nformation\n$\\underline{\\textbf{DE}}$tecto$\\underline{\\textbf{R}}$ ($\\textit{CIDER})$, a\nplug-and-play jailbreaking detector designed to identify maliciously perturbed\nimage inputs, utilizing the cross-modal similarity between harmful queries and\nadversarial images. This simple yet effective cross-modality information\ndetector, $\\textit{CIDER}$, is independent of the target VLMs and requires less\ncomputation cost. Extensive experimental results demonstrate the effectiveness\nand efficiency of $\\textit{CIDER}$, as well as its transferability to both\nwhite-box and black-box VLMs.\n","authors":["Yue Xu","Xiuyuan Qi","Zhan Qin","Wenjie Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21659v2.pdf","comment":"12 pages, 9 figures"},{"id":"http://arxiv.org/abs/2310.07161v3","updated":"2024-08-01T11:37:16Z","published":"2023-10-11T03:19:22Z","title":"Psychoacoustic Challenges Of Speech Enhancement On VoIP Platforms","summary":"  Within the ambit of VoIP (Voice over Internet Protocol) telecommunications,\nthe complexities introduced by acoustic transformations merit rigorous\nanalysis. This research, rooted in the exploration of proprietary sender-side\ndenoising effects, meticulously evaluates platforms such as Google Meets and\nZoom. The study draws upon the Deep Noise Suppression (DNS) 2020 dataset,\nensuring a structured examination tailored to various denoising settings and\nreceiver interfaces. A methodological novelty is introduced via Blinder-Oaxaca\ndecomposition, traditionally an econometric tool, repurposed herein to analyze\nacoustic-phonetic perturbations within VoIP systems. To further ground the\nimplications of these transformations, psychoacoustic metrics, specifically\nPESQ and STOI, were used to explain of perceptual quality and intelligibility.\nCumulatively, the insights garnered underscore the intricate landscape of\nVoIP-influenced acoustic dynamics. In addition to the primary findings, a\nmultitude of metrics are reported, extending the research purview. Moreover,\nout-of-domain benchmarking for both time and time-frequency domain speech\nenhancement models is included, thereby enhancing the depth and applicability\nof this inquiry.\n","authors":["Joseph Konan","Shikhar Agnihotri","Ojas Bhargave","Shuo Han","Yunyang Zeng","Ankit Shah","Bhiksha Raj"],"pdf_url":"https://arxiv.org/pdf/2310.07161v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09713v2","updated":"2024-08-01T11:24:13Z","published":"2024-03-11T15:15:27Z","title":"A Hybrid Intelligence Method for Argument Mining","summary":"  Large-scale survey tools enable the collection of citizen feedback in opinion\ncorpora. Extracting the key arguments from a large and noisy set of opinions\nhelps in understanding the opinions quickly and accurately. Fully automated\nmethods can extract arguments but (1) require large labeled datasets that\ninduce large annotation costs and (2) work well for known viewpoints, but not\nfor novel points of view. We propose HyEnA, a hybrid (human + AI) method for\nextracting arguments from opinionated texts, combining the speed of automated\nprocessing with the understanding and reasoning capabilities of humans. We\nevaluate HyEnA on three citizen feedback corpora. We find that, on the one\nhand, HyEnA achieves higher coverage and precision than a state-of-the-art\nautomated method when compared to a common set of diverse opinions, justifying\nthe need for human insight. On the other hand, HyEnA requires less human effort\nand does not compromise quality compared to (fully manual) expert analysis,\ndemonstrating the benefit of combining human and artificial intelligence.\n","authors":["Michiel van der Meer","Enrico Liscio","Catholijn M. Jonker","Aske Plaat","Piek Vossen","Pradeep K. Murukannaiah"],"pdf_url":"https://arxiv.org/pdf/2403.09713v2.pdf","comment":"Published in JAIR"},{"id":"http://arxiv.org/abs/2407.21441v2","updated":"2024-08-01T10:35:57Z","published":"2024-07-31T08:44:29Z","title":"QuestGen: Effectiveness of Question Generation Methods for Fact-Checking\n  Applications","summary":"  Verifying fact-checking claims poses a significant challenge, even for\nhumans. Recent approaches have demonstrated that decomposing claims into\nrelevant questions to gather evidence enhances the efficiency of the\nfact-checking process. In this paper, we provide empirical evidence showing\nthat this question decomposition can be effectively automated. We demonstrate\nthat smaller generative models, fine-tuned for the question generation task\nusing data augmentation from various datasets, outperform large language models\nby up to 8%. Surprisingly, in some cases, the evidence retrieved using\nmachine-generated questions proves to be significantly more effective for\nfact-checking than that obtained from human-written questions. We also perform\nmanual evaluation of the decomposed questions to assess the quality of the\nquestions generated.\n","authors":["Ritvik Setty","Vinay Setty"],"pdf_url":"https://arxiv.org/pdf/2407.21441v2.pdf","comment":"Accepted in CIKM 2024 as a short paper 4 pages and 1 page references.\n  Fixed typo in author name"},{"id":"http://arxiv.org/abs/2406.12925v2","updated":"2024-08-01T10:09:15Z","published":"2024-06-14T13:54:29Z","title":"GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks","summary":"  Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.\n","authors":["Ihor Stepanov","Mykhailo Shtopko"],"pdf_url":"https://arxiv.org/pdf/2406.12925v2.pdf","comment":"11 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2310.10981v3","updated":"2024-08-01T09:53:49Z","published":"2023-10-17T04:03:00Z","title":"Instructive Dialogue Summarization with Query Aggregations","summary":"  Conventional dialogue summarization methods directly generate summaries and\ndo not consider user's specific interests. This poses challenges in cases where\nthe users are more focused on particular topics or aspects. With the\nadvancement of instruction-finetuned language models, we introduce\ninstruction-tuning to dialogues to expand the capability set of dialogue\nsummarization models. To overcome the scarcity of instructive dialogue\nsummarization data, we propose a three-step approach to synthesize high-quality\nquery-based summarization triples. This process involves summary-anchored query\ngeneration, query filtering, and query-based summary generation. By training a\nunified model called InstructDS (Instructive Dialogue Summarization) on three\nsummarization datasets with multi-purpose instructive triples, we expand the\ncapability of dialogue summarization models. We evaluate our method on four\ndatasets, including dialogue summarization and dialogue reading comprehension.\nExperimental results show that our approach outperforms the state-of-the-art\nmodels and even models with larger sizes. Additionally, our model exhibits\nhigher generalizability and faithfulness, as confirmed by human subjective\nevaluations.\n","authors":["Bin Wang","Zhengyuan Liu","Nancy F. Chen"],"pdf_url":"https://arxiv.org/pdf/2310.10981v3.pdf","comment":"EMNLP 2023 Main Conference - Summarization (update for\n  acknowledgement)"},{"id":"http://arxiv.org/abs/2407.20685v2","updated":"2024-08-01T09:34:15Z","published":"2024-07-30T09:26:43Z","title":"CultureVo: The Serious Game of Utilizing Gen AI for Enhancing Cultural\n  Intelligence","summary":"  CultureVo, Inc. has developed the Integrated Culture Learning Suite (ICLS) to\ndeliver foundational knowledge of world cultures through a combination of\ninteractive lessons and gamified experiences. This paper explores how\nGenerative AI powered by open source Large Langauge Models are utilized within\nthe ICLS to enhance cultural intelligence. The suite employs Generative AI\ntechniques to automate the assessment of learner knowledge, analyze behavioral\npatterns, and manage interactions with non-player characters using real time\nlearner assessment. Additionally, ICLS provides contextual hint and recommend\ncourse content by assessing learner proficiency, while Generative AI\nfacilitates the automated creation and validation of educational content.\n","authors":["Ajita Agarwala","Anupam Purwar","Viswanadhasai Rao"],"pdf_url":"https://arxiv.org/pdf/2407.20685v2.pdf","comment":"Fourth International Conference on AI-ML Systems, 8-11 October, 2024,\n  Louisiana, USA"},{"id":"http://arxiv.org/abs/2407.03104v2","updated":"2024-08-01T08:08:43Z","published":"2024-07-03T13:41:44Z","title":"KeyVideoLLM: Towards Large-scale Video Keyframe Selection","summary":"  Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets.\n","authors":["Hao Liang","Jiapeng Li","Tianyi Bai","Xijie Huang","Linzhuang Sun","Zhengren Wang","Conghui He","Bin Cui","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15220v4","updated":"2024-08-01T07:51:25Z","published":"2024-02-23T09:29:19Z","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition","summary":"  Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.\n","authors":["Lu Ye","Ze Tao","Yong Huang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.15220v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2402.02563v3","updated":"2024-08-01T07:46:54Z","published":"2024-02-04T16:45:01Z","title":"DefInt: A Default-interventionist Framework for Efficient Reasoning with\n  Hybrid Large Language Models","summary":"  Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose a Default-Interventionist framework\n(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,\nDefInt uses smaller-scale language models to generate low-cost reasoning\nthoughts, which resembles the fast intuitions produced by System 1. If the\nintuitions are considered with low confidence, DefInt will invoke the\nreflective reasoning of scaled-up language models as the intervention of System\n2, which can override the default thoughts and rectify the reasoning process.\nExperiments on five representative reasoning tasks show that DefInt\nconsistently achieves state-of-the-art reasoning accuracy and solution\ndiversity. More importantly, it substantially reduces the token cost by 49%-79%\ncompared to the second accurate baselines. Specifically, the open-ended tasks\nhave an average 75% token cost reduction. Code repo with all prompts will be\nreleased upon publication.\n","authors":["Yu Shang","Yu Li","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.02563v3.pdf","comment":"18 pages, 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2405.13292v2","updated":"2024-08-01T07:46:25Z","published":"2024-05-22T02:19:13Z","title":"Metadata Integration for Spam Reviews Detection on Vietnamese E-commerce\n  Websites","summary":"  The problem of detecting spam reviews (opinions) has received significant\nattention in recent years, especially with the rapid development of e-commerce.\nSpam reviews are often classified based on comment content, but in some cases,\nit is insufficient for models to accurately determine the review label. In this\nwork, we introduce the ViSpamReviews v2 dataset, which includes metadata of\nreviews with the objective of integrating supplementary attributes for spam\nreview classification. We propose a novel approach to simultaneously integrate\nboth textual and categorical attributes into the classification model. In our\nexperiments, the product category proved effective when combined with deep\nneural network (DNN) models, while text features performed well on both DNN\nmodels and the model achieved state-of-the-art performance in the problem of\ndetecting spam reviews on Vietnamese e-commerce websites, namely PhoBERT.\nSpecifically, the PhoBERT model achieves the highest accuracy when combined\nwith product description features generated from the SPhoBert model, which is\nthe combination of PhoBERT and SentenceBERT. Using the macro-averaged F1 score,\nthe task of classifying spam reviews achieved 87.22% (an increase of 1.64%\ncompared to the baseline), while the task of identifying the type of spam\nreviews achieved an accuracy of 73.49% (an increase of 1.93% compared to the\nbaseline).\n","authors":["Co Van Dinh","Son T. Luu"],"pdf_url":"https://arxiv.org/pdf/2405.13292v2.pdf","comment":"Published in the International Journal of Asian Language Processing\n  (IJALP)"},{"id":"http://arxiv.org/abs/2407.00072v4","updated":"2024-08-01T06:56:15Z","published":"2024-06-21T08:52:11Z","title":"Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy\n  Retrieval-Augmented Generation","summary":"  In Greek mythology, Pistis symbolized good faith, trust, and reliability.\nDrawing inspiration from these principles, Pistis-RAG is a scalable multi-stage\nframework designed to address the challenges of large-scale retrieval-augmented\ngeneration (RAG) systems. This framework consists of distinct stages: matching,\npre-ranking, ranking, reasoning, and aggregating. Each stage contributes to\nnarrowing the search space, prioritizing semantically relevant documents,\naligning with the large language model's (LLM) preferences, supporting complex\nchain-of-thought (CoT) methods, and combining information from multiple\nsources.\n  Our ranking stage introduces a significant innovation by recognizing that\nsemantic relevance alone may not lead to improved generation quality, due to\nthe sensitivity of the few-shot prompt order, as noted in previous research.\nThis critical aspect is often overlooked in current RAG frameworks.\n  We argue that the alignment issue between LLMs and external knowledge ranking\nmethods is tied to the model-centric paradigm dominant in RAG systems. We\npropose a content-centric approach, emphasizing seamless integration between\nLLMs and external information sources to optimize content transformation for\nspecific tasks.\n  Our novel ranking stage is designed specifically for RAG systems,\nincorporating principles of information retrieval while considering the unique\nbusiness scenarios reflected in LLM preferences and user feedback. We simulated\nfeedback signals on the MMLU benchmark, resulting in a 9.3% performance\nimprovement. Our model and code will be open-sourced on GitHub. Additionally,\nexperiments on real-world, large-scale data validate the scalability of our\nframework.\n","authors":["Yu Bai","Yukai Miao","Li Chen","Dan Li","Yanyu Ren","Hongtao Xie","Ce Yang","Xuhui Cai"],"pdf_url":"https://arxiv.org/pdf/2407.00072v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04695v2","updated":"2024-08-01T06:23:58Z","published":"2024-01-09T17:44:36Z","title":"Narrowing the Knowledge Evaluation Gap: Open-Domain Question Answering\n  with Multi-Granularity Answers","summary":"  Factual questions typically can be answered correctly at different levels of\ngranularity. For example, both ``August 4, 1961'' and ``1961'' are correct\nanswers to the question ``When was Barack Obama born?''. Standard question\nanswering (QA) evaluation protocols, however, do not explicitly take this into\naccount and compare a predicted answer against answers of a single granularity\nlevel. In this work, we propose GRANOLA QA, a novel evaluation setting where a\npredicted answer is evaluated in terms of accuracy and informativeness against\na set of multi-granularity answers. We present a simple methodology for\nenriching existing datasets with multi-granularity answers, and create\nGRANOLA-EQ, a multi-granularity version of the EntityQuestions dataset. We\nevaluate a range of decoding methods on GRANOLA-EQ, including a new algorithm,\ncalled Decoding with Response Aggregation (DRAG), that is geared towards\naligning the response granularity with the model's uncertainty. Our experiments\nshow that large language models with standard decoding tend to generate\nspecific answers, which are often incorrect. In contrast, when evaluated on\nmulti-granularity answers, DRAG yields a nearly 20 point increase in accuracy\non average, which further increases for rare entities. Overall, this reveals\nthat standard evaluation and decoding schemes may significantly underestimate\nthe knowledge encapsulated in LMs.\n","authors":["Gal Yona","Roee Aharoni","Mor Geva"],"pdf_url":"https://arxiv.org/pdf/2401.04695v2.pdf","comment":"To appear in ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2402.17934v2","updated":"2024-08-01T05:52:51Z","published":"2024-02-27T23:12:45Z","title":"Inducing Generalization across Languages and Tasks using Featurized\n  Low-Rank Mixtures","summary":"  Adapting pretrained large language models (LLMs) to various downstream tasks\nin tens or hundreds of human languages is computationally expensive.\nParameter-efficient fine-tuning (PEFT) significantly reduces the adaptation\ncost, by tuning only a small amount of parameters. However, common PEFT methods\nLoRA (Hu et al., 2022) suffer from suboptimal performance on diverse dataset\nmixtures, due to aggressive parameter tying and negative interference among\ndifferent datasets. In this work, we propose Featurized Low-rank Mixtures\n(FLix), a novel PEFT method designed for effective multitask multilingual\nadaptation. FLix associates each unique dataset feature, such as the dataset's\nlanguage or task, with its own low-rank weight update parameters. By composing\nfeature-specific parameters for each dataset, FLix can accommodate diverse\ndataset mixtures and generalize better to unseen datasets. Our experiments show\nthat FLix leads to significant improvements over a variety of tasks for both\nsupervised learning and zero-shot settings with gains of up to $14.2$ inexact\nmatch points in zero-shot semantic parsing.\n","authors":["Chu-Cheng Lin","Xinyi Wang","Jonathan H. Clark","Han Lu","Yun Zhu","Chenxi Whitehouse","Hongkun Yu"],"pdf_url":"https://arxiv.org/pdf/2402.17934v2.pdf","comment":"Revised version"},{"id":"http://arxiv.org/abs/2401.11864v5","updated":"2024-08-01T04:03:32Z","published":"2024-01-22T11:37:18Z","title":"Distilling Mathematical Reasoning Capabilities into Small Language\n  Models","summary":"  This work addresses the challenge of democratizing advanced Large Language\nModels (LLMs) by compressing their mathematical reasoning capabilities into\nsub-billion parameter Small Language Models (SLMs) without compromising\nperformance. We introduce Equation-of-Thought Distillation (EoTD), a novel\ntechnique that encapsulates the reasoning process into equation-based\nrepresentations to construct an EoTD dataset for fine-tuning SLMs.\nAdditionally, we propose the Ensemble Thoughts Distillation (ETD) framework to\nenhance the reasoning performance of SLMs. This involves creating a reasoning\ndataset with multiple thought processes, including Chain-of-Thought (CoT),\nProgram-of-Thought (PoT), and Equation-of-Thought (EoT), and using it for\nfine-tuning. Our experimental performance demonstrates that EoTD significantly\nboosts the reasoning abilities of SLMs, while ETD enables these models to\nachieve state-of-the-art reasoning performance.\n","authors":["Xunyu Zhu","Jian Li","Yong Liu","Can Ma","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2401.11864v5.pdf","comment":"Accepted for publication in Neural Networks"},{"id":"http://arxiv.org/abs/2407.20756v2","updated":"2024-08-01T04:01:39Z","published":"2024-07-30T11:57:40Z","title":"SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision\n  Language Models","summary":"  Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).\n","authors":["Zheng Liu","Hao Liang","Xijie Huang","Wentao Xiong","Qinhan Yu","Linzhuang Sun","Chong Chen","Conghui He","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.20756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11539v3","updated":"2024-08-01T03:19:03Z","published":"2023-12-15T23:34:05Z","title":"KGLens: Towards Efficient and Effective Knowledge Probing of Large\n  Language Models with Knowledge Graphs","summary":"  Large Language Models (LLMs) might hallucinate facts, while curated Knowledge\nGraph (KGs) are typically factually reliable especially with domain-specific\nknowledge. Measuring the alignment between KGs and LLMs can effectively probe\nthe factualness and identify the knowledge blind spots of LLMs. However,\nverifying the LLMs over extensive KGs can be expensive. In this paper, we\npresent KGLens, a Thompson-sampling-inspired framework aimed at effectively and\nefficiently measuring the alignment between KGs and LLMs. KGLens features a\ngraph-guided question generator for converting KGs into natural language, along\nwith a carefully designed importance sampling strategy based on parameterized\nKG structure to expedite KG traversal. Our simulation experiment compares the\nbrute force method with KGLens under six different sampling methods,\ndemonstrating that our approach achieves superior probing efficiency.\nLeveraging KGLens, we conducted in-depth analyses of the factual accuracy of\nten LLMs across three large domain-specific KGs from Wikidata, composing over\n19K edges, 700 relations, and 21K entities. Human evaluation results indicate\nthat KGLens can assess LLMs with a level of accuracy nearly equivalent to that\nof human annotators, achieving 95.7% of the accuracy rate.\n","authors":["Shangshang Zheng","He Bai","Yizhe Zhang","Yi Su","Xiaochuan Niu","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2312.11539v3.pdf","comment":"ACL 2024 Workshop Towards Knowledgeable Language Models"},{"id":"http://arxiv.org/abs/2404.07221v2","updated":"2024-08-01T03:02:44Z","published":"2024-03-23T00:49:40Z","title":"Improving Retrieval for RAG based Question Answering Models on Financial\n  Documents","summary":"  The effectiveness of Large Language Models (LLMs) in generating accurate\nresponses relies heavily on the quality of input provided, particularly when\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\nsignificant advancements in LLMs' response quality in recent years, users may\nstill encounter inaccuracies or irrelevant answers; these issues often stem\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\nthe RAG process. This paper explores the existing constraints of RAG pipelines\nand introduces methodologies for enhancing text retrieval. It delves into\nstrategies such as sophisticated chunking techniques, query expansion, the\nincorporation of metadata annotations, the application of re-ranking\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\napproaches can substantially improve the retrieval quality, thereby elevating\nthe overall performance and reliability of LLMs in processing and responding to\nqueries.\n","authors":["Spurthi Setty","Harsh Thakkar","Alyssa Lee","Eden Chung","Natan Vidra"],"pdf_url":"https://arxiv.org/pdf/2404.07221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21491v2","updated":"2024-08-01T01:18:04Z","published":"2024-07-31T10:02:21Z","title":"Generative Expressive Conversational Speech Synthesis","summary":"  Conversational Speech Synthesis (CSS) aims to express a target utterance with\nthe proper speaking style in a user-agent conversation setting. Existing CSS\nmethods employ effective multi-modal context modeling techniques to achieve\nempathy understanding and expression. However, they often need to design\ncomplex network architectures and meticulously optimize the modules within\nthem. In addition, due to the limitations of small-scale datasets containing\nscripted recording styles, they often fail to simulate real natural\nconversational styles. To address the above issues, we propose a novel\ngenerative expressive CSS system, termed GPT-Talker.We transform the multimodal\ninformation of the multi-turn dialogue history into discrete token sequences\nand seamlessly integrate them to form a comprehensive user-agent dialogue\ncontext. Leveraging the power of GPT, we predict the token sequence, that\nincludes both semantic and style knowledge, of response for the agent. After\nthat, the expressive conversational speech is synthesized by the\nconversation-enriched VITS to deliver feedback to the user.Furthermore, we\npropose a large-scale Natural CSS Dataset called NCSSD, that includes both\nnaturally recorded conversational speech in improvised styles and dialogues\nextracted from TV shows. It encompasses both Chinese and English languages,\nwith a total duration of 236 hours.We conducted comprehensive experiments on\nthe reliability of the NCSSD and the effectiveness of our GPT-Talker. Both\nsubjective and objective evaluations demonstrate that our model outperforms\nother state-of-the-art CSS systems significantly in terms of naturalness and\nexpressiveness. The Code, Dataset, and Pre-trained Model are available at:\nhttps://github.com/AI-S2-Lab/GPT-Talker.\n","authors":["Rui Liu","Yifan Hu","Yi Ren","Xiang Yin","Haizhou Li"],"pdf_url":"https://arxiv.org/pdf/2407.21491v2.pdf","comment":"14 pages, 6 figures, 8 tables. Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.21315v2","updated":"2024-08-01T01:17:34Z","published":"2024-07-31T03:53:14Z","title":"Beyond Silent Letters: Amplifying LLMs in Emotion Recognition with Vocal\n  Nuances","summary":"  This paper introduces a novel approach to emotion detection in speech using\nLarge Language Models (LLMs). We address the limitation of LLMs in processing\naudio inputs by translating speech characteristics into natural language\ndescriptions. Our method integrates these descriptions into text prompts,\nenabling LLMs to perform multimodal emotion analysis without architectural\nmodifications. We evaluate our approach on two datasets: IEMOCAP and MELD,\ndemonstrating significant improvements in emotion recognition accuracy,\nparticularly for high-quality audio data. Our experiments show that\nincorporating speech descriptions yields a 2 percentage point increase in\nweighted F1 score on IEMOCAP (from 70.111\\% to 72.596\\%). We also compare\nvarious LLM architectures and explore the effectiveness of different feature\nrepresentations. Our findings highlight the potential of this approach in\nenhancing emotion detection capabilities of LLMs and underscore the importance\nof audio quality in speech-based emotion recognition tasks. We'll release the\nsource code on Github.\n","authors":["Zehui Wu","Ziwei Gong","Lin Ai","Pengyuan Shi","Kaan Donbekci","Julia Hirschberg"],"pdf_url":"https://arxiv.org/pdf/2407.21315v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00765v1","updated":"2024-08-01T17:59:54Z","published":"2024-08-01T17:59:54Z","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","summary":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.\n","authors":["Weihao Yu","Zhengyuan Yang","Linfeng Ren","Linjie Li","Jianfeng Wang","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00765v1.pdf","comment":"Extension of MM-Vet: arXiv:2308.02490"},{"id":"http://arxiv.org/abs/2408.00764v1","updated":"2024-08-01T17:59:46Z","published":"2024-08-01T17:59:46Z","title":"AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation","summary":"  Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.\n","authors":["Mengkang Hu","Pu Zhao","Can Xu","Qingfeng Sun","Jianguang Lou","Qingwei Lin","Ping Luo","Saravan Rajmohan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00761v1","updated":"2024-08-01T17:59:12Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v1.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2408.00728v1","updated":"2024-08-01T17:20:24Z","published":"2024-08-01T17:20:24Z","title":"CERT-ED: Certifiably Robust Text Classification for Edit Distance","summary":"  With the growing integration of AI in daily life, ensuring the robustness of\nsystems to inference-time attacks is crucial. Among the approaches for\ncertifying robustness to such adversarial examples, randomized smoothing has\nemerged as highly promising due to its nature as a wrapper around arbitrary\nblack-box models. Previous work on randomized smoothing in natural language\nprocessing has primarily focused on specific subsets of edit distance\noperations, such as synonym substitution or word insertion, without exploring\nthe certification of all edit operations. In this paper, we adapt Randomized\nDeletion (Huang et al., 2023) and propose, CERTified Edit Distance defense\n(CERT-ED) for natural language classification. Through comprehensive\nexperiments, we demonstrate that CERT-ED outperforms the existing Hamming\ndistance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of\nboth accuracy and the cardinality of the certificate. By covering various\nthreat models, including 5 direct and 5 transfer attacks, our method improves\nempirical robustness in 38 out of 50 settings.\n","authors":["Zhuoqun Huang","Neil G Marchant","Olga Ohrimenko","Benjamin I. P. Rubinstein"],"pdf_url":"https://arxiv.org/pdf/2408.00728v1.pdf","comment":"22 pages, 3 figures, 12 tables. Include 11 pages of appendices"},{"id":"http://arxiv.org/abs/2408.00727v1","updated":"2024-08-01T17:18:17Z","published":"2024-08-01T17:18:17Z","title":"Improving Retrieval-Augmented Generation in Medicine with Iterative\n  Follow-up Questions","summary":"  The emergent abilities of large language models (LLMs) have demonstrated\ngreat potential in solving medical questions. They can possess considerable\nmedical knowledge, but may still hallucinate and are inflexible in the\nknowledge updates. While Retrieval-Augmented Generation (RAG) has been proposed\nto enhance the medical question-answering capabilities of LLMs with external\nknowledge bases, it may still fail in complex cases where multiple rounds of\ninformation-seeking are required. To address such an issue, we propose\niterative RAG for medicine (i-MedRAG), where LLMs can iteratively ask follow-up\nqueries based on previous information-seeking attempts. In each iteration of\ni-MedRAG, the follow-up queries will be answered by a vanilla RAG system and\nthey will be further used to guide the query generation in the next iteration.\nOur experiments show the improved performance of various LLMs brought by\ni-MedRAG compared with vanilla RAG on complex questions from clinical vignettes\nin the United States Medical Licensing Examination (USMLE), as well as various\nknowledge tests in the Massive Multitask Language Understanding (MMLU) dataset.\nNotably, our zero-shot i-MedRAG outperforms all existing prompt engineering and\nfine-tuning methods on GPT-3.5, achieving an accuracy of 69.68\\% on the MedQA\ndataset. In addition, we characterize the scaling properties of i-MedRAG with\ndifferent iterations of follow-up queries and different numbers of queries per\niteration. Our case studies show that i-MedRAG can flexibly ask follow-up\nqueries to form reasoning chains, providing an in-depth analysis of medical\nquestions. To the best of our knowledge, this is the first-of-its-kind study on\nincorporating follow-up queries into medical RAG.\n","authors":["Guangzhi Xiong","Qiao Jin","Xiao Wang","Minjia Zhang","Zhiyong Lu","Aidong Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00727v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00690v1","updated":"2024-08-01T16:31:35Z","published":"2024-08-01T16:31:35Z","title":"Improving Text Embeddings for Smaller Language Models Using Contrastive\n  Fine-tuning","summary":"  While Large Language Models show remarkable performance in natural language\nunderstanding, their resource-intensive nature makes them less accessible. In\ncontrast, smaller language models such as MiniCPM offer more sustainable\nscalability, but often underperform without specialized optimization. In this\npaper, we explore the enhancement of smaller language models through the\nimprovement of their text embeddings. We select three language models, MiniCPM,\nPhi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our\nresults demonstrate that this fine-tuning method enhances the quality of text\nembeddings for all three models across various benchmarks, with MiniCPM showing\nthe most significant improvements of an average 56.33\\% performance gain. The\ncontrastive fine-tuning code is publicly available at\nhttps://github.com/trapoom555/Language-Model-STS-CFT.\n","authors":["Trapoom Ukarapol","Zhicheng Lee","Amy Xin"],"pdf_url":"https://arxiv.org/pdf/2408.00690v1.pdf","comment":"Code: https://github.com/trapoom555/Language-Model-STS-CFT,\n  Huggingface:\n  https://huggingface.co/collections/trapoom555/small-lms-text-embedding-663b3ec87527788a577f6852"},{"id":"http://arxiv.org/abs/2408.00684v1","updated":"2024-08-01T16:25:54Z","published":"2024-08-01T16:25:54Z","title":"Assessing the Variety of a Concept Space Using an Unbiased Estimate of\n  Rao's Quadratic Index","summary":"  Past research relates design creativity to 'divergent thinking,' i.e., how\nwell the concept space is explored during the early phase of design.\nResearchers have argued that generating several concepts would increase the\nchances of producing better design solutions. 'Variety' is one of the\nparameters by which one can quantify the breadth of a concept space explored by\nthe designers. It is useful to assess variety at the conceptual design stage\nbecause, at this stage, designers have the freedom to explore different\nsolution principles so as to satisfy a design problem with substantially novel\nconcepts. This article elaborates on and critically examines the existing\nvariety metrics from the engineering design literature, discussing their\nlimitations. A new distance-based variety metric is proposed, along with a\nprescriptive framework to support the assessment process. This framework uses\nthe SAPPhIRE model of causality as a knowledge representation scheme to measure\nthe real-valued distance between two design concepts. The proposed framework is\nimplemented in a software tool called 'VariAnT.' Furthermore, the tool's\napplication is demonstrated through an illustrative example.\n","authors":["Anubhab Majumder","Ujjwal Pal","Amaresh Chakrabarti"],"pdf_url":"https://arxiv.org/pdf/2408.00684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00675v1","updated":"2024-08-01T16:18:09Z","published":"2024-08-01T16:18:09Z","title":"Leveraging Entailment Judgements in Cross-Lingual Summarisation","summary":"  Synthetically created Cross-Lingual Summarisation (CLS) datasets are prone to\ninclude document-summary pairs where the reference summary is unfaithful to the\ncorresponding document as it contains content not supported by the document\n(i.e., hallucinated content). This low data quality misleads model learning and\nobscures evaluation results. Automatic ways to assess hallucinations and\nimprove training have been proposed for monolingual summarisation,\npredominantly in English. For CLS, we propose to use off-the-shelf\ncross-lingual Natural Language Inference (X-NLI) to evaluate faithfulness of\nreference and model generated summaries. Then, we study training approaches\nthat are aware of faithfulness issues in the training data and propose an\napproach that uses unlikelihood loss to teach a model about unfaithful summary\nsequences. Our results show that it is possible to train CLS models that yield\nmore faithful summaries while maintaining comparable or better informativess.\n","authors":["Huajian Zhang","Laura Perez-Beltrachini"],"pdf_url":"https://arxiv.org/pdf/2408.00675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00662v1","updated":"2024-08-01T15:58:05Z","published":"2024-08-01T15:58:05Z","title":"Aligning Multiple Knowledge Graphs in a Single Pass","summary":"  Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass.\n","authors":["Yaming Yang","Zhe Wang","Ziyu Guan","Wei Zhao","Weigang Lu","Xinyan Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00655v1","updated":"2024-08-01T15:45:19Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Faster, Longer and More Accurate Inference with\n  Next-sentence Prediction for Large Language Models","summary":"  Contemporary large language models (LLMs) predominantly utilize a next-token\nprediction method for inference, which significantly impedes their processing\nspeed. In this paper, we introduce a novel inference methodology termed\nnext-sentence prediction, aimed at enhancing the inference efficiency of LLMs.\nWe present SentenceVAE, a tiny model consisting of an encoder and a decoder.\nThe encoder effectively condenses the information within a sentence into a\nsingular token, while the decoder reconstructs this compressed data back into\nits original sentential form. By integrating SentenceVAE into the input and\noutput layers of LLMs, we develop Sentence-level LLMs (SLLMs) that employ a\nsentence-by-sentence inference approach, markedly accelerating inference\nspeeds. SentenceVAE also maintains the integrity of the original semantic\ncontent by segmenting the text into sentences, thereby preserving accuracy\nwhile boosting inference speeds. Compared to traditional LLMs, SLLMs process\nfewer tokens over equivalent context lengths, significantly reducing memory\ndemands for Self-Attention computations and facilitating the handling of longer\ncontexts. Our experimental findings reveal that this method can increase\ninference speeds by 204~365%, reduce perplexity (PPL) to 46~75% of its original\nmetric, and decrease memory overhead by 86~91% for the same context length. The\nadvantages of this approach are further amplified with increases in model\nparameters.\n","authors":["Hongjun An","Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v1.pdf","comment":"First preview version"},{"id":"http://arxiv.org/abs/2408.00624v1","updated":"2024-08-01T15:09:32Z","published":"2024-08-01T15:09:32Z","title":"SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data","summary":"  In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.\n","authors":["Yichen Lu","Jiaqi Song","Xuankai Chang","Hengwei Bian","Soumi Maiti","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2408.00624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00620v1","updated":"2024-08-01T15:05:42Z","published":"2024-08-01T15:05:42Z","title":"Are Bigger Encoders Always Better in Vision Large Models?","summary":"  In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.\n","authors":["Bozhou Li","Hao Liang","Zimo Meng","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00612v1","updated":"2024-08-01T14:52:04Z","published":"2024-08-01T14:52:04Z","title":"Downstream bias mitigation is all you need","summary":"  The advent of transformer-based architectures and large language models\n(LLMs) have significantly advanced the performance of natural language\nprocessing (NLP) models. Since these LLMs are trained on huge corpuses of data\nfrom the web and other sources, there has been a major concern about harmful\nprejudices that may potentially be transferred from the data. In many\napplications, these pre-trained LLMs are fine-tuned on task specific datasets,\nwhich can further contribute to biases. This paper studies the extent of biases\nabsorbed by LLMs during pre-training as well as task-specific behaviour after\nfine-tuning. We found that controlled interventions on pre-trained LLMs, prior\nto fine-tuning, have minimal effect on lowering biases in classifiers. However,\nthe biases present in domain-specific datasets play a much bigger role, and\nhence mitigating them at this stage has a bigger impact. While pre-training\ndoes matter, but after the model has been pre-trained, even slight changes to\nco-occurrence rates in the fine-tuning dataset has a significant effect on the\nbias of the model.\n","authors":["Arkadeep Baksi","Rahul Singh","Tarun Joshi"],"pdf_url":"https://arxiv.org/pdf/2408.00612v1.pdf","comment":"21 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.00584v1","updated":"2024-08-01T14:14:15Z","published":"2024-08-01T14:14:15Z","title":"Non Verbis, Sed Rebus: Large Language Models are Weak Solvers of Italian\n  Rebuses","summary":"  Rebuses are puzzles requiring constrained multi-step reasoning to identify a\nhidden phrase from a set of images and letters. In this work, we introduce a\nlarge collection of verbalized rebuses for the Italian language and use it to\nassess the rebus-solving capabilities of state-of-the-art large language\nmodels. While general-purpose systems such as LLaMA-3 and GPT-4o perform poorly\non this task, ad-hoc fine-tuning seems to improve models' performance. However,\nwe find that performance gains from training are largely motivated by\nmemorization. Our results suggest that rebus solving remains a challenging test\nbed to evaluate large language models' linguistic proficiency and sequential\ninstruction-following skills.\n","authors":["Gabriele Sarti","Tommaso Caselli","Malvina Nissim","Arianna Bisazza"],"pdf_url":"https://arxiv.org/pdf/2408.00584v1.pdf","comment":"Code: https://github.com/gsarti/verbalized-rebus. Artifacts:\n  https://huggingface.co/collections/gsarti/verbalized-rebus-clic-it-2024-66ab8f11cb04e68bdf4fb028"},{"id":"http://arxiv.org/abs/2408.00555v1","updated":"2024-08-01T13:38:58Z","published":"2024-08-01T13:38:58Z","title":"Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation","summary":"  Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.\n","authors":["Xiaoye Qu","Qiyuan Chen","Wei Wei","Jishuo Sun","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00550v1","updated":"2024-08-01T13:34:35Z","published":"2024-08-01T13:34:35Z","title":"Mitigating Multilingual Hallucination in Large Vision-Language Models","summary":"  While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR\n","authors":["Xiaoye Qu","Mingyang Song","Wei Wei","Jianfeng Dong","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.00550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00539v1","updated":"2024-08-01T13:22:01Z","published":"2024-08-01T13:22:01Z","title":"Intermittent Semi-working Mask: A New Masking Paradigm for LLMs","summary":"  Multi-turn dialogues are a key interaction method between humans and Large\nLanguage Models (LLMs), as conversations extend over multiple rounds, keeping\nLLMs' high generation quality and low latency is a challenge. Mainstream LLMs\ncan be grouped into two categories based on masking strategy: causal LLM and\nprefix LLM. Several works have demonstrated that prefix LLMs tend to outperform\ncausal ones in scenarios that heavily depend on historical context such as\nmulti-turn dialogues or in-context learning, thanks to their bidirectional\nattention on prefix sequences. However, prefix LLMs have an inherent\ninefficient training problem in multi-turn dialogue datasets. In addition, the\nattention mechanism of prefix LLM makes it unable to reuse Key-Value Cache (KV\nCache) across dialogue rounds to reduce generation latency. In this paper, we\npropose a novel masking scheme called Intermittent Semi-working Mask (ISM) to\naddress these problems. Specifically, we apply alternate bidirectional and\nunidirectional attention on queries and answers in the dialogue history. In\nthis way, ISM is able to maintain the high quality of prefix LLM and low\ngeneration latency of causal LLM, simultaneously. Extensive experiments\nillustrate that our ISM achieves significant performance.\n","authors":["Mingcong Lu","Jiangcai Zhu","Wang Hao","Zheng Li","Shusheng Zhang","Kailai Shao","Chao Chen","Nan Li","Feng Wang","Xin Lu"],"pdf_url":"https://arxiv.org/pdf/2408.00539v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00534v1","updated":"2024-08-01T13:10:35Z","published":"2024-08-01T13:10:35Z","title":"The Monetisation of Toxicity: Analysing YouTube Content Creators and\n  Controversy-Driven Engagement","summary":"  YouTube is a major social media platform that plays a significant role in\ndigital culture, with content creators at its core. These creators often engage\nin controversial behaviour to drive engagement, which can foster toxicity. This\npaper presents a quantitative analysis of controversial content on YouTube,\nfocusing on the relationship between controversy, toxicity, and monetisation.\nWe introduce a curated dataset comprising 20 controversial YouTube channels\nextracted from Reddit discussions, including 16,349 videos and more than 105\nmillion comments. We identify and categorise monetisation cues from video\ndescriptions into various models, including affiliate marketing and direct\nselling, using lists of URLs and keywords. Additionally, we train a machine\nlearning model to measure the toxicity of comments in these videos. Our\nfindings reveal that while toxic comments correlate with higher engagement,\nthey negatively impact monetisation, indicating that controversy-driven\ninteraction does not necessarily lead to financial gain. We also observed\nsignificant variation in monetisation strategies, with some creators showing\nextensive monetisation despite high toxicity levels. Our study introduces a\ncurated dataset, lists of URLs and keywords to categorise monetisation, a\nmachine learning model to measure toxicity, and is a significant step towards\nunderstanding the complex relationship between controversy, engagement, and\nmonetisation on YouTube. The lists used for detecting and categorising\nmonetisation cues are available on https://github.com/thalesbertaglia/toxmon.\n","authors":["Thales Bertaglia","Catalina Goanta","Adriana Iamnitchi"],"pdf_url":"https://arxiv.org/pdf/2408.00534v1.pdf","comment":"Accept for publication at the 4th International Workshop on Open\n  Challenges in Online Social Networks (OASIS) held in conjunction with 35th\n  ACM Conference on Hypertext and Social Media (HT24)"},{"id":"http://arxiv.org/abs/2408.00491v1","updated":"2024-08-01T11:52:56Z","published":"2024-08-01T11:52:56Z","title":"GalleryGPT: Analyzing Paintings with Large Multimodal Models","summary":"  Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.\n","authors":["Yi Bin","Wenhao Shi","Yujuan Ding","Zhiqiang Hu","Zheng Wang","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00491v1.pdf","comment":"Accepted as Oral Presentation at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.00397v1","updated":"2024-08-01T09:07:32Z","published":"2024-08-01T09:07:32Z","title":"In-Context Example Selection via Similarity Search Improves Low-Resource\n  Machine Translation","summary":"  The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. In this paper, we focus\non machine translation (MT), a task that has been shown to benefit from\nin-context translation examples. However no systematic studies have been\npublished on how best to select examples, and mixed results have been reported\non the usefulness of similarity-based selection over random selection. We\nprovide a study covering multiple LLMs and multiple in-context example\nretrieval strategies, comparing multilingual sentence embeddings. We cover\nseveral language directions, representing different levels of language\nresourcedness (English into French, German, Swahili and Wolof). Contrarily to\npreviously published results, we find that sentence embedding similarity can\nimprove MT, especially for low-resource language directions, and discuss the\nbalance between selection pool diversity and quality. We also highlight\npotential problems with the evaluation of LLM-based MT and suggest a more\nappropriate evaluation protocol, adapting the COMET metric to the evaluation of\nLLMs. Code and outputs are freely available at\nhttps://github.com/ArmelRandy/ICL-MT.\n","authors":["Armel Zebaze","Benot Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2408.00397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00357v1","updated":"2024-08-01T07:54:52Z","published":"2024-08-01T07:54:52Z","title":"DeliLaw: A Chinese Legal Counselling System Based on a Large Language\n  Model","summary":"  Traditional legal retrieval systems designed to retrieve legal documents,\nstatutes, precedents, and other legal information are unable to give\nsatisfactory answers due to lack of semantic understanding of specific\nquestions. Large Language Models (LLMs) have achieved excellent results in a\nvariety of natural language processing tasks, which inspired us that we train a\nLLM in the legal domain to help legal retrieval. However, in the Chinese legal\ndomain, due to the complexity of legal questions and the rigour of legal\narticles, there is no legal large model with satisfactory practical application\nyet. In this paper, we present DeliLaw, a Chinese legal counselling system\nbased on a large language model. DeliLaw integrates a legal retrieval module\nand a case retrieval module to overcome the model hallucination. Users can\nconsult professional legal questions, search for legal articles and relevant\njudgement cases, etc. on the DeliLaw system in a dialogue mode. In addition,\nDeliLaw supports the use of English for counseling. we provide the address of\nthe system: https://data.delilegal.com/lawQuestion.\n","authors":["Nan Xie","Yuelin Bai","Hengyuan Gao","Feiteng Fang","Qixuan Zhao","Zhijian Li","Ziqiang Xue","Liang Zhu","Shiwen Ni","Min Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00357v1.pdf","comment":"CIKM 2024, 5 pages with 3 figures"},{"id":"http://arxiv.org/abs/2408.00307v1","updated":"2024-08-01T06:06:25Z","published":"2024-08-01T06:06:25Z","title":"ABC Align: Large Language Model Alignment for Safety & Accuracy","summary":"  Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.\n","authors":["Gareth Seneque","Lap-Hang Ho","Ariel Kuperman","Nafise Erfanian Saeedi","Jeffrey Molendijk"],"pdf_url":"https://arxiv.org/pdf/2408.00307v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00284v1","updated":"2024-08-01T04:57:31Z","published":"2024-08-01T04:57:31Z","title":"Bailing-TTS: Chinese Dialectal Speech Synthesis Towards Human-like\n  Spontaneous Representation","summary":"  Large-scale text-to-speech (TTS) models have made significant progress\nrecently.However, they still fall short in the generation of Chinese dialectal\nspeech. Toaddress this, we propose Bailing-TTS, a family of large-scale TTS\nmodels capable of generating high-quality Chinese dialectal speech. Bailing-TTS\nserves as a foundation model for Chinese dialectal speech generation. First,\ncontinual semi-supervised learning is proposed to facilitate the alignment of\ntext tokens and speech tokens. Second, the Chinese dialectal representation\nlearning is developed using a specific transformer architecture and multi-stage\ntraining processes. With the proposed design of novel network architecture and\ncorresponding strategy, Bailing-TTS is able to generate Chinese dialectal\nspeech from text effectively and efficiently. Experiments demonstrate that\nBailing-TTS generates Chinese dialectal speech towards human-like spontaneous\nrepresentation. Readers are encouraged to listen to demos at\n\\url{https://c9412600.github.io/bltts_tech_report/index.html}.\n","authors":["Xinhan Di","Zihao Chen","Yunming Liang","Junjie Zheng","Yihua Wang","Chaofan Ding"],"pdf_url":"https://arxiv.org/pdf/2408.00284v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.00283v1","updated":"2024-08-01T04:56:13Z","published":"2024-08-01T04:56:13Z","title":"Navigating Text-to-Image Generative Bias across Indic Languages","summary":"  This research investigates biases in text-to-image (TTI) models for the Indic\nlanguages widely spoken across India. It evaluates and compares the generative\nperformance and cultural relevance of leading TTI models in these languages\nagainst their performance in English. Using the proposed IndicTTI benchmark, we\ncomprehensively assess the performance of 30 Indic languages with two\nopen-source diffusion models and two commercial generation APIs. The primary\nobjective of this benchmark is to evaluate the support for Indic languages in\nthese models and identify areas needing improvement. Given the linguistic\ndiversity of 30 languages spoken by over 1.4 billion people, this benchmark\naims to provide a detailed and insightful analysis of TTI models' effectiveness\nwithin the Indic linguistic landscape. The data and code for the IndicTTI\nbenchmark can be accessed at\nhttps://iab-rubric.org/resources/other-databases/indictti.\n","authors":["Surbhi Mittal","Arnav Sudan","Mayank Vatsa","Richa Singh","Tamar Glaser","Tal Hassner"],"pdf_url":"https://arxiv.org/pdf/2408.00283v1.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00274v1","updated":"2024-08-01T04:28:38Z","published":"2024-08-01T04:28:38Z","title":"QUITO: Accelerating Long-Context Reasoning through Query-Guided Context\n  Compression","summary":"  In-context learning (ICL) capabilities are foundational to the success of\nlarge language models (LLMs). Recently, context compression has attracted\ngrowing interest since it can largely reduce reasoning complexities and\ncomputation costs of LLMs. In this paper, we introduce a novel Query-gUIded\naTtention cOmpression (QUITO) method, which leverages attention of the question\nover the contexts to filter useless information. Specifically, we take a\ntrigger token to calculate the attention distribution of the context in\nresponse to the question. Based on the distribution, we propose three different\nfiltering methods to satisfy the budget constraints of the context length. We\nevaluate the QUITO using two widely-used datasets, namely, NaturalQuestions and\nASQA. Experimental results demonstrate that QUITO significantly outperforms\nestablished baselines across various datasets and downstream LLMs, underscoring\nits effectiveness. Our code is available at\nhttps://github.com/Wenshansilvia/attention_compressor.\n","authors":["Wenshan Wang","Yihang Wang","Yixing Fan","Huaming Liao","Jiafeng Guo"],"pdf_url":"https://arxiv.org/pdf/2408.00274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00264v1","updated":"2024-08-01T03:43:32Z","published":"2024-08-01T03:43:32Z","title":"Clover-2: Accurate Inference for Regressive Lightweight Speculative\n  Decoding","summary":"  Large Language Models (LLMs) frequently suffer from inefficiencies, largely\nattributable to the discord between the requirements of auto-regressive\ndecoding and the architecture of contemporary GPUs. Recently, regressive\nlightweight speculative decoding has garnered attention for its notable\nefficiency improvements in text generation tasks. This approach utilizes a\nlightweight regressive draft model, like a Recurrent Neural Network (RNN) or a\nsingle transformer decoder layer, leveraging sequential information to\niteratively predict potential tokens. Specifically, RNN draft models are\ncomputationally economical but tend to deliver lower accuracy, while attention\ndecoder layer models exhibit the opposite traits. This paper presents Clover-2,\nan advanced iteration of Clover, an RNN-based draft model designed to achieve\ncomparable accuracy to that of attention decoder layer models while maintaining\nminimal computational overhead. Clover-2 enhances the model architecture and\nincorporates knowledge distillation to increase Clover's accuracy and improve\noverall efficiency. We conducted experiments using the open-source Vicuna 7B\nand LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses\nexisting methods across various model architectures, showcasing its efficacy\nand robustness.\n","authors":["Bin Xiao","Lujun Gui","Lei Su","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.00264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00244v1","updated":"2024-08-01T02:49:58Z","published":"2024-08-01T02:49:58Z","title":"Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms","summary":"  Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.\n","authors":["Tian Meng","Yang Tao","Wuliang Yin"],"pdf_url":"https://arxiv.org/pdf/2408.00244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00230v1","updated":"2024-08-01T01:54:17Z","published":"2024-08-01T01:54:17Z","title":"Lost in Translation: Latent Concept Misalignment in Text-to-Image\n  Diffusion Models","summary":"  Advancements in text-to-image diffusion models have broadened extensive\ndownstream practical applications, but such models often encounter misalignment\nissues between text and image. Taking the generation of a combination of two\ndisentangled concepts as an example, say given the prompt \"a tea cup of iced\ncoke\", existing models usually generate a glass cup of iced coke because the\niced coke usually co-occurs with the glass cup instead of the tea one during\nmodel training. The root of such misalignment is attributed to the confusion in\nthe latent semantic space of text-to-image diffusion models, and hence we refer\nto the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment\n(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate\nthe scope of LC-Mis, and develop an automated pipeline for aligning the latent\nsemantics of diffusion models to text prompts. Empirical assessments confirm\nthe effectiveness of our approach, substantially reducing LC-Mis errors and\nenhancing the robustness and versatility of text-to-image diffusion models. Our\ncode and dataset have been available online for reference.\n","authors":["Juntu Zhao","Junyu Deng","Yixin Ye","Chongxuan Li","Zhijie Deng","Dequan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00230v1.pdf","comment":"33 pages, 19 figures"},{"id":"http://arxiv.org/abs/2408.00205v1","updated":"2024-08-01T00:18:21Z","published":"2024-08-01T00:18:21Z","title":"Sentence-wise Speech Summarization: Task, Datasets, and End-to-End\n  Modeling with LM Knowledge Distillation","summary":"  This paper introduces a novel approach called sentence-wise speech\nsummarization (Sen-SSum), which generates text summaries from a spoken document\nin a sentence-by-sentence manner. Sen-SSum combines the real-time processing of\nautomatic speech recognition (ASR) with the conciseness of speech\nsummarization. To explore this approach, we present two datasets for Sen-SSum:\nMega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of\nTransformer-based models: 1) cascade models that combine ASR and strong text\nsummarization models, and 2) end-to-end (E2E) models that directly convert\nspeech into a text summary. While E2E models are appealing to develop\ncompute-efficient models, they perform worse than cascade models. Therefore, we\npropose knowledge distillation for E2E models using pseudo-summaries generated\nby the cascade models. Our experiments show that this proposed knowledge\ndistillation effectively improves the performance of the E2E model on both\ndatasets.\n","authors":["Kohei Matsuura","Takanori Ashihara","Takafumi Moriya","Masato Mimura","Takatomo Kano","Atsunori Ogawa","Marc Delcroix"],"pdf_url":"https://arxiv.org/pdf/2408.00205v1.pdf","comment":"Accepted to Interspeech2024. Dataset:\n  https://huggingface.co/datasets/komats/mega-ssum"},{"id":"http://arxiv.org/abs/2408.00203v1","updated":"2024-08-01T00:00:43Z","published":"2024-08-01T00:00:43Z","title":"OmniParser for Pure Vision Based GUI Agent","summary":"  The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.\n","authors":["Yadong Lu","Jianwei Yang","Yelong Shen","Ahmed Awadallah"],"pdf_url":"https://arxiv.org/pdf/2408.00203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00948v1","updated":"2024-08-01T23:06:06Z","published":"2024-08-01T23:06:06Z","title":"Leveraging Large Language Models (LLMs) for Traffic Management at Urban\n  Intersections: The Case of Mixed Traffic Scenarios","summary":"  Urban traffic management faces significant challenges due to the dynamic\nenvironments, and traditional algorithms fail to quickly adapt to this\nenvironment in real-time and predict possible conflicts. This study explores\nthe ability of a Large Language Model (LLM), specifically, GPT-4o-mini to\nimprove traffic management at urban intersections. We recruited GPT-4o-mini to\nanalyze, predict position, detect and resolve the conflicts at an intersection\nin real-time for various basic scenarios. The key findings of this study to\ninvestigate whether LLMs can logically reason and understand the scenarios to\nenhance the traffic efficiency and safety by providing real-time analysis. The\nstudy highlights the potential of LLMs in urban traffic management creating\nmore intelligent and more adaptive systems. Results showed the GPT-4o-mini was\neffectively able to detect and resolve conflicts in heavy traffic, congestion,\nand mixed-speed conditions. The complex scenario of multiple intersections with\nobstacles and pedestrians saw successful conflict management as well. Results\nshow that the integration of LLMs promises to improve the effectiveness of\ntraffic control for safer and more efficient urban intersection management.\n","authors":["Sari Masri","Huthaifa I. Ashqar","Mohammed Elhenawy"],"pdf_url":"https://arxiv.org/pdf/2408.00948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00932v1","updated":"2024-08-01T21:50:23Z","published":"2024-08-01T21:50:23Z","title":"Towards Zero-Shot Annotation of the Built Environment with\n  Vision-Language Models (Vision Paper)","summary":"  Equitable urban transportation applications require high-fidelity digital\nrepresentations of the built environment: not just streets and sidewalks, but\nbike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,\ntraffic signals, signage, street markings, potholes, and more. Direct\ninspections and manual annotations are prohibitively expensive at scale.\nConventional machine learning methods require substantial annotated training\ndata for adequate performance. In this paper, we consider vision language\nmodels as a mechanism for annotating diverse urban features from satellite\nimages, reducing the dependence on human annotation to produce large training\nsets. While these models have achieved impressive results in describing common\nobjects in images captured from a human perspective, their training sets are\nless likely to include strong signals for esoteric features in the built\nenvironment, and their performance in these settings is therefore unclear. We\ndemonstrate proof-of-concept combining a state-of-the-art vision language model\nand variants of a prompting strategy that asks the model to consider segmented\nelements independently of the original image. Experiments on two urban features\n-- stop lines and raised tables -- show that while direct zero-shot prompting\ncorrectly annotates nearly zero images, the pre-segmentation strategies can\nannotate images with near 40% intersection-over-union accuracy. We describe how\nthese results inform a new research agenda in automatic annotation of the built\nenvironment to improve equity, accessibility, and safety at broad scale and in\ndiverse environments.\n","authors":["Bin Han","Yiwei Yang","Anat Caspi","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.00932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00921v1","updated":"2024-08-01T21:22:16Z","published":"2024-08-01T21:22:16Z","title":"Automatic Pull Request Description Generation Using LLMs: A T5 Model\n  Approach","summary":"  Developers create pull request (PR) descriptions to provide an overview of\ntheir changes and explain the motivations behind them. These descriptions help\nreviewers and fellow developers quickly understand the updates. Despite their\nimportance, some developers omit these descriptions. To tackle this problem, we\npropose an automated method for generating PR descriptions based on commit\nmessages and source code comments. This method frames the task as a text\nsummarization problem, for which we utilized the T5 text-to-text transfer\nmodel. We fine-tuned a pre-trained T5 model using a dataset containing 33,466\nPRs. The model's effectiveness was assessed using ROUGE metrics, which are\nrecognized for their strong alignment with human evaluations. Our findings\nreveal that the T5 model significantly outperforms LexRank, which served as our\nbaseline for comparison.\n","authors":["Md Nazmus Sakib","Md Athikul Islam","Md Mashrur Arifin"],"pdf_url":"https://arxiv.org/pdf/2408.00921v1.pdf","comment":"Accepted to 2nd International Conference on Artificial Intelligence,\n  Blockchain, and Internet of Things (AIBThings-2024), September 07-08, 2024,\n  Michigan, USA"},{"id":"http://arxiv.org/abs/2404.00859v2","updated":"2024-08-01T21:21:28Z","published":"2024-04-01T02:01:28Z","title":"Do language models plan ahead for future tokens?","summary":"  Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.\n","authors":["Wilson Wu","John X. Morris","Lionel Levine"],"pdf_url":"https://arxiv.org/pdf/2404.00859v2.pdf","comment":"24 pages, 11 figures. Camera-ready for COLM 2024"},{"id":"http://arxiv.org/abs/2408.00914v1","updated":"2024-08-01T21:08:07Z","published":"2024-08-01T21:08:07Z","title":"Granting GPT-4 License and Opportunity: Enhancing Accuracy and\n  Confidence Estimation for Few-Shot Event Detection","summary":"  Large Language Models (LLMs) such as GPT-4 have shown enough promise in the\nfew-shot learning context to suggest use in the generation of \"silver\" data and\nrefinement of new ontologies through iterative application and review. Such\nworkflows become more effective with reliable confidence estimation.\nUnfortunately, confidence estimation is a documented weakness of models such as\nGPT-4, and established methods to compensate require significant additional\ncomplexity and computation. The present effort explores methods for effective\nconfidence estimation with GPT-4 with few-shot learning for event detection in\nthe BETTER ontology as a vehicle. The key innovation is expanding the prompt\nand task presented to GPT-4 to provide License to speculate when unsure and\nOpportunity to quantify and explain its uncertainty (L&O). This approach\nimproves accuracy and provides usable confidence measures (0.759 AUC) with no\nadditional machinery.\n","authors":["Steven Fincke","Adrien Bibal","Elizabeth Boschee"],"pdf_url":"https://arxiv.org/pdf/2408.00914v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00884v1","updated":"2024-08-01T19:29:18Z","published":"2024-08-01T19:29:18Z","title":"Hybrid Querying Over Relational Databases and Large Language Models","summary":"  Database queries traditionally operate under the closed-world assumption,\nproviding no answers to questions that require information beyond the data\nstored in the database. Hybrid querying using SQL offers an alternative by\nintegrating relational databases with large language models (LLMs) to answer\nbeyond-database questions. In this paper, we present the first cross-domain\nbenchmark, SWAN, containing 120 beyond-database questions over four real-world\ndatabases. To leverage state-of-the-art language models in addressing these\ncomplex questions in SWAN, we present, HQDL, a preliminary solution for hybrid\nquerying, and also discuss potential future directions. Our evaluation\ndemonstrates that HQDL using GPT-4 Turbo with few-shot prompts, achieves 40.0\\%\nin execution accuracy and 48.2\\% in data factuality. These results highlights\nboth the potential and challenges for hybrid querying. We believe that our work\nwill inspire further research in creating more efficient and accurate data\nsystems that seamlessly integrate relational databases and large language\nmodels to address beyond-database questions.\n","authors":["Fuheng Zhao","Divyakant Agrawal","Amr El Abbadi"],"pdf_url":"https://arxiv.org/pdf/2408.00884v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00863v1","updated":"2024-08-01T18:31:31Z","published":"2024-08-01T18:31:31Z","title":"UniMoT: Unified Molecule-Text Language Model with Discrete Token\n  Representation","summary":"  The remarkable success of Large Language Models (LLMs) across diverse tasks\nhas driven the research community to extend their capabilities to molecular\napplications. However, most molecular LLMs employ adapter-based architectures\nthat do not treat molecule and text modalities equally and lack a supervision\nsignal for the molecule modality. To address these issues, we introduce UniMoT,\na Unified Molecule-Text LLM adopting a tokenizer-based architecture that\nexpands the vocabulary of LLM with molecule tokens. Specifically, we introduce\na Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge\nthe modality gap between molecule and text. This tokenizer transforms molecules\ninto sequences of molecule tokens with causal dependency, encapsulating\nhigh-level molecular and textual information. Equipped with this tokenizer,\nUniMoT can unify molecule and text modalities under a shared token\nrepresentation and an autoregressive training paradigm, enabling it to\ninterpret molecules as a foreign language and generate them as text. Following\na four-stage training scheme, UniMoT emerges as a multi-modal generalist\ncapable of performing both molecule-to-text and text-to-molecule tasks.\nExtensive experiments demonstrate that UniMoT achieves state-of-the-art\nperformance across a wide range of molecule comprehension and generation tasks.\n","authors":["Juzheng Zhang","Yatao Bian","Yongqiang Chen","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2408.00863v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2308.05846v2","updated":"2024-08-01T17:46:38Z","published":"2023-08-10T19:56:15Z","title":"Seed Kernel Counting using Domain Randomization and Object Tracking\n  Neural Networks","summary":"  High-throughput phenotyping (HTP) of seeds, also known as seed phenotyping,\nis the comprehensive assessment of complex seed traits such as growth,\ndevelopment, tolerance, resistance, ecology, yield, and the measurement of\nparameters that form more complex traits. One of the key aspects of seed\nphenotyping is cereal yield estimation that the seed production industry relies\nupon to conduct their business. While mechanized seed kernel counters are\navailable in the market currently, they are often priced high and sometimes\noutside the range of small scale seed production firms' affordability. The\ndevelopment of object tracking neural network models such as You Only Look Once\n(YOLO) enables computer scientists to design algorithms that can estimate\ncereal yield inexpensively. The key bottleneck with neural network models is\nthat they require a plethora of labelled training data before they can be put\nto task. We demonstrate that the use of synthetic imagery serves as a feasible\nsubstitute to train neural networks for object tracking that includes the tasks\nof object classification and detection. Furthermore, we propose a seed kernel\ncounter that uses a low-cost mechanical hopper, trained YOLOv8 neural network\nmodel, and object tracking algorithms on StrongSORT and ByteTrack to estimate\ncereal yield from videos. The experiment yields a seed kernel count with an\naccuracy of 95.2\\% and 93.2\\% for Soy and Wheat respectively using the\nStrongSORT algorithm, and an accuray of 96.8\\% and 92.4\\% for Soy and Wheat\nrespectively using the ByteTrack algorithm.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Mitchell Neilsen"],"pdf_url":"https://arxiv.org/pdf/2308.05846v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18320v2","updated":"2024-08-01T17:43:19Z","published":"2024-05-28T16:11:11Z","title":"Self-Supervised Learning Based Handwriting Verification","summary":"  We present SSL-HV: Self-Supervised Learning approaches applied to the task of\nHandwriting Verification. This task involves determining whether a given pair\nof handwritten images originate from the same or different writer distribution.\nWe have compared the performance of multiple generative, contrastive SSL\napproaches against handcrafted feature extractors and supervised learning on\nCEDAR AND dataset. We show that ResNet based Variational Auto-Encoder (VAE)\noutperforms other generative approaches achieving 76.3% accuracy, while\nResNet-18 fine-tuned using Variance-Invariance-Covariance Regularization\n(VICReg) outperforms other contrastive approaches achieving 78% accuracy. Using\na pre-trained VAE and VICReg for the downstream task of writer verification we\nobserved a relative improvement in accuracy of 6.7% and 9% over ResNet-18\nsupervised baseline with 10% writer labels.\n","authors":["Mihir Chauhan","Mohammad Abuzar Hashemi","Abhishek Satbhai","Mir Basheer Ali","Bina Ramamurthy","Mingchen Gao","Siwei Lyu","Sargur Srihari"],"pdf_url":"https://arxiv.org/pdf/2405.18320v2.pdf","comment":"8 pages, 2 figures, 2 tables, Accepted at Irish Machine Vision and\n  Image Processing Conference 2024"},{"id":"http://arxiv.org/abs/2402.10344v2","updated":"2024-08-01T17:34:51Z","published":"2024-02-15T22:17:17Z","title":"Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions","summary":"  We evaluate different Neural Radiance Fields (NeRFs) techniques for\nreconstructing (3D) plants in varied environments, from indoor settings to\noutdoor fields. Traditional techniques often struggle to capture the complex\ndetails of plants, which is crucial for botanical and agricultural\nunderstanding. We evaluate three scenarios with increasing complexity and\ncompare the results with the point cloud obtained using LiDAR as ground truth\ndata. In the most realistic field scenario, the NeRF models achieve a 74.65% F1\nscore with 30 minutes of training on the GPU, highlighting the efficiency and\naccuracy of NeRFs in challenging environments. These findings not only\ndemonstrate the potential of NeRF in detailed and realistic 3D plant modeling\nbut also suggest practical approaches for enhancing the speed and efficiency of\nthe 3D reconstruction process.\n","authors":["Muhammad Arbab Arshad","Talukder Jubery","James Afful","Anushrut Jignasu","Aditya Balu","Baskar Ganapathysubramanian","Soumik Sarkar","Adarsh Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.10344v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14360v2","updated":"2024-08-01T17:14:17Z","published":"2024-06-20T14:33:51Z","title":"Deblurring Neural Radiance Fields with Event-driven Bundle Adjustment","summary":"  Neural Radiance Fields (NeRF) achieves impressive 3D representation learning\nand novel view synthesis results with high-quality multi-view images as input.\nHowever, motion blur in images often occurs in low-light and high-speed motion\nscenes, which significantly degrades the reconstruction quality of NeRF.\nPrevious deblurring NeRF methods struggle to estimate pose and lighting changes\nduring the exposure time, making them unable to accurately model the motion\nblur. The bio-inspired event camera measuring intensity changes with high\ntemporal resolution makes up this information deficiency. In this paper, we\npropose Event-driven Bundle Adjustment for Deblurring Neural Radiance Fields\n(EBAD-NeRF) to jointly optimize the learnable poses and NeRF parameters by\nleveraging the hybrid event-RGB data. An intensity-change-metric event loss and\na photo-metric blur loss are introduced to strengthen the explicit modeling of\ncamera motion blur. Experiments on both synthetic and real-captured data\ndemonstrate that EBAD-NeRF can obtain accurate camera trajectory during the\nexposure time and learn a sharper 3D representations compared to prior works.\n","authors":["Yunshan Qi","Lin Zhu","Yifan Zhao","Nan Bao","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2406.14360v2.pdf","comment":"Accepted by 32nd ACM International Conference on Multimedia (MM 2024)"},{"id":"http://arxiv.org/abs/2407.20021v3","updated":"2024-08-01T16:13:45Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v3.pdf","comment":"Author Preprint"},{"id":"http://arxiv.org/abs/2307.09067v2","updated":"2024-08-01T16:09:50Z","published":"2023-07-18T08:37:58Z","title":"Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image\n  Segmentation with U-Net","summary":"  Fetal head segmentation is a crucial step in measuring the fetal head\ncircumference (HC) during gestation, an important biometric in obstetrics for\nmonitoring fetal growth. However, manual biometry generation is time-consuming\nand results in inconsistent accuracy. To address this issue, convolutional\nneural network (CNN) models have been utilized to improve the efficiency of\nmedical biometry. But training a CNN network from scratch is a challenging\ntask, we proposed a Transfer Learning (TL) method. Our approach involves\nfine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to\nperform segmentation on a set of fetal head ultrasound (US) images with limited\neffort. This method addresses the challenges associated with training a CNN\nnetwork from scratch. It suggests that our proposed FT strategy yields\nsegmentation performance that is comparable when trained with a reduced number\nof parameters by 85.8%. And our proposed FT strategy outperforms other\nstrategies with smaller trainable parameter sizes below 4.4 million. Thus, we\ncontend that it can serve as a dependable FT approach for reducing the size of\nmodels in medical image analysis. Our key findings highlight the importance of\nthe balance between model performance and size in developing Artificial\nIntelligence (AI) applications by TL methods. Code is available at\nhttps://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.\n","authors":["Fangyijie Wang","Gunol Silvestre","Kathleen M. Curran"],"pdf_url":"https://arxiv.org/pdf/2307.09067v2.pdf","comment":"Irish Machine Vision and Image Processing Conference Proceedings 2023"},{"id":"http://arxiv.org/abs/2406.11551v3","updated":"2024-08-01T16:00:04Z","published":"2024-06-17T13:49:12Z","title":"Towards Self-Supervised FG-SBIR with Unified Sample Feature Alignment\n  and Multi-Scale Token Recycling","summary":"  Fine-Grained Sketch-Based Image Retrieval (FG-SBIR) aims to minimize the\ndistance between sketches and corresponding images in the embedding space.\nHowever, scalability is hindered by the growing complexity of solutions, mainly\ndue to the abstract nature of fine-grained sketches. In this paper, we propose\nan effective approach to narrow the gap between the two domains. It mainly\nfacilitates unified mutual information sharing both intra- and inter-samples,\nrather than treating them as a single feature alignment problem between\nmodalities. Specifically, our approach includes: (i) Employing dual\nweight-sharing networks to optimize alignment within the sketch and image\ndomain, which also effectively mitigates model learning saturation issues. (ii)\nIntroducing an objective optimization function based on contrastive loss to\nenhance the model's ability to align features in both intra- and inter-samples.\n(iii) Presenting a self-supervised Multi-Scale Token Recycling (MSTR) Module\nfeatured by recycling discarded patch tokens in multi-scale features, further\nenhancing representation capability and retrieval performance. Our framework\nachieves excellent results on CNN- and ViT-based backbones. Extensive\nexperiments demonstrate its superiority over existing methods. We also\nintroduce Cloths-V1, the first professional fashion sketch-image dataset,\nutilized to validate our method and will be beneficial for other applications\n","authors":["Jianan Jiang","Hao Tang","Zhilin Jiang","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2406.11551v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.14168v4","updated":"2024-08-01T15:56:43Z","published":"2024-01-25T13:27:03Z","title":"Vivim: a Video Vision Mamba for Medical Video Segmentation","summary":"  Medical video segmentation gains increasing attention in clinical practice\ndue to the redundant dynamic references in video frames. However, traditional\nconvolutional neural networks have a limited receptive field and\ntransformer-based networks are mediocre in constructing long-term dependency\nfrom the perspective of computational complexity. This bottleneck poses a\nsignificant challenge when processing longer sequences in medical video\nanalysis tasks using available devices with limited memory. Recently, state\nspace models (SSMs), famous by Mamba, have exhibited impressive achievements in\nefficient long sequence modeling, which develops deep neural networks by\nexpanding the receptive field on many vision tasks significantly.\nUnfortunately, vanilla SSMs failed to simultaneously capture causal temporal\ncues and preserve non-casual spatial information. To this end, this paper\npresents a Video Vision Mamba-based framework, dubbed as Vivim, for medical\nvideo segmentation tasks. Our Vivim can effectively compress the long-term\nspatiotemporal representation into sequences at varying scales with our\ndesigned Temporal Mamba Block. We also introduce an improved boundary-aware\naffine constraint across frames to enhance the discriminative ability of Vivim\non ambiguous lesions. Extensive experiments on thyroid segmentation, breast\nlesion segmentation in ultrasound videos, and polyp segmentation in colonoscopy\nvideos demonstrate the effectiveness and efficiency of our Vivim, superior to\nexisting methods. The code is available at:\nhttps://github.com/scott-yjyang/Vivim. The dataset will be released once\naccepted.\n","authors":["Yijun Yang","Zhaohu Xing","Lequan Yu","Chunwang Huang","Huazhu Fu","Lei Zhu"],"pdf_url":"https://arxiv.org/pdf/2401.14168v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13312v2","updated":"2024-08-01T15:54:29Z","published":"2023-04-26T06:33:31Z","title":"Technical Note: Defining and Quantifying AND-OR Interactions for\n  Faithful and Concise Explanation of DNNs","summary":"  In this technical note, we aim to explain a deep neural network (DNN) by\nquantifying the encoded interactions between input variables, which reflects\nthe DNN's inference logic. Specifically, we first rethink the definition of\ninteractions, and then formally define faithfulness and conciseness for\ninteraction-based explanation. To this end, we propose two kinds of\ninteractions, i.e., the AND interaction and the OR interaction. For\nfaithfulness, we prove the uniqueness of the AND (OR) interaction in\nquantifying the effect of the AND (OR) relationship between input variables.\nBesides, based on AND-OR interactions, we design techniques to boost the\nconciseness of the explanation, while not hurting the faithfulness. In this\nway, the inference logic of a DNN can be faithfully and concisely explained by\na set of symbolic concepts.\n","authors":["Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13312v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.06206"},{"id":"http://arxiv.org/abs/2401.14832v3","updated":"2024-08-01T15:46:43Z","published":"2024-01-26T13:01:28Z","title":"Text Image Inpainting via Global Structure-Guided Diffusion Models","summary":"  Real-world text can be damaged by corrosion issues caused by environmental or\nhuman factors, which hinder the preservation of the complete styles of texts,\ne.g., texture and structure. These corrosion issues, such as graffiti signs and\nincomplete signatures, bring difficulties in understanding the texts, thereby\nposing significant challenges to downstream applications, e.g., scene text\nrecognition and signature identification. Notably, current inpainting\ntechniques often fail to adequately address this problem and have difficulties\nrestoring accurate text images along with reasonable and consistent styles.\nFormulating this as an open problem of text image inpainting, this paper aims\nto build a benchmark to facilitate its study. In doing so, we establish two\nspecific text inpainting datasets which contain scene text images and\nhandwritten text images, respectively. Each of them includes images revamped by\nreal-life and synthetic datasets, featuring pairs of original images, corrupted\nimages, and other assistant information. On top of the datasets, we further\ndevelop a novel neural framework, Global Structure-guided Diffusion Model\n(GSDM), as a potential solution. Leveraging the global structure of the text as\na prior, the proposed GSDM develops an efficient diffusion model to recover\nclean texts. The efficacy of our approach is demonstrated by thorough empirical\nstudy, including a substantial boost in both recognition accuracy and image\nquality. These findings not only highlight the effectiveness of our method but\nalso underscore its potential to enhance the broader field of text image\nunderstanding and processing. Code and datasets are available at:\nhttps://github.com/blackprotoss/GSDM.\n","authors":["Shipeng Zhu","Pengfei Fang","Chenjie Zhu","Zuoyan Zhao","Qiang Xu","Hui Xue"],"pdf_url":"https://arxiv.org/pdf/2401.14832v3.pdf","comment":"Accepted by AAAI-24"},{"id":"http://arxiv.org/abs/2311.00048v2","updated":"2024-08-01T15:37:52Z","published":"2023-10-31T18:01:41Z","title":"SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image\n  Classification","summary":"  Multiple Instance Learning (MIL) has been widely used in weakly supervised\nwhole slide image (WSI) classification. Typical MIL methods include a feature\nembedding part, which embeds the instances into features via a pre-trained\nfeature extractor, and an MIL aggregator that combines instance embeddings into\npredictions. Most efforts have typically focused on improving these parts. This\ninvolves refining the feature embeddings through self-supervised pre-training\nas well as modeling the correlations between instances separately.\n  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that\naddresses those two aspects at the same time by leveraging sparse dictionary\nlearning. The sparse dictionary learning captures the similarities of instances\nby expressing them as sparse linear combinations of atoms in an over-complete\ndictionary. In addition, imposing sparsity improves instance feature embeddings\nby suppressing irrelevant instances while retaining the most relevant ones. To\nmake the conventional sparse coding algorithm compatible with deep learning, we\nunrolled it into a sparsely coded module leveraging deep unrolling. The\nproposed SC module can be incorporated into any existing MIL framework in a\nplug-and-play manner with an acceptable computational cost. The experimental\nresults on multiple datasets demonstrated that the proposed SC module could\nsubstantially boost the performance of state-of-the-art MIL methods. The codes\nare available at\n\\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.\n","authors":["Peijie Qiu","Pan Xiao","Wenhui Zhu","Yalin Wang","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2311.00048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02111v3","updated":"2024-08-01T14:01:56Z","published":"2023-12-04T18:43:45Z","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology","summary":"  Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.\n","authors":["Lucas Farndale","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.02111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12715v2","updated":"2024-08-01T13:53:43Z","published":"2022-09-26T14:11:05Z","title":"Enhancing convolutional neural network generalizability via low-rank\n  weight approximation","summary":"  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n","authors":["Chenyin Gao","Shu Yang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.12715v2.pdf","comment":"accepted by IET Image Processing"},{"id":"http://arxiv.org/abs/2407.12927v2","updated":"2024-08-01T13:51:26Z","published":"2024-07-17T18:01:25Z","title":"Textualized and Feature-based Models for Compound Multimodal Emotion\n  Recognition in the Wild","summary":"  Systems for multimodal emotion recognition (ER) are commonly trained to\nextract features from different modalities (e.g., visual, audio, and textual)\nthat are combined to predict individual basic emotions. However, compound\nemotions often occur in real-world scenarios, and the uncertainty of\nrecognizing such complex emotions over diverse modalities is challenging for\nfeature-based models As an alternative, emerging multimodal large language\nmodels (LLMs) like BERT and LLaMA rely on explicit non-verbal cues that may be\ntranslated from different non-textual modalities (e.g., audio and visual) into\ntext. Textualization of modalities augments data with emotional cues to help\nthe LLM encode the interconnections between all modalities in a shared text\nspace. In such text-based models, prior knowledge of ER tasks is leveraged to\ntextualize relevant nonverbal cues such as audio tone from vocal expressions,\nand action unit intensity from facial expressions. Since the pre-trained\nweights are publicly available for many LLMs, training on large-scale datasets\nis unnecessary, allowing fine-tuning for downstream tasks such as compound ER\n(CER). This paper compares the potential of text- and feature-based approaches\nfor compound multimodal ER in videos. Experiments were conducted on the\nchallenging C-EXPR-DB dataset in the wild for CER, and contrasted with results\non the MELD dataset for basic ER. Our results indicate that multimodal\ntextualization provides lower accuracy than feature-based models on C-EXPR-DB,\nwhere text transcripts are captured in the wild. However, higher accuracy can\nbe achieved when the video data has rich transcripts. Our code is available.\n","authors":["Nicolas Richet","Soufiane Belharbi","Haseeb Aslam","Meike Emilie Schadt","Manuela Gonzlez-Gonzlez","Gustave Cortal","Alessandro Lameiras Koerich","Marco Pedersoli","Alain Finkel","Simon Bacon","Eric Granger"],"pdf_url":"https://arxiv.org/pdf/2407.12927v2.pdf","comment":"13 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.11356v2","updated":"2024-08-01T12:58:29Z","published":"2024-07-16T03:41:48Z","title":"The Devil is in the Statistics: Mitigating and Exploiting Statistics\n  Difference for Generalizable Semi-supervised Medical Image Segmentation","summary":"  Despite the recent success of domain generalization in medical image\nsegmentation, voxel-wise annotation for all source domains remains a huge\nburden. Semi-supervised domain generalization has been proposed very recently\nto combat this challenge by leveraging limited labeled data along with abundant\nunlabeled data collected from multiple medical institutions, depending on\nprecisely harnessing unlabeled data while improving generalization\nsimultaneously. In this work, we observe that domain shifts between medical\ninstitutions cause disparate feature statistics, which significantly\ndeteriorates pseudo-label quality due to an unexpected normalization process.\nNevertheless, this phenomenon could be exploited to facilitate unseen domain\ngeneralization. Therefore, we propose 1) multiple statistics-individual\nbranches to mitigate the impact of domain shifts for reliable pseudo-labels and\n2) one statistics-aggregated branch for domain-invariant feature learning.\nFurthermore, to simulate unseen domains with statistics difference, we approach\nthis from two aspects, i.e., a perturbation with histogram matching at image\nlevel and a random batch normalization selection strategy at feature level,\nproducing diverse statistics to expand the training distribution. Evaluation\nresults on three medical image datasets demonstrate the effectiveness of our\nmethod compared with recent SOTA methods. The code is available at\nhttps://github.com/qiumuyang/SIAB.\n","authors":["Muyang Qiu","Jian Zhang","Lei Qi","Qian Yu","Yinghuan Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11356v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2404.04526v2","updated":"2024-08-01T11:17:28Z","published":"2024-04-06T06:48:16Z","title":"DATENeRF: Depth-Aware Text-based Editing of NeRFs","summary":"  Recent advancements in diffusion models have shown remarkable proficiency in\nediting 2D images based on text prompts. However, extending these techniques to\nedit scenes in Neural Radiance Fields (NeRF) is complex, as editing individual\n2D frames can result in inconsistencies across multiple views. Our crucial\ninsight is that a NeRF scene's geometry can serve as a bridge to integrate\nthese 2D edits. Utilizing this geometry, we employ a depth-conditioned\nControlNet to enhance the coherence of each 2D image modification. Moreover, we\nintroduce an inpainting approach that leverages the depth information of NeRF\nscenes to distribute 2D edits across different images, ensuring robustness\nagainst errors and resampling challenges. Our results reveal that this\nmethodology achieves more consistent, lifelike, and detailed edits than\nexisting leading methods for text-driven NeRF scene editing.\n","authors":["Sara Rojas","Julien Philip","Kai Zhang","Sai Bi","Fujun Luan","Bernard Ghanem","Kalyan Sunkavall"],"pdf_url":"https://arxiv.org/pdf/2404.04526v2.pdf","comment":"3D Scene Editing, Neural Rendering, Diffusion Models, Accepted to\n  ECCV24"},{"id":"http://arxiv.org/abs/2404.10572v2","updated":"2024-08-01T10:34:47Z","published":"2024-04-16T13:47:27Z","title":"Label merge-and-split: A graph-colouring approach for memory-efficient\n  brain parcellation","summary":"  Whole brain parcellation requires inferring hundreds of segmentation labels\nin large image volumes and thus presents significant practical challenges for\ndeep learning approaches. We introduce label merge-and-split, a method that\nfirst greatly reduces the effective number of labels required for\nlearning-based whole brain parcellation and then recovers original labels.\nUsing a greedy graph colouring algorithm, our method automatically groups and\nmerges multiple spatially separate labels prior to model training and\ninference. The merged labels may be semantically unrelated. A deep learning\nmodel is trained to predict merged labels. At inference time, original labels\nare restored using atlas-based influence regions. In our experiments, the\nproposed approach reduces the number of labels by up to 68% while achieving\nsegmentation accuracy comparable to the baseline method without label merging\nand splitting. Moreover, model training and inference times as well as GPU\nmemory requirements were reduced significantly. The proposed method can be\napplied to all semantic segmentation tasks with a large number of spatially\nseparate classes within an atlas-based prior.\n","authors":["Aaron Kujawa","Reuben Dorent","Sebastien Ourselin","Tom Vercauteren"],"pdf_url":"https://arxiv.org/pdf/2404.10572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.07558v2","updated":"2024-08-01T09:57:28Z","published":"2024-04-21T04:37:24Z","title":"An AI-Enabled Framework Within Reach for Enhancing Healthcare\n  Sustainability and Fairness","summary":"  Good health and well-being is among key issues in the United Nations 2030\nSustainable Development Goals. The rising prevalence of large-scale infectious\ndiseases and the accelerated aging of the global population are driving the\ntransformation of healthcare technologies. In this context, establishing\nlarge-scale public health datasets, developing medical models, and creating\ndecision-making systems with a human-centric approach are of strategic\nsignificance. Recently, by leveraging the extraordinary number of accessible\ncameras, groundbreaking advancements have emerged in AI methods for\nphysiological signal monitoring and disease diagnosis using camera sensors.\nThese approaches, requiring no specialized medical equipment, offer convenient\nmanners of collecting large-scale medical data in response to public health\nevents. Therefore, we outline a prospective framework and heuristic vision for\na camera-based public health (CBPH) framework utilizing visual physiological\nmonitoring technology. The CBPH can be considered as a convenient and universal\nframework for public health, advancing the United Nations Sustainable\nDevelopment Goals, particularly in promoting the universality, sustainability,\nand equity of healthcare in low- and middle-income countries or regions.\nFurthermore, CBPH provides a comprehensive solution for building a large-scale\nand human-centric medical database, and a multi-task large medical model for\npublic health and medical scientific discoveries. It has a significant\npotential to revolutionize personal monitoring technologies, digital medicine,\ntelemedicine, and primary health care in public health. Therefore, it can be\ndeemed that the outcomes of this paper will contribute to the establishment of\na sustainable and fair framework for public health, which serves as a crucial\nbridge for advancing scientific discoveries in the realm of AI for medicine\n(AI4Medicine).\n","authors":["Bin Huang","Changchen Zhao","Zimeng Liu","Shenda Hong","Baochang Zhang","Hao Lu","Zhijun Liu","Wenjin Wang","Hui Liu"],"pdf_url":"https://arxiv.org/pdf/2406.07558v2.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.20660v2","updated":"2024-08-01T09:56:15Z","published":"2024-07-30T08:52:51Z","title":"What makes for good morphology representations for spatial omics?","summary":"  Spatial omics has transformed our understanding of tissue architecture by\npreserving spatial context of gene expression patterns. Simultaneously,\nadvances in imaging AI have enabled extraction of morphological features\ndescribing the tissue. The intersection of spatial omics and imaging AI\npresents opportunities for a more holistic understanding. In this review we\nintroduce a framework for categorizing spatial omics-morphology combination\nmethods, focusing on how morphological features can be translated or integrated\ninto spatial omics analyses. By translation we mean finding morphological\nfeatures that spatially correlate with gene expression patterns with the\npurpose of predicting gene expression. Such features can be used to generate\nsuper-resolution gene expression maps or infer genetic information from\nclinical H&E-stained samples. By integration we mean finding morphological\nfeatures that spatially complement gene expression patterns with the purpose of\nenriching information. Such features can be used to define spatial domains,\nespecially where gene expression has preceded morphological changes and where\nmorphology remains after gene expression. We discuss learning strategies and\ndirections for further development of the field.\n","authors":["Eduard Chelebian","Christophe Avenel","Carolina Whlby"],"pdf_url":"https://arxiv.org/pdf/2407.20660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.18401v3","updated":"2024-08-01T09:04:39Z","published":"2024-04-29T03:36:05Z","title":"Spectral-Spatial Mamba for Hyperspectral Image Classification","summary":"  Recently, deep learning models have achieved excellent performance in\nhyperspectral image (HSI) classification. Among the many deep models,\nTransformer has gradually attracted interest for its excellence in modeling the\nlong-range dependencies of spatial-spectral features in HSI. However,\nTransformer has the problem of quadratic computational complexity due to the\nself-attention mechanism, which is heavier than other models and thus has\nlimited adoption in HSI processing. Fortunately, the recently emerging state\nspace model-based Mamba shows great computational efficiency while achieving\nthe modeling power of Transformers. Therefore, in this paper, we make a\npreliminary attempt to apply the Mamba to HSI classification, leading to the\nproposed spectral-spatial Mamba (SS-Mamba). Specifically, the proposed SS-Mamba\nmainly consists of spectral-spatial token generation module and several stacked\nspectral-spatial Mamba blocks. Firstly, the token generation module converts\nany given HSI cube to spatial and spectral tokens as sequences. And then these\ntokens are sent to stacked spectral-spatial mamba blocks (SS-MB). Each SS-MB\nblock consists of two basic mamba blocks and a spectral-spatial feature\nenhancement module. The spatial and spectral tokens are processed separately by\nthe two basic mamba blocks, respectively. Besides, the feature enhancement\nmodule modulates spatial and spectral tokens using HSI sample's center region\ninformation. In this way, the spectral and spatial tokens cooperate with each\nother and achieve information fusion within each block. The experimental\nresults conducted on widely used HSI datasets reveal that the proposed model\nachieves competitive results compared with the state-of-the-art methods. The\nMamba-based method opens a new window for HSI classification.\n","authors":["Lingbo Huang","Yushi Chen","Xin He"],"pdf_url":"https://arxiv.org/pdf/2404.18401v3.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2405.01409v3","updated":"2024-08-01T08:58:40Z","published":"2024-05-02T16:01:58Z","title":"Goal-conditioned reinforcement learning for ultrasound navigation\n  guidance","summary":"  Transesophageal echocardiography (TEE) plays a pivotal role in cardiology for\ndiagnostic and interventional procedures. However, using it effectively\nrequires extensive training due to the intricate nature of image acquisition\nand interpretation. To enhance the efficiency of novice sonographers and reduce\nvariability in scan acquisitions, we propose a novel ultrasound (US) navigation\nassistance method based on contrastive learning as goal-conditioned\nreinforcement learning (GCRL). We augment the previous framework using a novel\ncontrastive patient batching method (CPB) and a data-augmented contrastive\nloss, both of which we demonstrate are essential to ensure generalization to\nanatomical variations across patients. The proposed framework enables\nnavigation to both standard diagnostic as well as intricate interventional\nviews with a single model. Our method was developed with a large dataset of 789\npatients and obtained an average error of 6.56 mm in position and 9.36 degrees\nin angle on a testing dataset of 140 patients, which is competitive or superior\nto models trained on individual views. Furthermore, we quantitatively validate\nour method's ability to navigate to interventional views such as the Left\nAtrial Appendage (LAA) view used in LAA closure. Our approach holds promise in\nproviding valuable guidance during transesophageal ultrasound examinations,\ncontributing to the advancement of skill acquisition for cardiac ultrasound\npractitioners.\n","authors":["Abdoul Aziz Amadou","Vivek Singh","Florin C. Ghesu","Young-Ho Kim","Laura Stanciulescu","Harshitha P. Sai","Puneet Sharma","Alistair Young","Ronak Rajani","Kawal Rhode"],"pdf_url":"https://arxiv.org/pdf/2405.01409v3.pdf","comment":"Accepted in MICCAI 2024; 11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2303.04664v2","updated":"2024-08-01T08:39:23Z","published":"2023-03-08T15:34:57Z","title":"Centroid-centered Modeling for Efficient Vision Transformer Pre-training","summary":"  Masked Image Modeling (MIM) is a new self-supervised vision pre-training\nparadigm using a Vision Transformer (ViT). Previous works can be pixel-based or\ntoken-based, using original pixels or discrete visual tokens from parametric\ntokenizer models, respectively. Our proposed centroid-based approach, CCViT,\nleverages k-means clustering to obtain centroids for image modeling without\nsupervised training of the tokenizer model, which only takes seconds to create.\nThis non-parametric centroid tokenizer only takes seconds to create and is\nfaster for token inference. The centroids can represent both patch pixels and\nindex tokens with the property of local invariance. Specifically, we adopt\npatch masking and centroid replacing strategies to construct corrupted inputs,\nand two stacked encoder blocks to predict corrupted patch tokens and\nreconstruct original patch pixels. Experiments show that our CCViT achieves\n84.4% top-1 accuracy on ImageNet-1K classification with ViT-B and 86.0% with\nViT-L. We also transfer our pre-trained model to other downstream tasks. Our\napproach achieves competitive results with recent baselines without external\nsupervision and distillation training from other models.\n","authors":["Xin Yan","Zuchao Li","Lefei Zhang"],"pdf_url":"https://arxiv.org/pdf/2303.04664v2.pdf","comment":"Codes are available at https://github.com/Cakeyan/CCViT_Public"},{"id":"http://arxiv.org/abs/2407.03104v2","updated":"2024-08-01T08:08:43Z","published":"2024-07-03T13:41:44Z","title":"KeyVideoLLM: Towards Large-scale Video Keyframe Selection","summary":"  Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets.\n","authors":["Hao Liang","Jiapeng Li","Tianyi Bai","Xijie Huang","Linzhuang Sun","Zhengren Wang","Conghui He","Bin Cui","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08604v2","updated":"2024-08-01T07:55:49Z","published":"2024-06-12T19:17:17Z","title":"GRU-Net: Gaussian Attention Aided Dense Skip Connection Based\n  MultiResUNet for Breast Histopathology Image Segmentation","summary":"  Breast cancer is a major global health concern. Pathologists face challenges\nin analyzing complex features from pathological images, which is a\ntime-consuming and labor-intensive task. Therefore, efficient computer-based\ndiagnostic tools are needed for early detection and treatment planning. This\npaper presents a modified version of MultiResU-Net for histopathology image\nsegmentation, which is selected as the backbone for its ability to analyze and\nsegment complex features at multiple scales and ensure effective feature flow\nvia skip connections. The modified version also utilizes the Gaussian\ndistribution-based Attention Module (GdAM) to incorporate\nhistopathology-relevant text information in a Gaussian distribution. The\nsampled features from the Gaussian text feature-guided distribution highlight\nspecific spatial regions based on prior knowledge. Finally, using the\nControlled Dense Residual Block (CDRB) on skip connections of MultiResU-Net,\nthe information is transferred from the encoder layers to the decoder layers in\na controlled manner using a scaling parameter derived from the extracted\nspatial features. We validate our approach on two diverse breast cancer\nhistopathology image datasets: TNBC and MonuSeg, demonstrating superior\nsegmentation performance compared to state-of-the-art methods. The code for our\nproposed model is available on https://github.com/AyushRoy2001/GRU-Net.\n","authors":["Ayush Roy","Payel Pramanik","Sohom Ghosal","Daria Valenkova","Dmitrii Kaplun","Ram Sarkar"],"pdf_url":"https://arxiv.org/pdf/2406.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20843v2","updated":"2024-08-01T07:43:11Z","published":"2024-07-30T14:16:09Z","title":"DFE-IANet: A Method for Polyp Image Classification Based on Dual-domain\n  Feature Extraction and Interaction Attention","summary":"  It is helpful in preventing colorectal cancer to detect and treat polyps in\nthe gastrointestinal tract early. However, there have been few studies to date\non designing polyp image classification networks that balance efficiency and\naccuracy. This challenge is mainly attributed to the fact that polyps are\nsimilar to other pathologies and have complex features influenced by texture,\ncolor, and morphology. In this paper, we propose a novel network DFE-IANet\nbased on both spectral transformation and feature interaction. Firstly, to\nextract detailed features and multi-scale features, the features are\ntransformed by the multi-scale frequency domain feature extraction (MSFD) block\nto extract texture details at the fine-grained level in the frequency domain.\nSecondly, the multi-scale interaction attention (MSIA) block is designed to\nenhance the network's capability of extracting critical features. This block\nintroduces multi-scale features into self-attention, aiming to adaptively guide\nthe network to concentrate on vital regions. Finally, with a compact parameter\nof only 4M, DFE-IANet outperforms the latest and classical networks in terms of\nefficiency. Furthermore, DFE-IANet achieves state-of-the-art (SOTA) results on\nthe challenging Kvasir dataset, demonstrating a remarkable Top-1 accuracy of\n93.94%. This outstanding accuracy surpasses ViT by 8.94%, ResNet50 by 1.69%,\nand VMamba by 1.88%. Our code is publicly available at\nhttps://github.com/PURSUETHESUN/DFE-IANet.\n","authors":["Wei Wang","Jixing He","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2407.20843v2.pdf","comment":"This paper has been accepted by 2024 International Conference on\n  Intelligent Computing (ICIC 2024). It can be accessed at\n  http://poster-openaccess.com"},{"id":"http://arxiv.org/abs/2312.03781v4","updated":"2024-08-01T07:29:47Z","published":"2023-12-06T09:39:38Z","title":"Lite-Mind: Towards Efficient and Robust Brain Representation Network","summary":"  The limited data availability and the low signal-to-noise ratio of fMRI\nsignals lead to the challenging task of fMRI-to-image retrieval.\nState-of-the-art MindEye remarkably improves fMRI-to-image retrieval\nperformance by leveraging a large model, i.e., a 996M MLP Backbone per subject,\nto align fMRI embeddings to the final hidden layer of CLIP's Vision Transformer\n(ViT). However, significant individual variations exist among subjects, even\nunder identical experimental setups, mandating the training of large\nsubject-specific models. The substantial parameters pose significant challenges\nin deploying fMRI decoding on practical devices. To this end, we propose\nLite-Mind, a lightweight, efficient, and robust brain representation learning\nparadigm based on Discrete Fourier Transform (DFT), which efficiently aligns\nfMRI voxels to fine-grained information of CLIP. We elaborately design a DFT\nbackbone with Spectrum Compression and Frequency Projector modules to learn\ninformative and robust voxel embeddings. Our experiments demonstrate that\nLite-Mind achieves an impressive 94.6% fMRI-to-image retrieval accuracy on the\nNSD dataset for Subject 1, with 98.7% fewer parameters than MindEye. Lite-Mind\nis also proven to be able to be migrated to smaller fMRI datasets and\nestablishes a new state-of-the-art for zero-shot classification on the GOD\ndataset.\n","authors":["Zixuan Gong","Qi Zhang","Guangyin Bao","Lei Zhu","Ke Liu","Liang Hu","Duoqian Miao","Yu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.03781v4.pdf","comment":"17 pages, ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2404.01065v2","updated":"2024-08-01T07:01:49Z","published":"2024-04-01T11:57:40Z","title":"T-Mamba: A unified framework with Long-Range Dependency in dual-domain\n  for 2D & 3D Tooth Segmentation","summary":"  Tooth segmentation is a pivotal step in modern digital dentistry, essential\nfor applications across orthodontic diagnosis and treatment planning. Despite\nits importance, this process is fraught with challenges due to the high noise\nand low contrast inherent in 2D and 3D tooth data. Both Convolutional Neural\nNetworks (CNNs) and Transformers has shown promise in medical image\nsegmentation, yet each method has limitations in handling long-range\ndependencies and computational complexity. To address this issue, this paper\nintroduces T-Mamba, integrating frequency-based features and shared\nbi-positional encoding into vision mamba to address limitations in efficient\nglobal feature modeling. Besides, we design a gate selection unit to integrate\ntwo features in spatial domain and one feature in frequency domain adaptively.\nT-Mamba is the first work to introduce frequency-based features into vision\nmamba, and its flexibility allows it to process both 2D and 3D tooth data\nwithout the need for separate modules. Also, the TED3, a large-scale public\ntooth 2D dental X-ray dataset, has been presented in this paper. Extensive\nexperiments demonstrate that T-Mamba achieves new SOTA results on a public\ntooth CBCT dataset and outperforms previous SOTA methods on TED3 dataset. The\ncode and models are publicly available at: https://github.com/isbrycee/T-Mamba.\n","authors":["Jing Hao","Yonghui Zhu","Lei He","Moyun Liu","James Kit Hon Tsoi","Kuo Feng Hung"],"pdf_url":"https://arxiv.org/pdf/2404.01065v2.pdf","comment":"25 pages, 10 figures, 7 tables"},{"id":"http://arxiv.org/abs/2404.17100v2","updated":"2024-08-01T06:46:04Z","published":"2024-04-26T01:21:08Z","title":"Open-Set Video-based Facial Expression Recognition with Human\n  Expression-sensitive Prompting","summary":"  In Video-based Facial Expression Recognition (V-FER), models are typically\ntrained on closed-set datasets with a fixed number of known classes. However,\nthese models struggle with unknown classes common in real-world scenarios. In\nthis paper, we introduce a challenging Open-set Video-based Facial Expression\nRecognition (OV-FER) task, aiming to identify both known and new, unseen facial\nexpressions. While existing approaches use large-scale vision-language models\nlike CLIP to identify unseen classes, we argue that these methods may not\nadequately capture the subtle human expressions needed for OV-FER. To address\nthis limitation, we propose a novel Human Expression-Sensitive Prompting (HESP)\nmechanism to significantly enhance CLIP's ability to model video-based facial\nexpression details effectively. Our proposed HESP comprises three components:\n1) a textual prompting module with learnable prompts to enhance CLIP's textual\nrepresentation of both known and unknown emotions, 2) a visual prompting module\nthat encodes temporal emotional information from video frames using\nexpression-sensitive attention, equipping CLIP with a new visual modeling\nability to extract emotion-rich information, and 3) an open-set multi-task\nlearning scheme that promotes interaction between the textual and visual\nmodules, improving the understanding of novel human emotions in video\nsequences. Extensive experiments conducted on four OV-FER task settings\ndemonstrate that HESP can significantly boost CLIP's performance (a relative\nimprovement of 17.93% on AUROC and 106.18% on OSCR) and outperform other\nstate-of-the-art open-set video understanding methods by a large margin. Code\nis available at https://github.com/cosinehuang/HESP.\n","authors":["Yuanyuan Liu","Yuxuan Huang","Shuyang Liu","Yibing Zhan","Zijing Chen","Zhe Chen"],"pdf_url":"https://arxiv.org/pdf/2404.17100v2.pdf","comment":"Accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2210.03437v2","updated":"2024-08-01T06:02:27Z","published":"2022-10-07T10:13:30Z","title":"KRF: Keypoint Refinement with Fusion Network for 6D Pose Estimation","summary":"  Some robust point cloud registration approaches with controllable pose\nrefinement magnitude, such as ICP and its variants, are commonly used to\nimprove 6D pose estimation accuracy. However, the effectiveness of these\nmethods gradually diminishes with the advancement of deep learning techniques\nand the enhancement of initial pose accuracy, primarily due to their lack of\nspecific design for pose refinement. In this paper, we propose Point Cloud\nCompletion and Keypoint Refinement with Fusion Data (PCKRF), a new pose\nrefinement pipeline for 6D pose estimation. The pipeline consists of two steps.\nFirst, it completes the input point clouds via a novel pose-sensitive point\ncompletion network. The network uses both local and global features with pose\ninformation during point completion. Then, it registers the completed object\npoint cloud with the corresponding target point cloud by our proposed Color\nsupported Iterative KeyPoint (CIKP) method. The CIKP method introduces color\ninformation into registration and registers a point cloud around each keypoint\nto increase stability. The PCKRF pipeline can be integrated with existing\npopular 6D pose estimation methods, such as the full flow bidirectional fusion\nnetwork, to further improve their pose estimation accuracy. Experiments\ndemonstrate that our method exhibits superior stability compared to existing\napproaches when optimizing initial poses with relatively high precision.\nNotably, the results indicate that our method effectively complements most\nexisting pose estimation techniques, leading to improved performance in most\ncases. Furthermore, our method achieves promising results even in challenging\nscenarios involving textureless and symmetrical objects. Our source code is\navailable at https://github.com/zhanhz/KRF.\n","authors":["Yiheng Han","Irvin Haozhe Zhan","Long Zeng","Yu-Ping Wang","Ran Yi","Minjing Yu","Matthieu Gaetan Lin","Jenny Sheng","Yong-Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2210.03437v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10224v3","updated":"2024-08-01T05:18:48Z","published":"2024-01-18T18:59:09Z","title":"The Manga Whisperer: Automatically Generating Transcriptions for Comics","summary":"  In the past few decades, Japanese comics, commonly referred to as Manga, have\ntranscended both cultural and linguistic boundaries to become a true worldwide\nsensation. Yet, the inherent reliance on visual cues and illustration within\nmanga renders it largely inaccessible to individuals with visual impairments.\nIn this work, we seek to address this substantial barrier, with the aim of\nensuring that manga can be appreciated and actively engaged by everyone.\nSpecifically, we tackle the problem of diarisation i.e. generating a\ntranscription of who said what and when, in a fully automatic way.\n  To this end, we make the following contributions: (1) we present a unified\nmodel, Magi, that is able to (a) detect panels, text boxes and character boxes,\n(b) cluster characters by identity (without knowing the number of clusters\napriori), and (c) associate dialogues to their speakers; (2) we propose a novel\napproach that is able to sort the detected text boxes in their reading order\nand generate a dialogue transcript; (3) we annotate an evaluation benchmark for\nthis task using publicly available [English] manga pages. The code, evaluation\ndatasets and the pre-trained model can be found at:\nhttps://github.com/ragavsachdeva/magi.\n","authors":["Ragav Sachdeva","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2401.10224v3.pdf","comment":"Accepted at CVPR'24"},{"id":"http://arxiv.org/abs/2303.08046v2","updated":"2024-08-01T05:15:43Z","published":"2023-03-07T09:20:39Z","title":"Ultra-High-Resolution Detector Simulation with Intra-Event Aware GAN and\n  Self-Supervised Relational Reasoning","summary":"  Simulating high-resolution detector responses is a computationally intensive\nprocess that has long been challenging in Particle Physics. Despite the ability\nof generative models to streamline it, full ultra-high-granularity detector\nsimulation still proves to be difficult as it contains correlated and\nfine-grained information. To overcome these limitations, we propose Intra-Event\nAware Generative Adversarial Network (IEA-GAN). IEA-GAN presents a Relational\nReasoning Module that approximates an event in detector simulation, generating\ncontextualized high-resolution full detector responses with a proper relational\ninductive bias. IEA-GAN also introduces a Self-Supervised intra-event aware\nloss and Uniformity loss, significantly enhancing sample fidelity and\ndiversity. We demonstrate IEA-GAN's application in generating sensor-dependent\nimages for the ultra-high-granularity Pixel Vertex Detector (PXD), with more\nthan 7.5 M information channels at the Belle II Experiment. Applications of\nthis work span from Foundation Models for high-granularity detector simulation,\nsuch as at the HL-LHC (High Luminosity LHC), to simulation-based inference and\nfine-grained density estimation. To our knowledge, IEA-GAN is the first\nalgorithm for faithful ultra-high-granularity full detector simulation with\nevent-based reasoning.\n","authors":["Baran Hashemi","Nikolai Hartmann","Sahand Sharifzadeh","James Kahn","Thomas Kuhr"],"pdf_url":"https://arxiv.org/pdf/2303.08046v2.pdf","comment":"Published at Nature Communications"},{"id":"http://arxiv.org/abs/2406.13642v5","updated":"2024-08-01T04:46:58Z","published":"2024-06-19T15:41:30Z","title":"SpatialBot: Precise Spatial Understanding with Vision Language Models","summary":"  Vision Language Models (VLMs) have achieved impressive performance in 2D\nimage understanding, however they are still struggling with spatial\nunderstanding which is the foundation of Embodied AI. In this paper, we propose\nSpatialBot for better spatial understanding by feeding both RGB and depth\nimages. Additionally, we have constructed the SpatialQA dataset, which involves\nmulti-level depth-related questions to train VLMs for depth understanding.\nFinally, we present SpatialBench to comprehensively evaluate VLMs' capabilities\nin spatial understanding at different levels. Extensive experiments on our\nspatial-understanding benchmark, general VLM benchmarks and Embodied AI tasks,\ndemonstrate the remarkable improvements of SpatialBot trained on SpatialQA. The\nmodel, code and data are available at https://github.com/BAAI-DCAI/SpatialBot.\n","authors":["Wenxiao Cai","Yaroslav Ponomarenko","Jianhao Yuan","Xiaoqi Li","Wankou Yang","Hao Dong","Bo Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.13642v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19394v2","updated":"2024-08-01T04:22:29Z","published":"2024-07-28T04:23:40Z","title":"Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets","summary":"  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n","authors":["Tianxiao Zhang","Wenju Xu","Bo Luo","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20756v2","updated":"2024-08-01T04:01:39Z","published":"2024-07-30T11:57:40Z","title":"SynthVLM: High-Efficiency and High-Quality Synthetic Data for Vision\n  Language Models","summary":"  Recently, with the rise of web images, managing and understanding large-scale\nimage datasets has become increasingly important. Vision Large Language Models\n(VLLMs) have recently emerged due to their robust vision-understanding\ncapabilities. However, training these models requires vast amounts of data,\nposing challenges to efficiency, effectiveness, data quality, and privacy. In\nthis paper, we introduce SynthVLM, a novel data synthesis pipeline for VLLMs.\nUnlike existing methods that generate captions from images, SynthVLM employs\nadvanced diffusion models and high-quality captions to automatically generate\nand select high-resolution images from captions, creating precisely aligned\nimage-text pairs. Leveraging these pairs, we achieve state-of-the-art (SoTA)\nperformance on various vision question answering tasks, maintaining high\nalignment quality and preserving advanced language abilities. Moreover,\nSynthVLM surpasses traditional GPT-4 Vision-based caption generation methods in\nperformance while significantly reducing computational overhead. Crucially, our\nmethod's reliance on purely generated data ensures the preservation of privacy,\nachieving SoTA performance with just 100k data points (only 18% of the official\ndataset size).\n","authors":["Zheng Liu","Hao Liang","Xijie Huang","Wentao Xiong","Qinhan Yu","Linzhuang Sun","Chong Chen","Conghui He","Bin Cui","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.20756v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09786v4","updated":"2024-08-01T03:57:24Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v4.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2407.21740v2","updated":"2024-08-01T03:16:43Z","published":"2024-07-31T16:52:00Z","title":"Contrastive Factor Analysis","summary":"  Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.\n","authors":["Zhibin Duan","Tiansheng Wen","Yifei Wang","Chen Zhu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.21740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13211v2","updated":"2024-08-01T03:07:49Z","published":"2024-07-18T06:50:39Z","title":"Research on Image Super-Resolution Reconstruction Mechanism based on\n  Convolutional Neural Network","summary":"  Super-resolution reconstruction techniques entail the utilization of software\nalgorithms to transform one or more sets of low-resolution images captured from\nthe same scene into high-resolution images. In recent years, considerable\nadvancement has been observed in the domain of single-image super-resolution\nalgorithms, particularly those based on deep learning techniques. Nevertheless,\nthe extraction of image features and nonlinear mapping methods in the\nreconstruction process remain challenging for existing algorithms. These issues\nresult in the network architecture being unable to effectively utilize the\ndiverse range of information at different levels. The loss of high-frequency\ndetails is significant, and the final reconstructed image features are overly\nsmooth, with a lack of fine texture details. This negatively impacts the\nsubjective visual quality of the image. The objective is to recover\nhigh-quality, high-resolution images from low-resolution images. In this work,\nan enhanced deep convolutional neural network model is employed, comprising\nmultiple convolutional layers, each of which is configured with specific\nfilters and activation functions to effectively capture the diverse features of\nthe image. Furthermore, a residual learning strategy is employed to accelerate\ntraining and enhance the convergence of the network, while sub-pixel\nconvolutional layers are utilized to refine the high-frequency details and\ntextures of the image. The experimental analysis demonstrates the superior\nperformance of the proposed model on multiple public datasets when compared\nwith the traditional bicubic interpolation method and several other\nlearning-based super-resolution methods. Furthermore, it proves the model's\nefficacy in maintaining image edges and textures.\n","authors":["Hao Yan","Zixiang Wang","Zhengjia Xu","Zhuoyue Wang","Zhizhong Wu","Ranran Lyu"],"pdf_url":"https://arxiv.org/pdf/2407.13211v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11816v2","updated":"2024-08-01T02:49:00Z","published":"2023-12-19T03:15:50Z","title":"A Dual-way Enhanced Framework from Text Matching Point of View for\n  Multimodal Entity Linking","summary":"  Multimodal Entity Linking (MEL) aims at linking ambiguous mentions with\nmultimodal information to entity in Knowledge Graph (KG) such as Wikipedia,\nwhich plays a key role in many applications. However, existing methods suffer\nfrom shortcomings, including modality impurity such as noise in raw image and\nambiguous textual entity representation, which puts obstacles to MEL. We\nformulate multimodal entity linking as a neural text matching problem where\neach multimodal information (text and image) is treated as a query, and the\nmodel learns the mapping from each query to the relevant entity from candidate\nentities. This paper introduces a dual-way enhanced (DWE) framework for MEL:\n(1) our model refines queries with multimodal data and addresses semantic gaps\nusing cross-modal enhancers between text and image information. Besides, DWE\ninnovatively leverages fine-grained image attributes, including facial\ncharacteristic and scene feature, to enhance and refine visual features. (2)By\nusing Wikipedia descriptions, DWE enriches entity semantics and obtains more\ncomprehensive textual representation, which reduces between textual\nrepresentation and the entities in KG. Extensive experiments on three public\nbenchmarks demonstrate that our method achieves state-of-the-art (SOTA)\nperformance, indicating the superiority of our model. The code is released on\nhttps://github.com/season1blue/DWE\n","authors":["Shezheng Song","Shan Zhao","Chengyu Wang","Tianwei Yan","Shasha Li","Xiaoguang Mao","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2312.11816v2.pdf","comment":"AAAI23 Accept"},{"id":"http://arxiv.org/abs/2407.14055v2","updated":"2024-08-01T02:19:08Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v2.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding. Author affiliation updated\n  for v2. Acknowledgements and funding information added for v2"},{"id":"http://arxiv.org/abs/2407.21266v2","updated":"2024-08-01T01:59:58Z","published":"2024-07-31T01:07:21Z","title":"DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image\n  Segmentation on Multiple GPUs","summary":"  The segmentation of ultra-high resolution images poses challenges such as\nloss of spatial information or computational inefficiency. In this work, a\nnovel approach that combines encoder-decoder architectures with domain\ndecomposition strategies to address these challenges is proposed. Specifically,\na domain decomposition-based U-Net (DDU-Net) architecture is introduced, which\npartitions input images into non-overlapping patches that can be processed\nindependently on separate devices. A communication network is added to\nfacilitate inter-patch information exchange to enhance the understanding of\nspatial context. Experimental validation is performed on a synthetic dataset\nthat is designed to measure the effectiveness of the communication network.\nThen, the performance is tested on the DeepGlobe land cover classification\ndataset as a real-world benchmark data set. The results demonstrate that the\napproach, which includes inter-patch communication for images divided into\n$16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher\nintersection over union (IoU) score compared to the same network without\ninter-patch communication. The performance of the network which includes\ncommunication is equivalent to that of a baseline U-Net trained on the full\nimage, showing that our model provides an effective solution for segmenting\nultra-high-resolution images while preserving spatial context. The code is\navailable at https://github.com/corne00/HiRes-Seg-CNN.\n","authors":["Corn Verburg","Alexander Heinlein","Eric C. Cyr"],"pdf_url":"https://arxiv.org/pdf/2407.21266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21263v2","updated":"2024-08-01T01:59:39Z","published":"2024-07-31T00:56:06Z","title":"Outlier Detection in Large Radiological Datasets using UMAP","summary":"  The success of machine learning algorithms heavily relies on the quality of\nsamples and the accuracy of their corresponding labels. However, building and\nmaintaining large, high-quality datasets is an enormous task. This is\nespecially true for biomedical data and for meta-sets that are compiled from\nsmaller ones, as variations in image quality, labeling, reports, and archiving\ncan lead to errors, inconsistencies, and repeated samples. Here, we show that\nthe uniform manifold approximation and projection (UMAP) algorithm can find\nthese anomalies essentially by forming independent clusters that are distinct\nfrom the main (good) data but similar to other points with the same error type.\nAs a representative example, we apply UMAP to discover outliers in the publicly\navailable ChestX-ray14, CheXpert, and MURA datasets. While the results are\narchival and retrospective and focus on radiological images, the graph-based\nmethods work for any data type and will prove equally beneficial for curation\nat the time of dataset creation.\n","authors":["Mohammad Tariqul Islam","Jason W. Fleischer"],"pdf_url":"https://arxiv.org/pdf/2407.21263v2.pdf","comment":"Accepted in MICCAI-2024 Workshop on Topology- and Graph-Informed\n  Imaging Informatics (TGI3)"},{"id":"http://arxiv.org/abs/2408.00766v1","updated":"2024-08-01T17:59:59Z","published":"2024-08-01T17:59:59Z","title":"Optimizing Diffusion Models for Joint Trajectory Prediction and\n  Controllable Generation","summary":"  Diffusion models are promising for joint trajectory prediction and\ncontrollable generation in autonomous driving, but they face challenges of\ninefficient inference steps and high computational demands. To tackle these\nchallenges, we introduce Optimal Gaussian Diffusion (OGD) and Estimated Clean\nManifold (ECM) Guidance. OGD optimizes the prior distribution for a small\ndiffusion time $T$ and starts the reverse diffusion process from it. ECM\ndirectly injects guidance gradients to the estimated clean manifold,\neliminating extensive gradient backpropagation throughout the network. Our\nmethodology streamlines the generative process, enabling practical applications\nwith reduced computational overhead. Experimental validation on the large-scale\nArgoverse 2 dataset demonstrates our approach's superior performance, offering\na viable solution for computationally efficient, high-quality joint trajectory\nprediction and controllable generation for autonomous driving. Our project\nwebpage is at https://yixiaowang7.github.io/OptTrajDiff_Page/.\n","authors":["Yixiao Wang","Chen Tang","Lingfeng Sun","Simone Rossi","Yichen Xie","Chensheng Peng","Thomas Hannagan","Stefano Sabatini","Nicola Poerio","Masayoshi Tomizuka","Wei Zhan"],"pdf_url":"https://arxiv.org/pdf/2408.00766v1.pdf","comment":"30 pages, 20 figures, Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00765v1","updated":"2024-08-01T17:59:54Z","published":"2024-08-01T17:59:54Z","title":"MM-Vet v2: A Challenging Benchmark to Evaluate Large Multimodal Models\n  for Integrated Capabilities","summary":"  MM-Vet, with open-ended vision-language questions targeting at evaluating\nintegrated capabilities, has become one of the most popular benchmarks for\nlarge multimodal model evaluation. MM-Vet assesses six core vision-language\n(VL) capabilities: recognition, knowledge, spatial awareness, language\ngeneration, OCR, and math. However, its question format is restricted to single\nimage-text pairs, lacking the interleaved image and text sequences prevalent in\nreal-world scenarios. To address this limitation, we introduce MM-Vet v2, which\nincludes a new VL capability called \"image-text sequence understanding\",\nevaluating models' ability to process VL sequences. Furthermore, we maintain\nthe high quality of evaluation samples while further expanding the evaluation\nset size. Using MM-Vet v2 to benchmark large multimodal models, we found that\nClaude 3.5 Sonnet is the best model with a score of 71.8, slightly\noutperforming GPT-4o which scored 71.0. Among open-weight models,\nInternVL2-Llama3-76B leads with a score of 68.4.\n","authors":["Weihao Yu","Zhengyuan Yang","Linfeng Ren","Linjie Li","Jianfeng Wang","Kevin Lin","Chung-Ching Lin","Zicheng Liu","Lijuan Wang","Xinchao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00765v1.pdf","comment":"Extension of MM-Vet: arXiv:2308.02490"},{"id":"http://arxiv.org/abs/2408.00762v1","updated":"2024-08-01T17:59:27Z","published":"2024-08-01T17:59:27Z","title":"UniTalker: Scaling up Audio-Driven 3D Facial Animation through A Unified\n  Model","summary":"  Audio-driven 3D facial animation aims to map input audio to realistic facial\nmotion. Despite significant progress, limitations arise from inconsistent 3D\nannotations, restricting previous models to training on specific annotations\nand thereby constraining the training scale. In this work, we present\nUniTalker, a unified model featuring a multi-head architecture designed to\neffectively leverage datasets with varied annotations. To enhance training\nstability and ensure consistency among multi-head outputs, we employ three\ntraining strategies, namely, PCA, model warm-up, and pivot identity embedding.\nTo expand the training scale and diversity, we assemble A2F-Bench, comprising\nfive publicly available datasets and three newly curated datasets. These\ndatasets contain a wide range of audio domains, covering multilingual speech\nvoices and songs, thereby scaling the training data from commonly employed\ndatasets, typically less than 1 hour, to 18.5 hours. With a single trained\nUniTalker model, we achieve substantial lip vertex error reductions of 9.2% for\nBIWI dataset and 13.7% for Vocaset. Additionally, the pre-trained UniTalker\nexhibits promise as the foundation model for audio-driven facial animation\ntasks. Fine-tuning the pre-trained UniTalker on seen datasets further enhances\nperformance on each dataset, with an average error reduction of 6.3% on\nA2F-Bench. Moreover, fine-tuning UniTalker on an unseen dataset with only half\nthe data surpasses prior state-of-the-art models trained on the full dataset.\nThe code and dataset are available at the project page\nhttps://github.com/X-niper/UniTalker.\n","authors":["Xiangyu Fan","Jiaqi Li","Zhiqian Lin","Weiye Xiao","Lei Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00760v1","updated":"2024-08-01T17:59:09Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00759v1","updated":"2024-08-01T17:58:19Z","published":"2024-08-01T17:58:19Z","title":"Text-Guided Video Masked Autoencoder","summary":"  Recent video masked autoencoder (MAE) works have designed improved masking\nalgorithms focused on saliency. These works leverage visual cues such as motion\nto mask the most salient regions. However, the robustness of such visual cues\ndepends on how often input videos match underlying assumptions. On the other\nhand, natural language description is an information dense representation of\nvideo that implicitly captures saliency without requiring modality-specific\nassumptions, and has not been explored yet for video MAE. To this end, we\nintroduce a novel text-guided masking algorithm (TGM) that masks the video\nregions with highest correspondence to paired captions. Without leveraging any\nexplicit visual cues for saliency, our TGM is competitive with state-of-the-art\nmasking algorithms such as motion-guided masking. To further benefit from the\nsemantics of natural language for masked reconstruction, we next introduce a\nunified framework for joint MAE and masked video-text contrastive learning. We\nshow that across existing masking algorithms, unifying MAE and masked\nvideo-text contrastive learning improves downstream performance compared to\npure MAE on a variety of video recognition tasks, especially for linear probe.\nWithin this unified framework, our TGM achieves the best relative performance\non five action recognition and one egocentric datasets, highlighting the\ncomplementary nature of natural language for masked video modeling.\n","authors":["David Fan","Jue Wang","Shuai Liao","Zhikang Zhang","Vimal Bhat","Xinyu Li"],"pdf_url":"https://arxiv.org/pdf/2408.00759v1.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00756v1","updated":"2024-08-01T17:57:25Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment a variety of objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we provide an extensive evaluation of SAM\n2's ability to segment both 2D and 3D medical images. We collect 18 medical\nimaging datasets, including common 3D modalities such as computed tomography\n(CT), magnetic resonance imaging (MRI), and positron emission tomography (PET)\nas well as 2D modalities such as X-ray and ultrasound. We consider two\nevaluation pipelines of SAM 2: (1) multi-frame 3D segmentation, where prompts\nare provided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. We learn that SAM 2 exhibits similar performance as SAM\nunder single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v1.pdf","comment":"11 pages, 7 figures. A first attempt on evaluating SAM 2 on medical\n  images"},{"id":"http://arxiv.org/abs/2408.00754v1","updated":"2024-08-01T17:57:12Z","published":"2024-08-01T17:57:12Z","title":"Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal\n  Language Model","summary":"  Multimodal language models (MLLMs) are increasingly being implemented in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Despite their potential, current top models\nwithin our community still fall short in adequately understanding spatial and\ntemporal dimensions. We introduce Coarse Correspondence, a simple,\ntraining-free, effective, and general-purpose visual prompting method to elicit\n3D and temporal understanding in multimodal LLMs. Our method uses a lightweight\ntracking model to find object correspondences between frames in a video or\nbetween sets of image viewpoints. It selects the most frequent object instances\nand visualizes them with markers with unique IDs in the image. With this simple\napproach, we achieve state-of-the-art results on 3D understanding benchmarks\nincluding ScanQA (+20.5\\%) and a subset of OpenEQA (+9.7\\%), and on long-form\nvideo benchmarks such as EgoSchema (+6.0\\%). We also curate a small diagnostic\ndataset to evaluate whether MLLMs can reason about space from a described\nviewpoint other than the camera viewpoint. Again, Coarse Correspondence\nimproves spatial perspective-taking abilities but we highlight that MLLMs\nstruggle with this task. Together, we demonstrate that our simple prompting\nmethod can significantly aid downstream tasks that require 3D or temporal\nreasoning.\n","authors":["Benlin Liu","Yuhao Dong","Yiqin Wang","Yongming Rao","Yansong Tang","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2408.00754v1.pdf","comment":"project page: https://coarse-correspondence.github.io"},{"id":"http://arxiv.org/abs/2408.00749v1","updated":"2024-08-01T17:52:10Z","published":"2024-08-01T17:52:10Z","title":"Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer","summary":"  Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Trevor Rife"],"pdf_url":"https://arxiv.org/pdf/2408.00749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00744v1","updated":"2024-08-01T17:48:08Z","published":"2024-08-01T17:48:08Z","title":"Collaborative Vision-Text Representation Optimizing for Open-Vocabulary\n  Segmentation","summary":"  Pre-trained vision-language models, e.g. CLIP, have been increasingly used to\naddress the challenging Open-Vocabulary Segmentation (OVS) task, benefiting\nfrom their well-aligned vision-text embedding space. Typical solutions involve\neither freezing CLIP during training to unilaterally maintain its zero-shot\ncapability, or fine-tuning CLIP vision encoder to achieve perceptual\nsensitivity to local regions. However, few of them incorporate vision-text\ncollaborative optimization. Based on this, we propose the Content-Dependent\nTransfer to adaptively enhance each text embedding by interacting with the\ninput image, which presents a parameter-efficient way to optimize the text\nrepresentation. Besides, we additionally introduce a Representation\nCompensation strategy, reviewing the original CLIP-V representation as\ncompensation to maintain the zero-shot capability of CLIP. In this way, the\nvision and text representation of CLIP are optimized collaboratively, enhancing\nthe alignment of the vision-text feature space. To the best of our knowledge,\nwe are the first to establish the collaborative vision-text optimizing\nmechanism within the OVS field. Extensive experiments demonstrate our method\nachieves superior performance on popular OVS benchmarks. In open-vocabulary\nsemantic segmentation, our method outperforms the previous state-of-the-art\napproaches by +0.5, +2.3, +3.4, +0.4 and +1.1 mIoU, respectively on A-847,\nA-150, PC-459, PC-59 and PAS-20. Furthermore, in a panoptic setting on ADE20K,\nwe achieve the performance of 27.1 PQ, 73.5 SQ, and 32.9 RQ. Code will be\navailable at https://github.com/jiaosiyu1999/MAFT-Plus.git .\n","authors":["Siyu Jiao","Hongguang Zhu","Jiannan Huang","Yao Zhao","Yunchao Wei","Humphrey Shi"],"pdf_url":"https://arxiv.org/pdf/2408.00744v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00738v1","updated":"2024-08-01T17:35:58Z","published":"2024-08-01T17:35:58Z","title":"Virchow 2: Scaling Self-Supervised Mixed Magnification Models in\n  Pathology","summary":"  Foundation models are rapidly being developed for computational pathology\napplications. However, it remains an open question which factors are most\nimportant for downstream performance with data scale and diversity, model size,\nand training algorithm all playing a role. In this work, we present the result\nof scaling both data and model size, surpassing previous studies in both\ndimensions, and introduce two new models: Virchow 2, a 632M parameter vision\ntransformer, and Virchow 2G, a 1.85B parameter vision transformer, each trained\nwith 3.1M histopathology whole slide images. To support this scale, we propose\ndomain-inspired adaptations to the DINOv2 training algorithm, which is quickly\nbecoming the default method in self-supervised learning for computational\npathology. We achieve state of the art performance on twelve tile-level tasks,\nas compared to the top performing competing models. Our results suggest that\ndata diversity and domain-specific training can outperform models that only\nscale in the number of parameters, but, on average, performance benefits from\ndomain-tailoring, data scale, and model scale.\n","authors":["Eric Zimmermann","Eugene Vorontsov","Julian Viret","Adam Casson","Michal Zelechowski","George Shaikovski","Neil Tenenholtz","James Hall","Thomas Fuchs","Nicolo Fusi","Siqi Liu","Kristen Severson"],"pdf_url":"https://arxiv.org/pdf/2408.00738v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00735v1","updated":"2024-08-01T17:27:28Z","published":"2024-08-01T17:27:28Z","title":"TurboEdit: Text-Based Image Editing Using Few-Step Diffusion Models","summary":"  Diffusion models have opened the path to a wide range of text-based image\nediting frameworks. However, these typically build on the multi-step nature of\nthe diffusion backwards process, and adapting them to distilled, fast-sampling\nmethods has proven surprisingly challenging. Here, we focus on a popular line\nof text-based editing frameworks - the ``edit-friendly'' DDPM-noise inversion\napproach. We analyze its application to fast sampling methods and categorize\nits failures into two classes: the appearance of visual artifacts, and\ninsufficient editing strength. We trace the artifacts to mismatched noise\nstatistics between inverted noises and the expected noise schedule, and suggest\na shifted noise schedule which corrects for this offset. To increase editing\nstrength, we propose a pseudo-guidance approach that efficiently increases the\nmagnitude of edits without introducing new artifacts. All in all, our method\nenables text-based image editing with as few as three diffusion steps, while\nproviding novel insights into the mechanisms behind popular text-based editing\napproaches.\n","authors":["Gilad Deutch","Rinon Gal","Daniel Garibi","Or Patashnik","Daniel Cohen-Or"],"pdf_url":"https://arxiv.org/pdf/2408.00735v1.pdf","comment":"Project page: https://turboedit-paper.github.io/"},{"id":"http://arxiv.org/abs/2408.00714v1","updated":"2024-08-01T17:00:08Z","published":"2024-08-01T17:00:08Z","title":"SAM 2: Segment Anything in Images and Videos","summary":"  We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing a version of our model, the dataset\nand an interactive demo.\n","authors":["Nikhila Ravi","Valentin Gabeur","Yuan-Ting Hu","Ronghang Hu","Chaitanya Ryali","Tengyu Ma","Haitham Khedr","Roman Rdle","Chloe Rolland","Laura Gustafson","Eric Mintun","Junting Pan","Kalyan Vasudev Alwala","Nicolas Carion","Chao-Yuan Wu","Ross Girshick","Piotr Dollr","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2408.00714v1.pdf","comment":"Website: https://ai.meta.com/sam2"},{"id":"http://arxiv.org/abs/2408.00712v1","updated":"2024-08-01T16:58:50Z","published":"2024-08-01T16:58:50Z","title":"MotionFix: Text-Driven 3D Human Motion Editing","summary":"  The focus of this paper is 3D motion editing. Given a 3D human motion and a\ntextual description of the desired modification, our goal is to generate an\nedited motion as described by the text. The challenges include the lack of\ntraining data and the design of a model that faithfully edits the source\nmotion. In this paper, we address both these challenges. We build a methodology\nto semi-automatically collect a dataset of triplets in the form of (i) a source\nmotion, (ii) a target motion, and (iii) an edit text, and create the new\nMotionFix dataset. Having access to such data allows us to train a conditional\ndiffusion model, TMED, that takes both the source motion and the edit text as\ninput. We further build various baselines trained only on text-motion pairs\ndatasets, and show superior performance of our model trained on triplets. We\nintroduce new retrieval-based metrics for motion editing and establish a new\nbenchmark on the evaluation set of MotionFix. Our results are encouraging,\npaving the way for further research on finegrained motion generation. Code and\nmodels will be made publicly available.\n","authors":["Nikos Athanasiou","Alpr Ceske","Markos Diomataris","Michael J. Black","Gl Varol"],"pdf_url":"https://arxiv.org/pdf/2408.00712v1.pdf","comment":"arXiv v1"},{"id":"http://arxiv.org/abs/2408.00707v1","updated":"2024-08-01T16:54:11Z","published":"2024-08-01T16:54:11Z","title":"Synthetic dual image generation for reduction of labeling efforts in\n  semantic segmentation of micrographs with a customized metric function","summary":"  Training of semantic segmentation models for material analysis requires\nmicrographs and their corresponding masks. It is quite unlikely that perfect\nmasks will be drawn, especially at the edges of objects, and sometimes the\namount of data that can be obtained is small, since only a few samples are\navailable. These aspects make it very problematic to train a robust model. We\ndemonstrate a workflow for the improvement of semantic segmentation models of\nmicrographs through the generation of synthetic microstructural images in\nconjunction with masks. The workflow only requires joining a few micrographs\nwith their respective masks to create the input for a Vector\nQuantised-Variational AutoEncoder model that includes an embedding space, which\nis trained such that a generative model (PixelCNN) learns the distribution of\neach input, transformed into discrete codes, and can be used to sample new\ncodes. The latter will eventually be decoded by VQ-VAE to generate images\nalongside corresponding masks for semantic segmentation. To evaluate the\nsynthetic data, we have trained U-Net models with different amounts of these\nsynthetic data in conjunction with real data. These models were then evaluated\nusing non-synthetic images only. Additionally, we introduce a customized metric\nderived from the mean Intersection over Union (mIoU). The proposed metric\nprevents a few falsely predicted pixels from greatly reducing the value of the\nmIoU. We have achieved a reduction in sample preparation and acquisition times,\nas well as the efforts, needed for image processing and labeling tasks, are\nless when it comes to training semantic segmentation model. The approach could\nbe generalized to various types of image data such that it serves as a\nuser-friendly solution for training models with a small number of real images.\n","authors":["Matias Oscar Volman Stern","Dominic Hohs","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.00707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00706v1","updated":"2024-08-01T16:52:39Z","published":"2024-08-01T16:52:39Z","title":"Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM","summary":"  Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.\n","authors":["Xiaofeng Liu","Jonghye Woo","Chao Ma","Jinsong Ouyang","Georges El Fakhri"],"pdf_url":"https://arxiv.org/pdf/2408.00706v1.pdf","comment":"2024 IEEE Nuclear Science Symposium and Medical Imaging Conference"},{"id":"http://arxiv.org/abs/2408.00701v1","updated":"2024-08-01T16:48:03Z","published":"2024-08-01T16:48:03Z","title":"Joint Neural Networks for One-shot Object Recognition and Detection","summary":"  This paper presents a novel joint neural networks approach to address the\nchallenging one-shot object recognition and detection tasks. Inspired by\nSiamese neural networks and state-of-art multi-box detection approaches, the\njoint neural networks are able to perform object recognition and detection for\ncategories that remain unseen during the training process. Following the\none-shot object recognition/detection constraints, the training and testing\ndatasets do not contain overlapped classes, in other words, all the test\nclasses remain unseen during training. The joint networks architecture is able\nto effectively compare pairs of images via stacked convolutional layers of the\nquery and target inputs, recognising patterns of the same input query category\nwithout relying on previous training around this category. The proposed\napproach achieves 61.41% accuracy for one-shot object recognition on the\nMiniImageNet dataset and 47.1% mAP for one-shot object detection when trained\non the COCO dataset and tested using the Pascal VOC dataset. Code available at\nhttps://github.com/cjvargasc/JNN recog and https://github.com/cjvargasc/JNN\ndetection/\n","authors":["Camilo J. Vargas","Qianni Zhang","Ebroul Izquierdo"],"pdf_url":"https://arxiv.org/pdf/2408.00701v1.pdf","comment":"published as part of the PhD thesis:\n  https://qmro.qmul.ac.uk/xmlui/handle/123456789/72758"},{"id":"http://arxiv.org/abs/2408.00677v1","updated":"2024-08-01T16:20:02Z","published":"2024-08-01T16:20:02Z","title":"Scaling Backwards: Minimal Synthetic Pre-training?","summary":"  Pre-training and transfer learning are an important building block of current\ncomputer vision systems. While pre-training is usually performed on large\nreal-world image datasets, in this paper we ask whether this is truly\nnecessary. To this end, we search for a minimal, purely synthetic pre-training\ndataset that allows us to achieve performance similar to the 1 million images\nof ImageNet-1k. We construct such a dataset from a single fractal with\nperturbations. With this, we contribute three main findings. (i) We show that\npre-training is effective even with minimal synthetic images, with performance\non par with large-scale pre-training datasets like ImageNet-1k for full\nfine-tuning. (ii) We investigate the single parameter with which we construct\nartificial categories for our dataset. We find that while the shape differences\ncan be indistinguishable to humans, they are crucial for obtaining strong\nperformances. (iii) Finally, we investigate the minimal requirements for\nsuccessful pre-training. Surprisingly, we find that a substantial reduction of\nsynthetic images from 1k to 1 can even lead to an increase in pre-training\nperformance, a motivation to further investigate ``scaling backwards''.\nFinally, we extend our method from synthetic images to real images to see if a\nsingle real image can show similar pre-training effect through shape\naugmentation. We find that the use of grayscale images and affine\ntransformations allows even real images to ``scale backwards''.\n","authors":["Ryo Nakamura","Ryu Tadokoro","Ryosuke Yamada","Yuki M. Asano","Iro Laina","Christian Rupprecht","Nakamasa Inoue","Rio Yokota","Hirokatsu Kataoka"],"pdf_url":"https://arxiv.org/pdf/2408.00677v1.pdf","comment":"Accepted to ECCV2024"},{"id":"http://arxiv.org/abs/2408.00672v1","updated":"2024-08-01T16:13:07Z","published":"2024-08-01T16:13:07Z","title":"ExpertAF: Expert Actionable Feedback from Video","summary":"  Feedback is essential for learning a new skill or improving one's current\nskill-level. However, current methods for skill-assessment from video only\nprovide scores or compare demonstrations, leaving the burden of knowing what to\ndo differently on the user. We introduce a novel method to generate actionable\nfeedback from video of a person doing a physical activity, such as basketball\nor soccer. Our method takes a video demonstration and its accompanying 3D body\npose and generates (1) free-form expert commentary describing what the person\nis doing well and what they could improve, and (2) a visual expert\ndemonstration that incorporates the required corrections. We show how to\nleverage Ego-Exo4D's videos of skilled activity and expert commentary together\nwith a strong language model to create a weakly-supervised training dataset for\nthis task, and we devise a multimodal video-language model to infer coaching\nfeedback. Our method is able to reason across multi-modal input combinations to\noutput full-spectrum, actionable coaching -- expert commentary, expert video\nretrieval, and the first-of-its-kind expert pose generation -- outperforming\nstrong vision-language models on both established metrics and human preference\nstudies.\n","authors":["Kumar Ashutosh","Tushar Nagarajan","Georgios Pavlakos","Kris Kitani","Kristen Grauman"],"pdf_url":"https://arxiv.org/pdf/2408.00672v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2408.00653v1","updated":"2024-08-01T15:41:57Z","published":"2024-08-01T15:41:57Z","title":"SF3D: Stable Fast 3D Mesh Reconstruction with UV-unwrapping and\n  Illumination Disentanglement","summary":"  We present SF3D, a novel method for rapid and high-quality textured object\nmesh reconstruction from a single image in just 0.5 seconds. Unlike most\nexisting approaches, SF3D is explicitly trained for mesh generation,\nincorporating a fast UV unwrapping technique that enables swift texture\ngeneration rather than relying on vertex colors. The method also learns to\npredict material parameters and normal maps to enhance the visual quality of\nthe reconstructed 3D meshes. Furthermore, SF3D integrates a delighting step to\neffectively remove low-frequency illumination effects, ensuring that the\nreconstructed meshes can be easily used in novel illumination conditions.\nExperiments demonstrate the superior performance of SF3D over the existing\ntechniques. Project page: https://stable-fast-3d.github.io\n","authors":["Mark Boss","Zixuan Huang","Aaryaman Vasishta","Varun Jampani"],"pdf_url":"https://arxiv.org/pdf/2408.00653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00644v1","updated":"2024-08-01T15:35:44Z","published":"2024-08-01T15:35:44Z","title":"Towards End-to-End Explainable Facial Action Unit Recognition via\n  Vision-Language Joint Learning","summary":"  Facial action units (AUs), as defined in the Facial Action Coding System\n(FACS), have received significant research interest owing to their diverse\nrange of applications in facial state analysis. Current mainstream FAU\nrecognition models have a notable limitation, i.e., focusing only on the\naccuracy of AU recognition and overlooking explanations of corresponding AU\nstates. In this paper, we propose an end-to-end Vision-Language joint learning\nnetwork for explainable FAU recognition (termed VL-FAU), which aims to\nreinforce AU representation capability and language interpretability through\nthe integration of joint multimodal tasks. Specifically, VL-FAU brings together\nlanguage models to generate fine-grained local muscle descriptions and\ndistinguishable global face description when optimising FAU recognition.\nThrough this, the global facial representation and its local AU representations\nwill achieve higher distinguishability among different AUs and different\nsubjects. In addition, multi-level AU representation learning is utilised to\nimprove AU individual attention-aware representation capabilities based on\nmulti-scale combined facial stem feature. Extensive experiments on DISFA and\nBP4D AU datasets show that the proposed approach achieves superior performance\nover the state-of-the-art methods on most of the metrics. In addition, compared\nwith mainstream FAU recognition methods, VL-FAU can provide local- and\nglobal-level interpretability language descriptions with the AUs' predictions.\n","authors":["Xuri Ge","Junchen Fu","Fuhai Chen","Shan An","Nicu Sebe","Joemon M. Jose"],"pdf_url":"https://arxiv.org/pdf/2408.00644v1.pdf","comment":"10 pages, 5 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.00640v1","updated":"2024-08-01T15:27:48Z","published":"2024-08-01T15:27:48Z","title":"AMAES: Augmented Masked Autoencoder Pretraining on Public Brain MRI Data\n  for 3D-Native Segmentation","summary":"  This study investigates the impact of self-supervised pretraining of 3D\nsemantic segmentation models on a large-scale, domain-specific dataset. We\nintroduce BRAINS-45K, a dataset of 44,756 brain MRI volumes from public\nsources, the largest public dataset available, and revisit a number of design\nchoices for pretraining modern segmentation architectures by simplifying and\noptimizing state-of-the-art methods, and combining them with a novel\naugmentation strategy. The resulting AMAES framework is based on\nmasked-image-modeling and intensity-based augmentation reversal and balances\nmemory usage, runtime, and finetuning performance. Using the popular U-Net and\nthe recent MedNeXt architecture as backbones, we evaluate the effect of\npretraining on three challenging downstream tasks, covering single-sequence,\nlow-resource settings, and out-of-domain generalization. The results highlight\nthat pretraining on the proposed dataset with AMAES significantly improves\nsegmentation performance in the majority of evaluated cases, and that it is\nbeneficial to pretrain the model with augmentations, despite pretraing on a\nlarge-scale dataset. Code and model checkpoints for reproducing results, as\nwell as the BRAINS-45K dataset are available at\n\\url{https://github.com/asbjrnmunk/amaes}.\n","authors":["Asbjrn Munk","Jakob Ambsdorf","Sebastian Llambias","Mads Nielsen"],"pdf_url":"https://arxiv.org/pdf/2408.00640v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00639v1","updated":"2024-08-01T15:26:24Z","published":"2024-08-01T15:26:24Z","title":"Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs","summary":"  Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .\n","authors":["Francesco Di Salvo","David Tafler","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.00639v1.pdf","comment":"Accepted at BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00636v1","updated":"2024-08-01T15:20:20Z","published":"2024-08-01T15:20:20Z","title":"Deep Learning in Medical Image Classification from MRI-based Brain Tumor\n  Images","summary":"  Brain tumors are among the deadliest diseases in the world. Magnetic\nResonance Imaging (MRI) is one of the most effective ways to detect brain\ntumors. Accurate detection of brain tumors based on MRI scans is critical, as\nit can potentially save many lives and facilitate better decision-making at the\nearly stages of the disease. Within our paper, four different types of\nMRI-based images have been collected from the database: glioma tumor, no tumor,\npituitary tumor, and meningioma tumor. Our study focuses on making predictions\nfor brain tumor classification. Five models, including four pre-trained models\n(MobileNet, EfficientNet-B0, ResNet-18, and VGG16) and one new model,\nMobileNet-BT, have been proposed for this study.\n","authors":["Xiaoyi Liu","Zhuoyue Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00629v1","updated":"2024-08-01T15:14:10Z","published":"2024-08-01T15:14:10Z","title":"Empowering Snapshot Compressive Imaging: Spatial-Spectral State Space\n  Model with Across-Scanning and Local Enhancement","summary":"  Snapshot Compressive Imaging (SCI) relies on decoding algorithms such as CNN\nor Transformer to reconstruct the hyperspectral image (HSI) from its compressed\nmeasurement. Although existing CNN and Transformer-based methods have proven\neffective, CNNs are limited by their inadequate modeling of long-range\ndependencies, while Transformer ones face high computational costs due to\nquadratic complexity. Recent Mamba models have demonstrated superior\nperformance over CNN and Transformer-based architectures in some visual tasks,\nbut these models have not fully utilized the local similarities in both spatial\nand spectral dimensions. Moreover, the long-sequence modeling capability of SSM\nmay offer an advantage in processing the numerous spectral bands for HSI\nreconstruction, which has not yet been explored. In this paper, we introduce a\nState Space Model with Across-Scanning and Local Enhancement, named ASLE-SSM,\nthat employs a Spatial-Spectral SSM for global-local balanced context encoding\nand cross-channel interaction promoting. Specifically, we introduce local\nscanning in the spatial dimension to balance the global and local receptive\nfields, and then propose our across-scanning method based on spatial-spectral\nlocal cubes to leverage local similarities between adjacent spectral bands and\npixels to guide the reconstruction process. These two scanning mechanisms\nextract the HSI's local features while balancing the global perspective without\nany additional costs. Experimental results illustrate ASLE-SSM's superiority\nover existing state-of-the-art methods, with an inference speed 2.4 times\nfaster than Transformer-based MST and saving 0.12 (M) of parameters, achieving\nthe lowest computational cost and parameter count.\n","authors":["Wenzhe Tian","Haijin Zeng","Yin-Ping Zhao","Yongyong Chen","Zhen Wang","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00629v1.pdf","comment":"12 pages,6 figures"},{"id":"http://arxiv.org/abs/2408.00624v1","updated":"2024-08-01T15:09:32Z","published":"2024-08-01T15:09:32Z","title":"SynesLM: A Unified Approach for Audio-visual Speech Recognition and\n  Translation via Language Model and Synthetic Data","summary":"  In this work, we present SynesLM, an unified model which can perform three\nmultimodal language understanding tasks: audio-visual automatic speech\nrecognition(AV-ASR) and visual-aided speech/machine translation(VST/VMT).\nUnlike previous research that focused on lip motion as visual cues for speech\nsignals, our work explores more general visual information within entire\nframes, such as objects and actions. Additionally, we use synthetic image data\nto enhance the correlation between image and speech data. We benchmark SynesLM\nagainst the How2 dataset, demonstrating performance on par with\nstate-of-the-art (SOTA) models dedicated to AV-ASR while maintaining our\nmultitasking framework. Remarkably, for zero-shot AV-ASR, SynesLM achieved SOTA\nperformance by lowering the Word Error Rate (WER) from 43.4% to 39.4% on the\nVisSpeech Dataset. Furthermore, our results in VST and VMT outperform the\nprevious results, improving the BLEU score to 43.5 from 37.2 for VST, and to\n54.8 from 54.4 for VMT.\n","authors":["Yichen Lu","Jiaqi Song","Xuankai Chang","Hengwei Bian","Soumi Maiti","Shinji Watanabe"],"pdf_url":"https://arxiv.org/pdf/2408.00624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00620v1","updated":"2024-08-01T15:05:42Z","published":"2024-08-01T15:05:42Z","title":"Are Bigger Encoders Always Better in Vision Large Models?","summary":"  In recent years, multimodal large language models (MLLMs) have shown strong\npotential in real-world applications. They are developing rapidly due to their\nremarkable ability to comprehend multimodal information and their inherent\npowerful cognitive and reasoning capabilities. Among MLLMs, vision language\nmodels (VLM) stand out for their ability to understand vision information.\nHowever, the scaling trend of VLMs under the current mainstream paradigm has\nnot been extensively studied. Whether we can achieve better performance by\ntraining even larger models is still unclear. To address this issue, we\nconducted experiments on the pretraining stage of MLLMs. We conduct our\nexperiment using different encoder sizes and large language model (LLM) sizes.\nOur findings indicate that merely increasing the size of encoders does not\nnecessarily enhance the performance of VLMs. Moreover, we analyzed the effects\nof LLM backbone parameter size and data quality on the pretraining outcomes.\nAdditionally, we explored the differences in scaling laws between LLMs and\nVLMs.\n","authors":["Bozhou Li","Hao Liang","Zimo Meng","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00620v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00619v1","updated":"2024-08-01T15:01:07Z","published":"2024-08-01T15:01:07Z","title":"Harnessing Uncertainty-aware Bounding Boxes for Unsupervised 3D Object\n  Detection","summary":"  Unsupervised 3D object detection aims to identify objects of interest from\nunlabeled raw data, such as LiDAR points. Recent approaches usually adopt\npseudo 3D bounding boxes (3D bboxes) from clustering algorithm to initialize\nthe model training, and then iteratively updating both pseudo labels and the\ntrained model. However, pseudo bboxes inevitably contain noises, and such\ninaccurate annotation accumulates to the final model, compromising the\nperformance. Therefore, in an attempt to mitigate the negative impact of pseudo\nbboxes, we introduce a new uncertainty-aware framework. In particular, Our\nmethod consists of two primary components: uncertainty estimation and\nuncertainty regularization. (1) In the uncertainty estimation phase, we\nincorporate an extra auxiliary detection branch alongside the primary detector.\nThe prediction disparity between the primary and auxiliary detectors is\nleveraged to estimate uncertainty at the box coordinate level, including\nposition, shape, orientation. (2) Based on the assessed uncertainty, we\nregularize the model training via adaptively adjusting every 3D bboxes\ncoordinates. For pseudo bbox coordinates with high uncertainty, we assign a\nrelatively low loss weight. Experiment verifies that the proposed method is\nrobust against the noisy pseudo bboxes, yielding substantial improvements on\nnuScenes and Lyft compared to existing techniques, with increases of 6.9% in\nAP$_{BEV}$ and 2.5% in AP$_{3D}$ on nuScenes, and 2.2% in AP$_{BEV}$ and 1.0%\nin AP$_{3D}$ on Lyft.\n","authors":["Ruiyang Zhang","Hu Zhang","Hang Yu","Zhedong Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.00619v1.pdf","comment":"Preprint, 14 pages, 4 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.00599v1","updated":"2024-08-01T14:31:06Z","published":"2024-08-01T14:31:06Z","title":"Learned Compression of Point Cloud Geometry and Attributes in a Single\n  Model through Multimodal Rate-Control","summary":"  Point cloud compression is essential to experience volumetric multimedia as\nit drastically reduces the required streaming data rates. Point attributes,\nspecifically colors, extend the challenge of lossy compression beyond geometric\nrepresentation to achieving joint reconstruction of texture and geometry.\nState-of-the-art methods separate geometry and attributes to compress them\nindividually. This comes at a computational cost, requiring an encoder and a\ndecoder for each modality. Additionally, as attribute compression methods\nrequire the same geometry for encoding and decoding, the encoder emulates the\ndecoder-side geometry reconstruction as an input step to project and compress\nthe attributes. In this work, we propose to learn joint compression of geometry\nand attributes using a single, adaptive autoencoder model, embedding both\nmodalities into a unified latent space which is then entropy encoded. Key to\nthe technique is to replace the search for trade-offs between rate, attribute\nquality and geometry quality, through conditioning the model on the desired\nqualities of both modalities, bypassing the need for training model ensembles.\nTo differentiate important point cloud regions during encoding or to allow\nview-dependent compression for user-centered streaming, conditioning is\npointwise, which allows for local quality and rate variation. Our evaluation\nshows comparable performance to state-of-the-art compression methods for\ngeometry and attributes, while reducing complexity compared to related\ncompression methods.\n","authors":["Michael Rudolph","Aron Riemenschneider","Amr Rizk"],"pdf_url":"https://arxiv.org/pdf/2408.00599v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.00591v1","updated":"2024-08-01T14:20:47Z","published":"2024-08-01T14:20:47Z","title":"Regional quality estimation for echocardiography using deep learning","summary":"  Automatic estimation of cardiac ultrasound image quality can be beneficial\nfor guiding operators and ensuring the accuracy of clinical measurements.\nPrevious work often fails to distinguish the view correctness of the\nechocardiogram from the image quality. Additionally, previous studies only\nprovide a global image quality value, which limits their practical utility. In\nthis work, we developed and compared three methods to estimate image quality:\n1) classic pixel-based metrics like the generalized contrast-to-noise ratio\n(gCNR) on myocardial segments as region of interest and left ventricle lumen as\nbackground, obtained using a U-Net segmentation 2) local image coherence\nderived from a U-Net model that predicts coherence from B-Mode images 3) a deep\nconvolutional network that predicts the quality of each region directly in an\nend-to-end fashion. We evaluate each method against manual regional image\nquality annotations by three experienced cardiologists. The results indicate\npoor performance of the gCNR metric, with Spearman correlation to the\nannotations of \\r{ho} = 0.24. The end-to-end learning model obtains the best\nresult, \\r{ho} = 0.69, comparable to the inter-observer correlation, \\r{ho} =\n0.63. Finally, the coherence-based method, with \\r{ho} = 0.58, outperformed the\nclassical metrics and is more generic than the end-to-end approach.\n","authors":["Gilles Van De Vyver","Svein-Erik Msy","Hvard Dalen","Bjrnar Leangen Grenne","Espen Holte","Sindre Hellum Olaisen","John Nyberg","Andreas stvik","Lasse Lvstakken","Erik Smistad"],"pdf_url":"https://arxiv.org/pdf/2408.00591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00565v1","updated":"2024-08-01T13:52:18Z","published":"2024-08-01T13:52:18Z","title":"MUFASA: Multi-View Fusion and Adaptation Network with Spatial Awareness\n  for Radar Object Detection","summary":"  In recent years, approaches based on radar object detection have made\nsignificant progress in autonomous driving systems due to their robustness\nunder adverse weather compared to LiDAR. However, the sparsity of radar point\nclouds poses challenges in achieving precise object detection, highlighting the\nimportance of effective and comprehensive feature extraction technologies. To\naddress this challenge, this paper introduces a comprehensive feature\nextraction method for radar point clouds. This study first enhances the\ncapability of detection networks by using a plug-and-play module, GeoSPA. It\nleverages the Lalonde features to explore local geometric patterns.\nAdditionally, a distributed multi-view attention mechanism, DEMVA, is designed\nto integrate the shared information across the entire dataset with the global\ninformation of each individual frame. By employing the two modules, we present\nour method, MUFASA, which enhances object detection performance through\nimproved feature extraction. The approach is evaluated on the VoD and\nTJ4DRaDSet datasets to demonstrate its effectiveness. In particular, we achieve\nstate-of-the-art results among radar-based methods on the VoD dataset with the\nmAP of 50.24%.\n","authors":["Xiangyuan Peng","Miao Tang","Huawei Sun","Kay Bierzynski","Lorenzo Servadei","Robert Wille"],"pdf_url":"https://arxiv.org/pdf/2408.00565v1.pdf","comment":"Accepted by ICANN 2024"},{"id":"http://arxiv.org/abs/2408.00555v1","updated":"2024-08-01T13:38:58Z","published":"2024-08-01T13:38:58Z","title":"Alleviating Hallucination in Large Vision-Language Models with Active\n  Retrieval Augmentation","summary":"  Despite the remarkable ability of large vision-language models (LVLMs) in\nimage comprehension, these models frequently generate plausible yet factually\nincorrect responses, a phenomenon known as hallucination.Recently, in large\nlanguage models (LLMs), augmenting LLMs by retrieving information from external\nknowledge resources has been proven as a promising solution to mitigate\nhallucinations.However, the retrieval augmentation in LVLM significantly lags\nbehind the widespread applications of LVLM. Moreover, when transferred to\naugmenting LVLMs, sometimes the hallucination degree of the model is even\nexacerbated.Motivated by the research gap and counter-intuitive phenomenon, we\nintroduce a novel framework, the Active Retrieval-Augmented large\nvision-language model (ARA), specifically designed to address hallucinations by\nincorporating three critical dimensions: (i) dissecting the retrieval targets\nbased on the inherent hierarchical structures of images. (ii) pinpointing the\nmost effective retrieval methods and filtering out the reliable retrieval\nresults. (iii) timing the retrieval process to coincide with episodes of low\ncertainty, while circumventing unnecessary retrieval during periods of high\ncertainty. To assess the capability of our proposed ARA model in reducing\nhallucination, we employ three widely used LVLM models (LLaVA-1.5, Qwen-VL, and\nmPLUG-Owl2) across four benchmarks. Our empirical observations suggest that by\nutilizing fitting retrieval mechanisms and timing the retrieval judiciously, we\ncan effectively mitigate the hallucination problem. We hope that this study can\nprovide deeper insights into how to adapt the retrieval augmentation to LVLMs\nfor reducing hallucinations with more effective retrieval and minimal retrieval\noccurrences.\n","authors":["Xiaoye Qu","Qiyuan Chen","Wei Wei","Jishuo Sun","Jianfeng Dong"],"pdf_url":"https://arxiv.org/pdf/2408.00555v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00550v1","updated":"2024-08-01T13:34:35Z","published":"2024-08-01T13:34:35Z","title":"Mitigating Multilingual Hallucination in Large Vision-Language Models","summary":"  While Large Vision-Language Models (LVLMs) have exhibited remarkable\ncapabilities across a wide range of tasks, they suffer from hallucination\nproblems, where models generate plausible yet incorrect answers given the input\nimage-query pair. This hallucination phenomenon is even more severe when\nquerying the image in non-English languages, while existing methods for\nmitigating hallucinations in LVLMs only consider the English scenarios. In this\npaper, we make the first attempt to mitigate this important multilingual\nhallucination in LVLMs. With thorough experiment analysis, we found that\nmultilingual hallucination in LVLMs is a systemic problem that could arise from\ndeficiencies in multilingual capabilities or inadequate multimodal abilities.\nTo this end, we propose a two-stage Multilingual Hallucination Removal (MHR)\nframework for LVLMs, aiming to improve resistance to hallucination for both\nhigh-resource and low-resource languages. Instead of relying on the intricate\nmanual annotations of multilingual resources, we fully leverage the inherent\ncapabilities of the LVLM and propose a novel cross-lingual alignment method,\nwhich generates multiple responses for each image-query input and then\nidentifies the hallucination-aware pairs for each language. These data pairs\nare finally used for direct preference optimization to prompt the LVLMs to\nfavor non-hallucinating responses. Experimental results show that our MHR\nachieves a substantial reduction in hallucination generation for LVLMs.\nNotably, on our extended multilingual POPE benchmark, our framework delivers an\naverage increase of 19.0% in accuracy across 13 different languages. Our code\nand model weights are available at https://github.com/ssmisya/MHR\n","authors":["Xiaoye Qu","Mingyang Song","Wei Wei","Jianfeng Dong","Yu Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.00550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00538v1","updated":"2024-08-01T13:21:34Z","published":"2024-08-01T13:21:34Z","title":"High-Quality, ROS Compatible Video Encoding and Decoding for\n  High-Definition Datasets","summary":"  Robotic datasets are important for scientific benchmarking and developing\nalgorithms, for example for Simultaneous Localization and Mapping (SLAM).\nModern robotic datasets feature video data of high resolution and high\nframerates. Storing and sharing those datasets becomes thus very costly,\nespecially if more than one camera is used for the datasets. It is thus\nessential to store this video data in a compressed format. This paper\ninvestigates the use of modern video encoders for robotic datasets. We provide\na software that can replay mp4 videos within ROS 1 and ROS 2 frameworks,\nsupporting the synchronized playback in simulated time. Furthermore, the paper\nevaluates different encoders and their settings to find optimal configurations\nin terms of resulting size, quality and encoding time. Through this work we\nshow that it is possible to store and share even highest quality video datasets\nwithin reasonable storage constraints.\n","authors":["Jian Li","Bowen Xu","Sren Schwertfeger"],"pdf_url":"https://arxiv.org/pdf/2408.00538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00498v1","updated":"2024-08-01T12:08:20Z","published":"2024-08-01T12:08:20Z","title":"How Effective are Self-Supervised Models for Contact Identification in\n  Videos","summary":"  The exploration of video content via Self-Supervised Learning (SSL) models\nhas unveiled a dynamic field of study, emphasizing both the complex challenges\nand unique opportunities inherent in this area. Despite the growing body of\nresearch, the ability of SSL models to detect physical contacts in videos\nremains largely unexplored, particularly the effectiveness of methods such as\ndownstream supervision with linear probing or full fine-tuning. This work aims\nto bridge this gap by employing eight different convolutional neural networks\n(CNNs) based video SSL models to identify instances of physical contact within\nvideo sequences specifically. The Something-Something v2 (SSv2) and\nEpic-Kitchen (EK-100) datasets were chosen for evaluating these approaches due\nto the promising results on UCF101 and HMDB51, coupled with their limited prior\nassessment on SSv2 and EK-100. Additionally, these datasets feature diverse\nenvironments and scenarios, essential for testing the robustness and accuracy\nof video-based models. This approach not only examines the effectiveness of\neach model in recognizing physical contacts but also explores the performance\nin the action recognition downstream task. By doing so, valuable insights into\nthe adaptability of SSL models in interpreting complex, dynamic visual\ninformation are contributed.\n","authors":["Malitha Gunawardhana","Limalka Sadith","Liel David","Daniel Harari","Muhammad Haris Khan"],"pdf_url":"https://arxiv.org/pdf/2408.00498v1.pdf","comment":"15 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00496v1","updated":"2024-08-01T12:05:02Z","published":"2024-08-01T12:05:02Z","title":"SegStitch: Multidimensional Transformer for Robust and Efficient Medical\n  Imaging Segmentation","summary":"  Medical imaging segmentation plays a significant role in the automatic\nrecognition and analysis of lesions. State-of-the-art methods, particularly\nthose utilizing transformers, have been prominently adopted in 3D semantic\nsegmentation due to their superior performance in scalability and\ngeneralizability. However, plain vision transformers encounter challenges due\nto their neglect of local features and their high computational complexity. To\naddress these challenges, we introduce three key contributions: Firstly, we\nproposed SegStitch, an innovative architecture that integrates transformers\nwith denoising ODE blocks. Instead of taking whole 3D volumes as inputs, we\nadapt axial patches and customize patch-wise queries to ensure semantic\nconsistency. Additionally, we conducted extensive experiments on the BTCV and\nACDC datasets, achieving improvements up to 11.48% and 6.71% respectively in\nmDSC, compared to state-of-the-art methods. Lastly, our proposed method\ndemonstrates outstanding efficiency, reducing the number of parameters by 36.7%\nand the number of FLOPS by 10.7% compared to UNETR. This advancement holds\npromising potential for adapting our method to real-world clinical practice.\nThe code will be available at https://github.com/goblin327/SegStitch\n","authors":["Shengbo Tan","Zeyu Zhang","Ying Cai","Daji Ergu","Lin Wu","Binbin Hu","Pengzhang Yu","Yang Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.00496v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00493v1","updated":"2024-08-01T11:53:44Z","published":"2024-08-01T11:53:44Z","title":"Explainable Emotion Decoding for Human and Computer Vision","summary":"  Modern Machine Learning (ML) has significantly advanced various research\nfields, but the opaque nature of ML models hinders their adoption in several\ndomains. Explainable AI (XAI) addresses this challenge by providing additional\ninformation to help users understand the internal decision-making process of ML\nmodels. In the field of neuroscience, enriching a ML model for brain decoding\nwith attribution-based XAI techniques means being able to highlight which brain\nareas correlate with the task at hand, thus offering valuable insights to\ndomain experts. In this paper, we analyze human and Computer Vision (CV)\nsystems in parallel, training and explaining two ML models based respectively\non functional Magnetic Resonance Imaging (fMRI) and movie frames. We do so by\nleveraging the \"StudyForrest\" dataset, which includes functional Magnetic\nResonance Imaging (fMRI) scans of subjects watching the \"Forrest Gump\" movie,\nemotion annotations, and eye-tracking data. For human vision the ML task is to\nlink fMRI data with emotional annotations, and the explanations highlight the\nbrain regions strongly correlated with the label. On the other hand, for\ncomputer vision, the input data is movie frames, and the explanations are\npixel-level heatmaps. We cross-analyzed our results, linking human attention\n(obtained through eye-tracking) with XAI saliency on CV models and brain region\nactivations. We show how a parallel analysis of human and computer vision can\nprovide useful information for both the neuroscience community (allocation\ntheory) and the ML community (biological plausibility of convolutional models).\n","authors":["Alessio Borriero","Martina Milazzo","Matteo Diano","Davide Orsenigo","Maria Chiara Villa","Chiara Di Fazio","Marco Tamietto","Alan Perotti"],"pdf_url":"https://arxiv.org/pdf/2408.00493v1.pdf","comment":"This work has been accepted to be presented to The 2nd World\n  Conference on eXplainable Artificial Intelligence (xAI 2024), July 17-19,\n  2024 - Malta"},{"id":"http://arxiv.org/abs/2408.00491v1","updated":"2024-08-01T11:52:56Z","published":"2024-08-01T11:52:56Z","title":"GalleryGPT: Analyzing Paintings with Large Multimodal Models","summary":"  Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.\n","authors":["Yi Bin","Wenhao Shi","Yujuan Ding","Zhiqiang Hu","Zheng Wang","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00491v1.pdf","comment":"Accepted as Oral Presentation at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.00489v1","updated":"2024-08-01T11:51:50Z","published":"2024-08-01T11:51:50Z","title":"Multi-label Sewer Pipe Defect Recognition with Mask Attention Feature\n  Enhancement and Label Correlation Learning","summary":"  The coexistence of multiple defect categories as well as the substantial\nclass imbalance problem significantly impair the detection of sewer pipeline\ndefects. To solve this problem, a multi-label pipe defect recognition method is\nproposed based on mask attention guided feature enhancement and label\ncorrelation learning. The proposed method can achieve current approximate\nstate-of-the-art classification performance using just 1/16 of the Sewer-ML\ntraining dataset and exceeds the current best method by 11.87\\% in terms of F2\nmetric on the full dataset, while also proving the superiority of the model.\nThe major contribution of this study is the development of a more efficient\nmodel for identifying and locating multiple defects in sewer pipe images for a\nmore accurate sewer pipeline condition assessment. Moreover, by employing class\nactivation maps, our method can accurately pinpoint multiple defect categories\nin the image which demonstrates a strong model interpretability. Our code is\navailable at\n\\href{https://github.com/shengyu27/MA-Q2L}{\\textcolor{black}{https://github.com/shengyu27/MA-Q2L.}\n","authors":["Xin Zuo","Yu Sheng","Jifeng Shen","Yongwei Shan"],"pdf_url":"https://arxiv.org/pdf/2408.00489v1.pdf","comment":"Accepted by the Journal of Computing in Civil Engineering"},{"id":"http://arxiv.org/abs/2408.00483v1","updated":"2024-08-01T11:39:45Z","published":"2024-08-01T11:39:45Z","title":"A Systematic Review on Long-Tailed Learning","summary":"  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n","authors":["Chongsheng Zhang","George Almpanidis","Gaojuan Fan","Binquan Deng","Yanbo Zhang","Ji Liu","Aouaidjia Kamel","Paolo Soda","Joo Gama"],"pdf_url":"https://arxiv.org/pdf/2408.00483v1.pdf","comment":"Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]"},{"id":"http://arxiv.org/abs/2408.00470v1","updated":"2024-08-01T11:16:26Z","published":"2024-08-01T11:16:26Z","title":"Image Super-Resolution with Taylor Expansion Approximation and Large\n  Field Reception","summary":"  Self-similarity techniques are booming in blind super-resolution (SR) due to\naccurate estimation of the degradation types involved in low-resolution images.\nHowever, high-dimensional matrix multiplication within self-similarity\ncomputation prohibitively consumes massive computational costs. We find that\nthe high-dimensional attention map is derived from the matrix multiplication\nbetween Query and Key, followed by a softmax function. This softmax makes the\nmatrix multiplication between Query and Key inseparable, posing a great\nchallenge in simplifying computational complexity. To address this issue, we\nfirst propose a second-order Taylor expansion approximation (STEA) to separate\nthe matrix multiplication of Query and Key, resulting in the complexity\nreduction from $\\mathcal{O}(N^2)$ to $\\mathcal{O}(N)$. Then, we design a\nmulti-scale large field reception (MLFR) to compensate for the performance\ndegradation caused by STEA. Finally, we apply these two core designs to\nlaboratory and real-world scenarios by constructing LabNet and RealNet,\nrespectively. Extensive experimental results tested on five synthetic datasets\ndemonstrate that our LabNet sets a new benchmark in qualitative and\nquantitative evaluations. Tested on the RealWorld38 dataset, our RealNet\nachieves superior visual quality over existing methods. Ablation studies\nfurther verify the contributions of STEA and MLFR towards both LabNet and\nRealNet frameworks.\n","authors":["Jiancong Feng","Yuan-Gen Wang","Mingjie Li","Fengchuang Xing"],"pdf_url":"https://arxiv.org/pdf/2408.00470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00458v1","updated":"2024-08-01T10:55:20Z","published":"2024-08-01T10:55:20Z","title":"Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual\n  Inversion","summary":"  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n","authors":["Manuel Kansy","Jacek Naruniec","Christopher Schroers","Markus Gross","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2408.00458v1.pdf","comment":"Preprint. All videos in this paper are best viewed as animations with\n  Acrobat Reader by pressing the highlighted frame of each video"},{"id":"http://arxiv.org/abs/2408.00441v1","updated":"2024-08-01T10:25:14Z","published":"2024-08-01T10:25:14Z","title":"Focus, Distinguish, and Prompt: Unleashing CLIP for Efficient and\n  Flexible Scene Text Retrieval","summary":"  Scene text retrieval aims to find all images containing the query text from\nan image gallery. Current efforts tend to adopt an Optical Character\nRecognition (OCR) pipeline, which requires complicated text detection and/or\nrecognition processes, resulting in inefficient and inflexible retrieval.\nDifferent from them, in this work we propose to explore the intrinsic potential\nof Contrastive Language-Image Pre-training (CLIP) for OCR-free scene text\nretrieval. Through empirical analysis, we observe that the main challenges of\nCLIP as a text retriever are: 1) limited text perceptual scale, and 2)\nentangled visual-semantic concepts. To this end, a novel model termed FDP\n(Focus, Distinguish, and Prompt) is developed. FDP first focuses on scene text\nvia shifting the attention to the text area and probing the hidden text\nknowledge, and then divides the query text into content word and function word\nfor processing, in which a semantic-aware prompting scheme and a distracted\nqueries assistance module are utilized. Extensive experiments show that FDP\nsignificantly enhances the inference speed while achieving better or\ncompetitive retrieval accuracy compared to existing methods. Notably, on the\nIIIT-STR benchmark, FDP surpasses the state-of-the-art model by 4.37% with a 4\ntimes faster speed. Furthermore, additional experiments under phrase-level and\nattribute-aware scene text retrieval settings validate FDP's particular\nadvantages in handling diverse forms of query text. The source code will be\npublicly available at https://github.com/Gyann-z/FDP.\n","authors":["Gangyan Zeng","Yuan Zhang","Jin Wei","Dongbao Yang","Peng Zhang","Yiwen Gao","Xugong Qin","Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.00441v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.00438v1","updated":"2024-08-01T10:16:58Z","published":"2024-08-01T10:16:58Z","title":"MonoMM: A Multi-scale Mamba-Enhanced Network for Real-time Monocular 3D\n  Object Detection","summary":"  Recent advancements in transformer-based monocular 3D object detection\ntechniques have exhibited exceptional performance in inferring 3D attributes\nfrom single 2D images. However, most existing methods rely on\nresource-intensive transformer architectures, which often lead to significant\ndrops in computational efficiency and performance when handling long sequence\ndata. To address these challenges and advance monocular 3D object detection\ntechnology, we propose an innovative network architecture, MonoMM, a\nMulti-scale \\textbf{M}amba-Enhanced network for real-time Monocular 3D object\ndetection. This well-designed architecture primarily includes the following two\ncore modules: Focused Multi-Scale Fusion (FMF) Module, which focuses on\neffectively preserving and fusing image information from different scales with\nlower computational resource consumption. By precisely regulating the\ninformation flow, the FMF module enhances the model adaptability and robustness\nto scale variations while maintaining image details. Depth-Aware Feature\nEnhancement Mamba (DMB) Module: It utilizes the fused features from image\ncharacteristics as input and employs a novel adaptive strategy to globally\nintegrate depth information and visual information. This depth fusion strategy\nnot only improves the accuracy of depth estimation but also enhances the model\nperformance under different viewing angles and environmental conditions.\nMoreover, the modular design of MonoMM provides high flexibility and\nscalability, facilitating adjustments and optimizations according to specific\napplication needs. Extensive experiments conducted on the KITTI dataset show\nthat our method outperforms previous monocular methods and achieves real-time\ndetection.\n","authors":["Youjia Fu","Zihao Xu","Junsong Fu","Huixia Xue","Shuqiu Tan","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2408.00438v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00427v1","updated":"2024-08-01T09:59:57Z","published":"2024-08-01T09:59:57Z","title":"CARMIL: Context-Aware Regularization on Multiple Instance Learning\n  models for Whole Slide Images","summary":"  Multiple Instance Learning (MIL) models have proven effective for cancer\nprognosis from Whole Slide Images. However, the original MIL formulation\nincorrectly assumes the patches of the same image to be independent, leading to\na loss of spatial context as information flows through the network.\nIncorporating contextual knowledge into predictions is particularly important\ngiven the inclination for cancerous cells to form clusters and the presence of\nspatial indicators for tumors. State-of-the-art methods often use attention\nmechanisms eventually combined with graphs to capture spatial knowledge. In\nthis paper, we take a novel and transversal approach, addressing this issue\nthrough the lens of regularization. We propose Context-Aware Regularization for\nMultiple Instance Learning (CARMIL), a versatile regularization scheme designed\nto seamlessly integrate spatial knowledge into any MIL model. Additionally, we\npresent a new and generic metric to quantify the Context-Awareness of any MIL\nmodel when applied to Whole Slide Images, resolving a previously unexplored gap\nin the field. The efficacy of our framework is evaluated for two survival\nanalysis tasks on glioblastoma (TCGA GBM) and colon cancer data (TCGA COAD).\n","authors":["Thiziri Nait Saada","Valentina Di-Proietto","Benoit Schmauch","Katharina Von Loga","Lucas Fidon"],"pdf_url":"https://arxiv.org/pdf/2408.00427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00420v1","updated":"2024-08-01T09:42:44Z","published":"2024-08-01T09:42:44Z","title":"MPT-PAR:Mix-Parameters Transformer for Panoramic Activity Recognition","summary":"  The objective of the panoramic activity recognition task is to identify\nbehaviors at various granularities within crowded and complex environments,\nencompassing individual actions, social group activities, and global\nactivities. Existing methods generally use either parameter-independent modules\nto capture task-specific features or parameter-sharing modules to obtain common\nfeatures across all tasks. However, there is often a strong interrelatedness\nand complementary effect between tasks of different granularities that previous\nmethods have yet to notice. In this paper, we propose a model called MPT-PAR\nthat considers both the unique characteristics of each task and the synergies\nbetween different tasks simultaneously, thereby maximizing the utilization of\nfeatures across multi-granularity activity recognition. Furthermore, we\nemphasize the significance of temporal and spatial information by introducing a\nspatio-temporal relation-enhanced module and a scene representation learning\nmodule, which integrate the the spatio-temporal context of action and global\nscene into the feature map of each granularity. Our method achieved an overall\nF1 score of 47.5\\% on the JRDB-PAR dataset, significantly outperforming all the\nstate-of-the-art methods.\n","authors":["Wenqing Gan","Yan Sun","Feiran Liu","Xiangfeng Luo"],"pdf_url":"https://arxiv.org/pdf/2408.00420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00418v1","updated":"2024-08-01T09:39:27Z","published":"2024-08-01T09:39:27Z","title":"Towards Reliable Advertising Image Generation Using Human Feedback","summary":"  In the e-commerce realm, compelling advertising images are pivotal for\nattracting customer attention. While generative models automate image\ngeneration, they often produce substandard images that may mislead customers\nand require significant labor costs to inspect. This paper delves into\nincreasing the rate of available generated images. We first introduce a\nmulti-modal Reliable Feedback Network (RFNet) to automatically inspect the\ngenerated images. Combining the RFNet into a recurrent process, Recurrent\nGeneration, results in a higher number of available advertising images. To\nfurther enhance production efficiency, we fine-tune diffusion models with an\ninnovative Consistent Condition regularization utilizing the feedback from\nRFNet (RFFT). This results in a remarkable increase in the available rate of\ngenerated images, reducing the number of attempts in Recurrent Generation, and\nproviding a highly efficient production process without sacrificing visual\nappeal. We also construct a Reliable Feedback 1 Million (RF1M) dataset which\ncomprises over one million generated advertising images annotated by human,\nwhich helps to train RFNet to accurately assess the availability of generated\nimages and faithfully reflect the human feedback. Generally speaking, our\napproach offers a reliable solution for advertising image generation.\n","authors":["Zhenbang Du","Wei Feng","Haohan Wang","Yaoyu Li","Jingsen Wang","Jian Li","Zheng Zhang","Jingjing Lv","Xin Zhu","Junsheng Jin","Junjie Shen","Zhangang Lin","Jingping Shao"],"pdf_url":"https://arxiv.org/pdf/2408.00418v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.00415v1","updated":"2024-08-01T09:32:01Z","published":"2024-08-01T09:32:01Z","title":"DriveArena: A Closed-loop Generative Simulation Platform for Autonomous\n  Driving","summary":"  This paper presented DriveArena, the first high-fidelity closed-loop\nsimulation system designed for driving agents navigating in real scenarios.\nDriveArena features a flexible, modular architecture, allowing for the seamless\ninterchange of its core components: Traffic Manager, a traffic simulator\ncapable of generating realistic traffic flow on any worldwide street map, and\nWorld Dreamer, a high-fidelity conditional generative model with infinite\nautoregression. This powerful synergy empowers any driving agent capable of\nprocessing real-world images to navigate in DriveArena's simulated environment.\nThe agent perceives its surroundings through images generated by World Dreamer\nand output trajectories. These trajectories are fed into Traffic Manager,\nachieving realistic interactions with other vehicles and producing a new scene\nlayout. Finally, the latest scene layout is relayed back into World Dreamer,\nperpetuating the simulation cycle. This iterative process fosters closed-loop\nexploration within a highly realistic environment, providing a valuable\nplatform for developing and evaluating driving agents across diverse and\nchallenging scenarios. DriveArena signifies a substantial leap forward in\nleveraging generative image data for the driving simulation platform, opening\ninsights for closed-loop autonomous driving. Code will be available soon on\nGitHub: https://github.com/PJLab-ADG/DriveArena\n","authors":["Xuemeng Yang","Licheng Wen","Yukai Ma","Jianbiao Mei","Xin Li","Tiantian Wei","Wenjie Lei","Daocheng Fu","Pinlong Cai","Min Dou","Botian Shi","Liang He","Yong Liu","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.00415v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.00388v1","updated":"2024-08-01T08:57:47Z","published":"2024-08-01T08:57:47Z","title":"Deepfake Media Forensics: State of the Art and Challenges Ahead","summary":"  AI-generated synthetic media, also called Deepfakes, have significantly\ninfluenced so many domains, from entertainment to cybersecurity. Generative\nAdversarial Networks (GANs) and Diffusion Models (DMs) are the main frameworks\nused to create Deepfakes, producing highly realistic yet fabricated content.\nWhile these technologies open up new creative possibilities, they also bring\nsubstantial ethical and security risks due to their potential misuse. The rise\nof such advanced media has led to the development of a cognitive bias known as\nImpostor Bias, where individuals doubt the authenticity of multimedia due to\nthe awareness of AI's capabilities. As a result, Deepfake detection has become\na vital area of research, focusing on identifying subtle inconsistencies and\nartifacts with machine learning techniques, especially Convolutional Neural\nNetworks (CNNs). Research in forensic Deepfake technology encompasses five main\nareas: detection, attribution and recognition, passive authentication,\ndetection in realistic scenarios, and active authentication. Each area tackles\nspecific challenges, from tracing the origins of synthetic media and examining\nits inherent characteristics for authenticity. This paper reviews the primary\nalgorithms that address these challenges, examining their advantages,\nlimitations, and future prospects.\n","authors":["Irene Amerini","Mauro Barni","Sebastiano Battiato","Paolo Bestagini","Giulia Boato","Tania Sari Bonaventura","Vittoria Bruni","Roberto Caldelli","Francesco De Natale","Rocco De Nicola","Luca Guarnera","Sara Mandelli","Gian Luca Marcialis","Marco Micheletto","Andrea Montibeller","Giulia Orru'","Alessandro Ortis","Pericle Perazzo","Davide Salvi","Stefano Tubaro","Claudia Melis Tonti","Massimo Villari","Domenico Vitulano"],"pdf_url":"https://arxiv.org/pdf/2408.00388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00380v1","updated":"2024-08-01T08:41:13Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that \\name{} achieves excellent performance relative to the number of WSIs\nused and the model's parameter count. This suggests that the application of\nstain normalization has substantially improved the model's efficiency and\ngeneralization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.00374v1","updated":"2024-08-01T08:32:03Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00372v1","updated":"2024-08-01T08:29:42Z","published":"2024-08-01T08:29:42Z","title":"Few-shot Defect Image Generation based on Consistency Modeling","summary":"  Image generation can solve insufficient labeled data issues in defect\ndetection. Most defect generation methods are only trained on a single product\nwithout considering the consistencies among multiple products, leading to poor\nquality and diversity of generated results. To address these issues, we propose\nDefectDiffu, a novel text-guided diffusion method to model both intra-product\nbackground consistency and inter-product defect consistency across multiple\nproducts and modulate the consistency perturbation directions to control\nproduct type and defect strength, achieving diversified defect image\ngeneration. Firstly, we leverage a text encoder to separately provide\nconsistency prompts for background, defect, and fusion parts of the\ndisentangled integrated architecture, thereby disentangling defects and normal\nbackgrounds. Secondly, we propose the double-free strategy to generate defect\nimages through two-stage perturbation of consistency direction, thereby\ncontrolling product type and defect strength by adjusting the perturbation\nscale. Besides, DefectDiffu can generate defect mask annotations utilizing\ncross-attention maps from the defect part. Finally, to improve the generation\nquality of small defects and masks, we propose the adaptive attention-enhance\nloss to increase the attention to defects. Experimental results demonstrate\nthat DefectDiffu surpasses state-of-the-art methods in terms of generation\nquality and diversity, thus effectively improving downstream defection\nperformance. Moreover, defect perturbation directions can be transferred among\nvarious products to achieve zero-shot defect generation, which is highly\nbeneficial for addressing insufficient data issues. The code are available at\nhttps://github.com/FFDD-diffusion/DefectDiffu.\n","authors":["Qingfeng Shi","Jing Wei","Fei Shen","Zhengtao Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00365v1","updated":"2024-08-01T08:10:32Z","published":"2024-08-01T08:10:32Z","title":"Multimodal Fusion and Coherence Modeling for Video Topic Segmentation","summary":"  The video topic segmentation (VTS) task segments videos into intelligible,\nnon-overlapping topics, facilitating efficient comprehension of video content\nand quick access to specific content. VTS is also critical to various\ndownstream video understanding tasks. Traditional VTS methods using shallow\nfeatures or unsupervised approaches struggle to accurately discern the nuances\nof topical transitions. Recently, supervised approaches have achieved superior\nperformance on video action or scene segmentation over unsupervised approaches.\nIn this work, we improve supervised VTS by thoroughly exploring multimodal\nfusion and multimodal coherence modeling. Specifically, (1) we enhance\nmultimodal fusion by exploring different architectures using cross-attention\nand mixture of experts. (2) To generally strengthen multimodality alignment and\nfusion, we pre-train and fine-tune the model with multimodal contrastive\nlearning. (3) We propose a new pre-training task tailored for the VTS task, and\na novel fine-tuning task for enhancing multimodal coherence modeling for VTS.\nWe evaluate the proposed approaches on educational videos, in the form of\nlectures, due to the vital role of topic segmentation of educational videos in\nboosting learning experiences. Additionally, we introduce a large-scale Chinese\nlecture video dataset to augment the existing English corpus, promoting further\nresearch in VTS. Experiments on both English and Chinese lecture datasets\ndemonstrate that our model achieves superior VTS performance compared to\ncompetitive unsupervised and supervised baselines.\n","authors":["Hai Yu","Chong Deng","Qinglin Zhang","Jiaqing Liu","Qian Chen","Wen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00361v1","updated":"2024-08-01T08:03:13Z","published":"2024-08-01T08:03:13Z","title":"High-Precision Self-Supervised Monocular Depth Estimation with\n  Rich-Resource Prior","summary":"  In the area of self-supervised monocular depth estimation, models that\nutilize rich-resource inputs, such as high-resolution and multi-frame inputs,\ntypically achieve better performance than models that use ordinary single image\ninput. However, these rich-resource inputs may not always be available,\nlimiting the applicability of these methods in general scenarios. In this\npaper, we propose Rich-resource Prior Depth estimator (RPrDepth), which only\nrequires single input image during the inference phase but can still produce\nhighly accurate depth estimations comparable to rich resource based methods.\nSpecifically, we treat rich-resource data as prior information and extract\nfeatures from it as reference features in an offline manner. When estimating\nthe depth for a single-image image, we search for similar pixels from the\nrich-resource features and use them as prior information to estimate the depth.\nExperimental results demonstrate that our model outperform other single-image\nmodel and can achieve comparable or even better performance than models with\nrich-resource inputs, only using low-resolution single-image input.\n","authors":["Wencheng Han","Jianbing Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00361v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.00355v1","updated":"2024-08-01T07:52:07Z","published":"2024-08-01T07:52:07Z","title":"DNTextSpotter: Arbitrary-Shaped Scene Text Spotting via Improved\n  Denoising Training","summary":"  More and more end-to-end text spotting methods based on Transformer\narchitecture have demonstrated superior performance. These methods utilize a\nbipartite graph matching algorithm to perform one-to-one optimal matching\nbetween predicted objects and actual objects. However, the instability of\nbipartite graph matching can lead to inconsistent optimization targets, thereby\naffecting the training performance of the model. Existing literature applies\ndenoising training to solve the problem of bipartite graph matching instability\nin object detection tasks. Unfortunately, this denoising training method cannot\nbe directly applied to text spotting tasks, as these tasks need to perform\nirregular shape detection tasks and more complex text recognition tasks than\nclassification. To address this issue, we propose a novel denoising training\nmethod (DNTextSpotter) for arbitrary-shaped text spotting. Specifically, we\ndecompose the queries of the denoising part into noised positional queries and\nnoised content queries. We use the four Bezier control points of the Bezier\ncenter curve to generate the noised positional queries. For the noised content\nqueries, considering that the output of the text in a fixed positional order is\nnot conducive to aligning position with content, we employ a masked character\nsliding method to initialize noised content queries, thereby assisting in the\nalignment of text content and position. To improve the model's perception of\nthe background, we further utilize an additional loss function for background\ncharacters classification in the denoising training part.Although DNTextSpotter\nis conceptually simple, it outperforms the state-of-the-art methods on four\nbenchmarks (Total-Text, SCUT-CTW1500, ICDAR15, and Inverse-Text), especially\nyielding an improvement of 11.3% against the best approach in Inverse-Text\ndataset.\n","authors":["Yu Xie","Qian Qiao","Jun Gao","Tianxiang Wu","Shaoyao Huang","Jiaqing Fan","Ziqiang Cao","Zili Wang","Yue Zhang","Jielei Zhang","Huyang Sun"],"pdf_url":"https://arxiv.org/pdf/2408.00355v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.00352v1","updated":"2024-08-01T07:44:11Z","published":"2024-08-01T07:44:11Z","title":"Autonomous LLM-Enhanced Adversarial Attack for Text-to-Motion","summary":"  Human motion generation driven by deep generative models has enabled\ncompelling applications, but the ability of text-to-motion (T2M) models to\nproduce realistic motions from text prompts raises security concerns if\nexploited maliciously. Despite growing interest in T2M, few methods focus on\nsafeguarding these models against adversarial attacks, with existing work on\ntext-to-image models proving insufficient for the unique motion domain. In the\npaper, we propose ALERT-Motion, an autonomous framework leveraging large\nlanguage models (LLMs) to craft targeted adversarial attacks against black-box\nT2M models. Unlike prior methods modifying prompts through predefined rules,\nALERT-Motion uses LLMs' knowledge of human motion to autonomously generate\nsubtle yet powerful adversarial text descriptions. It comprises two key\nmodules: an adaptive dispatching module that constructs an LLM-based agent to\niteratively refine and search for adversarial prompts; and a multimodal\ninformation contrastive module that extracts semantically relevant motion\ninformation to guide the agent's search. Through this LLM-driven approach,\nALERT-Motion crafts adversarial prompts querying victim models to produce\noutputs closely matching targeted motions, while avoiding obvious\nperturbations. Evaluations across popular T2M models demonstrate ALERT-Motion's\nsuperiority over previous methods, achieving higher attack success rates with\nstealthier adversarial prompts. This pioneering work on T2M adversarial attacks\nhighlights the urgency of developing defensive measures as motion generation\ntechnology advances, urging further research into safe and responsible\ndeployment.\n","authors":["Honglei Miao","Fan Ma","Ruijie Quan","Kun Zhan","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00352v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00351v1","updated":"2024-08-01T07:42:45Z","published":"2024-08-01T07:42:45Z","title":"Hierarchically Structured Neural Bones for Reconstructing Animatable\n  Objects from Casual Videos","summary":"  We propose a new framework for creating and easily manipulating 3D models of\narbitrary objects using casually captured videos. Our core ingredient is a\nnovel hierarchy deformation model, which captures motions of objects with a\ntree-structured bones. Our hierarchy system decomposes motions based on the\ngranularity and reveals the correlations between parts without exploiting any\nprior structural knowledge. We further propose to regularize the bones to be\npositioned at the basis of motions, centers of parts, sufficiently covering\nrelated surfaces of the part. This is achieved by our bone occupancy function,\nwhich identifies whether a given 3D point is placed within the bone. Coupling\nthe proposed components, our framework offers several clear advantages: (1)\nusers can obtain animatable 3D models of the arbitrary objects in improved\nquality from their casual videos, (2) users can manipulate 3D models in an\nintuitive manner with minimal costs, and (3) users can interactively add or\ndelete control points as necessary. The experimental results demonstrate the\nefficacy of our framework on diverse instances, in reconstruction quality,\ninterpretability and easier manipulation. Our code is available at\nhttps://github.com/subin6/HSNB.\n","authors":["Subin Jeon","In Cho","Minsu Kim","Woong Oh Cho","Seon Joo Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00351v1.pdf","comment":"ECCV 2024 accepted"},{"id":"http://arxiv.org/abs/2408.00350v1","updated":"2024-08-01T07:40:00Z","published":"2024-08-01T07:40:00Z","title":"A Simple Background Augmentation Method for Object Detection with\n  Diffusion Model","summary":"  In computer vision, it is well-known that a lack of data diversity will\nimpair model performance. In this study, we address the challenges of enhancing\nthe dataset diversity problem in order to benefit various downstream tasks such\nas object detection and instance segmentation. We propose a simple yet\neffective data augmentation approach by leveraging advancements in generative\nmodels, specifically text-to-image synthesis technologies like Stable\nDiffusion. Our method focuses on generating variations of labeled real images,\nutilizing generative object and background augmentation via inpainting to\naugment existing training data without the need for additional annotations. We\nfind that background augmentation, in particular, significantly improves the\nmodels' robustness and generalization capabilities. We also investigate how to\nadjust the prompt and mask to ensure the generated content comply with the\nexisting annotations. The efficacy of our augmentation techniques is validated\nthrough comprehensive evaluations of the COCO dataset and several other key\nobject detection benchmarks, demonstrating notable enhancements in model\nperformance across diverse scenarios. This approach offers a promising solution\nto the challenges of dataset enhancement, contributing to the development of\nmore accurate and robust computer vision models.\n","authors":["Yuhang Li","Xin Dong","Chen Chen","Weiming Zhuang","Lingjuan Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.00350v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00347v1","updated":"2024-08-01T07:35:54Z","published":"2024-08-01T07:35:54Z","title":"Advancing Medical Image Segmentation: Morphology-Driven Learning with\n  Diffusion Transformer","summary":"  Understanding the morphological structure of medical images and precisely\nsegmenting the region of interest or abnormality is an important task that can\nassist in diagnosis. However, the unique properties of medical imaging make\nclear segmentation difficult, and the high cost and time-consuming task of\nlabeling leads to a coarse-grained representation of ground truth. Facing with\nthese problems, we propose a novel Diffusion Transformer Segmentation (DTS)\nmodel for robust segmentation in the presence of noise. We propose an\nalternative to the dominant Denoising U-Net encoder through experiments\napplying a transformer architecture, which captures global dependency through\nself-attention. Additionally, we propose k-neighbor label smoothing, reverse\nboundary attention, and self-supervised learning with morphology-driven\nlearning to improve the ability to identify complex structures. Our model,\nwhich analyzes the morphological representation of images, shows better results\nthan the previous models in various medical imaging modalities, including CT,\nMRI, and lesion images.\n","authors":["Sungmin Kang","Jaeha Song","Jihie Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00347v1.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00343v1","updated":"2024-08-01T07:27:54Z","published":"2024-08-01T07:27:54Z","title":"IN-Sight: Interactive Navigation through Sight","summary":"  Current visual navigation systems often treat the environment as static,\nlacking the ability to adaptively interact with obstacles. This limitation\nleads to navigation failure when encountering unavoidable obstructions. In\nresponse, we introduce IN-Sight, a novel approach to self-supervised path\nplanning, enabling more effective navigation strategies through interaction\nwith obstacles. Utilizing RGB-D observations, IN-Sight calculates\ntraversability scores and incorporates them into a semantic map, facilitating\nlong-range path planning in complex, maze-like environments. To precisely\nnavigate around obstacles, IN-Sight employs a local planner, trained\nimperatively on a differentiable costmap using representation learning\ntechniques. The entire framework undergoes end-to-end training within the\nstate-of-the-art photorealistic Intel SPEAR Simulator. We validate the\neffectiveness of IN-Sight through extensive benchmarking in a variety of\nsimulated scenarios and ablation studies. Moreover, we demonstrate the system's\nreal-world applicability with zero-shot sim-to-real transfer, deploying our\nplanner on the legged robot platform ANYmal, showcasing its practical potential\nfor interactive navigation in real environments.\n","authors":["Philipp Schoch","Fan Yang","Yuntao Ma","Stefan Leutenegger","Marco Hutter","Quentin Leboute"],"pdf_url":"https://arxiv.org/pdf/2408.00343v1.pdf","comment":"The 2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.00337v1","updated":"2024-08-01T07:17:10Z","published":"2024-08-01T07:17:10Z","title":"DistillGrasp: Integrating Features Correlation with Knowledge\n  Distillation for Depth Completion of Transparent Objects","summary":"  Due to the visual properties of reflection and refraction, RGB-D cameras\ncannot accurately capture the depth of transparent objects, leading to\nincomplete depth maps. To fill in the missing points, recent studies tend to\nexplore new visual features and design complex networks to reconstruct the\ndepth, however, these approaches tremendously increase computation, and the\ncorrelation of different visual features remains a problem. To this end, we\npropose an efficient depth completion network named DistillGrasp which\ndistillates knowledge from the teacher branch to the student branch.\nSpecifically, in the teacher branch, we design a position correlation block\n(PCB) that leverages RGB images as the query and key to search for the\ncorresponding values, guiding the model to establish correct correspondence\nbetween two features and transfer it to the transparent areas. For the student\nbranch, we propose a consistent feature correlation module (CFCM) that retains\nthe reliable regions of RGB images and depth maps respectively according to the\nconsistency and adopts a CNN to capture the pairwise relationship for depth\ncompletion. To avoid the student branch only learning regional features from\nthe teacher branch, we devise a distillation loss that not only considers the\ndistance loss but also the object structure and edge information. Extensive\nexperiments conducted on the ClearGrasp dataset manifest that our teacher\nnetwork outperforms state-of-the-art methods in terms of accuracy and\ngeneralization, and the student network achieves competitive results with a\nhigher speed of 48 FPS. In addition, the significant improvement in a\nreal-world robotic grasping system illustrates the effectiveness and robustness\nof our proposed system.\n","authors":["Yiheng Huang","Junhong Chen","Nick Michiels","Muhammad Asim","Luc Claesen","Wenyin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00337v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.00332v1","updated":"2024-08-01T07:10:45Z","published":"2024-08-01T07:10:45Z","title":"Vision-based Wearable Steering Assistance for People with Impaired\n  Vision in Jogging","summary":"  Outdoor sports pose a challenge for people with impaired vision. The demand\nfor higher-speed mobility inspired us to develop a vision-based wearable\nsteering assistance. To ensure broad applicability, we focused on a\nrepresentative sports environment, the athletics track. Our efforts centered on\nimproving the speed and accuracy of perception, enhancing planning adaptability\nfor the real world, and providing swift and safe assistance for people with\nimpaired vision. In perception, we engineered a lightweight multitask network\ncapable of simultaneously detecting track lines and obstacles. Additionally,\ndue to the limitations of existing datasets for supporting multi-task detection\nin athletics tracks, we diligently collected and annotated a new dataset (MAT)\ncontaining 1000 images. In planning, we integrated the methods of sampling and\nspline curves, addressing the planning challenges of curves. Meanwhile, we\nutilized the positions of the track lines and obstacles as constraints to guide\npeople with impaired vision safely along the current track. Our system is\ndeployed on an embedded device, Jetson Orin NX. Through outdoor experiments, it\ndemonstrated adaptability in different sports scenarios, assisting users in\nachieving free movement of 400-meter at an average speed of 1.34 m/s, meeting\nthe level of normal people in jogging. Our MAT dataset is publicly available\nfrom https://github.com/snoopy-l/MAT\n","authors":["Xiaotong Liu","Binglu Wang","Zhijun Li"],"pdf_url":"https://arxiv.org/pdf/2408.00332v1.pdf","comment":"Accepted to ICRA 2024"},{"id":"http://arxiv.org/abs/2408.00331v1","updated":"2024-08-01T07:08:11Z","published":"2024-08-01T07:08:11Z","title":"DECIDER: Leveraging Foundation Model Priors for Improved Model Failure\n  Detection and Explanation","summary":"  Reliably detecting when a deployed machine learning model is likely to fail\non a given input is crucial for ensuring safe operation. In this work, we\npropose DECIDER (Debiasing Classifiers to Identify Errors Reliably), a novel\napproach that leverages priors from large language models (LLMs) and\nvision-language models (VLMs) to detect failures in image classification\nmodels. DECIDER utilizes LLMs to specify task-relevant core attributes and\nconstructs a ``debiased'' version of the classifier by aligning its visual\nfeatures to these core attributes using a VLM, and detects potential failure by\nmeasuring disagreement between the original and debiased models. In addition to\nproactively identifying samples on which the model would fail, DECIDER also\nprovides human-interpretable explanations for failure through a novel\nattribute-ablation strategy. Through extensive experiments across diverse\nbenchmarks spanning subpopulation shifts (spurious correlations, class\nimbalance) and covariate shifts (synthetic corruptions, domain shifts), DECIDER\nconsistently achieves state-of-the-art failure detection performance,\nsignificantly outperforming baselines in terms of the overall Matthews\ncorrelation coefficient as well as failure and success recall. Our codes can be\naccessed at~\\url{https://github.com/kowshikthopalli/DECIDER/}\n","authors":["Rakshith Subramanyam","Kowshik Thopalli","Vivek Narayanaswamy","Jayaraman J. Thiagarajan"],"pdf_url":"https://arxiv.org/pdf/2408.00331v1.pdf","comment":"Accepted at ECCV (European Conference on Computer Vision) 2024"},{"id":"http://arxiv.org/abs/2408.00315v1","updated":"2024-08-01T06:26:05Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.00311v1","updated":"2024-08-01T06:14:37Z","published":"2024-08-01T06:14:37Z","title":"Translating Imaging to Genomics: Leveraging Transformers for Predictive\n  Modeling","summary":"  In this study, we present a novel approach for predicting genomic information\nfrom medical imaging modalities using a transformer-based model. We aim to\nbridge the gap between imaging and genomics data by leveraging transformer\nnetworks, allowing for accurate genomic profile predictions from CT/MRI images.\nPresently most studies rely on the use of whole slide images (WSI) for the\nassociation, which are obtained via invasive methodologies. We propose using\nonly available CT/MRI images to predict genomic sequences. Our transformer\nbased approach is able to efficiently generate associations between multiple\nsequences based on CT/MRI images alone. This work paves the way for the use of\nnon-invasive imaging modalities for precise and personalized healthcare,\nallowing for a better understanding of diseases and treatment.\n","authors":["Aiman Farooq","Deepak Mishra","Santanu Chaudhury"],"pdf_url":"https://arxiv.org/pdf/2408.00311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00303v1","updated":"2024-08-01T06:02:59Z","published":"2024-08-01T06:02:59Z","title":"Neural Octahedral Field: Octahedral prior for simultaneous smoothing and\n  sharp edge regularization","summary":"  Neural implicit representation, the parameterization of distance function as\na coordinate neural field, has emerged as a promising lead in tackling surface\nreconstruction from unoriented point clouds. To enforce consistent orientation,\nexisting methods focus on regularizing the gradient of the distance function,\nsuch as constraining it to be of the unit norm, minimizing its divergence, or\naligning it with the eigenvector of Hessian that corresponds to zero\neigenvalue. However, under the presence of large scanning noise, they tend to\neither overfit the noise input or produce an excessively smooth reconstruction.\nIn this work, we propose to guide the surface reconstruction under a new\nvariant of neural field, the octahedral field, leveraging the spherical\nharmonics representation of octahedral frames originated in the hexahedral\nmeshing. Such field automatically snaps to geometry features when constrained\nto be smooth, and naturally preserves sharp angles when interpolated over\ncreases. By simultaneously fitting and smoothing the octahedral field alongside\nthe implicit geometry, it behaves analogously to bilateral filtering, resulting\nin smooth reconstruction while preserving sharp edges. Despite being operated\npurely pointwise, our method outperforms various traditional and neural\napproaches across extensive experiments, and is very competitive with methods\nthat require normal and data priors. Our full implementation is available at:\nhttps://github.com/Ankbzpx/frame-field.\n","authors":["Ruichen Zheng","Tao Yu"],"pdf_url":"https://arxiv.org/pdf/2408.00303v1.pdf","comment":"project page: https://github.com/Ankbzpx/frame-field"},{"id":"http://arxiv.org/abs/2408.00300v1","updated":"2024-08-01T05:56:34Z","published":"2024-08-01T05:56:34Z","title":"Towards Flexible Evaluation for Generative Visual Question Answering","summary":"  Throughout rapid development of multimodal large language models, a crucial\ningredient is a fair and accurate evaluation of their multimodal comprehension\nabilities. Although Visual Question Answering (VQA) could serve as a developed\ntest field, limitations of VQA evaluation, like the inflexible pattern of Exact\nMatch, have hindered MLLMs from demonstrating their real capability and\ndiscourage rich responses. Therefore, this paper proposes the use of\nsemantics-based evaluators for assessing unconstrained open-ended responses on\nVQA datasets. As characteristics of VQA have made such evaluation significantly\ndifferent than the traditional Semantic Textual Similarity (STS) task, to\nsystematically analyze the behaviour and compare the performance of various\nevaluators including LLM-based ones, we proposes three key properties, i.e.,\nAlignment, Consistency and Generalization, and a corresponding dataset\nAssessing VQA Evaluators (AVE) to facilitate analysis. In addition, this paper\nproposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design\nbased on the unique features of VQA evaluation. Experimental results verify the\nfeasibility of model-based VQA evaluation and effectiveness of the proposed\nevaluator that surpasses existing semantic evaluators by a large margin. The\nproposed training scheme generalizes to both the BERT-like encoders and\ndecoder-only LLM.\n","authors":["Huishan Ji","Qingyi Si","Zheng Lin","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00298v1","updated":"2024-08-01T05:47:04Z","published":"2024-08-01T05:47:04Z","title":"Tails Tell Tales: Chapter-Wide Manga Transcriptions with Character Names","summary":"  Enabling engagement of manga by visually impaired individuals presents a\nsignificant challenge due to its inherently visual nature. With the goal of\nfostering accessibility, this paper aims to generate a dialogue transcript of a\ncomplete manga chapter, entirely automatically, with a particular emphasis on\nensuring narrative consistency. This entails identifying (i) what is being\nsaid, i.e., detecting the texts on each page and classifying them into\nessential vs non-essential, and (ii) who is saying it, i.e., attributing each\ndialogue to its speaker, while ensuring the same characters are named\nconsistently throughout the chapter.\n  To this end, we introduce: (i) Magiv2, a model that is capable of generating\nhigh-quality chapter-wide manga transcripts with named characters and\nsignificantly higher precision in speaker diarisation over prior works; (ii) an\nextension of the PopManga evaluation dataset, which now includes annotations\nfor speech-bubble tail boxes, associations of text to corresponding tails,\nclassifications of text as essential or non-essential, and the identity for\neach character box; and (iii) a new character bank dataset, which comprises\nover 11K characters from 76 manga series, featuring 11.5K exemplar character\nimages in total, as well as a list of chapters in which they appear. The code,\ntrained model, and both datasets can be found at:\nhttps://github.com/ragavsachdeva/magi\n","authors":["Ragav Sachdeva","Gyungin Shin","Andrew Zisserman"],"pdf_url":"https://arxiv.org/pdf/2408.00298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00297v1","updated":"2024-08-01T05:46:57Z","published":"2024-08-01T05:46:57Z","title":"EmoTalk3D: High-Fidelity Free-View Synthesis of Emotional 3D Talking\n  Head","summary":"  We present a novel approach for synthesizing 3D talking heads with\ncontrollable emotion, featuring enhanced lip synchronization and rendering\nquality. Despite significant progress in the field, prior methods still suffer\nfrom multi-view consistency and a lack of emotional expressiveness. To address\nthese issues, we collect EmoTalk3D dataset with calibrated multi-view videos,\nemotional annotations, and per-frame 3D geometry. By training on the EmoTalk3D\ndataset, we propose a \\textit{`Speech-to-Geometry-to-Appearance'} mapping\nframework that first predicts faithful 3D geometry sequence from the audio\nfeatures, then the appearance of a 3D talking head represented by 4D Gaussians\nis synthesized from the predicted geometry. The appearance is further\ndisentangled into canonical and dynamic Gaussians, learned from multi-view\nvideos, and fused to render free-view talking head animation. Moreover, our\nmodel enables controllable emotion in the generated talking heads and can be\nrendered in wide-range views. Our method exhibits improved rendering quality\nand stability in lip motion generation while capturing dynamic facial details\nsuch as wrinkles and subtle expressions. Experiments demonstrate the\neffectiveness of our approach in generating high-fidelity and\nemotion-controllable 3D talking heads. The code and EmoTalk3D dataset are\nreleased at https://nju-3dv.github.io/projects/EmoTalk3D.\n","authors":["Qianyun He","Xinya Ji","Yicheng Gong","Yuanxun Lu","Zhengyu Diao","Linjia Huang","Yao Yao","Siyu Zhu","Zhan Ma","Songcen Xu","Xiaofei Wu","Zixiao Zhang","Xun Cao","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00297v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00296v1","updated":"2024-08-01T05:46:06Z","published":"2024-08-01T05:46:06Z","title":"Head360: Learning a Parametric 3D Full-Head for Free-View Synthesis in\n  360","summary":"  Creating a 360{\\deg} parametric model of a human head is a very challenging\ntask. While recent advancements have demonstrated the efficacy of leveraging\nsynthetic data for building such parametric head models, their performance\nremains inadequate in crucial areas such as expression-driven animation,\nhairstyle editing, and text-based modifications. In this paper, we build a\ndataset of artist-designed high-fidelity human heads and propose to create a\nnovel parametric 360{\\deg} renderable parametric head model from it. Our scheme\ndecouples the facial motion/shape and facial appearance, which are represented\nby a classic parametric 3D mesh model and an attached neural texture,\nrespectively. We further propose a training method for decompositing hairstyle\nand facial appearance, allowing free-swapping of the hairstyle. A novel\ninversion fitting method is presented based on single image input with high\ngeneralization and fidelity. To the best of our knowledge, our model is the\nfirst parametric 3D full-head that achieves 360{\\deg} free-view synthesis,\nimage-based fitting, appearance editing, and animation within a single model.\nExperiments show that facial motions and appearances are well disentangled in\nthe parametric space, leading to SOTA performance in rendering and animating\nquality. The code and SynHead100 dataset are released at\nhttps://nju-3dv.github.io/projects/Head360.\n","authors":["Yuxiao He","Yiyu Zhuang","Yanwen Wang","Yao Yao","Siyu Zhu","Xiaoyu Li","Qi Zhang","Xun Cao","Hao Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.00296v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00294v1","updated":"2024-08-01T05:41:59Z","published":"2024-08-01T05:41:59Z","title":"RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace","summary":"  With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.\n","authors":["Lu Ou","Shaolin Liao","Shihui Gao","Guandong Huang","Zheng Qi"],"pdf_url":"https://arxiv.org/pdf/2408.00294v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00290v1","updated":"2024-08-01T05:24:20Z","published":"2024-08-01T05:24:20Z","title":"Multi-Modal Parameter-Efficient Fine-tuning via Graph Neural Network","summary":"  With the advent of the era of foundation models, pre-training and fine-tuning\nhave become common paradigms. Recently, parameter-efficient fine-tuning has\ngarnered widespread attention due to its better balance between the number of\nlearnable parameters and performance. However, some current parameter-efficient\nfine-tuning methods only model a single modality and lack the utilization of\nstructural knowledge in downstream tasks. To address this issue, this paper\nproposes a multi-modal parameter-efficient fine-tuning method based on graph\nnetworks. Each image is fed into a multi-modal large language model (MLLM) to\ngenerate a text description. The image and its corresponding text description\nare then processed by a frozen image encoder and text encoder to generate image\nfeatures and text features, respectively. A graph is constructed based on the\nsimilarity of the multi-modal feature nodes, and knowledge and relationships\nrelevant to these features are extracted from each node. Additionally, Elastic\nWeight Consolidation (EWC) regularization is incorporated into the loss\nfunction to mitigate the problem of forgetting during task learning. The\nproposed model achieves test accuracies on the OxfordPets, Flowers102, and\nFood101 datasets that improve by 4.45%, 2.92%, and 0.23%, respectively. The\ncode is available at https://github.com/yunche0/GA-Net/tree/master.\n","authors":["Bin Cheng","Jiaxuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.00290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00288v1","updated":"2024-08-01T05:22:41Z","published":"2024-08-01T05:22:41Z","title":"Gradient Harmonization in Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation (UDA) intends to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Many current methods focus\non learning feature representations that are both discriminative for\nclassification and invariant across domains by simultaneously optimizing domain\nalignment and classification tasks. However, these methods often overlook a\ncrucial challenge: the inherent conflict between these two tasks during\ngradient-based optimization. In this paper, we delve into this issue and\nintroduce two effective solutions known as Gradient Harmonization, including GH\nand GH++, to mitigate the conflict between domain alignment and classification\ntasks. GH operates by altering the gradient angle between different tasks from\nan obtuse angle to an acute angle, thus resolving the conflict and trade-offing\nthe two tasks in a coordinated manner. Yet, this would cause both tasks to\ndeviate from their original optimization directions. We thus further propose an\nimproved version, GH++, which adjusts the gradient angle between tasks from an\nobtuse angle to a vertical angle. This not only eliminates the conflict but\nalso minimizes deviation from the original gradient directions. Finally, for\noptimization convenience and efficiency, we evolve the gradient harmonization\nstrategies into a dynamically weighted loss function using an integral operator\non the harmonized gradient. Notably, GH/GH++ are orthogonal to UDA and can be\nseamlessly integrated into most existing UDA models. Theoretical insights and\nexperimental analyses demonstrate that the proposed approaches not only enhance\npopular UDA baselines but also improve recent state-of-the-art models.\n","authors":["Fuxiang Huang","Suqi Song","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00288v1.pdf","comment":"IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2408.00286v1","updated":"2024-08-01T05:04:22Z","published":"2024-08-01T05:04:22Z","title":"Diff3DETR:Agent-based Diffusion Model for Semi-supervised 3D Object\n  Detection","summary":"  3D object detection is essential for understanding 3D scenes. Contemporary\ntechniques often require extensive annotated training data, yet obtaining\npoint-wise annotations for point clouds is time-consuming and laborious. Recent\ndevelopments in semi-supervised methods seek to mitigate this problem by\nemploying a teacher-student framework to generate pseudo-labels for unlabeled\npoint clouds. However, these pseudo-labels frequently suffer from insufficient\ndiversity and inferior quality. To overcome these hurdles, we introduce an\nAgent-based Diffusion Model for Semi-supervised 3D Object Detection\n(Diff3DETR). Specifically, an agent-based object query generator is designed to\nproduce object queries that effectively adapt to dynamic scenes while striking\na balance between sampling locations and content embedding. Additionally, a\nbox-aware denoising module utilizes the DDIM denoising process and the\nlong-range attention in the transformer decoder to refine bounding boxes\nincrementally. Extensive experiments on ScanNet and SUN RGB-D datasets\ndemonstrate that Diff3DETR outperforms state-of-the-art semi-supervised 3D\nobject detection methods.\n","authors":["Jiacheng Deng","Jiahao Lu","Tianzhu Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00286v1.pdf","comment":"Accepted to ECCV2024"},{"id":"http://arxiv.org/abs/2408.00283v1","updated":"2024-08-01T04:56:13Z","published":"2024-08-01T04:56:13Z","title":"Navigating Text-to-Image Generative Bias across Indic Languages","summary":"  This research investigates biases in text-to-image (TTI) models for the Indic\nlanguages widely spoken across India. It evaluates and compares the generative\nperformance and cultural relevance of leading TTI models in these languages\nagainst their performance in English. Using the proposed IndicTTI benchmark, we\ncomprehensively assess the performance of 30 Indic languages with two\nopen-source diffusion models and two commercial generation APIs. The primary\nobjective of this benchmark is to evaluate the support for Indic languages in\nthese models and identify areas needing improvement. Given the linguistic\ndiversity of 30 languages spoken by over 1.4 billion people, this benchmark\naims to provide a detailed and insightful analysis of TTI models' effectiveness\nwithin the Indic linguistic landscape. The data and code for the IndicTTI\nbenchmark can be accessed at\nhttps://iab-rubric.org/resources/other-databases/indictti.\n","authors":["Surbhi Mittal","Arnav Sudan","Mayank Vatsa","Richa Singh","Tamar Glaser","Tal Hassner"],"pdf_url":"https://arxiv.org/pdf/2408.00283v1.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2408.00279v1","updated":"2024-08-01T04:39:36Z","published":"2024-08-01T04:39:36Z","title":"DMESA: Densely Matching Everything by Segmenting Anything","summary":"  We propose MESA and DMESA as novel feature matching methods, which utilize\nSegment Anything Model (SAM) to effectively mitigate matching redundancy. The\nkey insight of our methods is to establish implicit-semantic area matching\nprior to point matching, based on advanced image understanding of SAM. Then,\ninformative area matches with consistent internal semantic are able to undergo\ndense feature comparison, facilitating precise inside-area point matching.\nSpecifically, MESA adopts a sparse matching framework and first obtains\ncandidate areas from SAM results through a novel Area Graph (AG). Then, area\nmatching among the candidates is formulated as graph energy minimization and\nsolved by graphical models derived from AG. To address the efficiency issue of\nMESA, we further propose DMESA as its dense counterpart, applying a dense\nmatching framework. After candidate areas are identified by AG, DMESA\nestablishes area matches through generating dense matching distributions. The\ndistributions are produced from off-the-shelf patch matching utilizing the\nGaussian Mixture Model and refined via the Expectation Maximization. With less\nrepetitive computation, DMESA showcases a speed improvement of nearly five\ntimes compared to MESA, while maintaining competitive accuracy. Our methods are\nextensively evaluated on five datasets encompassing indoor and outdoor scenes.\nThe results illustrate consistent performance improvements from our methods for\nfive distinct point matching baselines across all datasets. Furthermore, our\nmethods exhibit promise generalization and improved robustness against image\nresolution variations. The code is publicly available at\nhttps://github.com/Easonyesheng/A2PM-MESA.\n","authors":["Yesheng Zhang","Xu Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.00279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00273v1","updated":"2024-08-01T04:27:10Z","published":"2024-08-01T04:27:10Z","title":"3D U-KAN Implementation for Multi-modal MRI Brain Tumor Segmentation","summary":"  We explore the application of U-KAN, a U-Net based network enhanced with\nKolmogorov-Arnold Network (KAN) layers, for 3D brain tumor segmentation using\nmulti-modal MRI data. We adapt the original 2D U-KAN model to the 3D task, and\nintroduce a variant called UKAN-SE, which incorporates Squeeze-and-Excitation\nmodules for global attention. We compare the performance of U-KAN and UKAN-SE\nagainst existing methods such as U-Net, Attention U-Net, and Swin UNETR, using\nthe BraTS 2024 dataset. Our results show that U-KAN and UKAN-SE, with\napproximately 10.6 million parameters, achieve exceptional efficiency,\nrequiring only about 1/4 of the training time of U-Net and Attention U-Net, and\n1/6 that of Swin UNETR, while surpassing these models across most evaluation\nmetrics. Notably, UKAN-SE slightly outperforms U-KAN.\n","authors":["Tianze Tang","Yanbing Chen","Hai Shu"],"pdf_url":"https://arxiv.org/pdf/2408.00273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00258v1","updated":"2024-08-01T03:31:45Z","published":"2024-08-01T03:31:45Z","title":"Improving Image De-raining Using Reference-Guided Transformers","summary":"  Image de-raining is a critical task in computer vision to improve visibility\nand enhance the robustness of outdoor vision systems. While recent advances in\nde-raining methods have achieved remarkable performance, the challenge remains\nto produce high-quality and visually pleasing de-rained results. In this paper,\nwe present a reference-guided de-raining filter, a transformer network that\nenhances de-raining results using a reference clean image as guidance. We\nleverage the capabilities of the proposed module to further refine the images\nde-rained by existing methods. We validate our method on three datasets and\nshow that our module can improve the performance of existing prior-based,\nCNN-based, and transformer-based approaches.\n","authors":["Zihao Ye","Jaehoon Cho","Changjae Oh"],"pdf_url":"https://arxiv.org/pdf/2408.00258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00255v1","updated":"2024-08-01T03:27:48Z","published":"2024-08-01T03:27:48Z","title":"Revocable Backdoor for Deep Model Trading","summary":"  Deep models are being applied in numerous fields and have become a new\nimportant digital product. Meanwhile, previous studies have shown that deep\nmodels are vulnerable to backdoor attacks, in which compromised models return\nattacker-desired results when a trigger appears. Backdoor attacks severely\nbreak the trust-worthiness of deep models. In this paper, we turn this weakness\nof deep models into a strength, and propose a novel revocable backdoor and deep\nmodel trading scenario. Specifically, we aim to compromise deep models without\ndegrading their performance, meanwhile, we can easily detoxify poisoned models\nwithout re-training the models. We design specific mask matrices to manage the\ninternal feature maps of the models. These mask matrices can be used to\ndeactivate the backdoors. The revocable backdoor can be adopted in the deep\nmodel trading scenario. Sellers train models with revocable backdoors as a\ntrial version. Buyers pay a deposit to sellers and obtain a trial version of\nthe deep model. If buyers are satisfied with the trial version, they pay a\nfinal payment to sellers and sellers send mask matrices to buyers to withdraw\nrevocable backdoors. We demonstrate the feasibility and robustness of our\nrevocable backdoor by various datasets and network architectures.\n","authors":["Yiran Xu","Nan Zhong","Zhenxing Qian","Xinpeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00255v1.pdf","comment":"to appear in ECAI 2024"},{"id":"http://arxiv.org/abs/2408.00254v1","updated":"2024-08-01T03:26:50Z","published":"2024-08-01T03:26:50Z","title":"LoopSparseGS: Loop Based Sparse-View Friendly Gaussian Splatting","summary":"  Despite the photorealistic novel view synthesis (NVS) performance achieved by\nthe original 3D Gaussian splatting (3DGS), its rendering quality significantly\ndegrades with sparse input views. This performance drop is mainly caused by the\nlimited number of initial points generated from the sparse input, insufficient\nsupervision during the training process, and inadequate regularization of the\noversized Gaussian ellipsoids. To handle these issues, we propose the\nLoopSparseGS, a loop-based 3DGS framework for the sparse novel view synthesis\ntask. In specific, we propose a loop-based Progressive Gaussian Initialization\n(PGI) strategy that could iteratively densify the initialized point cloud using\nthe rendered pseudo images during the training process. Then, the sparse and\nreliable depth from the Structure from Motion, and the window-based dense\nmonocular depth are leveraged to provide precise geometric supervision via the\nproposed Depth-alignment Regularization (DAR). Additionally, we introduce a\nnovel Sparse-friendly Sampling (SFS) strategy to handle oversized Gaussian\nellipsoids leading to large pixel errors. Comprehensive experiments on four\ndatasets demonstrate that LoopSparseGS outperforms existing state-of-the-art\nmethods for sparse-input novel view synthesis, across indoor, outdoor, and\nobject-level scenes with various image resolutions.\n","authors":["Zhenyu Bao","Guibiao Liao","Kaichen Zhou","Kanglin Liu","Qing Li","Guoping Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.00254v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.00249v1","updated":"2024-08-01T03:06:56Z","published":"2024-08-01T03:06:56Z","title":"Task-Adapter: Task-specific Adaptation of Image Models for Few-shot\n  Action Recognition","summary":"  Existing works in few-shot action recognition mostly fine-tune a pre-trained\nimage model and design sophisticated temporal alignment modules at feature\nlevel. However, simply fully fine-tuning the pre-trained model could cause\noverfitting due to the scarcity of video samples. Additionally, we argue that\nthe exploration of task-specific information is insufficient when relying\nsolely on well extracted abstract features. In this work, we propose a simple\nbut effective task-specific adaptation method (Task-Adapter) for few-shot\naction recognition. By introducing the proposed Task-Adapter into the last\nseveral layers of the backbone and keeping the parameters of the original\npre-trained model frozen, we mitigate the overfitting problem caused by full\nfine-tuning and advance the task-specific mechanism into the process of feature\nextraction. In each Task-Adapter, we reuse the frozen self-attention layer to\nperform task-specific self-attention across different videos within the given\ntask to capture both distinctive information among classes and shared\ninformation within classes, which facilitates task-specific adaptation and\nenhances subsequent metric measurement between the query feature and support\nprototypes. Experimental results consistently demonstrate the effectiveness of\nour proposed Task-Adapter on four standard few-shot action recognition\ndatasets. Especially on temporal challenging SSv2 dataset, our method\noutperforms the state-of-the-art methods by a large margin.\n","authors":["Congqi Cao","Yueran Zhang","Yating Yu","Qinyi Lv","Lingtong Min","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00249v1.pdf","comment":"Accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2408.00221v1","updated":"2024-08-01T01:23:02Z","published":"2024-08-01T01:23:02Z","title":"multiGradICON: A Foundation Model for Multimodal Medical Image\n  Registration","summary":"  Modern medical image registration approaches predict deformations using deep\nnetworks. These approaches achieve state-of-the-art (SOTA) registration\naccuracy and are generally fast. However, deep learning (DL) approaches are, in\ncontrast to conventional non-deep-learning-based approaches, anatomy-specific.\nRecently, a universal deep registration approach, uniGradICON, has been\nproposed. However, uniGradICON focuses on monomodal image registration. In this\nwork, we therefore develop multiGradICON as a first step towards universal\n*multimodal* medical image registration. Specifically, we show that 1) we can\ntrain a DL registration model that is suitable for monomodal *and* multimodal\nregistration; 2) loss function randomization can increase multimodal\nregistration accuracy; and 3) training a model with multimodal data helps\nmultimodal generalization. Our code and the multiGradICON model are available\nat https://github.com/uncbiag/uniGradICON.\n","authors":["Basar Demir","Lin Tian","Thomas Hastings Greer","Roland Kwitt","Francois-Xavier Vialard","Raul San Jose Estepar","Sylvain Bouix","Richard Jarrett Rushmore","Ebrahim Ebrahim","Marc Niethammer"],"pdf_url":"https://arxiv.org/pdf/2408.00221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00210v1","updated":"2024-08-01T00:40:17Z","published":"2024-08-01T00:40:17Z","title":"A Prior Embedding-Driven Architecture for Long Distance Blind Iris\n  Recognition","summary":"  Blind iris images, which result from unknown degradation during the process\nof iris recognition at long distances, often lead to decreased iris recognition\nrates. Currently, little existing literature offers a solution to this problem.\nIn response, we propose a prior embedding-driven architecture for long distance\nblind iris recognition. We first proposed a blind iris image restoration\nnetwork called Iris-PPRGAN. To effectively restore the texture of the blind\niris, Iris-PPRGAN includes a Generative Adversarial Network (GAN) used as a\nPrior Decoder, and a DNN used as the encoder. To extract iris features more\nefficiently, we then proposed a robust iris classifier by modifying the\nbottleneck module of InsightFace, which called Insight-Iris. A low-quality\nblind iris image is first restored by Iris-PPRGAN, then the restored iris image\nundergoes recognition via Insight-Iris. Experimental results on the public\nCASIA-Iris-distance dataset demonstrate that our proposed method significantly\nsuperior results to state-of-the-art blind iris restoration methods both\nquantitatively and qualitatively, Specifically, the recognition rate for\nlong-distance blind iris images reaches 90% after processing with our methods,\nrepresenting an improvement of approximately ten percentage points compared to\nimages without restoration.\n","authors":["Qi Xiong","Xinman Zhang","Jun Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00210v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00203v1","updated":"2024-08-01T00:00:43Z","published":"2024-08-01T00:00:43Z","title":"OmniParser for Pure Vision Based GUI Agent","summary":"  The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.\n","authors":["Yadong Lu","Jianwei Yang","Yelong Shen","Ahmed Awadallah"],"pdf_url":"https://arxiv.org/pdf/2408.00203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00950v1","updated":"2024-08-01T23:11:03Z","published":"2024-08-01T23:11:03Z","title":"PrivateGaze: Preserving User Privacy in Black-box Mobile Gaze Tracking\n  Services","summary":"  Eye gaze contains rich information about human attention and cognitive\nprocesses. This capability makes the underlying technology, known as gaze\ntracking, a critical enabler for many ubiquitous applications and has triggered\nthe development of easy-to-use gaze estimation services. Indeed, by utilizing\nthe ubiquitous cameras on tablets and smartphones, users can readily access\nmany gaze estimation services. In using these services, users must provide\ntheir full-face images to the gaze estimator, which is often a black box. This\nposes significant privacy threats to the users, especially when a malicious\nservice provider gathers a large collection of face images to classify\nsensitive user attributes. In this work, we present PrivateGaze, the first\napproach that can effectively preserve users' privacy in black-box gaze\ntracking services without compromising gaze estimation performance.\nSpecifically, we proposed a novel framework to train a privacy preserver that\nconverts full-face images into obfuscated counterparts, which are effective for\ngaze estimation while containing no privacy information. Evaluation on four\ndatasets shows that the obfuscated image can protect users' private\ninformation, such as identity and gender, against unauthorized attribute\nclassification. Meanwhile, when used directly by the black-box gaze estimator\nas inputs, the obfuscated images lead to comparable tracking performance to the\nconventional, unprotected full-face images.\n","authors":["Lingyu Du","Jinyuan Jia","Xucong Zhang","Guohao Lan"],"pdf_url":"https://arxiv.org/pdf/2408.00950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00943v1","updated":"2024-08-01T22:25:06Z","published":"2024-08-01T22:25:06Z","title":"Data-Driven Traffic Simulation for an Intersection in a Metropolis","summary":"  We present a novel data-driven simulation environment for modeling traffic in\nmetropolitan street intersections. Using real-world tracking data collected\nover an extended period of time, we train trajectory forecasting models to\nlearn agent interactions and environmental constraints that are difficult to\ncapture conventionally. Trajectories of new agents are first coarsely generated\nby sampling from the spatial and temporal generative distributions, then\nrefined using state-of-the-art trajectory forecasting models. The simulation\ncan run either autonomously, or under explicit human control conditioned on the\ngenerative distributions. We present the experiments for a variety of model\nconfigurations. Under an iterative prediction scheme, the way-point-supervised\nTrajNet++ model obtained 0.36 Final Displacement Error (FDE) in 20 FPS on an\nNVIDIA A100 GPU.\n","authors":["Chengbo Zang","Mehmet Kerem Turkcan","Gil Zussman","Javad Ghaderi","Zoran Kostic"],"pdf_url":"https://arxiv.org/pdf/2408.00943v1.pdf","comment":"CVPR 2024 Workshop POETS Oral"},{"id":"http://arxiv.org/abs/2408.00940v1","updated":"2024-08-01T22:08:52Z","published":"2024-08-01T22:08:52Z","title":"A dual-task mutual learning framework for predicting post-thrombectomy\n  cerebral hemorrhage","summary":"  Ischemic stroke is a severe condition caused by the blockage of brain blood\nvessels, and can lead to the death of brain tissue due to oxygen deprivation.\nThrombectomy has become a common treatment choice for ischemic stroke due to\nits immediate effectiveness. But, it carries the risk of postoperative cerebral\nhemorrhage. Clinically, multiple CT scans within 0-72 hours post-surgery are\nused to monitor for hemorrhage. However, this approach exposes radiation dose\nto patients, and may delay the detection of cerebral hemorrhage. To address\nthis dilemma, we propose a novel prediction framework for measuring\npostoperative cerebral hemorrhage using only the patient's initial CT scan.\nSpecifically, we introduce a dual-task mutual learning framework to takes the\ninitial CT scan as input and simultaneously estimates both the follow-up CT\nscan and prognostic label to predict the occurrence of postoperative cerebral\nhemorrhage. Our proposed framework incorporates two attention mechanisms, i.e.,\nself-attention and interactive attention. Specifically, the self-attention\nmechanism allows the model to focus more on high-density areas in the image,\nwhich are critical for diagnosis (i.e., potential hemorrhage areas). The\ninteractive attention mechanism further models the dependencies between the\ninterrelated generation and classification tasks, enabling both tasks to\nperform better than the case when conducted individually. Validated on clinical\ndata, our method can generate follow-up CT scans better than state-of-the-art\nmethods, and achieves an accuracy of 86.37% in predicting follow-up prognostic\nlabels. Thus, our work thus contributes to the timely screening of\npost-thrombectomy cerebral hemorrhage, and could significantly reform the\nclinical process of thrombectomy and other similar operations related to\nstroke.\n","authors":["Caiwen Jiang","Tianyu Wang","Xiaodan Xing","Mianxin Liu","Guang Yang","Zhongxiang Ding","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00938v1","updated":"2024-08-01T22:01:42Z","published":"2024-08-01T22:01:42Z","title":"CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting\n  Idiopathic Pulmonary Fibrosis Progression","summary":"  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.\n","authors":["Caiwen Jiang","Xiaodan Xing","Zaixin Ou","Mianxin Liu","Walsh Simon","Guang Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00938v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00932v1","updated":"2024-08-01T21:50:23Z","published":"2024-08-01T21:50:23Z","title":"Towards Zero-Shot Annotation of the Built Environment with\n  Vision-Language Models (Vision Paper)","summary":"  Equitable urban transportation applications require high-fidelity digital\nrepresentations of the built environment: not just streets and sidewalks, but\nbike lanes, marked and unmarked crossings, curb ramps and cuts, obstructions,\ntraffic signals, signage, street markings, potholes, and more. Direct\ninspections and manual annotations are prohibitively expensive at scale.\nConventional machine learning methods require substantial annotated training\ndata for adequate performance. In this paper, we consider vision language\nmodels as a mechanism for annotating diverse urban features from satellite\nimages, reducing the dependence on human annotation to produce large training\nsets. While these models have achieved impressive results in describing common\nobjects in images captured from a human perspective, their training sets are\nless likely to include strong signals for esoteric features in the built\nenvironment, and their performance in these settings is therefore unclear. We\ndemonstrate proof-of-concept combining a state-of-the-art vision language model\nand variants of a prompting strategy that asks the model to consider segmented\nelements independently of the original image. Experiments on two urban features\n-- stop lines and raised tables -- show that while direct zero-shot prompting\ncorrectly annotates nearly zero images, the pre-segmentation strategies can\nannotate images with near 40% intersection-over-union accuracy. We describe how\nthese results inform a new research agenda in automatic annotation of the built\nenvironment to improve equity, accessibility, and safety at broad scale and in\ndiverse environments.\n","authors":["Bin Han","Yiwei Yang","Anat Caspi","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.00932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.13651v4","updated":"2024-08-01T21:49:31Z","published":"2023-08-25T19:40:56Z","title":"PCNN: Probable-Class Nearest-Neighbor Explanations Improve Fine-Grained\n  Image Classification Accuracy for AIs and Humans","summary":"  Nearest neighbors (NN) are traditionally used to compute final decisions,\ne.g., in Support Vector Machines or k-NN classifiers, and to provide users with\nexplanations for the model's decision. In this paper, we show a novel utility\nof nearest neighbors: To improve predictions of a frozen, pretrained classifier\nC. We leverage an image comparator S that (1) compares the input image with NN\nimages from the top-K most probable classes; and (2) uses S' output scores to\nweight the confidence scores of C. Our method consistently improves\nfine-grained image classification accuracy on CUB-200, Cars-196, and Dogs-120.\nAlso, a human study finds that showing lay users our probable-class nearest\nneighbors (PCNN) reduces over-reliance on AI, thus improving their decision\naccuracy over prior work which only shows only the top-1 class examples.\n","authors":["Giang Nguyen","Valerie Chen","Mohammad Reza Taesiri","Anh Totti Nguyen"],"pdf_url":"https://arxiv.org/pdf/2308.13651v4.pdf","comment":"Accepted to Transaction of Machine Learning Research"},{"id":"http://arxiv.org/abs/2408.00923v1","updated":"2024-08-01T21:27:31Z","published":"2024-08-01T21:27:31Z","title":"Reclaiming Residual Knowledge: A Novel Paradigm to Low-Bit Quantization","summary":"  This paper explores a novel paradigm in low-bit (i.e. 4-bits or lower)\nquantization, differing from existing state-of-the-art methods, by framing\noptimal quantization as an architecture search problem within convolutional\nneural networks (ConvNets). Our framework, dubbed \\textbf{CoRa} (Optimal\nQuantization Residual \\textbf{Co}nvolutional Operator Low-\\textbf{Ra}nk\nAdaptation), is motivated by two key aspects. Firstly, quantization residual\nknowledge, i.e. the lost information between floating-point weights and\nquantized weights, has long been neglected by the research community.\nReclaiming the critical residual knowledge, with an infinitesimal extra\nparameter cost, can reverse performance degradation without training. Secondly,\nstate-of-the-art quantization frameworks search for optimal quantized weights\nto address the performance degradation. Yet, the vast search spaces in weight\noptimization pose a challenge for the efficient optimization in large models.\nFor example, state-of-the-art BRECQ necessitates $2 \\times 10^4$ iterations to\nquantize models. Fundamentally differing from existing methods, \\textbf{CoRa}\nsearches for the optimal architectures of low-rank adapters, reclaiming\ncritical quantization residual knowledge, within the search spaces smaller\ncompared to the weight spaces, by many orders of magnitude. The low-rank\nadapters approximate the quantization residual weights, discarded in previous\nmethods. We evaluate our approach over multiple pre-trained ConvNets on\nImageNet. \\textbf{CoRa} achieves comparable performance against both\nstate-of-the-art quantization-aware training and post-training quantization\nbaselines, in $4$-bit and $3$-bit quantization, by using less than $250$\niterations on a small calibration set with $1600$ images. Thus, \\textbf{CoRa}\nestablishes a new state-of-the-art in terms of the optimization efficiency in\nlow-bit quantization.\n","authors":["Risn Luo","Alexandru Drimbarean","James McDermott","Colm O'Riordan"],"pdf_url":"https://arxiv.org/pdf/2408.00923v1.pdf","comment":"Accepted by The 35th British Machine Vision Conference (BMVC 2024)"},{"id":"http://arxiv.org/abs/2406.02529v2","updated":"2024-08-01T20:53:09Z","published":"2024-06-04T17:51:08Z","title":"ReLUs Are Sufficient for Learning Implicit Neural Representations","summary":"  Motivated by the growing theoretical understanding of neural networks that\nemploy the Rectified Linear Unit (ReLU) as their activation function, we\nrevisit the use of ReLU activation functions for learning implicit neural\nrepresentations (INRs). Inspired by second order B-spline wavelets, we\nincorporate a set of simple constraints to the ReLU neurons in each layer of a\ndeep neural network (DNN) to remedy the spectral bias. This in turn enables its\nuse for various INR tasks. Empirically, we demonstrate that, contrary to\npopular belief, one can learn state-of-the-art INRs based on a DNN composed of\nonly ReLU neurons. Next, by leveraging recent theoretical works which\ncharacterize the kinds of functions ReLU neural networks learn, we provide a\nway to quantify the regularity of the learned function. This offers a\nprincipled approach to selecting the hyperparameters in INR architectures. We\nsubstantiate our claims through experiments in signal representation, super\nresolution, and computed tomography, demonstrating the versatility and\neffectiveness of our method. The code for all experiments can be found at\nhttps://github.com/joeshenouda/relu-inrs.\n","authors":["Joseph Shenouda","Yamin Zhou","Robert D. Nowak"],"pdf_url":"https://arxiv.org/pdf/2406.02529v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2403.14837v2","updated":"2024-08-01T20:01:58Z","published":"2024-03-21T21:13:53Z","title":"Osmosis: RGBD Diffusion Prior for Underwater Image Restoration","summary":"  Underwater image restoration is a challenging task because of water effects\nthat increase dramatically with distance. This is worsened by lack of ground\ntruth data of clean scenes without water. Diffusion priors have emerged as\nstrong image restoration priors. However, they are often trained with a dataset\nof the desired restored output, which is not available in our case. We also\nobserve that using only color data is insufficient, and therefore augment the\nprior with a depth channel. We train an unconditional diffusion model prior on\nthe joint space of color and depth, using standard RGBD datasets of natural\noutdoor scenes in air. Using this prior together with a novel guidance method\nbased on the underwater image formation model, we generate posterior samples of\nclean images, removing the water effects. Even though our prior did not see any\nunderwater images during training, our method outperforms state-of-the-art\nbaselines for image restoration on very challenging scenes. Our code, models\nand data are available on the project website.\n","authors":["Opher Bar Nathan","Deborah Levy","Tali Treibitz","Dan Rosenbaum"],"pdf_url":"https://arxiv.org/pdf/2403.14837v2.pdf","comment":"ECCV 2024. Project page with results and code:\n  https://osmosis-diffusion.github.io/"},{"id":"http://arxiv.org/abs/2408.00891v1","updated":"2024-08-01T20:00:18Z","published":"2024-08-01T20:00:18Z","title":"Temporal Evolution of Knee Osteoarthritis: A Diffusion-based Morphing\n  Model for X-ray Medical Image Synthesis","summary":"  Knee Osteoarthritis (KOA) is a common musculoskeletal disorder that\nsignificantly affects the mobility of older adults. In the medical domain,\nimages containing temporal data are frequently utilized to study temporal\ndynamics and statistically monitor disease progression. While deep\nlearning-based generative models for natural images have been widely\nresearched, there are comparatively few methods available for synthesizing\ntemporal knee X-rays. In this work, we introduce a novel deep-learning model\ndesigned to synthesize intermediate X-ray images between a specific patient's\nhealthy knee and severe KOA stages. During the testing phase, based on a\nhealthy knee X-ray, the proposed model can produce a continuous and effective\nsequence of KOA X-ray images with varying degrees of severity. Specifically, we\nintroduce a Diffusion-based Morphing Model by modifying the Denoising Diffusion\nProbabilistic Model. Our approach integrates diffusion and morphing modules,\nenabling the model to capture spatial morphing details between source and\ntarget knee X-ray images and synthesize intermediate frames along a geodesic\npath. A hybrid loss consisting of diffusion loss, morphing loss, and\nsupervision loss was employed. We demonstrate that our proposed approach\nachieves the highest temporal frame synthesis performance, effectively\naugmenting data for classification models and simulating the progression of\nKOA.\n","authors":["Zhe Wang","Aladine Chetouani","Rachid Jennane","Yuhua Ru","Wasim Issa","Mohamed Jarraya"],"pdf_url":"https://arxiv.org/pdf/2408.00891v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.04943v2","updated":"2024-08-01T18:55:16Z","published":"2024-03-07T23:18:34Z","title":"AFreeCA: Annotation-Free Counting for All","summary":"  Object counting methods typically rely on manually annotated datasets. The\ncost of creating such datasets has restricted the versatility of these networks\nto count objects from specific classes (such as humans or penguins), and\ncounting objects from diverse categories remains a challenge. The availability\nof robust text-to-image latent diffusion models (LDMs) raises the question of\nwhether these models can be utilized to generate counting datasets. However,\nLDMs struggle to create images with an exact number of objects based solely on\ntext prompts but they can be used to offer a dependable \\textit{sorting} signal\nby adding and removing objects within an image. Leveraging this data, we\ninitially introduce an unsupervised sorting methodology to learn object-related\nfeatures that are subsequently refined and anchored for counting purposes using\ncounting data generated by LDMs. Further, we present a density\nclassifier-guided method for dividing an image into patches containing objects\nthat can be reliably counted. Consequently, we can generate counting data for\nany type of object and count them in an unsupervised manner. Our approach\noutperforms other unsupervised and few-shot alternatives and is not restricted\nto specific object classes for which counting data is available. Code to be\nreleased upon acceptance.\n","authors":["Adriano D'Alessandro","Ali Mahdavi-Amiri","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2403.04943v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.04421v2","updated":"2024-08-01T18:52:31Z","published":"2023-09-08T16:32:56Z","title":"SynthoGestures: A Novel Framework for Synthetic Dynamic Hand Gesture\n  Generation for Driving Scenarios","summary":"  Creating a diverse and comprehensive dataset of hand gestures for dynamic\nhuman-machine interfaces in the automotive domain can be challenging and\ntime-consuming. To overcome this challenge, we propose using synthetic gesture\ndatasets generated by virtual 3D models. Our framework utilizes Unreal Engine\nto synthesize realistic hand gestures, offering customization options and\nreducing the risk of overfitting. Multiple variants, including gesture speed,\nperformance, and hand shape, are generated to improve generalizability. In\naddition, we simulate different camera locations and types, such as RGB,\ninfrared, and depth cameras, without incurring additional time and cost to\nobtain these cameras. Experimental results demonstrate that our proposed\nframework, SynthoGestures (https://github.com/amrgomaaelhady/SynthoGestures),\nimproves gesture recognition accuracy and can replace or augment real-hand\ndatasets. By saving time and effort in the creation of the data set, our tool\naccelerates the development of gesture recognition systems for automotive\napplications.\n","authors":["Amr Gomaa","Robin Zitt","Guillermo Reyes","Antonio Krger"],"pdf_url":"https://arxiv.org/pdf/2309.04421v2.pdf","comment":"Accepted at IEEE IV'24. Shorter versions were accepted as\n  AutomotiveUI2023 Work in Progress and UIST2023 Poster Papers"},{"id":"http://arxiv.org/abs/2408.00874v1","updated":"2024-08-01T18:49:45Z","published":"2024-08-01T18:49:45Z","title":"Medical SAM 2: Segment medical images as video via Segment Anything\n  Model 2","summary":"  In this paper, we introduce Medical SAM 2 (MedSAM-2), an advanced\nsegmentation model that utilizes the SAM 2 framework to address both 2D and 3D\nmedical image segmentation tasks. By adopting the philosophy of taking medical\nimages as videos, MedSAM-2 not only applies to 3D medical images but also\nunlocks new One-prompt Segmentation capability. That allows users to provide a\nprompt for just one or a specific image targeting an object, after which the\nmodel can autonomously segment the same type of object in all subsequent\nimages, regardless of temporal relationships between the images. We evaluated\nMedSAM-2 across a variety of medical imaging modalities, including abdominal\norgans, optic discs, brain tumors, thyroid nodules, and skin lesions, comparing\nit against state-of-the-art models in both traditional and interactive\nsegmentation settings. Our findings show that MedSAM-2 not only surpasses\nexisting models in performance but also exhibits superior generalization across\na range of medical image segmentation tasks. Our code will be released at:\nhttps://github.com/MedicineToken/Medical-SAM2\n","authors":["Jiayuan Zhu","Yunli Qi","Junde Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10280v2","updated":"2024-08-01T18:49:11Z","published":"2023-03-17T23:23:55Z","title":"Synthetic-to-Real Domain Adaptation for Action Recognition: A Dataset\n  and Baseline Performances","summary":"  Human action recognition is a challenging problem, particularly when there is\nhigh variability in factors such as subject appearance, backgrounds and\nviewpoint. While deep neural networks (DNNs) have been shown to perform well on\naction recognition tasks, they typically require large amounts of high-quality\nlabeled data to achieve robust performance across a variety of conditions.\nSynthetic data has shown promise as a way to avoid the substantial costs and\npotential ethical concerns associated with collecting and labeling enormous\namounts of data in the real-world. However, synthetic data may differ from real\ndata in important ways. This phenomenon, known as \\textit{domain shift}, can\nlimit the utility of synthetic data in robotics applications. To mitigate the\neffects of domain shift, substantial effort is being dedicated to the\ndevelopment of domain adaptation (DA) techniques. Yet, much remains to be\nunderstood about how best to develop these techniques. In this paper, we\nintroduce a new dataset called Robot Control Gestures (RoCoG-v2). The dataset\nis composed of both real and synthetic videos from seven gesture classes, and\nis intended to support the study of synthetic-to-real domain shift for\nvideo-based action recognition. Our work expands upon existing datasets by\nfocusing the action classes on gestures for human-robot teaming, as well as by\nenabling investigation of domain shift in both ground and aerial views. We\npresent baseline results using state-of-the-art action recognition and domain\nadaptation algorithms and offer initial insight on tackling the\nsynthetic-to-real and ground-to-air domain shifts.\n","authors":["Arun V. Reddy","Ketul Shah","William Paul","Rohita Mocharla","Judy Hoffman","Kapil D. Katyal","Dinesh Manocha","Celso M. de Melo","Rama Chellappa"],"pdf_url":"https://arxiv.org/pdf/2303.10280v2.pdf","comment":"ICRA 2023. The first two authors contributed equally. Dataset\n  available at: https://github.com/reddyav1/RoCoG-v2"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2406.10251v3","updated":"2024-08-01T16:27:20Z","published":"2024-06-10T08:23:52Z","title":"The Impact of Quantization on Retrieval-Augmented Generation: An\n  Analysis of Small LLMs","summary":"  Post-training quantization reduces the computational demand of Large Language\nModels (LLMs) but can weaken some of their capabilities. Since LLM abilities\nemerge with scale, smaller LLMs are more sensitive to quantization. In this\npaper, we explore how quantization affects smaller LLMs' ability to perform\nretrieval-augmented generation (RAG), specifically in longer contexts. We chose\npersonalization for evaluation because it is a challenging domain to perform\nusing RAG as it requires long-context reasoning over multiple documents. We\ncompare the original FP16 and the quantized INT4 performance of multiple 7B and\n8B LLMs on two tasks while progressively increasing the number of retrieved\ndocuments to test how quantized models fare against longer contexts. To better\nunderstand the effect of retrieval, we evaluate three retrieval models in our\nexperiments. Our findings reveal that if a 7B LLM performs the task well,\nquantization does not impair its performance and long-context reasoning\ncapabilities. We conclude that it is possible to utilize RAG with quantized\nsmaller LLMs.\n","authors":["Mert Yazan","Suzan Verberne","Frederik Situmeang"],"pdf_url":"https://arxiv.org/pdf/2406.10251v3.pdf","comment":"Accepted to the IR-RAG Workshop at SIGIR 2024"},{"id":"http://arxiv.org/abs/2406.12925v2","updated":"2024-08-01T10:09:15Z","published":"2024-06-14T13:54:29Z","title":"GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks","summary":"  Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.\n","authors":["Ihor Stepanov","Mykhailo Shtopko"],"pdf_url":"https://arxiv.org/pdf/2406.12925v2.pdf","comment":"11 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2404.17591v2","updated":"2024-08-01T08:54:15Z","published":"2024-04-19T13:28:36Z","title":"Large Language Models for Next Point-of-Interest Recommendation","summary":"  The next Point of Interest (POI) recommendation task is to predict users'\nimmediate next POI visit given their historical data. Location-Based Social\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\ncomes with challenges. One frequently disregarded challenge is how to\neffectively use the abundant contextual information present in LBSN data.\nPrevious methods are limited by their numerical nature and fail to address this\nchallenge. In this paper, we propose a framework that uses pretrained Large\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\npreserve heterogeneous LBSN data in its original format, hence avoiding the\nloss of contextual information. Furthermore, our framework is capable of\ncomprehending the inherent meaning of contextual information due to the\ninclusion of commonsense knowledge. In experiments, we test our framework on\nthree real-world LBSN datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our analysis\ndemonstrates the effectiveness of the proposed framework in using contextual\ninformation as well as alleviating the commonly encountered cold-start and\nshort trajectory problems.\n","authors":["Peibo Li","Maarten de Rijke","Hao Xue","Shuang Ao","Yang Song","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2404.17591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00072v4","updated":"2024-08-01T06:56:15Z","published":"2024-06-21T08:52:11Z","title":"Pistis-RAG: A Scalable Cascading Framework Towards Trustworthy\n  Retrieval-Augmented Generation","summary":"  In Greek mythology, Pistis symbolized good faith, trust, and reliability.\nDrawing inspiration from these principles, Pistis-RAG is a scalable multi-stage\nframework designed to address the challenges of large-scale retrieval-augmented\ngeneration (RAG) systems. This framework consists of distinct stages: matching,\npre-ranking, ranking, reasoning, and aggregating. Each stage contributes to\nnarrowing the search space, prioritizing semantically relevant documents,\naligning with the large language model's (LLM) preferences, supporting complex\nchain-of-thought (CoT) methods, and combining information from multiple\nsources.\n  Our ranking stage introduces a significant innovation by recognizing that\nsemantic relevance alone may not lead to improved generation quality, due to\nthe sensitivity of the few-shot prompt order, as noted in previous research.\nThis critical aspect is often overlooked in current RAG frameworks.\n  We argue that the alignment issue between LLMs and external knowledge ranking\nmethods is tied to the model-centric paradigm dominant in RAG systems. We\npropose a content-centric approach, emphasizing seamless integration between\nLLMs and external information sources to optimize content transformation for\nspecific tasks.\n  Our novel ranking stage is designed specifically for RAG systems,\nincorporating principles of information retrieval while considering the unique\nbusiness scenarios reflected in LLM preferences and user feedback. We simulated\nfeedback signals on the MMLU benchmark, resulting in a 9.3% performance\nimprovement. Our model and code will be open-sourced on GitHub. Additionally,\nexperiments on real-world, large-scale data validate the scalability of our\nframework.\n","authors":["Yu Bai","Yukai Miao","Li Chen","Dan Li","Yanyu Ren","Hongtao Xie","Ce Yang","Xuhui Cai"],"pdf_url":"https://arxiv.org/pdf/2407.00072v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12384v4","updated":"2024-08-01T03:32:49Z","published":"2024-03-19T02:49:32Z","title":"AlignRec: Aligning and Training in Multimodal Recommendations","summary":"  With the development of multimedia systems, multimodal recommendations are\nplaying an essential role, as they can leverage rich contexts beyond\ninteractions. Existing methods mainly regard multimodal information as an\nauxiliary, using them to help learn ID features; However, there exist semantic\ngaps among multimodal content features and ID-based features, for which\ndirectly using multimodal information as an auxiliary would lead to\nmisalignment in representations of users and items. In this paper, we first\nsystematically investigate the misalignment issue in multimodal\nrecommendations, and propose a solution named AlignRec. In AlignRec, the\nrecommendation objective is decomposed into three alignments, namely alignment\nwithin contents, alignment between content and categorical ID, and alignment\nbetween users and items. Each alignment is characterized by a specific\nobjective function and is integrated into our multimodal recommendation\nframework. To effectively train AlignRec, we propose starting from pre-training\nthe first alignment to obtain unified multimodal features and subsequently\ntraining the following two alignments together with these features as input. As\nit is essential to analyze whether each multimodal feature helps in training\nand accelerate the iteration cycle of recommendation models, we design three\nnew classes of metrics to evaluate intermediate performance. Our extensive\nexperiments on three real-world datasets consistently verify the superiority of\nAlignRec compared to nine baselines. We also find that the multimodal features\ngenerated by AlignRec are better than currently used ones, which are to be\nopen-sourced in our repository https://github.com/sjtulyf123/AlignRec_CIKM24.\n","authors":["Yifan Liu","Kangning Zhang","Xiangyuan Ren","Yanhua Huang","Jiarui Jin","Yingjie Qin","Ruilong Su","Ruiwen Xu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12384v4.pdf","comment":"9 page paper, 2 page appendix. Accepted by CIKM24"},{"id":"http://arxiv.org/abs/2404.07221v2","updated":"2024-08-01T03:02:44Z","published":"2024-03-23T00:49:40Z","title":"Improving Retrieval for RAG based Question Answering Models on Financial\n  Documents","summary":"  The effectiveness of Large Language Models (LLMs) in generating accurate\nresponses relies heavily on the quality of input provided, particularly when\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\nsignificant advancements in LLMs' response quality in recent years, users may\nstill encounter inaccuracies or irrelevant answers; these issues often stem\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\nthe RAG process. This paper explores the existing constraints of RAG pipelines\nand introduces methodologies for enhancing text retrieval. It delves into\nstrategies such as sophisticated chunking techniques, query expansion, the\nincorporation of metadata annotations, the application of re-ranking\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\napproaches can substantially improve the retrieval quality, thereby elevating\nthe overall performance and reliability of LLMs in processing and responding to\nqueries.\n","authors":["Spurthi Setty","Harsh Thakkar","Alyssa Lee","Eden Chung","Natan Vidra"],"pdf_url":"https://arxiv.org/pdf/2404.07221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00490v1","updated":"2024-08-01T11:51:52Z","published":"2024-08-01T11:51:52Z","title":"Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation","summary":"  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.\n","authors":["Chu Zhao","Enneng Yang","Yuliang Liang","Pengxiang Lan","Yuting Liu","Jianzhe Zhao","Guibing Guo","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00490v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.00473v1","updated":"2024-08-01T11:23:42Z","published":"2024-08-01T11:23:42Z","title":"Towards Explainable and Interpretable Musical Difficulty Estimation: A\n  Parameter-efficient Approach","summary":"  Estimating music piece difficulty is important for organizing educational\nmusic collections. This process could be partially automatized to facilitate\nthe educator's role. Nevertheless, the decisions performed by prevalent\ndeep-learning models are hardly understandable, which may impair the acceptance\nof such a technology in music education curricula. Our work employs explainable\ndescriptors for difficulty estimation in symbolic music representations.\nFurthermore, through a novel parameter-efficient white-box model, we outperform\nprevious efforts while delivering interpretable results. These comprehensible\noutcomes emulate the functionality of a rubric, a tool widely used in music\neducation. Our approach, evaluated in piano repertoire categorized in 9\nclasses, achieved 41.4% accuracy independently, with a mean squared error (MSE)\nof 1.7, showing precise difficulty estimation. Through our baseline, we\nillustrate how building on top of past research can offer alternatives for\nmusic difficulty assessment which are explainable and interpretable. With this,\nwe aim to promote a more effective communication between the Music Information\nRetrieval (MIR) community and the music education one.\n","authors":["Pedro Ramoneda","Vsevolod Eremenko","Alexandre D'Hooge","Emilia Parada-Cabaleiro","Xavier Serra"],"pdf_url":"https://arxiv.org/pdf/2408.00473v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00447v1","updated":"2024-08-01T10:36:00Z","published":"2024-08-01T10:36:00Z","title":"DiscipLink: Unfolding Interdisciplinary Information Seeking Process via\n  Human-AI Co-Exploration","summary":"  Interdisciplinary studies often require researchers to explore literature in\ndiverse branches of knowledge. Yet, navigating through the highly scattered\nknowledge from unfamiliar disciplines poses a significant challenge. In this\npaper, we introduce DiscipLink, a novel interactive system that facilitates\ncollaboration between researchers and large language models (LLMs) in\ninterdisciplinary information seeking (IIS). Based on users' topics of\ninterest, DiscipLink initiates exploratory questions from the perspectives of\npossible relevant fields of study, and users can further tailor these\nquestions. DiscipLink then supports users in searching and screening papers\nunder selected questions by automatically expanding queries with\ndisciplinary-specific terminologies, extracting themes from retrieved papers,\nand highlighting the connections between papers and questions. Our evaluation,\ncomprising a within-subject comparative experiment and an open-ended\nexploratory study, reveals that DiscipLink can effectively support researchers\nin breaking down disciplinary boundaries and integrating scattered knowledge in\ndiverse fields. The findings underscore the potential of LLM-powered tools in\nfostering information-seeking practices and bolstering interdisciplinary\nresearch.\n","authors":["Chengbo Zheng","Yuanhao Zhang","Zeyu Huang","Chuhan Shi","Minrui Xu","Xiaojuan Ma"],"pdf_url":"https://arxiv.org/pdf/2408.00447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00337v1","updated":"2024-08-01T07:17:10Z","published":"2024-08-01T07:17:10Z","title":"DistillGrasp: Integrating Features Correlation with Knowledge\n  Distillation for Depth Completion of Transparent Objects","summary":"  Due to the visual properties of reflection and refraction, RGB-D cameras\ncannot accurately capture the depth of transparent objects, leading to\nincomplete depth maps. To fill in the missing points, recent studies tend to\nexplore new visual features and design complex networks to reconstruct the\ndepth, however, these approaches tremendously increase computation, and the\ncorrelation of different visual features remains a problem. To this end, we\npropose an efficient depth completion network named DistillGrasp which\ndistillates knowledge from the teacher branch to the student branch.\nSpecifically, in the teacher branch, we design a position correlation block\n(PCB) that leverages RGB images as the query and key to search for the\ncorresponding values, guiding the model to establish correct correspondence\nbetween two features and transfer it to the transparent areas. For the student\nbranch, we propose a consistent feature correlation module (CFCM) that retains\nthe reliable regions of RGB images and depth maps respectively according to the\nconsistency and adopts a CNN to capture the pairwise relationship for depth\ncompletion. To avoid the student branch only learning regional features from\nthe teacher branch, we devise a distillation loss that not only considers the\ndistance loss but also the object structure and edge information. Extensive\nexperiments conducted on the ClearGrasp dataset manifest that our teacher\nnetwork outperforms state-of-the-art methods in terms of accuracy and\ngeneralization, and the student network achieves competitive results with a\nhigher speed of 48 FPS. In addition, the significant improvement in a\nreal-world robotic grasping system illustrates the effectiveness and robustness\nof our proposed system.\n","authors":["Yiheng Huang","Junhong Chen","Nick Michiels","Muhammad Asim","Luc Claesen","Wenyin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00337v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.00326v1","updated":"2024-08-01T06:55:19Z","published":"2024-08-01T06:55:19Z","title":"Exploiting Preferences in Loss Functions for Sequential Recommendation\n  via Weak Transitivity","summary":"  A choice of optimization objective is immensely pivotal in the design of a\nrecommender system as it affects the general modeling process of a user's\nintent from previous interactions. Existing approaches mainly adhere to three\ncategories of loss functions: pairwise, pointwise, and setwise loss functions.\nDespite their effectiveness, a critical and common drawback of such objectives\nis viewing the next observed item as a unique positive while considering all\nremaining items equally negative. Such a binary label assignment is generally\nlimited to assuring a higher recommendation score of the positive item,\nneglecting potential structures induced by varying preferences between other\nunobserved items. To alleviate this issue, we propose a novel method that\nextends original objectives to explicitly leverage the different levels of\npreferences as relative orders between their scores. Finally, we demonstrate\nthe superior performance of our method compared to baseline objectives.\n","authors":["Hyunsoo Chung","Jungtaek Kim","Hyungeun Jo","Hyungwon Choi"],"pdf_url":"https://arxiv.org/pdf/2408.00326v1.pdf","comment":"Accepted to CIKM 2024, Short Research Paper Track"},{"id":"http://arxiv.org/abs/2408.00312v1","updated":"2024-08-01T06:14:42Z","published":"2024-08-01T06:14:42Z","title":"Adversarial Text Rewriting for Text-aware Recommender Systems","summary":"  Text-aware recommender systems incorporate rich textual features, such as\ntitles and descriptions, to generate item recommendations for users. The use of\ntextual features helps mitigate cold-start problems, and thus, such recommender\nsystems have attracted increased attention. However, we argue that the\ndependency on item descriptions makes the recommender system vulnerable to\nmanipulation by adversarial sellers on e-commerce platforms. In this paper, we\nexplore the possibility of such manipulation by proposing a new text rewriting\nframework to attack text-aware recommender systems. We show that the rewriting\nattack can be exploited by sellers to unfairly uprank their products, even\nthough the adversarially rewritten descriptions are perceived as realistic by\nhuman evaluators. Methodologically, we investigate two different variations to\ncarry out text rewriting attacks: (1) two-phase fine-tuning for greater attack\nperformance, and (2) in-context learning for higher text rewriting quality.\nExperiments spanning 3 different datasets and 4 existing approaches demonstrate\nthat recommender systems exhibit vulnerability against the proposed text\nrewriting attack. Our work adds to the existing literature around the\nrobustness of recommender systems, while highlighting a new dimension of\nvulnerability in the age of large-scale automated text generation.\n","authors":["Sejoon Oh","Gaurav Verma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2408.00312v1.pdf","comment":"Accepted for publication at: 33rd ACM International Conference on\n  Information and Knowledge Management (CIKM 2024). Code and data at:\n  https://github.com/sejoonoh/ATR"},{"id":"http://arxiv.org/abs/2408.00305v1","updated":"2024-08-01T06:04:44Z","published":"2024-08-01T06:04:44Z","title":"Leveraging Weak Cross-Modal Guidance for Coherence Modelling via\n  Iterative Learning","summary":"  Cross-modal coherence modeling is essential for intelligent systems to help\nthem organize and structure information, thereby understanding and creating\ncontent of the physical world coherently like human-beings. Previous work on\ncross-modal coherence modeling attempted to leverage the order information from\nanother modality to assist the coherence recovering of the target modality.\nDespite of the effectiveness, labeled associated coherency information is not\nalways available and might be costly to acquire, making the cross-modal\nguidance hard to leverage. To tackle this challenge, this paper explores a new\nway to take advantage of cross-modal guidance without gold labels on coherency,\nand proposes the Weak Cross-Modal Guided Ordering (WeGO) model. More\nspecifically, it leverages high-confidence predicted pairwise order in one\nmodality as reference information to guide the coherence modeling in another.\nAn iterative learning paradigm is further designed to jointly optimize the\ncoherence modeling in two modalities with selected guidance from each other.\nThe iterative cross-modal boosting also functions in inference to further\nenhance coherence prediction in each modality. Experimental results on two\npublic datasets have demonstrated that the proposed method outperforms existing\nmethods for cross-modal coherence modeling tasks. Major technical modules have\nbeen evaluated effective through ablation studies. Codes are available at:\n\\url{https://github.com/scvready123/IterWeGO}.\n","authors":["Yi Bin","Junrong Liao","Yujuan Ding","Haoxuan Li","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00305v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.00294v1","updated":"2024-08-01T05:41:59Z","published":"2024-08-01T05:41:59Z","title":"RDP: Ranked Differential Privacy for Facial Feature Protection in\n  Multiscale Sparsified Subspace","summary":"  With the widespread sharing of personal face images in applications' public\ndatabases, face recognition systems faces real threat of being breached by\npotential adversaries who are able to access users' face images and use them to\nintrude the face recognition systems. In this paper, we propose a novel privacy\nprotection method in the multiscale sparsified feature subspaces to protect\nsensitive facial features, by taking care of the influence or weight ranked\nfeature coefficients on the privacy budget, named \"Ranked Differential Privacy\n(RDP)\". After the multiscale feature decomposition, the lightweight Laplacian\nnoise is added to the dimension-reduced sparsified feature coefficients\naccording to the geometric superposition method. Then, we rigorously prove that\nthe RDP satisfies Differential Privacy. After that, the nonlinear Lagrange\nMultiplier (LM) method is formulated for the constraint optimization problem of\nmaximizing the utility of the visualization quality protected face images with\nsanitizing noise, under a given facial features privacy budget. Then, two\nmethods are proposed to solve the nonlinear LM problem and obtain the optimal\nnoise scale parameters: 1) the analytical Normalization Approximation (NA)\nmethod with identical average noise scale parameter for real-time online\napplications; and 2) the LM optimization Gradient Descent (LMGD) numerical\nmethod to obtain the nonlinear solution through iterative updating for more\naccurate offline applications. Experimental results on two real-world datasets\nshow that our proposed RDP outperforms other state-of-the-art methods: at a\nprivacy budget of 0.2, the PSNR (Peak Signal-to-Noise Ratio) of the RDP is\nabout ~10 dB higher than (10 times as high as) the highest PSNR of all compared\nmethods.\n","authors":["Lu Ou","Shaolin Liao","Shihui Gao","Guandong Huang","Zheng Qi"],"pdf_url":"https://arxiv.org/pdf/2408.00294v1.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00247v1","updated":"2024-08-01T02:54:23Z","published":"2024-08-01T02:54:23Z","title":"Simple but Efficient: A Multi-Scenario Nearline Retrieval Framework for\n  Recommendation on Taobao","summary":"  In recommendation systems, the matching stage is becoming increasingly\ncritical, serving as the upper limit for the entire recommendation process.\nRecently, some studies have started to explore the use of multi-scenario\ninformation for recommendations, such as model-based and data-based approaches.\nHowever, the matching stage faces significant challenges due to the need for\nultra-large-scale retrieval and meeting low latency requirements. As a result,\nthe methods applied at this stage (collaborative filtering and two-tower\nmodels) are often designed to be lightweight, hindering the full utilization of\nextensive information. On the other hand, the ranking stage features the most\nsophisticated models with the strongest scoring capabilities, but due to the\nlimited screen size of mobile devices, most of the ranked results may not gain\nexposure or be displayed. In this paper, we introduce an innovative\nmulti-scenario nearline retrieval framework. It operates by harnessing ranking\nlogs from various scenarios through Flink, allowing us to incorporate finely\nranked results from other scenarios into our matching stage in near real-time.\nBesides, we propose a streaming scoring module, which selects a crucial subset\nfrom the candidate pool. Implemented on the \"Guess You Like\" (homepage of the\nTaobao APP), China's premier e-commerce platform, our method has shown\nsubstantial improvements-most notably, a 5% uptick in product transactions.\nFurthermore, the proposed approach is not only model-free but also highly\nefficient, suggesting it can be quickly implemented in diverse scenarios and\ndemonstrate promising performance.\n","authors":["Yingcai Ma","Ziyang Wang","Yuliang Yan","Jian Wu","Yuning Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.00247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00878v1","updated":"2024-08-01T19:04:10Z","published":"2024-08-01T19:04:10Z","title":"Multi-Aspect Reviewed-Item Retrieval via LLM Query Decomposition and\n  Aspect Fusion","summary":"  While user-generated product reviews often contain large quantities of\ninformation, their utility in addressing natural language product queries has\nbeen limited, with a key challenge being the need to aggregate information from\nmultiple low-level sources (reviews) to a higher item level during retrieval.\nExisting methods for reviewed-item retrieval (RIR) typically take a late fusion\n(LF) approach which computes query-item scores by simply averaging the top-K\nquery-review similarity scores for an item. However, we demonstrate that for\nmulti-aspect queries and multi-aspect items, LF is highly sensitive to the\ndistribution of aspects covered by reviews in terms of aspect frequency and the\ndegree of aspect separation across reviews. To address these LF failures, we\npropose several novel aspect fusion (AF) strategies which include Large\nLanguage Model (LLM) query extraction and generative reranking. Our experiments\nshow that for imbalanced review corpora, AF can improve over LF by a MAP@10\nincrease from 0.36 to 0.52, while achieving equivalent performance for balanced\nreview corpora.\n","authors":["Anton Korikov","George Saad","Ethan Baron","Mustafa Khan","Manav Shah","Scott Sanner"],"pdf_url":"https://arxiv.org/pdf/2408.00878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00859v1","updated":"2024-08-01T18:17:25Z","published":"2024-08-01T18:17:25Z","title":"LICM: Effective and Efficient Long Interest Chain Modeling for News\n  Recommendation","summary":"  Accurately recommending personalized candidate news articles to users has\nalways been the core challenge of news recommendation system. News\nrecommendations often require modeling of user interests to match candidate\nnews. Recent efforts have primarily focused on extract local subgraph\ninformation, the lack of a comprehensive global news graph extraction has\nhindered the ability to utilize global news information collaboratively among\nsimilar users. To overcome these limitations, we propose an effective and\nefficient Long Interest Chain Modeling for News Recommendation(LICM), which\ncombines neighbor interest with long-chain interest distilled from a global\nnews click graph based on the collaborative of similar users to enhance news\nrecommendation. For a global news graph based on the click history of all\nusers, long chain interest generated from it can better utilize the\nhigh-dimensional information within it, enhancing the effectiveness of\ncollaborative recommendations. We therefore design a comprehensive selection\nmechanism and interest encoder to obtain long-chain interest from the global\ngraph. Finally, we use a gated network to integrate long-chain information with\nneighbor information to achieve the final user representation. Experiment\nresults on real-world datasets validate the effectiveness and efficiency of our\nmodel to improve the performance of news recommendation.\n","authors":["Zhen Yang","Wenhui Wang","Tao Qi","Peng Zhang","Tianyun Zhang","Ru Zhang","Jianyi Liu","Yongfeng Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00859v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2212.03733v3","updated":"2024-08-01T17:47:24Z","published":"2022-12-07T15:55:00Z","title":"Tiered Reward: Designing Rewards for Specification and Fast Learning of\n  Desired Behavior","summary":"  Reinforcement-learning agents seek to maximize a reward signal through\nenvironmental interactions. As humans, our job in the learning process is to\ndesign reward functions to express desired behavior and enable the agent to\nlearn such behavior swiftly. However, designing good reward functions to induce\nthe desired behavior is generally hard, let alone the question of which rewards\nmake learning fast. In this work, we introduce a family of a reward structures\nwe call Tiered Reward that addresses both of these questions. We consider the\nreward-design problem in tasks formulated as reaching desirable states and\navoiding undesirable states. To start, we propose a strict partial ordering of\nthe policy space to resolve trade-offs in behavior preference. We prefer\npolicies that reach the good states faster and with higher probability while\navoiding the bad states longer. Next, we introduce Tiered Reward, a class of\nenvironment-independent reward functions and show it is guaranteed to induce\npolicies that are Pareto-optimal according to our preference relation. Finally,\nwe demonstrate that Tiered Reward leads to fast learning with multiple tabular\nand deep reinforcement-learning algorithms.\n","authors":["Zhiyuan Zhou","Shreyas Sundara Raman","Henry Sowerby","Michael L. Littman"],"pdf_url":"https://arxiv.org/pdf/2212.03733v3.pdf","comment":"For code, see https://github.com/zhouzypaul/tiered-reward"},{"id":"http://arxiv.org/abs/2407.14435v3","updated":"2024-08-01T17:42:04Z","published":"2024-07-19T16:07:19Z","title":"Jumping Ahead: Improving Reconstruction Fidelity with JumpReLU Sparse\n  Autoencoders","summary":"  Sparse autoencoders (SAEs) are a promising unsupervised approach for\nidentifying causally relevant and interpretable linear features in a language\nmodel's (LM) activations. To be useful for downstream tasks, SAEs need to\ndecompose LM activations faithfully; yet to be interpretable the decomposition\nmust be sparse -- two objectives that are in tension. In this paper, we\nintroduce JumpReLU SAEs, which achieve state-of-the-art reconstruction fidelity\nat a given sparsity level on Gemma 2 9B activations, compared to other recent\nadvances such as Gated and TopK SAEs. We also show that this improvement does\nnot come at the cost of interpretability through manual and automated\ninterpretability studies. JumpReLU SAEs are a simple modification of vanilla\n(ReLU) SAEs -- where we replace the ReLU with a discontinuous JumpReLU\nactivation function -- and are similarly efficient to train and run. By\nutilising straight-through-estimators (STEs) in a principled manner, we show\nhow it is possible to train JumpReLU SAEs effectively despite the discontinuous\nJumpReLU function introduced in the SAE's forward pass. Similarly, we use STEs\nto directly train L0 to be sparse, instead of training on proxies such as L1,\navoiding problems like shrinkage.\n","authors":["Senthooran Rajamanoharan","Tom Lieberum","Nicolas Sonnerat","Arthur Conmy","Vikrant Varma","Jnos Kramr","Neel Nanda"],"pdf_url":"https://arxiv.org/pdf/2407.14435v3.pdf","comment":"v2: new appendix H comparing kernel functions & bug-fixes to\n  pseudo-code in Appendix J v3: further bug-fix to pseudo-code in Appendix J"},{"id":"http://arxiv.org/abs/2308.01674v4","updated":"2024-08-01T17:41:27Z","published":"2023-08-03T10:21:53Z","title":"End-to-End Reinforcement Learning of Koopman Models for Economic\n  Nonlinear Model Predictive Control","summary":"  (Economic) nonlinear model predictive control ((e)NMPC) requires dynamic\nmodels that are sufficiently accurate and computationally tractable.\nData-driven surrogate models for mechanistic models can reduce the\ncomputational burden of (e)NMPC; however, such models are typically trained by\nsystem identification for maximum prediction accuracy on simulation samples and\nperform suboptimally in (e)NMPC. We present a method for end-to-end\nreinforcement learning of Koopman surrogate models for optimal performance as\npart of (e)NMPC. We apply our method to two applications derived from an\nestablished nonlinear continuous stirred-tank reactor model. The controller\nperformance is compared to that of (e)NMPCs utilizing models trained using\nsystem identification, and model-free neural network controllers trained using\nreinforcement learning. We show that the end-to-end trained models outperform\nthose trained using system identification in (e)NMPC, and that, in contrast to\nthe neural network controllers, the (e)NMPC controllers can react to changes in\nthe control setting without retraining.\n","authors":["Daniel Mayfrank","Alexander Mitsos","Manuel Dahmen"],"pdf_url":"https://arxiv.org/pdf/2308.01674v4.pdf","comment":"manuscript (20 pages, 7 figures, 6 tables), supplementary materials\n  (3 pages, 2 tables)"},{"id":"http://arxiv.org/abs/2407.16020v3","updated":"2024-08-01T17:40:36Z","published":"2024-07-22T19:55:44Z","title":"Sparks of Quantum Advantage and Rapid Retraining in Machine Learning","summary":"  The advent of quantum computing holds the potential to revolutionize various\nfields by solving complex problems more efficiently than classical computers.\nDespite this promise, practical quantum advantage is hindered by current\nhardware limitations, notably the small number of qubits and high noise levels.\nIn this study, we leverage adiabatic quantum computers to optimize\nKolmogorov-Arnold Networks, a powerful neural network architecture for\nrepresenting complex functions with minimal parameters. By modifying the\nnetwork to use Bezier curves as the basis functions and formulating the\noptimization problem into a Quadratic Unconstrained Binary Optimization\nproblem, we create a fixed-sized solution space, independent of the number of\ntraining samples. Our approach demonstrates sparks of quantum advantage through\nfaster training times compared to classical optimizers such as the Adam,\nStochastic Gradient Descent, Adaptive Gradient, and simulated annealing.\nAdditionally, we introduce a novel rapid retraining capability, enabling the\nnetwork to be retrained with new data without reprocessing old samples, thus\nenhancing learning efficiency in dynamic environments. Experimental results on\ninitial training of classification and regression tasks validate the efficacy\nof our approach, showcasing significant speedups and comparable performance to\nclassical methods. While experiments on retraining demonstrate a sixty times\nspeed up using adiabatic quantum computing based optimization compared to that\nof the gradient descent based optimizers, with theoretical models allowing this\nspeed up to be even larger! Our findings suggest that with further advancements\nin quantum hardware and algorithm optimization, quantum-optimized machine\nlearning models could have broad applications across various domains, with\ninitial focus on rapid retraining.\n","authors":["William Troy"],"pdf_url":"https://arxiv.org/pdf/2407.16020v3.pdf","comment":"Major updates to the paper for timings and explanations of\n  optimization strategies used. Further optimized the code and updated the\n  figures to reflect the faster timings for v3"},{"id":"http://arxiv.org/abs/2310.12428v2","updated":"2024-08-01T17:38:27Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v2.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.15402v2","updated":"2024-08-01T16:31:00Z","published":"2024-02-23T16:05:51Z","title":"Grasp, See and Place: Efficient Unknown Object Rearrangement with Policy\n  Structure Prior","summary":"  We focus on the task of unknown object rearrangement, where a robot is\nsupposed to re-configure the objects into a desired goal configuration\nspecified by an RGB-D image. Recent works explore unknown object rearrangement\nsystems by incorporating learning-based perception modules. However, they are\nsensitive to perception error, and pay less attention to task-level\nperformance. In this paper, we aim to develop an effective system for unknown\nobject rearrangement amidst perception noise. We theoretically reveal the noisy\nperception impacts grasp and place in a decoupled way, and show such a\ndecoupled structure is valuable to improve task optimality. We propose GSP, a\ndual-loop system with the decoupled structure as prior. For the inner loop, we\nlearn a see policy for self-confident in-hand object matching. For the outer\nloop, we learn a grasp policy aware of object matching and grasp capability\nguided by task-level rewards. We leverage the foundation model CLIP for object\nmatching, policy learning and self-termination. A series of experiments\nindicate that GSP can conduct unknown object rearrangement with higher\ncompletion rates and fewer steps.\n","authors":["Kechun Xu","Zhongxiang Zhou","Jun Wu","Haojian Lu","Rong Xiong","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2402.15402v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00050v2","updated":"2024-08-01T16:14:27Z","published":"2024-03-25T15:11:15Z","title":"Grappa -- A Machine Learned Molecular Mechanics Force Field","summary":"  Simulating large molecular systems over long timescales requires force fields\nthat are both accurate and efficient. In recent years, E(3) equivariant neural\nnetworks have lifted the tension between computational efficiency and accuracy\nof force fields, but they are still several orders of magnitude more expensive\nthan established molecular mechanics (MM) force fields. Here, we propose\nGrappa, a machine learning framework to predict MM parameters from the\nmolecular graph, employing a graph attentional neural network and a transformer\nwith symmetry-preserving positional encoding. The resulting Grappa force field\noutperformstabulated and machine-learned MM force fields in terms of accuracy\nat the same computational efficiency and can be used in existing Molecular\nDynamics (MD) engines like GROMACS and OpenMM. It predicts energies and forces\nof small molecules, peptides, RNA and - showcasing its extensibility to\nuncharted regions of chemical space - radicals at state-of-the-art MM accuracy.\nWe demonstrate Grappa's transferability to macromolecules in MD simulations\nfrom a small fast folding protein up to a whole virus particle. Our force field\nsets the stage for biomolecular simulations closer to chemical accuracy, but\nwith the same computational cost as established protein force fields.\n","authors":["Leif Seute","Eric Hartmann","Jan Sthmer","Frauke Grter"],"pdf_url":"https://arxiv.org/pdf/2404.00050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20021v3","updated":"2024-08-01T16:13:45Z","published":"2024-07-29T13:57:40Z","title":"MimiQ: Low-Bit Data-Free Quantization of Vision Transformers with\n  Encouraging Inter-Head Attention Similarity","summary":"  Data-free quantization (DFQ) is a technique that creates a lightweight\nnetwork from its full-precision counterpart without the original training data,\noften through a synthetic dataset. Although several DFQ methods have been\nproposed for vision transformer (ViT) architectures, they fail to achieve\nefficacy in low-bit settings. Examining the existing methods, we identify that\ntheir synthetic data produce misaligned attention maps, while those of the real\nsamples are highly aligned. From the observation of aligned attention, we find\nthat aligning attention maps of synthetic data helps to improve the overall\nperformance of quantized ViTs. Motivated by this finding, we devise MimiQ, a\nnovel DFQ method designed for ViTs that focuses on inter-head attention\nsimilarity. First, we generate synthetic data by aligning head-wise attention\nresponses in relation to spatial query patches. Then, we apply head-wise\nstructural attention distillation to align the attention maps of the quantized\nnetwork to those of the full-precision teacher. The experimental results show\nthat the proposed method significantly outperforms baselines, setting a new\nstate-of-the-art performance for data-free ViT quantization.\n","authors":["Kanghyun Choi","Hye Yoon Lee","Dain Kwon","SunJong Park","Kyuyeun Kim","Noseong Park","Jinho Lee"],"pdf_url":"https://arxiv.org/pdf/2407.20021v3.pdf","comment":"Author Preprint"},{"id":"http://arxiv.org/abs/2307.09067v2","updated":"2024-08-01T16:09:50Z","published":"2023-07-18T08:37:58Z","title":"Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Image\n  Segmentation with U-Net","summary":"  Fetal head segmentation is a crucial step in measuring the fetal head\ncircumference (HC) during gestation, an important biometric in obstetrics for\nmonitoring fetal growth. However, manual biometry generation is time-consuming\nand results in inconsistent accuracy. To address this issue, convolutional\nneural network (CNN) models have been utilized to improve the efficiency of\nmedical biometry. But training a CNN network from scratch is a challenging\ntask, we proposed a Transfer Learning (TL) method. Our approach involves\nfine-tuning (FT) a U-Net network with a lightweight MobileNet as the encoder to\nperform segmentation on a set of fetal head ultrasound (US) images with limited\neffort. This method addresses the challenges associated with training a CNN\nnetwork from scratch. It suggests that our proposed FT strategy yields\nsegmentation performance that is comparable when trained with a reduced number\nof parameters by 85.8%. And our proposed FT strategy outperforms other\nstrategies with smaller trainable parameter sizes below 4.4 million. Thus, we\ncontend that it can serve as a dependable FT approach for reducing the size of\nmodels in medical image analysis. Our key findings highlight the importance of\nthe balance between model performance and size in developing Artificial\nIntelligence (AI) applications by TL methods. Code is available at\nhttps://github.com/13204942/FT_Methods_for_Fetal_Head_Segmentation.\n","authors":["Fangyijie Wang","Gunol Silvestre","Kathleen M. Curran"],"pdf_url":"https://arxiv.org/pdf/2307.09067v2.pdf","comment":"Irish Machine Vision and Image Processing Conference Proceedings 2023"},{"id":"http://arxiv.org/abs/2311.07315v2","updated":"2024-08-01T16:07:02Z","published":"2023-11-13T13:10:52Z","title":"An introduction to reinforcement learning for neuroscience","summary":"  Reinforcement learning has a rich history in neuroscience, from early work on\ndopamine as a reward prediction error signal for temporal difference learning\n(Schultz et al., 1997) to recent work suggesting that dopamine could implement\na form of 'distributional reinforcement learning' popularized in deep learning\n(Dabney et al., 2020). Throughout this literature, there has been a tight link\nbetween theoretical advances in reinforcement learning and neuroscientific\nexperiments and findings. As a result, the theories describing our experimental\ndata have become increasingly complex and difficult to navigate. In this\nreview, we cover the basic theory underlying classical work in reinforcement\nlearning and build up to an introductory overview of methods in modern deep\nreinforcement learning that have found applications in systems neuroscience. We\nstart with an overview of the reinforcement learning problem and classical\ntemporal difference algorithms, followed by a discussion of 'model-free' and\n'model-based' reinforcement learning together with methods such as DYNA and\nsuccessor representations that fall in between these two extremes. Throughout\nthese sections, we highlight the close parallels between such machine learning\nmethods and related work in both experimental and theoretical neuroscience. We\nthen provide an introduction to deep reinforcement learning with examples of\nhow these methods have been used to model different learning phenomena in\nsystems neuroscience, such as meta-reinforcement learning (Wang et al., 2018)\nand distributional reinforcement learning (Dabney et al., 2020). Code that\nimplements the methods discussed in this work and generates the figures is also\nprovided.\n","authors":["Kristopher T. Jensen"],"pdf_url":"https://arxiv.org/pdf/2311.07315v2.pdf","comment":"Code available at:\n  https://colab.research.google.com/drive/1ZC4lR8kTO48yySDZtcOEdMKd3NqY_ly1?usp=sharing"},{"id":"http://arxiv.org/abs/2403.05385v5","updated":"2024-08-01T16:02:06Z","published":"2024-03-08T15:30:58Z","title":"Switching the Loss Reduces the Cost in Batch (Offline) Reinforcement\n  Learning","summary":"  We propose training fitted Q-iteration with log-loss (FQI-log) for batch\nreinforcement learning (RL). We show that the number of samples needed to learn\na near-optimal policy with FQI-log scales with the accumulated cost of the\noptimal policy, which is zero in problems where acting optimally achieves the\ngoal and incurs no cost. In doing so, we provide a general framework for\nproving small-cost bounds, i.e. bounds that scale with the optimal achievable\ncost, in batch RL. Moreover, we empirically verify that FQI-log uses fewer\nsamples than FQI trained with squared loss on problems where the optimal policy\nreliably achieves the goal.\n","authors":["Alex Ayoub","Kaiwen Wang","Vincent Liu","Samuel Robertson","James McInerney","Dawen Liang","Nathan Kallus","Csaba Szepesvri"],"pdf_url":"https://arxiv.org/pdf/2403.05385v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.13312v2","updated":"2024-08-01T15:54:29Z","published":"2023-04-26T06:33:31Z","title":"Technical Note: Defining and Quantifying AND-OR Interactions for\n  Faithful and Concise Explanation of DNNs","summary":"  In this technical note, we aim to explain a deep neural network (DNN) by\nquantifying the encoded interactions between input variables, which reflects\nthe DNN's inference logic. Specifically, we first rethink the definition of\ninteractions, and then formally define faithfulness and conciseness for\ninteraction-based explanation. To this end, we propose two kinds of\ninteractions, i.e., the AND interaction and the OR interaction. For\nfaithfulness, we prove the uniqueness of the AND (OR) interaction in\nquantifying the effect of the AND (OR) relationship between input variables.\nBesides, based on AND-OR interactions, we design techniques to boost the\nconciseness of the explanation, while not hurting the faithfulness. In this\nway, the inference logic of a DNN can be faithfully and concisely explained by\na set of symbolic concepts.\n","authors":["Mingjie Li","Quanshi Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13312v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2111.06206"},{"id":"http://arxiv.org/abs/2304.13119v3","updated":"2024-08-01T15:52:32Z","published":"2023-04-25T19:48:54Z","title":"Application of Transformers for Nonlinear Channel Compensation in\n  Optical Systems","summary":"  In this paper, we introduce a new nonlinear optical channel equalizer based\non Transformers. By leveraging parallel computation and attending directly to\nthe memory across a sequence of symbols, we show that Transformers can be used\neffectively for nonlinear compensation (NLC) in coherent long-haul transmission\nsystems. For this application, we present an implementation of the encoder part\nof the Transformer and analyze its performance over a wide range of different\nhyper-parameters. It is shown that by proper embeddings and processing blocks\nof symbols at each iteration and also carefully selecting subsets of the\nencoder's output to be processed together, an efficient nonlinear equalization\ncan be achieved for different complexity constraints. To reduce the\ncomputational complexity of the attention mechanism, we further propose the use\nof a physic-informed mask inspired by nonlinear perturbation theory. We also\ncompare the Transformer-NLC with digital back-propagation (DBP) under different\ntransmission scenarios in order to demonstrate the flexibility and\ngeneralizability of the proposed data-driven solution.\n","authors":["Behnam Behinaein Hamgini","Hossein Najafi","Ali Bakhshali","Zhuhong Zhang"],"pdf_url":"https://arxiv.org/pdf/2304.13119v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11911v2","updated":"2024-08-01T15:44:19Z","published":"2024-06-16T16:46:55Z","title":"A Notion of Complexity for Theory of Mind via Discrete World Models","summary":"  Theory of Mind (ToM) can be used to assess the capabilities of Large Language\nModels (LLMs) in complex scenarios where social reasoning is required. While\nthe research community has proposed many ToM benchmarks, their hardness varies\ngreatly, and their complexity is not well defined. This work proposes a\nframework to measure the complexity of ToM tasks. We quantify a problem's\ncomplexity as the number of states necessary to solve it correctly. Our\ncomplexity measure also accounts for spurious states of a ToM problem designed\nto make it apparently harder. We use our method to assess the complexity of\nfive widely adopted ToM benchmarks. On top of this framework, we design a\nprompting technique that augments the information available to a model with a\ndescription of how the environment changes with the agents' interactions. We\nname this technique Discrete World Models (DWM) and show how it elicits\nsuperior performance on ToM tasks.\n","authors":["X. Angelo Huang","Emanuele La Malfa","Samuele Marro","Andrea Asperti","Anthony Cohn","Michael Wooldridge"],"pdf_url":"https://arxiv.org/pdf/2406.11911v2.pdf","comment":"https://flecart.github.io/complexity-tom-dwm"},{"id":"http://arxiv.org/abs/2311.00048v2","updated":"2024-08-01T15:37:52Z","published":"2023-10-31T18:01:41Z","title":"SC-MIL: Sparsely Coded Multiple Instance Learning for Whole Slide Image\n  Classification","summary":"  Multiple Instance Learning (MIL) has been widely used in weakly supervised\nwhole slide image (WSI) classification. Typical MIL methods include a feature\nembedding part, which embeds the instances into features via a pre-trained\nfeature extractor, and an MIL aggregator that combines instance embeddings into\npredictions. Most efforts have typically focused on improving these parts. This\ninvolves refining the feature embeddings through self-supervised pre-training\nas well as modeling the correlations between instances separately.\n  In this paper, we proposed a sparsely coding MIL (SC-MIL) method that\naddresses those two aspects at the same time by leveraging sparse dictionary\nlearning. The sparse dictionary learning captures the similarities of instances\nby expressing them as sparse linear combinations of atoms in an over-complete\ndictionary. In addition, imposing sparsity improves instance feature embeddings\nby suppressing irrelevant instances while retaining the most relevant ones. To\nmake the conventional sparse coding algorithm compatible with deep learning, we\nunrolled it into a sparsely coded module leveraging deep unrolling. The\nproposed SC module can be incorporated into any existing MIL framework in a\nplug-and-play manner with an acceptable computational cost. The experimental\nresults on multiple datasets demonstrated that the proposed SC module could\nsubstantially boost the performance of state-of-the-art MIL methods. The codes\nare available at\n\\href{https://github.com/sotiraslab/SCMIL.git}{https://github.com/sotiraslab/SCMIL.git}.\n","authors":["Peijie Qiu","Pan Xiao","Wenhui Zhu","Yalin Wang","Aristeidis Sotiras"],"pdf_url":"https://arxiv.org/pdf/2311.00048v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08448v2","updated":"2024-08-01T15:16:26Z","published":"2024-03-13T12:03:27Z","title":"Actor-Critic Physics-informed Neural Lyapunov Control","summary":"  Designing control policies for stabilization tasks with provable guarantees\nis a long-standing problem in nonlinear control. A crucial performance metric\nis the size of the resulting region of attraction, which essentially serves as\na robustness \"margin\" of the closed-loop system against uncertainties. In this\npaper, we propose a new method to train a stabilizing neural network controller\nalong with its corresponding Lyapunov certificate, aiming to maximize the\nresulting region of attraction while respecting the actuation constraints.\nCrucial to our approach is the use of Zubov's Partial Differential Equation\n(PDE), which precisely characterizes the true region of attraction of a given\ncontrol policy. Our framework follows an actor-critic pattern where we\nalternate between improving the control policy (actor) and learning a Zubov\nfunction (critic). Finally, we compute the largest certifiable region of\nattraction by invoking an SMT solver after the training procedure. Our\nnumerical experiments on several design problems show consistent and\nsignificant improvements in the size of the resulting region of attraction.\n","authors":["Jiarui Wang","Mahyar Fazlyab"],"pdf_url":"https://arxiv.org/pdf/2403.08448v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14814v3","updated":"2024-08-01T15:15:34Z","published":"2024-03-21T19:59:52Z","title":"The opportunities and risks of large language models in mental health","summary":"  Global rates of mental health concerns are rising, and there is increasing\nrealization that existing models of mental health care will not adequately\nexpand to meet the demand. With the emergence of large language models (LLMs)\nhas come great optimism regarding their promise to create novel, large-scale\nsolutions to support mental health. Despite their nascence, LLMs have already\nbeen applied to mental health related tasks. In this paper, we summarize the\nextant literature on efforts to use LLMs to provide mental health education,\nassessment, and intervention and highlight key opportunities for positive\nimpact in each area. We then highlight risks associated with LLMs' application\nto mental health and encourage the adoption of strategies to mitigate these\nrisks. The urgent need for mental health support must be balanced with\nresponsible development, testing, and deployment of mental health LLMs. It is\nespecially critical to ensure that mental health LLMs are fine-tuned for mental\nhealth, enhance mental health equity, and adhere to ethical standards and that\npeople, including those with lived experience with mental health concerns, are\ninvolved in all stages from development through deployment. Prioritizing these\nefforts will minimize potential harms to mental health and maximize the\nlikelihood that LLMs will positively impact mental health globally.\n","authors":["Hannah R. Lawrence","Renee A. Schneider","Susan B. Rubin","Maja J. Mataric","Daniel J. McDuff","Megan Jones Bell"],"pdf_url":"https://arxiv.org/pdf/2403.14814v3.pdf","comment":"15 pages, 2 tables, 4 figures"},{"id":"http://arxiv.org/abs/2404.10324v2","updated":"2024-08-01T15:10:45Z","published":"2024-04-16T07:08:04Z","title":"Graph neural network-based surrogate modelling for real-time hydraulic\n  prediction of urban drainage networks","summary":"  Physics-based models are computationally time-consuming and infeasible for\nreal-time scenarios of urban drainage networks, and a surrogate model is needed\nto accelerate the online predictive modelling. Fully-connected neural networks\n(NNs) are potential surrogate models, but may suffer from low interpretability\nand efficiency in fitting complex targets. Owing to the state-of-the-art\nmodelling power of graph neural networks (GNNs) and their match with urban\ndrainage networks in the graph structure, this work proposes a GNN-based\nsurrogate of the flow routing model for the hydraulic prediction problem of\ndrainage networks, which regards recent hydraulic states as initial conditions,\nand future runoff and control policy as boundary conditions. To incorporate\nhydraulic constraints and physical relationships into drainage modelling,\nphysics-guided mechanisms are designed on top of the surrogate model to\nrestrict the prediction variables with flow balance and flooding occurrence\nconstraints. According to case results in a stormwater network, the GNN-based\nmodel is more cost-effective with better hydraulic prediction accuracy than the\nNN-based model after equal training epochs, and the designed mechanisms further\nlimit prediction errors with interpretable domain knowledge. As the model\nstructure adheres to the flow routing mechanisms and hydraulic constraints in\nurban drainage networks, it provides an interpretable and effective solution\nfor data-driven surrogate modelling. Simultaneously, the surrogate model\naccelerates the predictive modelling of urban drainage networks for real-time\nuse compared with the physics-based model.\n","authors":["Zhiyu Zhang","Chenkaixiang Lu","Wenchong Tian","Zhenliang Liao","Zhiguo Yuan"],"pdf_url":"https://arxiv.org/pdf/2404.10324v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2111.04597v4","updated":"2024-08-01T14:38:06Z","published":"2021-11-08T16:09:39Z","title":"Neyman-Pearson Multi-class Classification via Cost-sensitive Learning","summary":"  Most existing classification methods aim to minimize the overall\nmisclassification error rate. However, in applications such as loan default\nprediction, different types of errors can have varying consequences. To address\nthis asymmetry issue, two popular paradigms have been developed: the\nNeyman-Pearson (NP) paradigm and the cost-sensitive (CS) paradigm. Previous\nstudies on the NP paradigm have primarily focused on the binary case, while the\nmulti-class NP problem poses a greater challenge due to its unknown\nfeasibility. In this work, we tackle the multi-class NP problem by establishing\na connection with the CS problem via strong duality and propose two algorithms.\nWe extend the concept of NP oracle inequalities, crucial in binary\nclassifications, to NP oracle properties in the multi-class context. Our\nalgorithms satisfy these NP oracle properties under certain conditions.\nFurthermore, we develop practical algorithms to assess the feasibility and\nstrong duality in multi-class NP problems, which can offer practitioners the\nlandscape of a multi-class NP problem with various target error levels.\nSimulations and real data studies validate the effectiveness of our algorithms.\nTo our knowledge, this is the first study to address the multi-class NP problem\nwith theoretical guarantees. The proposed algorithms have been implemented in\nthe R package \\texttt{npcs}, which is available on CRAN.\n","authors":["Ye Tian","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2111.04597v4.pdf","comment":"114 pages, 18 figures"},{"id":"http://arxiv.org/abs/2407.04822v3","updated":"2024-08-01T14:32:05Z","published":"2024-07-05T19:18:33Z","title":"YourMT3+: Multi-instrument Music Transcription with Enhanced Transformer\n  Architectures and Cross-dataset Stem Augmentation","summary":"  Multi-instrument music transcription aims to convert polyphonic music\nrecordings into musical scores assigned to each instrument. This task is\nchallenging for modeling as it requires simultaneously identifying multiple\ninstruments and transcribing their pitch and precise timing, and the lack of\nfully annotated data adds to the training difficulties. This paper introduces\nYourMT3+, a suite of models for enhanced multi-instrument music transcription\nbased on the recent language token decoding approach of MT3. We enhance its\nencoder by adopting a hierarchical attention transformer in the time-frequency\ndomain and integrating a mixture of experts. To address data limitations, we\nintroduce a new multi-channel decoding method for training with incomplete\nannotations and propose intra- and cross-stem augmentation for dataset mixing.\nOur experiments demonstrate direct vocal transcription capabilities,\neliminating the need for voice separation pre-processors. Benchmarks across ten\npublic datasets show our models' competitiveness with, or superiority to,\nexisting transcription models. Further testing on pop music recordings\nhighlights the limitations of current models. Fully reproducible code and\ndatasets are available with demos at \\url{https://github.com/mimbres/YourMT3}.\n","authors":["Sungkyun Chang","Emmanouil Benetos","Holger Kirchhoff","Simon Dixon"],"pdf_url":"https://arxiv.org/pdf/2407.04822v3.pdf","comment":"Accepted at IEEE International Workshop on Machine Learning for\n  Signal Processing (MLSP) 2024, London"},{"id":"http://arxiv.org/abs/2407.04724v2","updated":"2024-08-01T14:18:32Z","published":"2024-06-26T07:32:04Z","title":"A Likelihood-Based Generative Approach for Spatially Consistent\n  Precipitation Downscaling","summary":"  Deep learning has emerged as a promising tool for precipitation downscaling.\nHowever, current models rely on likelihood-based loss functions to properly\nmodel the precipitation distribution, leading to spatially inconsistent\nprojections when sampling. This work explores a novel approach by fusing the\nstrengths of likelihood-based and adversarial losses used in generative models.\nAs a result, we propose a likelihood-based generative approach for\nprecipitation downscaling, leveraging the benefits of both methods.\n","authors":["Jose Gonzlez-Abad"],"pdf_url":"https://arxiv.org/pdf/2407.04724v2.pdf","comment":"Accepted at ICML 2024 Machine Learning for Earth System Modeling\n  workshop"},{"id":"http://arxiv.org/abs/2309.07716v2","updated":"2024-08-01T14:16:23Z","published":"2023-09-14T13:48:16Z","title":"Understanding Vector-Valued Neural Networks and Their Relationship with\n  Real and Hypercomplex-Valued Neural Networks","summary":"  Despite the many successful applications of deep learning models for\nmultidimensional signal and image processing, most traditional neural networks\nprocess data represented by (multidimensional) arrays of real numbers. The\nintercorrelation between feature channels is usually expected to be learned\nfrom the training data, requiring numerous parameters and careful training. In\ncontrast, vector-valued neural networks are conceived to process arrays of\nvectors and naturally consider the intercorrelation between feature channels.\nConsequently, they usually have fewer parameters and often undergo more robust\ntraining than traditional neural networks. This paper aims to present a broad\nframework for vector-valued neural networks, referred to as V-nets. In this\ncontext, hypercomplex-valued neural networks are regarded as vector-valued\nmodels with additional algebraic properties. Furthermore, this paper explains\nthe relationship between vector-valued and traditional neural networks.\nPrecisely, a vector-valued neural network can be obtained by placing\nrestrictions on a real-valued model to consider the intercorrelation between\nfeature channels. Finally, we show how V-nets, including hypercomplex-valued\nneural networks, can be implemented in current deep-learning libraries as\nreal-valued networks.\n","authors":["Marcos Eduardo Valle"],"pdf_url":"https://arxiv.org/pdf/2309.07716v2.pdf","comment":"Accepted for publication in IEEE Signal Processing Magazine"},{"id":"http://arxiv.org/abs/2407.11054v2","updated":"2024-08-01T14:10:22Z","published":"2024-07-09T09:25:27Z","title":"Generative AI for Health Technology Assessment: Opportunities,\n  Challenges, and Policy Considerations","summary":"  This review introduces the transformative potential of generative Artificial\nIntelligence (AI) and foundation models, including large language models\n(LLMs), for health technology assessment (HTA). We explore their applications\nin four critical areas, evidence synthesis, evidence generation, clinical\ntrials and economic modeling: (1) Evidence synthesis: Generative AI has the\npotential to assist in automating literature reviews and meta-analyses by\nproposing search terms, screening abstracts, and extracting data with notable\naccuracy; (2) Evidence generation: These models can potentially facilitate\nautomating the process and analyze the increasingly available large collections\nof real-world data (RWD), including unstructured clinical notes and imaging,\nenhancing the speed and quality of real-world evidence (RWE) generation; (3)\nClinical trials: Generative AI can be used to optimize trial design, improve\npatient matching, and manage trial data more efficiently; and (4) Economic\nmodeling: Generative AI can also aid in the development of health economic\nmodels, from conceptualization to validation, thus streamlining the overall HTA\nprocess. Despite their promise, these technologies, while rapidly improving,\nare still nascent and continued careful evaluation in their applications to HTA\nis required. To ensure their responsible use and implementation, both\ndevelopers and users of research incorporating these tools, should familiarize\nthemselves with their current limitations, including the issues related to\nscientific validity, risk of bias, and consider equity and ethical\nimplications. We also surveyed the current policy landscape and provide\nsuggestions for HTA agencies on responsibly integrating generative AI into\ntheir workflows, emphasizing the importance of human oversight and the\nfast-evolving nature of these tools.\n","authors":["Rachael Fleurence","Jiang Bian","Xiaoyan Wang","Hua Xu","Dalia Dawoud","Mitch Higashi","Jagpreet Chhatwal"],"pdf_url":"https://arxiv.org/pdf/2407.11054v2.pdf","comment":"24 pages, 1 figure, 1 table, 2 boxes, 103 references"},{"id":"http://arxiv.org/abs/2407.20678v2","updated":"2024-08-01T14:09:12Z","published":"2024-07-30T09:20:15Z","title":"The Susceptibility of Example-Based Explainability Methods to Class\n  Outliers","summary":"  This study explores the impact of class outliers on the effectiveness of\nexample-based explainability methods for black-box machine learning models. We\nreformulate existing explainability evaluation metrics, such as correctness and\nrelevance, specifically for example-based methods, and introduce a new metric,\ndistinguishability. Using these metrics, we highlight the shortcomings of\ncurrent example-based explainability methods, including those who attempt to\nsuppress class outliers. We conduct experiments on two datasets, a text\nclassification dataset and an image classification dataset, and evaluate the\nperformance of four state-of-the-art explainability methods. Our findings\nunderscore the need for robust techniques to tackle the challenges posed by\nclass outliers.\n","authors":["Ikhtiyor Nematov","Dimitris Sacharidis","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2407.20678v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.16010"},{"id":"http://arxiv.org/abs/2407.03194v3","updated":"2024-08-01T14:08:53Z","published":"2024-07-03T15:26:02Z","title":"Prediction Instability in Machine Learning Ensembles","summary":"  In machine learning ensembles predictions from multiple models are\naggregated. Despite widespread use and strong performance of ensembles in\napplied problems little is known about the mathematical properties of\naggregating models and associated consequences for safe, explainable use of\nsuch models. In this paper we prove a theorem that shows that any ensemble will\nexhibit at least one of the following forms of prediction instability. It will\neither ignore agreement among all underlying models, change its mind when none\nof the underlying models have done so, or be manipulable through inclusion or\nexclusion of options it would never actually predict. As a consequence,\nensemble aggregation procedures will always need to balance the benefits of\ninformation use against the risk of these prediction instabilities. This\nanalysis also sheds light on what specific forms of prediction instability to\nexpect from particular ensemble algorithms; for example popular tree ensembles\nlike random forest, or xgboost will violate basic, intuitive fairness\nproperties. Finally, we show that this can be ameliorated by using consistent\nmodels in asymptotic conditions.\n","authors":["Jeremy Kedziora"],"pdf_url":"https://arxiv.org/pdf/2407.03194v3.pdf","comment":"15 pages, uses a modified version of ICML2024.sty"},{"id":"http://arxiv.org/abs/2312.02111v3","updated":"2024-08-01T14:01:56Z","published":"2023-12-04T18:43:45Z","title":"TriDeNT: Triple Deep Network Training for Privileged Knowledge\n  Distillation in Histopathology","summary":"  Computational pathology models rarely utilise data that will not be available\nfor inference. This means most models cannot learn from highly informative data\nsuch as additional immunohistochemical (IHC) stains and spatial\ntranscriptomics. We present TriDeNT, a novel self-supervised method for\nutilising privileged data that is not available during inference to improve\nperformance. We demonstrate the efficacy of this method for a range of\ndifferent paired data including immunohistochemistry, spatial transcriptomics\nand expert nuclei annotations. In all settings, TriDeNT outperforms other\nstate-of-the-art methods in downstream tasks, with observed improvements of up\nto 101%. Furthermore, we provide qualitative and quantitative measurements of\nthe features learned by these models and how they differ from baselines.\nTriDeNT offers a novel method to distil knowledge from scarce or costly data\nduring training, to create significantly better models for routine inputs.\n","authors":["Lucas Farndale","Robert Insall","Ke Yuan"],"pdf_url":"https://arxiv.org/pdf/2312.02111v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.10821v2","updated":"2024-08-01T13:53:48Z","published":"2023-08-21T16:13:23Z","title":"Optimized Deep Learning Models for Malware Detection under Concept Drift","summary":"  Despite the promising results of machine learning models in malicious files\ndetection, they face the problem of concept drift due to their constant\nevolution. This leads to declining performance over time, as the data\ndistribution of the new files differs from the training one, requiring frequent\nmodel update. In this work, we propose a model-agnostic protocol to improve a\nbaseline neural network against drift. We show the importance of feature\nreduction and training with the most recent validation set possible, and\npropose a loss function named Drift-Resilient Binary Cross-Entropy, an\nimprovement to the classical Binary Cross-Entropy more effective against drift.\nWe train our model on the EMBER dataset, published in2018, and evaluate it on a\ndataset of recent malicious files, collected between 2020 and 2023. Our\nimproved model shows promising results, detecting 15.2% more malware than a\nbaseline model.\n","authors":["William Maillet","Benjamin Marais"],"pdf_url":"https://arxiv.org/pdf/2308.10821v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2209.12715v2","updated":"2024-08-01T13:53:43Z","published":"2022-09-26T14:11:05Z","title":"Enhancing convolutional neural network generalizability via low-rank\n  weight approximation","summary":"  Noise is ubiquitous during image acquisition. Sufficient denoising is often\nan important first step for image processing. In recent decades, deep neural\nnetworks (DNNs) have been widely used for image denoising. Most DNN-based image\ndenoising methods require a large-scale dataset or focus on supervised\nsettings, in which single/pairs of clean images or a set of noisy images are\nrequired. This poses a significant burden on the image acquisition process.\nMoreover, denoisers trained on datasets of limited scale may incur\nover-fitting. To mitigate these issues, we introduce a new self-supervised\nframework for image denoising based on the Tucker low-rank tensor\napproximation. With the proposed design, we are able to characterize our\ndenoiser with fewer parameters and train it based on a single image, which\nconsiderably improves the model's generalizability and reduces the cost of data\nacquisition. Extensive experiments on both synthetic and real-world noisy\nimages have been conducted. Empirical results show that our proposed method\noutperforms existing non-learning-based methods (e.g., low-pass filter,\nnon-local mean), single-image unsupervised denoisers (e.g., DIP, NN+BM3D)\nevaluated on both in-sample and out-sample datasets. The proposed method even\nachieves comparable performances with some supervised methods (e.g., DnCNN).\n","authors":["Chenyin Gao","Shu Yang","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2209.12715v2.pdf","comment":"accepted by IET Image Processing"},{"id":"http://arxiv.org/abs/2401.14361v2","updated":"2024-08-01T13:21:24Z","published":"2024-01-25T18:07:50Z","title":"MoE-Infinity: Offloading-Efficient MoE Model Serving","summary":"  This paper presents MoE-Infinity, an offloading-efficient serving system for\nsparse mixture-of-experts (MoE) models. To optimize offloading, MoE-Infinity\nachieves novel request-level tracing for expert activation, capturing MoE's\nsparse execution patterns such as selective activation, group activation, and\nskewed reuse. Leveraging the request-level trace, MoE-Infinity performs\neffective expert prefetching and expert caching, achieving high efficiency in\ntransferring model parameters from host memory to GPU memory. Experimental\nresults demonstrate that MoE-Infinity achieves low latency comparable to\nexpensive full-GPU deployments, which require up to 4X more GPU resources than\nMoE-Infinity. Compared to offloading-supporting LLM serving systems such as\nDeepSpeed-Inference, Llama.cpp, Mixtral Offloading, and BrainStorm,\nMoE-Infinity exhibits superior latency performance, providing 2-20X\nimprovements when serving various MoE models for a large collection of LLM\ntasks. MoE-Infinity's source code is publicly available a\nhttps://github.com/TorchMoE/MoE-Infinity\n","authors":["Leyang Xue","Yao Fu","Zhan Lu","Luo Mai","Mahesh Marina"],"pdf_url":"https://arxiv.org/pdf/2401.14361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.11749v3","updated":"2024-08-01T12:37:13Z","published":"2023-11-20T13:21:10Z","title":"A causal intervention framework for synthesizing mobility data and\n  evaluating predictive neural networks","summary":"  Deep neural networks are increasingly utilized in mobility prediction tasks,\nyet their intricate internal workings pose challenges for interpretability,\nespecially in comprehending how various aspects of mobility behavior affect\npredictions. This study introduces a causal intervention framework to assess\nthe impact of mobility-related factors on neural networks designed for next\nlocation prediction -- a task focusing on predicting the immediate next\nlocation of an individual. To achieve this, we employ individual mobility\nmodels to synthesize location visit sequences and control behavior dynamics by\nintervening in their data generation process. We evaluate the interventional\nlocation sequences using mobility metrics and input them into well-trained\nnetworks to analyze performance variations. The results demonstrate the\neffectiveness in producing location sequences with distinct mobility behaviors,\nthereby facilitating the simulation of diverse yet realistic spatial and\ntemporal changes. These changes result in performance fluctuations in next\nlocation prediction networks, revealing impacts of critical mobility behavior\nfactors, including sequential patterns in location transitions, proclivity for\nexploring new locations, and preferences in location choices at population and\nindividual levels. The gained insights hold value for the real-world\napplication of mobility prediction networks, and the framework is expected to\npromote the use of causal inference to enhance the interpretability and\nrobustness of neural networks in mobility applications.\n","authors":["Ye Hong","Yanan Xin","Simon Dirmeier","Fernando Perez-Cruz","Martin Raubal"],"pdf_url":"https://arxiv.org/pdf/2311.11749v3.pdf","comment":"34 pages, 8 figures"},{"id":"http://arxiv.org/abs/2307.13124v3","updated":"2024-08-01T11:51:44Z","published":"2023-07-24T20:45:39Z","title":"Conformal prediction for frequency-severity modeling","summary":"  We present a model-agnostic framework for the construction of prediction\nintervals of insurance claims, with finite sample statistical guarantees,\nextending the technique of split conformal prediction to the domain of\ntwo-stage frequency-severity modeling. The framework effectiveness is showcased\nwith simulated and real datasets using classical parametric models and\ncontemporary machine learning methods. When the underlying severity model is a\nrandom forest, we extend the two-stage split conformal prediction algorithm,\nshowing how the out-of-bag mechanism can be leveraged to eliminate the need for\na calibration set in the conformal procedure.\n","authors":["Helton Graziadei","Paulo C. Marques F.","Eduardo F. L. de Melo","Rodrigo S. Targino"],"pdf_url":"https://arxiv.org/pdf/2307.13124v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.07484v6","updated":"2024-08-01T11:49:04Z","published":"2022-11-14T16:08:44Z","title":"Contextual Bandits with Packing and Covering Constraints: A Modular\n  Lagrangian Approach via Regression","summary":"  We consider contextual bandits with linear constraints (CBwLC), a variant of\ncontextual bandits in which the algorithm consumes multiple resources subject\nto linear constraints on total consumption. This problem generalizes contextual\nbandits with knapsacks (CBwK), allowing for packing and covering constraints,\nas well as positive and negative resource consumption. We provide the first\nalgorithm for CBwLC (or CBwK) that is based on regression oracles. The\nalgorithm is simple, computationally efficient, and statistically optimal under\nmild assumptions. Further, we provide the first vanishing-regret guarantees for\nCBwLC (or CBwK) that extend beyond the stochastic environment. We side-step\nstrong impossibility results from prior work by identifying a weaker (and,\narguably, fairer) benchmark to compare against. Our algorithm builds on\nLagrangeBwK (Immorlica et al., FOCS 2019), a Lagrangian-based technique for\nCBwK, and SquareCB (Foster and Rakhlin, ICML 2020), a regression-based\ntechnique for contextual bandits. Our analysis leverages the inherent\nmodularity of both techniques.\n","authors":["Aleksandrs Slivkins","Xingyu Zhou","Karthik Abinav Sankararaman","Dylan J. Foster"],"pdf_url":"https://arxiv.org/pdf/2211.07484v6.pdf","comment":"A preliminary version of this paper, authored by A. Slivkins, K.A.\n  Sankararaman and D.J. Foster, has been published at COLT 2023. The present\n  version features an important improvement, due to Xingyu Zhou. Specifically,\n  the $\\sqrt{T}$-regret result in Theorem 3.6(a) holds under a much weaker\n  assumption, and is now positioned as the main guarantee"},{"id":"http://arxiv.org/abs/2407.16888v2","updated":"2024-08-01T11:46:26Z","published":"2024-06-08T12:46:12Z","title":"A Nested Model for AI Design and Validation","summary":"  The growing AI field faces trust, transparency, fairness, and discrimination\nchallenges. Despite the need for new regulations, there is a mismatch between\nregulatory science and AI, preventing a consistent framework. A five-layer\nnested model for AI design and validation aims to address these issues and\nstreamline AI application design and validation, improving fairness, trust, and\nAI adoption. This model aligns with regulations, addresses AI practitioner's\ndaily challenges, and offers prescriptive guidance for determining appropriate\nevaluation approaches by identifying unique validity threats. We have three\nrecommendations motivated by this model: authors should distinguish between\nlayers when claiming contributions to clarify the specific areas in which the\ncontribution is made and to avoid confusion, authors should explicitly state\nupstream assumptions to ensure that the context and limitations of their AI\nsystem are clearly understood, AI venues should promote thorough testing and\nvalidation of AI systems and their compliance with regulatory requirements.\n","authors":["Akshat Dubey","Zewen Yang","Georges Hattab"],"pdf_url":"https://arxiv.org/pdf/2407.16888v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12925v2","updated":"2024-08-01T10:09:15Z","published":"2024-06-14T13:54:29Z","title":"GLiNER multi-task: Generalist Lightweight Model for Various Information\n  Extraction Tasks","summary":"  Information extraction tasks require both accurate, efficient, and\ngeneralisable models. Classical supervised deep learning approaches can achieve\nthe required performance, but they need large datasets and are limited in their\nability to adapt to different tasks. On the other hand, large language models\n(LLMs) demonstrate good generalization, meaning that they can adapt to many\ndifferent tasks based on user requests. However, LLMs are computationally\nexpensive and tend to fail to generate structured outputs. In this article, we\nwill introduce a new kind of GLiNER model that can be used for various\ninformation extraction tasks while being a small encoder model. Our model\nachieved SoTA performance on zero-shot NER benchmarks and leading performance\non question-answering, summarization and relation extraction tasks.\nAdditionally, in this article, we will cover experimental results on\nself-learning approaches for named entity recognition using GLiNER models.\n","authors":["Ihor Stepanov","Mykhailo Shtopko"],"pdf_url":"https://arxiv.org/pdf/2406.12925v2.pdf","comment":"11 pages, 1 figure, 6 tables"},{"id":"http://arxiv.org/abs/2402.16105v4","updated":"2024-08-01T09:53:03Z","published":"2024-02-25T15:08:37Z","title":"Informed Meta-Learning","summary":"  In noisy and low-data regimes prevalent in real-world applications, a key\nchallenge of machine learning lies in effectively incorporating inductive\nbiases that promote data efficiency and robustness. Meta-learning and informed\nML stand out as two approaches for incorporating prior knowledge into ML\npipelines. While the former relies on a purely data-driven source of priors,\nthe latter is guided by prior domain knowledge. In this paper, we formalise a\nhybrid paradigm, informed meta-learning, facilitating the incorporation of\npriors from unstructured knowledge representations, such as natural language;\nthus, unlocking complementarity in cross-task knowledge sharing of humans and\nmachines. We establish the foundational components of informed meta-learning\nand present a concrete instantiation of this framework--the Informed Neural\nProcess. Through a series of experiments, we demonstrate the potential benefits\nof informed meta-learning in improving data efficiency, robustness to\nobservational noise and task distribution shifts.\n","authors":["Katarzyna Kobalczyk","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.16105v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18615v2","updated":"2024-08-01T09:43:57Z","published":"2023-10-28T06:46:03Z","title":"Temporally Disentangled Representation Learning under Unknown\n  Nonstationarity","summary":"  In unsupervised causal representation learning for sequential data with\ntime-delayed latent causal influences, strong identifiability results for the\ndisentanglement of causally-related latent variables have been established in\nstationary settings by leveraging temporal structure. However, in nonstationary\nsetting, existing work only partially addressed the problem by either utilizing\nobserved auxiliary variables (e.g., class labels and/or domain indexes) as side\ninformation or assuming simplified latent causal dynamics. Both constrain the\nmethod to a limited range of scenarios. In this study, we further explored the\nMarkov Assumption under time-delayed causally related process in nonstationary\nsetting and showed that under mild conditions, the independent latent\ncomponents can be recovered from their nonlinear mixture up to a permutation\nand a component-wise transformation, without the observation of auxiliary\nvariables. We then introduce NCTRL, a principled estimation framework, to\nreconstruct time-delayed latent causal variables and identify their relations\nfrom measured sequential data only. Empirical evaluations demonstrated the\nreliable identification of time-delayed latent causal influences, with our\nmethodology substantially outperforming existing baselines that fail to exploit\nthe nonstationarity adequately and then, consequently, cannot distinguish\ndistribution shifts.\n","authors":["Xiangchen Song","Weiran Yao","Yewen Fan","Xinshuai Dong","Guangyi Chen","Juan Carlos Niebles","Eric Xing","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.18615v2.pdf","comment":"NeurIPS 2023. arXiv admin note: text overlap with arXiv:2210.13647"},{"id":"http://arxiv.org/abs/2405.05638v3","updated":"2024-08-01T09:25:20Z","published":"2024-05-09T09:27:18Z","title":"A Correlation-induced Finite Difference Estimator","summary":"  Finite difference (FD) approximation is a classic approach to stochastic\ngradient estimation when only noisy function realizations are available. In\nthis paper, we first provide a sample-driven method via the bootstrap technique\nto estimate the optimal perturbation, and then propose an efficient FD\nestimator based on correlated samples at the estimated optimal perturbation.\nFurthermore, theoretical analyses of both the perturbation estimator and the FD\nestimator reveal that, {\\it surprisingly}, the correlation enables the proposed\nFD estimator to achieve a reduction in variance and, in some cases, a decrease\nin bias compared to the traditional optimal FD estimator. Numerical results\nconfirm the efficiency of our estimators and align well with the theory\npresented, especially in scenarios with small sample sizes. Finally, we apply\nthe estimator to solve derivative-free optimization (DFO) problems, and\nnumerical studies show that DFO problems with 100 dimensions can be effectively\nsolved.\n","authors":["Guo Liang","Guangwu Liu","Kun Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.05638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.05012v2","updated":"2024-08-01T09:18:17Z","published":"2024-01-10T09:00:03Z","title":"HiMTM: Hierarchical Multi-Scale Masked Time Series Modeling with\n  Self-Distillation for Long-Term Forecasting","summary":"  Time series forecasting is a critical and challenging task in practical\napplication. Recent advancements in pre-trained foundation models for time\nseries forecasting have gained significant interest. However, current methods\noften overlook the multi-scale nature of time series, which is essential for\naccurate forecasting. To address this, we propose HiMTM, a hierarchical\nmulti-scale masked time series modeling with self-distillation for long-term\nforecasting. HiMTM integrates four key components: (1) hierarchical multi-scale\ntransformer (HMT) to capture temporal information at different scales; (2)\ndecoupled encoder-decoder (DED) that directs the encoder towards feature\nextraction while the decoder focuses on pretext tasks; (3) hierarchical\nself-distillation (HSD) for multi-stage feature-level supervision signals\nduring pre-training; and (4) cross-scale attention fine-tuning (CSA-FT) to\ncapture dependencies between different scales for downstream tasks. These\ncomponents collectively enhance multi-scale feature extraction in masked time\nseries modeling, improving forecasting accuracy. Extensive experiments on seven\nmainstream datasets show that HiMTM surpasses state-of-the-art self-supervised\nand end-to-end learning methods by a considerable margin of 3.16-68.54\\%.\nAdditionally, HiMTM outperforms the latest robust self-supervised learning\nmethod, PatchTST, in cross-domain forecasting by a significant margin of 2.3\\%.\nThe effectiveness of HiMTM is further demonstrated through its application in\nnatural gas demand forecasting.\n","authors":["Shubao Zhao","Ming Jin","Zhaoxiang Hou","Chengyi Yang","Zengxiang Li","Qingsong Wen","Yi Wang"],"pdf_url":"https://arxiv.org/pdf/2401.05012v2.pdf","comment":"accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2209.15224v3","updated":"2024-08-01T08:54:39Z","published":"2022-09-30T04:35:12Z","title":"Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture\n  Models","summary":"  Unsupervised learning has been widely used in many real-world applications.\nOne of the simplest and most important unsupervised learning models is the\nGaussian mixture model (GMM). In this work, we study the multi-task learning\nproblem on GMMs, which aims to leverage potentially similar GMM parameter\nstructures among tasks to obtain improved learning performance compared to\nsingle-task learning. We propose a multi-task GMM learning procedure based on\nthe EM algorithm that effectively utilizes unknown similarities between related\ntasks and is robust against a fraction of outlier tasks from arbitrary\ndistributions. The proposed procedure is shown to achieve the minimax optimal\nrate of convergence for both parameter estimation error and the excess\nmis-clustering error, in a wide range of regimes. Moreover, we generalize our\napproach to tackle the problem of transfer learning for GMMs, where similar\ntheoretical results are derived. Additionally, iterative unsupervised\nmulti-task and transfer learning methods may suffer from an initialization\nalignment problem, and two alignment algorithms are proposed to resolve the\nissue. Finally, we demonstrate the effectiveness of our methods through\nsimulations and real data examples. To the best of our knowledge, this is the\nfirst work studying multi-task and transfer learning on GMMs with theoretical\nguarantees.\n","authors":["Ye Tian","Haolei Weng","Lucy Xia","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2209.15224v3.pdf","comment":"162 pages, 15 figures, 2 tables"},{"id":"http://arxiv.org/abs/2404.17591v2","updated":"2024-08-01T08:54:15Z","published":"2024-04-19T13:28:36Z","title":"Large Language Models for Next Point-of-Interest Recommendation","summary":"  The next Point of Interest (POI) recommendation task is to predict users'\nimmediate next POI visit given their historical data. Location-Based Social\nNetwork (LBSN) data, which is often used for the next POI recommendation task,\ncomes with challenges. One frequently disregarded challenge is how to\neffectively use the abundant contextual information present in LBSN data.\nPrevious methods are limited by their numerical nature and fail to address this\nchallenge. In this paper, we propose a framework that uses pretrained Large\nLanguage Models (LLMs) to tackle this challenge. Our framework allows us to\npreserve heterogeneous LBSN data in its original format, hence avoiding the\nloss of contextual information. Furthermore, our framework is capable of\ncomprehending the inherent meaning of contextual information due to the\ninclusion of commonsense knowledge. In experiments, we test our framework on\nthree real-world LBSN datasets. Our results show that the proposed framework\noutperforms the state-of-the-art models in all three datasets. Our analysis\ndemonstrates the effectiveness of the proposed framework in using contextual\ninformation as well as alleviating the commonly encountered cold-start and\nshort trajectory problems.\n","authors":["Peibo Li","Maarten de Rijke","Hao Xue","Shuang Ao","Yang Song","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2404.17591v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06925v2","updated":"2024-08-01T08:51:46Z","published":"2024-01-12T23:14:34Z","title":"Modeling Latent Selection with Structural Causal Models","summary":"  Selection bias is ubiquitous in real-world data, and can lead to misleading\nresults if not dealt with properly. We introduce a conditioning operation on\nStructural Causal Models (SCMs) to model latent selection from a causal\nperspective. We show that the conditioning operation transforms an SCM with the\npresence of an explicit latent selection mechanism into an SCM without such\nselection mechanism, which partially encodes the causal semantics of the\nselected subpopulation according to the original SCM. Furthermore, we show that\nthis conditioning operation preserves the simplicity, acyclicity, and linearity\nof SCMs, and commutes with marginalization. Thanks to these properties,\ncombined with marginalization and intervention, the conditioning operation\noffers a valuable tool for conducting causal reasoning tasks within causal\nmodels where latent details have been abstracted away. We demonstrate by\nexample how classical results of causal inference can be generalized to include\nselection bias and how the conditioning operation helps with modeling of\nreal-world problems.\n","authors":["Leihao Chen","Onno Zoeter","Joris M. Mooij"],"pdf_url":"https://arxiv.org/pdf/2401.06925v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19872v2","updated":"2024-08-01T08:38:35Z","published":"2024-07-29T10:43:15Z","title":"OpenUAS: Embeddings of Cities in Japan with Anchor Data for Cross-city\n  Analysis of Area Usage Patterns","summary":"  We publicly release OpenUAS, a dataset of area embeddings based on urban\nusage patterns, including embeddings for over 1.3 million 50-meter square\nmeshes covering a total area of 3,300 square kilometers. This dataset is\nvaluable for analyzing area functions in fields such as market analysis, urban\nplanning, transportation infrastructure, and infection prediction. It captures\nthe characteristics of each area in the city, such as office districts and\nresidential areas, by employing an area embedding technique that utilizes\nlocation information typically obtained by GPS. Numerous area embedding\ntechniques have been proposed, and while the public release of such embedding\ndatasets is technically feasible, it has not been realized. One of the\nobstacles has been the integration of data from different cities and periods\ninto a unified space without sharing raw location data. We address this issue\nby developing an anchoring method that establishes anchors within a shared\nembedding space. We publicly release this anchor dataset along with area\nembedding datasets from several periods in eight major Japanese cities. This\ndataset allows users to analyze urban usage patterns in Japanese cities and\nembed their urban dataset into the same embedding space using the anchoring\nmethod. Our key contributions include the development of the anchoring method,\nreleasing area embedding datasets for Japanese cities, and providing tools for\neffective data utilization.\n","authors":["Naoki Tamura","Kazuyuki Shoji","Shin Katayama","Kenta Urano","Takuro Yonezawa","Nobuo Kawaguchi"],"pdf_url":"https://arxiv.org/pdf/2407.19872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08604v2","updated":"2024-08-01T07:55:49Z","published":"2024-06-12T19:17:17Z","title":"GRU-Net: Gaussian Attention Aided Dense Skip Connection Based\n  MultiResUNet for Breast Histopathology Image Segmentation","summary":"  Breast cancer is a major global health concern. Pathologists face challenges\nin analyzing complex features from pathological images, which is a\ntime-consuming and labor-intensive task. Therefore, efficient computer-based\ndiagnostic tools are needed for early detection and treatment planning. This\npaper presents a modified version of MultiResU-Net for histopathology image\nsegmentation, which is selected as the backbone for its ability to analyze and\nsegment complex features at multiple scales and ensure effective feature flow\nvia skip connections. The modified version also utilizes the Gaussian\ndistribution-based Attention Module (GdAM) to incorporate\nhistopathology-relevant text information in a Gaussian distribution. The\nsampled features from the Gaussian text feature-guided distribution highlight\nspecific spatial regions based on prior knowledge. Finally, using the\nControlled Dense Residual Block (CDRB) on skip connections of MultiResU-Net,\nthe information is transferred from the encoder layers to the decoder layers in\na controlled manner using a scaling parameter derived from the extracted\nspatial features. We validate our approach on two diverse breast cancer\nhistopathology image datasets: TNBC and MonuSeg, demonstrating superior\nsegmentation performance compared to state-of-the-art methods. The code for our\nproposed model is available on https://github.com/AyushRoy2001/GRU-Net.\n","authors":["Ayush Roy","Payel Pramanik","Sohom Ghosal","Daria Valenkova","Dmitrii Kaplun","Ram Sarkar"],"pdf_url":"https://arxiv.org/pdf/2406.08604v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15220v4","updated":"2024-08-01T07:51:25Z","published":"2024-02-23T09:29:19Z","title":"ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and\n  Two-Phase Partition","summary":"  Self-attention is an essential component of large language models (LLM) but a\nsignificant source of inference latency for long sequences. In multi-tenant LLM\nserving scenarios, the compute and memory operation cost of self-attention can\nbe optimized by using the probability that multiple LLM requests have shared\nsystem prompts in prefixes. In this paper, we introduce ChunkAttention, a\nprefix-aware self-attention module that can detect matching prompt prefixes\nacross multiple requests and share their key/value tensors in memory at runtime\nto improve the memory utilization of KV cache. This is achieved by breaking\nmonolithic key/value tensors into smaller chunks and structuring them into the\nauxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache,\nwe design an efficient self-attention kernel, where a two-phase partition\nalgorithm is implemented to improve the data locality during self-attention\ncomputation in the presence of shared system prompts. Experiments show that\nChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$\ncompared to the state-of-the-art implementation, with the length of the system\nprompt ranging from 1024 to 4096.\n","authors":["Lu Ye","Ze Tao","Yong Huang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2402.15220v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2402.02563v3","updated":"2024-08-01T07:46:54Z","published":"2024-02-04T16:45:01Z","title":"DefInt: A Default-interventionist Framework for Efficient Reasoning with\n  Hybrid Large Language Models","summary":"  Large language models (LLMs) have shown impressive emergent abilities in a\nwide range of tasks, but still face challenges in handling complex reasoning\nproblems. Previous works like chain-of-thought (CoT) and tree-of-thoughts (ToT)\nhave predominately focused on enhancing accuracy, but overlook the rapidly\nincreasing token cost, which could be particularly problematic for open-ended\nreal-world tasks with huge solution spaces. Motivated by the dual process\ntheory of human cognition, we propose a Default-Interventionist framework\n(DefInt) to unleash the synergistic potential of hybrid LLMs. By default,\nDefInt uses smaller-scale language models to generate low-cost reasoning\nthoughts, which resembles the fast intuitions produced by System 1. If the\nintuitions are considered with low confidence, DefInt will invoke the\nreflective reasoning of scaled-up language models as the intervention of System\n2, which can override the default thoughts and rectify the reasoning process.\nExperiments on five representative reasoning tasks show that DefInt\nconsistently achieves state-of-the-art reasoning accuracy and solution\ndiversity. More importantly, it substantially reduces the token cost by 49%-79%\ncompared to the second accurate baselines. Specifically, the open-ended tasks\nhave an average 75% token cost reduction. Code repo with all prompts will be\nreleased upon publication.\n","authors":["Yu Shang","Yu Li","Fengli Xu","Yong Li"],"pdf_url":"https://arxiv.org/pdf/2402.02563v3.pdf","comment":"18 pages, 10 figures, 14 tables"},{"id":"http://arxiv.org/abs/2406.03920v2","updated":"2024-08-01T07:29:42Z","published":"2024-06-06T10:02:49Z","title":"Towards Physically Consistent Deep Learning For Climate Model\n  Parameterizations","summary":"  Climate models play a critical role in understanding and projecting climate\nchange. Due to their complexity, their horizontal resolution of about 40-100 km\nremains too coarse to resolve processes such as clouds and convection, which\nneed to be approximated via parameterizations. These parameterizations are a\nmajor source of systematic errors and large uncertainties in climate\nprojections. Deep learning (DL)-based parameterizations, trained on data from\ncomputationally expensive short, high-resolution simulations, have shown great\npromise for improving climate models in that regard. However, their lack of\ninterpretability and tendency to learn spurious non-physical correlations\nresult in reduced trust in the climate simulation. We propose an efficient\nsupervised learning framework for DL-based parameterizations that leads to\nphysically consistent models with improved interpretability and negligible\ncomputational overhead compared to standard supervised training. First, key\nfeatures determining the target physical processes are uncovered. Subsequently,\nthe neural network is fine-tuned using only those relevant features. We show\nempirically that our method robustly identifies a small subset of the inputs as\nactual physical drivers, therefore, removing spurious non-physical\nrelationships. This results in by design physically consistent and\ninterpretable neural networks while maintaining the predictive performance of\nunconstrained black-box DL-based parameterizations.\n","authors":["Birgit Khbacher","Fernando Iglesias-Suarez","Niki Kilbertus","Veronika Eyring"],"pdf_url":"https://arxiv.org/pdf/2406.03920v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2210.09634v4","updated":"2024-08-01T05:09:34Z","published":"2022-10-18T07:03:14Z","title":"DPIS: An Enhanced Mechanism for Differentially Private SGD with\n  Importance Sampling","summary":"  Nowadays, differential privacy (DP) has become a well-accepted standard for\nprivacy protection, and deep neural networks (DNN) have been immensely\nsuccessful in machine learning. The combination of these two techniques, i.e.,\ndeep learning with differential privacy, promises the privacy-preserving\nrelease of high-utility models trained with sensitive data such as medical\nrecords. A classic mechanism for this purpose is DP-SGD, which is a\ndifferentially private version of the stochastic gradient descent (SGD)\noptimizer commonly used for DNN training. Subsequent approaches have improved\nvarious aspects of the model training process, including noise decay schedule,\nmodel architecture, feature engineering, and hyperparameter tuning. However,\nthe core mechanism for enforcing DP in the SGD optimizer remains unchanged ever\nsince the original DP-SGD algorithm, which has increasingly become a\nfundamental barrier limiting the performance of DP-compliant machine learning\nsolutions.\n  Motivated by this, we propose DPIS, a novel mechanism for differentially\nprivate SGD training that can be used as a drop-in replacement of the core\noptimizer of DP-SGD, with consistent and significant accuracy gains over the\nlatter. The main idea is to employ importance sampling (IS) in each SGD\niteration for mini-batch selection, which reduces both sampling variance and\nthe amount of random noise injected to the gradients that is required to\nsatisfy DP. Integrating IS into the complex mathematical machinery of DP-SGD is\nhighly non-trivial. DPIS addresses the challenge through novel mechanism\ndesigns, fine-grained privacy analysis, efficiency enhancements, and an\nadaptive gradient clipping optimization. Extensive experiments on four\nbenchmark datasets, namely MNIST, FMNIST, CIFAR-10 and IMDb, demonstrate the\nsuperior effectiveness of DPIS over existing solutions for deep learning with\ndifferential privacy.\n","authors":["Jianxin Wei","Ergute Bao","Xiaokui Xiao","Yin Yang"],"pdf_url":"https://arxiv.org/pdf/2210.09634v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12384v4","updated":"2024-08-01T03:32:49Z","published":"2024-03-19T02:49:32Z","title":"AlignRec: Aligning and Training in Multimodal Recommendations","summary":"  With the development of multimedia systems, multimodal recommendations are\nplaying an essential role, as they can leverage rich contexts beyond\ninteractions. Existing methods mainly regard multimodal information as an\nauxiliary, using them to help learn ID features; However, there exist semantic\ngaps among multimodal content features and ID-based features, for which\ndirectly using multimodal information as an auxiliary would lead to\nmisalignment in representations of users and items. In this paper, we first\nsystematically investigate the misalignment issue in multimodal\nrecommendations, and propose a solution named AlignRec. In AlignRec, the\nrecommendation objective is decomposed into three alignments, namely alignment\nwithin contents, alignment between content and categorical ID, and alignment\nbetween users and items. Each alignment is characterized by a specific\nobjective function and is integrated into our multimodal recommendation\nframework. To effectively train AlignRec, we propose starting from pre-training\nthe first alignment to obtain unified multimodal features and subsequently\ntraining the following two alignments together with these features as input. As\nit is essential to analyze whether each multimodal feature helps in training\nand accelerate the iteration cycle of recommendation models, we design three\nnew classes of metrics to evaluate intermediate performance. Our extensive\nexperiments on three real-world datasets consistently verify the superiority of\nAlignRec compared to nine baselines. We also find that the multimodal features\ngenerated by AlignRec are better than currently used ones, which are to be\nopen-sourced in our repository https://github.com/sjtulyf123/AlignRec_CIKM24.\n","authors":["Yifan Liu","Kangning Zhang","Xiangyuan Ren","Yanhua Huang","Jiarui Jin","Yingjie Qin","Ruilong Su","Ruiwen Xu","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.12384v4.pdf","comment":"9 page paper, 2 page appendix. Accepted by CIKM24"},{"id":"http://arxiv.org/abs/2406.01572v2","updated":"2024-08-01T03:29:32Z","published":"2024-06-03T17:51:54Z","title":"Unlocking Guidance for Discrete State-Space Diffusion and Flow Models","summary":"  Generative models on discrete state-spaces have a wide range of potential\napplications, particularly in the domain of natural sciences. In continuous\nstate-spaces, controllable and flexible generation of samples with desired\nproperties has been realized using guidance on diffusion and flow models.\nHowever, these guidance approaches are not readily amenable to discrete\nstate-space models. Consequently, we introduce a general and principled method\nfor applying guidance on such models. Our method depends on leveraging\ncontinuous-time Markov processes on discrete state-spaces, which unlocks\ncomputational tractability for sampling from a desired guided distribution. We\ndemonstrate the utility of our approach, Discrete Guidance, on a range of\napplications including guided generation of images, small-molecules, DNA\nsequences and protein sequences.\n","authors":["Hunter Nisonoff","Junhao Xiong","Stephan Allenspach","Jennifer Listgarten"],"pdf_url":"https://arxiv.org/pdf/2406.01572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11539v3","updated":"2024-08-01T03:19:03Z","published":"2023-12-15T23:34:05Z","title":"KGLens: Towards Efficient and Effective Knowledge Probing of Large\n  Language Models with Knowledge Graphs","summary":"  Large Language Models (LLMs) might hallucinate facts, while curated Knowledge\nGraph (KGs) are typically factually reliable especially with domain-specific\nknowledge. Measuring the alignment between KGs and LLMs can effectively probe\nthe factualness and identify the knowledge blind spots of LLMs. However,\nverifying the LLMs over extensive KGs can be expensive. In this paper, we\npresent KGLens, a Thompson-sampling-inspired framework aimed at effectively and\nefficiently measuring the alignment between KGs and LLMs. KGLens features a\ngraph-guided question generator for converting KGs into natural language, along\nwith a carefully designed importance sampling strategy based on parameterized\nKG structure to expedite KG traversal. Our simulation experiment compares the\nbrute force method with KGLens under six different sampling methods,\ndemonstrating that our approach achieves superior probing efficiency.\nLeveraging KGLens, we conducted in-depth analyses of the factual accuracy of\nten LLMs across three large domain-specific KGs from Wikidata, composing over\n19K edges, 700 relations, and 21K entities. Human evaluation results indicate\nthat KGLens can assess LLMs with a level of accuracy nearly equivalent to that\nof human annotators, achieving 95.7% of the accuracy rate.\n","authors":["Shangshang Zheng","He Bai","Yizhe Zhang","Yi Su","Xiaochuan Niu","Navdeep Jaitly"],"pdf_url":"https://arxiv.org/pdf/2312.11539v3.pdf","comment":"ACL 2024 Workshop Towards Knowledgeable Language Models"},{"id":"http://arxiv.org/abs/2407.21740v2","updated":"2024-08-01T03:16:43Z","published":"2024-07-31T16:52:00Z","title":"Contrastive Factor Analysis","summary":"  Factor analysis, often regarded as a Bayesian variant of matrix\nfactorization, offers superior capabilities in capturing uncertainty, modeling\ncomplex dependencies, and ensuring robustness. As the deep learning era\narrives, factor analysis is receiving less and less attention due to their\nlimited expressive ability. On the contrary, contrastive learning has emerged\nas a potent technique with demonstrated efficacy in unsupervised\nrepresentational learning. While the two methods are different paradigms,\nrecent theoretical analysis has revealed the mathematical equivalence between\ncontrastive learning and matrix factorization, providing a potential\npossibility for factor analysis combined with contrastive learning. Motivated\nby the interconnectedness of contrastive learning, matrix factorization, and\nfactor analysis, this paper introduces a novel Contrastive Factor Analysis\nframework, aiming to leverage factor analysis's advantageous properties within\nthe realm of contrastive learning. To further leverage the interpretability\nproperties of non-negative factor analysis, which can learn disentangled\nrepresentations, contrastive factor analysis is extended to a non-negative\nversion. Finally, extensive experimental validation showcases the efficacy of\nthe proposed contrastive (non-negative) factor analysis methodology across\nmultiple key properties, including expressiveness, robustness,\ninterpretability, and accurate uncertainty estimation.\n","authors":["Zhibin Duan","Tiansheng Wen","Yifei Wang","Chen Zhu","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.21740v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07221v2","updated":"2024-08-01T03:02:44Z","published":"2024-03-23T00:49:40Z","title":"Improving Retrieval for RAG based Question Answering Models on Financial\n  Documents","summary":"  The effectiveness of Large Language Models (LLMs) in generating accurate\nresponses relies heavily on the quality of input provided, particularly when\nemploying Retrieval Augmented Generation (RAG) techniques. RAG enhances LLMs by\nsourcing the most relevant text chunk(s) to base queries upon. Despite the\nsignificant advancements in LLMs' response quality in recent years, users may\nstill encounter inaccuracies or irrelevant answers; these issues often stem\nfrom suboptimal text chunk retrieval by RAG rather than the inherent\ncapabilities of LLMs. To augment the efficacy of LLMs, it is crucial to refine\nthe RAG process. This paper explores the existing constraints of RAG pipelines\nand introduces methodologies for enhancing text retrieval. It delves into\nstrategies such as sophisticated chunking techniques, query expansion, the\nincorporation of metadata annotations, the application of re-ranking\nalgorithms, and the fine-tuning of embedding algorithms. Implementing these\napproaches can substantially improve the retrieval quality, thereby elevating\nthe overall performance and reliability of LLMs in processing and responding to\nqueries.\n","authors":["Spurthi Setty","Harsh Thakkar","Alyssa Lee","Eden Chung","Natan Vidra"],"pdf_url":"https://arxiv.org/pdf/2404.07221v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01614v2","updated":"2024-08-01T02:56:58Z","published":"2024-06-28T01:46:10Z","title":"Enhancing Stability for Large Models Training in Constrained Bandwidth\n  Networks","summary":"  Training extremely large language models with billions of parameters is a\ncomputationally intensive task that pushes the limits of current data parallel\ntraining systems. While techniques like ZeRO++ have enabled efficient\ndistributed training of such giant models on inexpensive low-bandwidth\nclusters, they can suffer from convergence issues due to potential race\nconditions in the hierarchical partitioning (hpZ) scheme employed to reduce\ncross-machine communication. In this work, we first show how these race\nconditions cause instability when training models with billions of parameters.\nWe then propose a modification to the partitioning algorithm that addresses\nthese convergence challenges while maintaining competitive training efficiency.\nEmpirical evaluation on training the multi-billion parameters Falcon Models and\nLlama-2 models demonstrates the updated algorithm's ability to achieve reliable\nconvergence on these massive models, where stock ZeRO++ hpZ fails to converge.\nThe updated algorithm enables robust training of larger models with 98\\%\nthroughput and model training speed improvement without sacrificing the quality\nof convergence.\n","authors":["Yun Dai","Tejas Dharamsi","Byron Hsu","Tao Song","Hamed Firooz"],"pdf_url":"https://arxiv.org/pdf/2407.01614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14055v2","updated":"2024-08-01T02:19:08Z","published":"2024-07-19T06:31:22Z","title":"Quantum Hamiltonian Embedding of Images for Data Reuploading Classifiers","summary":"  When applying quantum computing to machine learning tasks, one of the first\nconsiderations is the design of the quantum machine learning model itself.\nConventionally, the design of quantum machine learning algorithms relies on the\n``quantisation\" of classical learning algorithms, such as using quantum linear\nalgebra to implement important subroutines of classical algorithms, if not the\nentire algorithm, seeking to achieve quantum advantage through possible\nrun-time accelerations brought by quantum computing. However, recent research\nhas started questioning whether quantum advantage via speedup is the right goal\nfor quantum machine learning [1]. Research also has been undertaken to exploit\nproperties that are unique to quantum systems, such as quantum contextuality,\nto better design quantum machine learning models [2]. In this paper, we take an\nalternative approach by incorporating the heuristics and empirical evidences\nfrom the design of classical deep learning algorithms to the design of quantum\nneural networks. We first construct a model based on the data reuploading\ncircuit [3] with the quantum Hamiltonian data embedding unitary [4]. Through\nnumerical experiments on images datasets, including the famous MNIST and\nFashionMNIST datasets, we demonstrate that our model outperforms the quantum\nconvolutional neural network (QCNN)[5] by a large margin (up to over 40% on\nMNIST test set). Based on the model design process and numerical results, we\nthen laid out six principles for designing quantum machine learning models,\nespecially quantum neural networks.\n","authors":["Peiyong Wang","Casey R. Myers","Lloyd C. L. Hollenberg","Udaya Parampalli"],"pdf_url":"https://arxiv.org/pdf/2407.14055v2.pdf","comment":"11 figures, 31 pages. Code available on\n  https://github.com/peiyong-addwater/HamEmbedding. Author affiliation updated\n  for v2. Acknowledgements and funding information added for v2"},{"id":"http://arxiv.org/abs/2407.21266v2","updated":"2024-08-01T01:59:58Z","published":"2024-07-31T01:07:21Z","title":"DDU-Net: A Domain Decomposition-based CNN for High-Resolution Image\n  Segmentation on Multiple GPUs","summary":"  The segmentation of ultra-high resolution images poses challenges such as\nloss of spatial information or computational inefficiency. In this work, a\nnovel approach that combines encoder-decoder architectures with domain\ndecomposition strategies to address these challenges is proposed. Specifically,\na domain decomposition-based U-Net (DDU-Net) architecture is introduced, which\npartitions input images into non-overlapping patches that can be processed\nindependently on separate devices. A communication network is added to\nfacilitate inter-patch information exchange to enhance the understanding of\nspatial context. Experimental validation is performed on a synthetic dataset\nthat is designed to measure the effectiveness of the communication network.\nThen, the performance is tested on the DeepGlobe land cover classification\ndataset as a real-world benchmark data set. The results demonstrate that the\napproach, which includes inter-patch communication for images divided into\n$16\\times16$ non-overlapping subimages, achieves a $2-3\\,\\%$ higher\nintersection over union (IoU) score compared to the same network without\ninter-patch communication. The performance of the network which includes\ncommunication is equivalent to that of a baseline U-Net trained on the full\nimage, showing that our model provides an effective solution for segmenting\nultra-high-resolution images while preserving spatial context. The code is\navailable at https://github.com/corne00/HiRes-Seg-CNN.\n","authors":["Corn Verburg","Alexander Heinlein","Eric C. Cyr"],"pdf_url":"https://arxiv.org/pdf/2407.21266v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.01593v6","updated":"2024-08-01T01:49:47Z","published":"2021-10-04T17:41:53Z","title":"Generalized Kernel Thinning","summary":"  The kernel thinning (KT) algorithm of Dwivedi and Mackey (2021) compresses a\nprobability distribution more effectively than independent sampling by\ntargeting a reproducing kernel Hilbert space (RKHS) and leveraging a less\nsmooth square-root kernel. Here we provide four improvements. First, we show\nthat KT applied directly to the target RKHS yields tighter, dimension-free\nguarantees for any kernel, any distribution, and any fixed function in the\nRKHS. Second, we show that, for analytic kernels like Gaussian, inverse\nmultiquadric, and sinc, target KT admits maximum mean discrepancy (MMD)\nguarantees comparable to or better than those of square-root KT without making\nexplicit use of a square-root kernel. Third, we prove that KT with a fractional\npower kernel yields better-than-Monte-Carlo MMD guarantees for non-smooth\nkernels, like Laplace and Mat\\'ern, that do not have square-roots. Fourth, we\nestablish that KT applied to a sum of the target and power kernels (a procedure\nwe call KT+) simultaneously inherits the improved MMD guarantees of power KT\nand the tighter individual function guarantees of target KT. In our experiments\nwith target KT and KT+, we witness significant improvements in integration\nerror even in $100$ dimensions and when compressing challenging differential\nequation posteriors.\n","authors":["Raaz Dwivedi","Lester Mackey"],"pdf_url":"https://arxiv.org/pdf/2110.01593v6.pdf","comment":"Corrected B-spline and Sinc rates in Table 3"},{"id":"http://arxiv.org/abs/2202.05525v2","updated":"2024-08-01T01:42:10Z","published":"2022-02-11T09:45:11Z","title":"From Unsupervised to Few-shot Graph Anomaly Detection: A Multi-scale\n  Contrastive Learning Approach","summary":"  Anomaly detection from graph data is an important data mining task in many\napplications such as social networks, finance, and e-commerce. Existing efforts\nin graph anomaly detection typically only consider the information in a single\nscale (view), thus inevitably limiting their capability in capturing anomalous\npatterns in complex graph data. To address this limitation, we propose a novel\nframework, graph ANomaly dEtection framework with Multi-scale cONtrastive\nlEarning (ANEMONE in short). By using a graph neural network as a backbone to\nencode the information from multiple graph scales (views), we learn better\nrepresentation for nodes in a graph. In maximizing the agreements between\ninstances at both the patch and context levels concurrently, we estimate the\nanomaly score of each node with a statistical anomaly estimator according to\nthe degree of agreement from multiple perspectives. To further exploit a\nhandful of ground-truth anomalies (few-shot anomalies) that may be collected in\nreal-life applications, we further propose an extended algorithm, ANEMONE-FS,\nto integrate valuable information in our method. We conduct extensive\nexperiments under purely unsupervised settings and few-shot anomaly detection\nsettings, and we demonstrate that the proposed method ANEMONE and its variant\nANEMONE-FS consistently outperform state-of-the-art algorithms on six benchmark\ndatasets.\n","authors":["Yu Zheng","Ming Jin","Yixin Liu","Lianhua Chi","Khoa T. Phan","Yi-Ping Phoebe Chen"],"pdf_url":"https://arxiv.org/pdf/2202.05525v2.pdf","comment":"13 pages, 5 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.20299v2","updated":"2024-08-01T01:33:48Z","published":"2024-07-29T04:02:17Z","title":"Dataset Distillation for Offline Reinforcement Learning","summary":"  Offline reinforcement learning often requires a quality dataset that we can\ntrain a policy on. However, in many situations, it is not possible to get such\na dataset, nor is it easy to train a policy to perform well in the actual\nenvironment given the offline data. We propose using data distillation to train\nand distill a better dataset which can then be used for training a better\npolicy model. We show that our method is able to synthesize a dataset where a\nmodel trained on it achieves similar performance to a model trained on the full\ndataset or a model trained using percentile behavioral cloning. Our project\nsite is available at\n$\\href{https://datasetdistillation4rl.github.io}{\\text{here}}$. We also provide\nour implementation at $\\href{https://github.com/ggflow123/DDRL}{\\text{this\nGitHub repository}}$.\n","authors":["Jonathan Light","Yuanzhe Liu","Ziniu Hu"],"pdf_url":"https://arxiv.org/pdf/2407.20299v2.pdf","comment":"ICML 2024 DMLR Workshop"},{"id":"http://arxiv.org/abs/2407.12254v2","updated":"2024-08-01T01:21:57Z","published":"2024-07-17T01:51:27Z","title":"COKE: Causal Discovery with Chronological Order and Expert Knowledge in\n  High Proportion of Missing Manufacturing Data","summary":"  Understanding causal relationships between machines is crucial for fault\ndiagnosis and optimization in manufacturing processes. Real-world datasets\nfrequently exhibit up to 90% missing data and high dimensionality from hundreds\nof sensors. These datasets also include domain-specific expert knowledge and\nchronological order information, reflecting the recording order across\ndifferent machines, which is pivotal for discerning causal relationships within\nthe manufacturing data. However, previous methods for handling missing data in\nscenarios akin to real-world conditions have not been able to effectively\nutilize expert knowledge. Conversely, prior methods that can incorporate expert\nknowledge struggle with datasets that exhibit missing values. Therefore, we\npropose COKE to construct causal graphs in manufacturing datasets by leveraging\nexpert knowledge and chronological order among sensors without imputing missing\ndata. Utilizing the characteristics of the recipe, we maximize the use of\nsamples with missing values, derive embeddings from intersections with an\ninitial graph that incorporates expert knowledge and chronological order, and\ncreate a sensor ordering graph. The graph-generating process has been optimized\nby an actor-critic architecture to obtain a final graph that has a maximum\nreward. Experimental evaluations in diverse settings of sensor quantities and\nmissing proportions demonstrate that our approach compared with the benchmark\nmethods shows an average improvement of 39.9% in the F1-score. Moreover, the\nF1-score improvement can reach 62.6% when considering the configuration similar\nto real-world datasets, and 85.0% in real-world semiconductor datasets. The\nsource code is available at https://github.com/OuTingYun/COKE.\n","authors":["Ting-Yun Ou","Ching Chang","Wen-Chih Peng"],"pdf_url":"https://arxiv.org/pdf/2407.12254v2.pdf","comment":"This paper has been accepted by the ACM International Conference on\n  Information and Knowledge Management (CIKM) 2024"},{"id":"http://arxiv.org/abs/2401.10467v2","updated":"2024-08-01T01:07:35Z","published":"2024-01-19T03:39:43Z","title":"Learning Backdoors for Mixed Integer Linear Programs with Contrastive\n  Learning","summary":"  Many real-world problems can be efficiently modeled as Mixed Integer Linear\nPrograms (MILPs) and solved with the Branch-and-Bound method. Prior work has\nshown the existence of MILP backdoors, small sets of variables such that\nprioritizing branching on them when possible leads to faster running times.\nHowever, finding high-quality backdoors that improve running times remains an\nopen question. Previous work learns to estimate the relative solver speed of\nrandomly sampled backdoors through ranking and then decide whether to use the\nhighest-ranked backdoor candidate. In this paper, we utilize the Monte-Carlo\ntree search method to collect backdoors for training, rather than relying on\nrandom sampling, and adapt a contrastive learning framework to train a Graph\nAttention Network model to predict backdoors. Our method, evaluated on several\ncommon MILP problem domains, demonstrates performance improvements over both\nGurobi and previous models.\n","authors":["Junyang Cai","Taoan Huang","Bistra Dilkina"],"pdf_url":"https://arxiv.org/pdf/2401.10467v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00764v1","updated":"2024-08-01T17:59:46Z","published":"2024-08-01T17:59:46Z","title":"AgentGen: Enhancing Planning Abilities for Large Language Model based\n  Agent via Environment and Task Generation","summary":"  Large Language Model (LLM) based agents have garnered significant attention\nand are becoming increasingly popular. Furthermore, planning ability is a\ncrucial component of an LLM-based agent, involving interaction with the\nenvironment and executing actions to complete a planning task, which generally\nentails achieving a desired goal from an initial state. This paper investigates\nenhancing the planning abilities of LLMs through instruction tuning, referred\nto as agent training. Recent studies have demonstrated that utilizing\nexpert-level trajectory for instruction-tuning LLMs effectively enhances their\nplanning capabilities. However, existing work primarily focuses on synthesizing\ntrajectories from manually designed planning tasks and environments. The\nlabor-intensive nature of creating these environments and tasks impedes the\ngeneration of sufficiently varied and extensive trajectories. To address this\nlimitation, this paper explores the automated synthesis of diverse environments\nand a gradual range of planning tasks, from easy to difficult. We introduce a\nframework, AgentGen, that leverages LLMs first to generate environments and\nsubsequently generate planning tasks conditioned on these environments.\nSpecifically, to improve environmental diversity, we propose using an\ninspiration corpus composed of various domain-specific text segments as the\ncontext for synthesizing environments. Moreover, to increase the difficulty\ndiversity of generated planning tasks, we propose a bidirectional evolution\nmethod, Bi-Evol, that evolves planning tasks from easier and harder directions\nto synthesize a task set with a smoother difficulty curve. The evaluation\nresults derived from AgentBoard show that AgentGen greatly improves LLMs'\nplanning ability, e.g., the AgentGen instruction-tuned Llama-3 8B surpasses\nGPT-3.5 in overall performance. Moreover, in certain tasks, it even outperforms\nGPT-4.\n","authors":["Mengkang Hu","Pu Zhao","Can Xu","Qingfeng Sun","Jianguang Lou","Qingwei Lin","Ping Luo","Saravan Rajmohan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00764v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00761v1","updated":"2024-08-01T17:59:12Z","published":"2024-08-01T17:59:12Z","title":"Tamper-Resistant Safeguards for Open-Weight LLMs","summary":"  Rapid advances in the capabilities of large language models (LLMs) have\nraised widespread concerns regarding their potential for malicious use.\nOpen-weight LLMs present unique challenges, as existing safeguards lack\nrobustness to tampering attacks that modify model weights. For example, recent\nworks have demonstrated that refusal and unlearning safeguards can be trivially\nremoved with a few steps of fine-tuning. These vulnerabilities necessitate new\napproaches for enabling the safe release of open-weight LLMs. We develop a\nmethod, called TAR, for building tamper-resistant safeguards into open-weight\nLLMs such that adversaries cannot remove the safeguards even after thousands of\nsteps of fine-tuning. In extensive evaluations and red teaming analyses, we\nfind that our method greatly improves tamper-resistance while preserving benign\ncapabilities. Our results demonstrate that tamper-resistance is a tractable\nproblem, opening up a promising new avenue to improve the safety and security\nof open-weight LLMs.\n","authors":["Rishub Tamirisa","Bhrugu Bharathi","Long Phan","Andy Zhou","Alice Gatti","Tarun Suresh","Maxwell Lin","Justin Wang","Rowan Wang","Ron Arel","Andy Zou","Dawn Song","Bo Li","Dan Hendrycks","Mantas Mazeika"],"pdf_url":"https://arxiv.org/pdf/2408.00761v1.pdf","comment":"Website: https://www.tamper-resistant-safeguards.com"},{"id":"http://arxiv.org/abs/2408.00760v1","updated":"2024-08-01T17:59:09Z","published":"2024-08-01T17:59:09Z","title":"Smoothed Energy Guidance: Guiding Diffusion Models with Reduced Energy\n  Curvature of Attention","summary":"  Conditional diffusion models have shown remarkable success in visual content\ngeneration, producing high-quality samples across various domains, largely due\nto classifier-free guidance (CFG). Recent attempts to extend guidance to\nunconditional models have relied on heuristic techniques, resulting in\nsuboptimal generation quality and unintended effects. In this work, we propose\nSmoothed Energy Guidance (SEG), a novel training- and condition-free approach\nthat leverages the energy-based perspective of the self-attention mechanism to\nenhance image generation. By defining the energy of self-attention, we\nintroduce a method to reduce the curvature of the energy landscape of attention\nand use the output as the unconditional prediction. Practically, we control the\ncurvature of the energy landscape by adjusting the Gaussian kernel parameter\nwhile keeping the guidance scale parameter fixed. Additionally, we present a\nquery blurring method that is equivalent to blurring the entire attention\nweights without incurring quadratic complexity in the number of tokens. In our\nexperiments, SEG achieves a Pareto improvement in both quality and the\nreduction of side effects. The code is available at\n\\url{https://github.com/SusungHong/SEG-SDXL}.\n","authors":["Susung Hong"],"pdf_url":"https://arxiv.org/pdf/2408.00760v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00754v1","updated":"2024-08-01T17:57:12Z","published":"2024-08-01T17:57:12Z","title":"Coarse Correspondence Elicit 3D Spacetime Understanding in Multimodal\n  Language Model","summary":"  Multimodal language models (MLLMs) are increasingly being implemented in\nreal-world environments, necessitating their ability to interpret 3D spaces and\ncomprehend temporal dynamics. Despite their potential, current top models\nwithin our community still fall short in adequately understanding spatial and\ntemporal dimensions. We introduce Coarse Correspondence, a simple,\ntraining-free, effective, and general-purpose visual prompting method to elicit\n3D and temporal understanding in multimodal LLMs. Our method uses a lightweight\ntracking model to find object correspondences between frames in a video or\nbetween sets of image viewpoints. It selects the most frequent object instances\nand visualizes them with markers with unique IDs in the image. With this simple\napproach, we achieve state-of-the-art results on 3D understanding benchmarks\nincluding ScanQA (+20.5\\%) and a subset of OpenEQA (+9.7\\%), and on long-form\nvideo benchmarks such as EgoSchema (+6.0\\%). We also curate a small diagnostic\ndataset to evaluate whether MLLMs can reason about space from a described\nviewpoint other than the camera viewpoint. Again, Coarse Correspondence\nimproves spatial perspective-taking abilities but we highlight that MLLMs\nstruggle with this task. Together, we demonstrate that our simple prompting\nmethod can significantly aid downstream tasks that require 3D or temporal\nreasoning.\n","authors":["Benlin Liu","Yuhao Dong","Yiqin Wang","Yongming Rao","Yansong Tang","Wei-Chiu Ma","Ranjay Krishna"],"pdf_url":"https://arxiv.org/pdf/2408.00754v1.pdf","comment":"project page: https://coarse-correspondence.github.io"},{"id":"http://arxiv.org/abs/2408.00751v1","updated":"2024-08-01T17:54:01Z","published":"2024-08-01T17:54:01Z","title":"A Policy-Gradient Approach to Solving Imperfect-Information Games with\n  Iterate Convergence","summary":"  Policy gradient methods have become a staple of any single-agent\nreinforcement learning toolbox, due to their combination of desirable\nproperties: iterate convergence, efficient use of stochastic trajectory\nfeedback, and theoretically-sound avoidance of importance sampling corrections.\nIn multi-agent imperfect-information settings (extensive-form games), however,\nit is still unknown whether the same desiderata can be guaranteed while\nretaining theoretical guarantees. Instead, sound methods for extensive-form\ngames rely on approximating counterfactual values (as opposed to Q values),\nwhich are incompatible with policy gradient methodologies. In this paper, we\ninvestigate whether policy gradient can be safely used in two-player zero-sum\nimperfect-information extensive-form games (EFGs). We establish positive\nresults, showing for the first time that a policy gradient method leads to\nprovable best-iterate convergence to a regularized Nash equilibrium in\nself-play.\n","authors":["Mingyang Liu","Gabriele Farina","Asuman Ozdaglar"],"pdf_url":"https://arxiv.org/pdf/2408.00751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00749v1","updated":"2024-08-01T17:52:10Z","published":"2024-08-01T17:52:10Z","title":"Leaf Angle Estimation using Mask R-CNN and LETR Vision Transformer","summary":"  Modern day studies show a high degree of correlation between high yielding\ncrop varieties and plants with upright leaf angles. It is observed that plants\nwith upright leaf angles intercept more light than those without upright leaf\nangles, leading to a higher rate of photosynthesis. Plant scientists and\nbreeders benefit from tools that can directly measure plant parameters in the\nfield i.e. on-site phenotyping. The estimation of leaf angles by manual means\nin a field setting is tedious and cumbersome. We mitigate the tedium using a\ncombination of the Mask R-CNN instance segmentation neural network, and Line\nSegment Transformer (LETR), a vision transformer. The proposed Computer Vision\n(CV) pipeline is applied on two image datasets, Summer 2015-Ames ULA and Summer\n2015- Ames MLA, with a combined total of 1,827 plant images collected in the\nfield using FieldBook, an Android application aimed at on-site phenotyping. The\nleaf angles estimated by the proposed pipeline on the image datasets are\ncompared to two independent manual measurements using ImageJ, a Java-based\nimage processing program developed at the National Institutes of Health and the\nLaboratory for Optical and Computational Instrumentation. The results, when\ncompared for similarity using the Cosine Similarity measure, exhibit 0.98\nsimilarity scores on both independent measurements of Summer 2015-Ames ULA and\nSummer 2015-Ames MLA image datasets, demonstrating the feasibility of the\nproposed pipeline for on-site measurement of leaf angles.\n","authors":["Venkat Margapuri","Prapti Thapaliya","Trevor Rife"],"pdf_url":"https://arxiv.org/pdf/2408.00749v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00728v1","updated":"2024-08-01T17:20:24Z","published":"2024-08-01T17:20:24Z","title":"CERT-ED: Certifiably Robust Text Classification for Edit Distance","summary":"  With the growing integration of AI in daily life, ensuring the robustness of\nsystems to inference-time attacks is crucial. Among the approaches for\ncertifying robustness to such adversarial examples, randomized smoothing has\nemerged as highly promising due to its nature as a wrapper around arbitrary\nblack-box models. Previous work on randomized smoothing in natural language\nprocessing has primarily focused on specific subsets of edit distance\noperations, such as synonym substitution or word insertion, without exploring\nthe certification of all edit operations. In this paper, we adapt Randomized\nDeletion (Huang et al., 2023) and propose, CERTified Edit Distance defense\n(CERT-ED) for natural language classification. Through comprehensive\nexperiments, we demonstrate that CERT-ED outperforms the existing Hamming\ndistance method RanMASK (Zeng et al., 2023) in 4 out of 5 datasets in terms of\nboth accuracy and the cardinality of the certificate. By covering various\nthreat models, including 5 direct and 5 transfer attacks, our method improves\nempirical robustness in 38 out of 50 settings.\n","authors":["Zhuoqun Huang","Neil G Marchant","Olga Ohrimenko","Benjamin I. P. Rubinstein"],"pdf_url":"https://arxiv.org/pdf/2408.00728v1.pdf","comment":"22 pages, 3 figures, 12 tables. Include 11 pages of appendices"},{"id":"http://arxiv.org/abs/2408.00716v1","updated":"2024-08-01T17:01:29Z","published":"2024-08-01T17:01:29Z","title":"A Natural Language Processing Framework for Hotel Recommendation Based\n  on Users' Text Reviews","summary":"  Recently, the application of Artificial Intelligence algorithms in hotel\nrecommendation systems has become an increasingly popular topic. One such\nmethod that has proven to be effective in this field is Deep Learning,\nespecially Natural Language processing models, which are able to extract\nsemantic knowledge from user's text reviews to create more efficient\nrecommendation systems. This can lead to the development of intelligent models\nthat can classify a user's preferences and emotions based on their feedback in\nthe form of text reviews about their hotel stay experience. In this study, we\npropose a Natural Language Processing framework that utilizes customer text\nreviews to provide personalized recommendations for the most appropriate hotel\nbased on their preferences. The framework is based on Bidirectional Encoder\nRepresentations from Transformers (BERT) and a fine-tuning/validation pipeline\nthat categorizes customer hotel review texts into \"Bad,\" \"Good,\" or \"Excellent\"\nrecommended hotels. Our findings indicate that the hotel recommendation system\nwe propose can significantly enhance the user experience of booking\naccommodations by providing personalized recommendations based on user\npreferences and previous booking history.\n","authors":["Lavrentia Aravani","Emmanuel Pintelas","Christos Pierrakeas","Panagiotis Pintelas"],"pdf_url":"https://arxiv.org/pdf/2408.00716v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00714v1","updated":"2024-08-01T17:00:08Z","published":"2024-08-01T17:00:08Z","title":"SAM 2: Segment Anything in Images and Videos","summary":"  We present Segment Anything Model 2 (SAM 2), a foundation model towards\nsolving promptable visual segmentation in images and videos. We build a data\nengine, which improves model and data via user interaction, to collect the\nlargest video segmentation dataset to date. Our model is a simple transformer\narchitecture with streaming memory for real-time video processing. SAM 2\ntrained on our data provides strong performance across a wide range of tasks.\nIn video segmentation, we observe better accuracy, using 3x fewer interactions\nthan prior approaches. In image segmentation, our model is more accurate and 6x\nfaster than the Segment Anything Model (SAM). We believe that our data, model,\nand insights will serve as a significant milestone for video segmentation and\nrelated perception tasks. We are releasing a version of our model, the dataset\nand an interactive demo.\n","authors":["Nikhila Ravi","Valentin Gabeur","Yuan-Ting Hu","Ronghang Hu","Chaitanya Ryali","Tengyu Ma","Haitham Khedr","Roman Rdle","Chloe Rolland","Laura Gustafson","Eric Mintun","Junting Pan","Kalyan Vasudev Alwala","Nicolas Carion","Chao-Yuan Wu","Ross Girshick","Piotr Dollr","Christoph Feichtenhofer"],"pdf_url":"https://arxiv.org/pdf/2408.00714v1.pdf","comment":"Website: https://ai.meta.com/sam2"},{"id":"http://arxiv.org/abs/2408.00713v1","updated":"2024-08-01T16:58:54Z","published":"2024-08-01T16:58:54Z","title":"Insurance Portfolio Pursuit with Reinforcement Learning","summary":"  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. Given a\ntarget portfolio, we term the problem of modulating offers to achieve this\ntarget portfolio the portfolio pursuit problem. We give a formulation of\nportfolio pursuit as a sequential decision making problem, and devise a novel\nreinforcement learning algorithm for its solution. We test our method on a\ncomplex synthetic market environment, and demonstrate that it outperforms a\nbaseline method which mimics current industry approaches to portfolio pursuit.\n","authors":["Edward James Young","Alistair Rogers","Elliott Tong","James Jordon"],"pdf_url":"https://arxiv.org/pdf/2408.00713v1.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.00707v1","updated":"2024-08-01T16:54:11Z","published":"2024-08-01T16:54:11Z","title":"Synthetic dual image generation for reduction of labeling efforts in\n  semantic segmentation of micrographs with a customized metric function","summary":"  Training of semantic segmentation models for material analysis requires\nmicrographs and their corresponding masks. It is quite unlikely that perfect\nmasks will be drawn, especially at the edges of objects, and sometimes the\namount of data that can be obtained is small, since only a few samples are\navailable. These aspects make it very problematic to train a robust model. We\ndemonstrate a workflow for the improvement of semantic segmentation models of\nmicrographs through the generation of synthetic microstructural images in\nconjunction with masks. The workflow only requires joining a few micrographs\nwith their respective masks to create the input for a Vector\nQuantised-Variational AutoEncoder model that includes an embedding space, which\nis trained such that a generative model (PixelCNN) learns the distribution of\neach input, transformed into discrete codes, and can be used to sample new\ncodes. The latter will eventually be decoded by VQ-VAE to generate images\nalongside corresponding masks for semantic segmentation. To evaluate the\nsynthetic data, we have trained U-Net models with different amounts of these\nsynthetic data in conjunction with real data. These models were then evaluated\nusing non-synthetic images only. Additionally, we introduce a customized metric\nderived from the mean Intersection over Union (mIoU). The proposed metric\nprevents a few falsely predicted pixels from greatly reducing the value of the\nmIoU. We have achieved a reduction in sample preparation and acquisition times,\nas well as the efforts, needed for image processing and labeling tasks, are\nless when it comes to training semantic segmentation model. The approach could\nbe generalized to various types of image data such that it serves as a\nuser-friendly solution for training models with a small number of real images.\n","authors":["Matias Oscar Volman Stern","Dominic Hohs","Andreas Jansche","Timo Bernthaler","Gerhard Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.00707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00706v1","updated":"2024-08-01T16:52:39Z","published":"2024-08-01T16:52:39Z","title":"Point-supervised Brain Tumor Segmentation with Box-prompted MedSAM","summary":"  Delineating lesions and anatomical structure is important for image-guided\ninterventions. Point-supervised medical image segmentation (PSS) has great\npotential to alleviate costly expert delineation labeling. However, due to the\nlack of precise size and boundary guidance, the effectiveness of PSS often\nfalls short of expectations. Although recent vision foundational models, such\nas the medical segment anything model (MedSAM), have made significant\nadvancements in bounding-box-prompted segmentation, it is not straightforward\nto utilize point annotation, and is prone to semantic ambiguity. In this\npreliminary study, we introduce an iterative framework to facilitate\nsemantic-aware point-supervised MedSAM. Specifically, the semantic box-prompt\ngenerator (SBPG) module has the capacity to convert the point input into\npotential pseudo bounding box suggestions, which are explicitly refined by the\nprototype-based semantic similarity. This is then succeeded by a prompt-guided\nspatial refinement (PGSR) module that harnesses the exceptional\ngeneralizability of MedSAM to infer the segmentation mask, which also updates\nthe box proposal seed in SBPG. Performance can be progressively improved with\nadequate iterations. We conducted an evaluation on BraTS2018 for the\nsegmentation of whole brain tumors and demonstrated its superior performance\ncompared to traditional PSS methods and on par with box-supervised methods.\n","authors":["Xiaofeng Liu","Jonghye Woo","Chao Ma","Jinsong Ouyang","Georges El Fakhri"],"pdf_url":"https://arxiv.org/pdf/2408.00706v1.pdf","comment":"2024 IEEE Nuclear Science Symposium and Medical Imaging Conference"},{"id":"http://arxiv.org/abs/2408.00700v1","updated":"2024-08-01T16:43:55Z","published":"2024-08-01T16:43:55Z","title":"You Can't Ignore Either: Unifying Structure and Feature Denoising for\n  Robust Graph Learning","summary":"  Recent research on the robustness of Graph Neural Networks (GNNs) under\nnoises or attacks has attracted great attention due to its importance in\nreal-world applications. Most previous methods explore a single noise source,\nrecovering corrupt node embedding by reliable structures bias or developing\nstructure learning with reliable node features. However, the noises and attacks\nmay come from both structures and features in graphs, making the graph\ndenoising a dilemma and challenging problem. In this paper, we develop a\nunified graph denoising (UGD) framework to unravel the deadlock between\nstructure and feature denoising. Specifically, a high-order neighborhood\nproximity evaluation method is proposed to recognize noisy edges, considering\nfeatures may be perturbed simultaneously. Moreover, we propose to refine noisy\nfeatures with reconstruction based on a graph auto-encoder. An iterative\nupdating algorithm is further designed to optimize the framework and acquire a\nclean graph, thus enabling robust graph learning for downstream tasks. Our UGD\nframework is self-supervised and can be easily implemented as a plug-and-play\nmodule. We carry out extensive experiments, which proves the effectiveness and\nadvantages of our method. Code is avalaible at\nhttps://github.com/YoungTimmy/UGD.\n","authors":["Tianmeng Yang","Jiahao Meng","Min Zhou","Yaming Yang","Yujing Wang","Xiangtai Li","Yunhai Tong"],"pdf_url":"https://arxiv.org/pdf/2408.00700v1.pdf","comment":"Accepted by CIKM'2024"},{"id":"http://arxiv.org/abs/2408.00699v1","updated":"2024-08-01T16:43:21Z","published":"2024-08-01T16:43:21Z","title":"Granular-Balls based Fuzzy Twin Support Vector Machine for\n  Classification","summary":"  The twin support vector machine (TWSVM) classifier has attracted increasing\nattention because of its low computational complexity. However, its performance\ntends to degrade when samples are affected by noise. The granular-ball fuzzy\nsupport vector machine (GBFSVM) classifier partly alleviates the adverse\neffects of noise, but it relies solely on the distance between the\ngranular-ball's center and the class center to design the granular-ball\nmembership function. In this paper, we first introduce the granular-ball twin\nsupport vector machine (GBTWSVM) classifier, which integrates granular-ball\ncomputing (GBC) with the twin support vector machine (TWSVM) classifier. By\nreplacing traditional point inputs with granular-balls, we demonstrate how to\nderive a pair of non-parallel hyperplanes for the GBTWSVM classifier by solving\na quadratic programming problem. Subsequently, we design the membership and\nnon-membership functions of granular-balls using Pythagorean fuzzy sets to\ndifferentiate the contributions of granular-balls in various regions.\nAdditionally, we develop the granular-ball fuzzy twin support vector machine\n(GBFTSVM) classifier by incorporating GBC with the fuzzy twin support vector\nmachine (FTSVM) classifier. We demonstrate how to derive a pair of non-parallel\nhyperplanes for the GBFTSVM classifier by solving a quadratic programming\nproblem. We also design algorithms for the GBTSVM classifier and the GBFTSVM\nclassifier. Finally, the superior classification performance of the GBTWSVM\nclassifier and the GBFTSVM classifier on 20 benchmark datasets underscores\ntheir scalability, efficiency, and robustness in tackling classification tasks.\n","authors":["Lixi Zhao","Weiping Ding","Duoqian Miao","Guangming Lang"],"pdf_url":"https://arxiv.org/pdf/2408.00699v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00695v1","updated":"2024-08-01T16:39:06Z","published":"2024-08-01T16:39:06Z","title":"Accelerating Full Waveform Inversion By Transfer Learning","summary":"  Full waveform inversion (FWI) is a powerful tool for reconstructing material\nfields based on sparsely measured data obtained by wave propagation. For\nspecific problems, discretizing the material field with a neural network (NN)\nimproves the robustness and reconstruction quality of the corresponding\noptimization problem. We call this method NN-based FWI. Starting from an\ninitial guess, the weights of the NN are iteratively updated to fit the\nsimulated wave signals to the sparsely measured data set. For gradient-based\noptimization, a suitable choice of the initial guess, i.e., a suitable NN\nweight initialization, is crucial for fast and robust convergence.\n  In this paper, we introduce a novel transfer learning approach to further\nimprove NN-based FWI. This approach leverages supervised pretraining to provide\na better NN weight initialization, leading to faster convergence of the\nsubsequent optimization problem. Moreover, the inversions yield physically more\nmeaningful local minima. The network is pretrained to predict the unknown\nmaterial field using the gradient information from the first iteration of\nconventional FWI. In our computational experiments on two-dimensional domains,\nthe training data set consists of reference simulations with arbitrarily\npositioned elliptical voids of different shapes and orientations. We compare\nthe performance of the proposed transfer learning NN-based FWI with three other\nmethods: conventional FWI, NN-based FWI without pretraining and conventional\nFWI with an initial guess predicted from the pretrained NN. Our results show\nthat transfer learning NN-based FWI outperforms the other methods in terms of\nconvergence speed and reconstruction quality.\n","authors":["Divya Shyam Singh","Leon Herrmann","Qing Sun","Tim Brchner","Felix Dietrich","Stefan Kollmannsberger"],"pdf_url":"https://arxiv.org/pdf/2408.00695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00681v1","updated":"2024-08-01T16:22:03Z","published":"2024-08-01T16:22:03Z","title":"Alpha-VI DeepONet: A prior-robust variational Bayesian approach for\n  enhancing DeepONets with uncertainty quantification","summary":"  We introduce a novel deep operator network (DeepONet) framework that\nincorporates generalised variational inference (GVI) using R\\'enyi's\n$\\alpha$-divergence to learn complex operators while quantifying uncertainty.\nBy incorporating Bayesian neural networks as the building blocks for the branch\nand trunk networks, our framework endows DeepONet with uncertainty\nquantification. The use of R\\'enyi's $\\alpha$-divergence, instead of the\nKullback-Leibler divergence (KLD), commonly used in standard variational\ninference, mitigates issues related to prior misspecification that are\nprevalent in Variational Bayesian DeepONets. This approach offers enhanced\nflexibility and robustness. We demonstrate that modifying the variational\nobjective function yields superior results in terms of minimising the mean\nsquared error and improving the negative log-likelihood on the test set. Our\nframework's efficacy is validated across various mechanical systems, where it\noutperforms both deterministic and standard KLD-based VI DeepONets in\npredictive accuracy and uncertainty quantification. The hyperparameter\n$\\alpha$, which controls the degree of robustness, can be tuned to optimise\nperformance for specific problems. We apply this approach to a range of\nmechanics problems, including gravity pendulum, advection-diffusion, and\ndiffusion-reaction systems. Our findings underscore the potential of\n$\\alpha$-VI DeepONet to advance the field of data-driven operator learning and\nits applications in engineering and scientific domains.\n","authors":["Soban Nasir Lone","Subhayan De","Rajdip Nayek"],"pdf_url":"https://arxiv.org/pdf/2408.00681v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00676v1","updated":"2024-08-01T16:19:08Z","published":"2024-08-01T16:19:08Z","title":"An effect analysis of the balancing techniques on the counterfactual\n  explanations of student success prediction models","summary":"  In the past decade, we have experienced a massive boom in the usage of\ndigital solutions in higher education. Due to this boom, large amounts of data\nhave enabled advanced data analysis methods to support learners and examine\nlearning processes. One of the dominant research directions in learning\nanalytics is predictive modeling of learners' success using various machine\nlearning methods. To build learners' and teachers' trust in such methods and\nsystems, exploring the methods and methodologies that enable relevant\nstakeholders to deeply understand the underlying machine-learning models is\nnecessary. In this context, counterfactual explanations from explainable\nmachine learning tools are promising. Several counterfactual generation methods\nhold much promise, but the features must be actionable and causal to be\neffective. Thus, obtaining which counterfactual generation method suits the\nstudent success prediction models in terms of desiderata, stability, and\nrobustness is essential. Although a few studies have been published in recent\nyears on the use of counterfactual explanations in educational sciences, they\nhave yet to discuss which counterfactual generation method is more suitable for\nthis problem. This paper analyzed the effectiveness of commonly used\ncounterfactual generation methods, such as WhatIf Counterfactual Explanations,\nMulti-Objective Counterfactual Explanations, and Nearest Instance\nCounterfactual Explanations after balancing. This contribution presents a case\nstudy using the Open University Learning Analytics dataset to demonstrate the\npractical usefulness of counterfactual explanations. The results illustrate the\nmethod's effectiveness and describe concrete steps that could be taken to alter\nthe model's prediction.\n","authors":["Mustafa Cavus","Jakub Kuzilek"],"pdf_url":"https://arxiv.org/pdf/2408.00676v1.pdf","comment":"19 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.00674v1","updated":"2024-08-01T16:16:29Z","published":"2024-08-01T16:16:29Z","title":"ChordSync: Conformer-Based Alignment of Chord Annotations to Music Audio","summary":"  In the Western music tradition, chords are the main constituent components of\nharmony, a fundamental dimension of music. Despite its relevance for several\nMusic Information Retrieval (MIR) tasks, chord-annotated audio datasets are\nlimited and need more diversity. One way to improve those resources is to\nleverage the large number of chord annotations available online, but this\nrequires aligning them with music audio. However, existing audio-to-score\nalignment techniques, which typically rely on Dynamic Time Warping (DTW), fail\nto address this challenge, as they require weakly aligned data for precise\nsynchronisation. In this paper, we introduce ChordSync, a novel conformer-based\nmodel designed to seamlessly align chord annotations with audio, eliminating\nthe need for weak alignment. We also provide a pre-trained model and a\nuser-friendly library, enabling users to synchronise chord annotations with\naudio tracks effortlessly. In this way, ChordSync creates opportunities for\nharnessing crowd-sourced chord data for MIR, especially in audio chord\nestimation, thereby facilitating the generation of novel datasets.\nAdditionally, our system extends its utility to music education, enhancing\nmusic learning experiences by providing accurately aligned annotations, thus\nenabling learners to engage in synchronised musical practices.\n","authors":["Andrea Poltronieri","Valentina Presutti","Martn Rocamora"],"pdf_url":"https://arxiv.org/pdf/2408.00674v1.pdf","comment":"8 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.00665v1","updated":"2024-08-01T16:01:51Z","published":"2024-08-01T16:01:51Z","title":"AutoM3L: An Automated Multimodal Machine Learning Framework with Large\n  Language Models","summary":"  Automated Machine Learning (AutoML) offers a promising approach to streamline\nthe training of machine learning models. However, existing AutoML frameworks\nare often limited to unimodal scenarios and require extensive manual\nconfiguration. Recent advancements in Large Language Models (LLMs) have\nshowcased their exceptional abilities in reasoning, interaction, and code\ngeneration, presenting an opportunity to develop a more automated and\nuser-friendly framework. To this end, we introduce AutoM3L, an innovative\nAutomated Multimodal Machine Learning framework that leverages LLMs as\ncontrollers to automatically construct multimodal training pipelines. AutoM3L\ncomprehends data modalities and selects appropriate models based on user\nrequirements, providing automation and interactivity. By eliminating the need\nfor manual feature engineering and hyperparameter optimization, our framework\nsimplifies user engagement and enables customization through directives,\naddressing the limitations of previous rule-based AutoML approaches. We\nevaluate the performance of AutoM3L on six diverse multimodal datasets spanning\nclassification, regression, and retrieval tasks, as well as a comprehensive set\nof unimodal datasets. The results demonstrate that AutoM3L achieves competitive\nor superior performance compared to traditional rule-based AutoML methods.\nFurthermore, a user study highlights the user-friendliness and usability of our\nframework, compared to the rule-based AutoML methods.\n","authors":["Daqin Luo","Chengjian Feng","Yuxuan Nong","Yiqing Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00665v1.pdf","comment":"Accpeted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.00662v1","updated":"2024-08-01T15:58:05Z","published":"2024-08-01T15:58:05Z","title":"Aligning Multiple Knowledge Graphs in a Single Pass","summary":"  Entity alignment (EA) is to identify equivalent entities across different\nknowledge graphs (KGs), which can help fuse these KGs into a more comprehensive\none. Previous EA methods mainly focus on aligning a pair of KGs, and to the\nbest of our knowledge, no existing EA method considers aligning multiple (more\nthan two) KGs. To fill this research gap, in this work, we study a novel\nproblem of aligning multiple KGs and propose an effective framework named\nMultiEA to solve the problem. First, we embed the entities of all the candidate\nKGs into a common feature space by a shared KG encoder. Then, we explore three\nalignment strategies to minimize the distances among pre-aligned entities. In\nparticular, we propose an innovative inference enhancement technique to improve\nthe alignment performance by incorporating high-order similarities. Finally, to\nverify the effectiveness of MultiEA, we construct two new real-world benchmark\ndatasets and conduct extensive experiments on them. The results show that our\nMultiEA can effectively and efficiently align multiple KGs in a single pass.\n","authors":["Yaming Yang","Zhe Wang","Ziyu Guan","Wei Zhao","Weigang Lu","Xinyan Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00662v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00657v1","updated":"2024-08-01T15:46:22Z","published":"2024-08-01T15:46:22Z","title":"Disentangling Dense Embeddings with Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) have shown promise in extracting interpretable\nfeatures from complex neural networks. We present one of the first applications\nof SAEs to dense text embeddings from large language models, demonstrating\ntheir effectiveness in disentangling semantic concepts. By training SAEs on\nembeddings of over 420,000 scientific paper abstracts from computer science and\nastronomy, we show that the resulting sparse representations maintain semantic\nfidelity while offering interpretability. We analyse these learned features,\nexploring their behaviour across different model capacities and introducing a\nnovel method for identifying ``feature families'' that represent related\nconcepts at varying levels of abstraction. To demonstrate the practical utility\nof our approach, we show how these interpretable features can be used to\nprecisely steer semantic search, allowing for fine-grained control over query\nsemantics. This work bridges the gap between the semantic richness of dense\nembeddings and the interpretability of sparse representations. We open source\nour embeddings, trained sparse autoencoders, and interpreted features, as well\nas a web app for exploring them.\n","authors":["Charles O'Neill","Christine Ye","Kartheik Iyer","John F. Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00657v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00652v1","updated":"2024-08-01T15:41:08Z","published":"2024-08-01T15:41:08Z","title":"Enhancing Multistep Prediction of Multivariate Market Indices Using\n  Weighted Optical Reservoir Computing","summary":"  We propose and experimentally demonstrate an innovative stock index\nprediction method using a weighted optical reservoir computing system. We\nconstruct fundamental market data combined with macroeconomic data and\ntechnical indicators to capture the broader behavior of the stock market. Our\napproach shows significant higher performance than state-of-the-art methods\nsuch as linear regression, decision trees, and neural network architectures\nincluding long short-term memory. It captures well the market's high volatility\nand nonlinear behaviors despite limited data, demonstrating great potential for\nreal-time, parallel, multi-dimensional data processing and predictions.\n","authors":["Fang Wang","Ting Bu","Yuping Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00641v1","updated":"2024-08-01T15:30:43Z","published":"2024-08-01T15:30:43Z","title":"Enhancing Ethereum Fraud Detection via Generative and Contrastive\n  Self-supervision","summary":"  The rampant fraudulent activities on Ethereum hinder the healthy development\nof the blockchain ecosystem, necessitating the reinforcement of regulations.\nHowever, multiple imbalances involving account interaction frequencies and\ninteraction types in the Ethereum transaction environment pose significant\nchallenges to data mining-based fraud detection research. To address this, we\nfirst propose the concept of meta-interactions to refine interaction behaviors\nin Ethereum, and based on this, we present a dual self-supervision enhanced\nEthereum fraud detection framework, named Meta-IFD. This framework initially\nintroduces a generative self-supervision mechanism to augment the interaction\nfeatures of accounts, followed by a contrastive self-supervision mechanism to\ndifferentiate various behavior patterns, and ultimately characterizes the\nbehavioral representations of accounts and mines potential fraud risks through\nmulti-view interaction feature learning. Extensive experiments on real Ethereum\ndatasets demonstrate the effectiveness and superiority of our framework in\ndetecting common Ethereum fraud behaviors such as Ponzi schemes and phishing\nscams. Additionally, the generative module can effectively alleviate the\ninteraction distribution imbalance in Ethereum data, while the contrastive\nmodule significantly enhances the framework's ability to distinguish different\nbehavior patterns. The source code will be released on GitHub soon.\n","authors":["Chenxiang Jin","Jiajun Zhou","Chenxuan Xie","Shanqing Yu","Qi Xuan","Xiaoniu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00639v1","updated":"2024-08-01T15:26:24Z","published":"2024-08-01T15:26:24Z","title":"Privacy-preserving datasets by capturing feature distributions with\n  Conditional VAEs","summary":"  Large and well-annotated datasets are essential for advancing deep learning\napplications, however often costly or impossible to obtain by a single entity.\nIn many areas, including the medical domain, approaches relying on data sharing\nhave become critical to address those challenges. While effective in increasing\ndataset size and diversity, data sharing raises significant privacy concerns.\nCommonly employed anonymization methods based on the k-anonymity paradigm often\nfail to preserve data diversity, affecting model robustness. This work\nintroduces a novel approach using Conditional Variational Autoencoders (CVAEs)\ntrained on feature vectors extracted from large pre-trained vision foundation\nmodels. Foundation models effectively detect and represent complex patterns\nacross diverse domains, allowing the CVAE to faithfully capture the embedding\nspace of a given data distribution to generate (sample) a diverse,\nprivacy-respecting, and potentially unbounded set of synthetic feature vectors.\nOur method notably outperforms traditional approaches in both medical and\nnatural image domains, exhibiting greater dataset diversity and higher\nrobustness against perturbations while preserving sample privacy. These results\nunderscore the potential of generative models to significantly impact deep\nlearning applications in data-scarce and privacy-sensitive environments. The\nsource code is available at\nhttps://github.com/francescodisalvo05/cvae-anonymization .\n","authors":["Francesco Di Salvo","David Tafler","Sebastian Doerrich","Christian Ledig"],"pdf_url":"https://arxiv.org/pdf/2408.00639v1.pdf","comment":"Accepted at BMVC 2024"},{"id":"http://arxiv.org/abs/2408.00613v1","updated":"2024-08-01T14:53:11Z","published":"2024-08-01T14:53:11Z","title":"Unlocking Fair Use in the Generative AI Supply Chain: A Systematized\n  Literature Review","summary":"  Through a systematization of generative AI (GenAI) stakeholder goals and\nexpectations, this work seeks to uncover what value different stakeholders see\nin their contributions to the GenAI supply line. This valuation enables us to\nunderstand whether fair use advocated by GenAI companies to train model\nprogresses the copyright law objective of promoting science and arts. While\nassessing the validity and efficacy of the fair use argument, we uncover\nresearch gaps and potential avenues for future works for researchers and\npolicymakers to address.\n","authors":["Amruta Mahuli","Asia Biega"],"pdf_url":"https://arxiv.org/pdf/2408.00613v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00611v1","updated":"2024-08-01T14:49:43Z","published":"2024-08-01T14:49:43Z","title":"Using CSNNs to Perform Event-based Data Processing & Classification on\n  ASL-DVS","summary":"  Recent advancements in bio-inspired visual sensing and neuromorphic computing\nhave led to the development of various highly efficient bio-inspired solutions\nwith real-world applications. One notable application integrates event-based\ncameras with spiking neural networks (SNNs) to process event-based sequences\nthat are asynchronous and sparse, making them difficult to handle. In this\nproject, we develop a convolutional spiking neural network (CSNN) architecture\nthat leverages convolutional operations and recurrent properties of a spiking\nneuron to learn the spatial and temporal relations in the ASL-DVS gesture\ndataset. The ASL-DVS gesture dataset is a neuromorphic dataset containing hand\ngestures when displaying 24 letters (A to Y, excluding J and Z due to the\nnature of their symbols) from the American Sign Language (ASL). We performed\nclassification on a pre-processed subset of the full ASL-DVS dataset to\nidentify letter signs and achieved 100\\% training accuracy. Specifically, this\nwas achieved by training in the Google Cloud compute platform while using a\nlearning rate of 0.0005, batch size of 25 (total of 20 batches), 200\niterations, and 10 epochs.\n","authors":["Ria Patel","Sujit Tripathy","Zachary Sublett","Seoyoung An","Riya Patel"],"pdf_url":"https://arxiv.org/pdf/2408.00611v1.pdf","comment":"8 pages, 14 figures"},{"id":"http://arxiv.org/abs/2408.00601v1","updated":"2024-08-01T14:35:24Z","published":"2024-08-01T14:35:24Z","title":"AutoPV: Automatically Design Your Photovoltaic Power Forecasting Model","summary":"  Photovoltaic power forecasting (PVPF) is a critical area in time series\nforecasting (TSF), enabling the efficient utilization of solar energy. With\nadvancements in machine learning and deep learning, various models have been\napplied to PVPF tasks. However, constructing an optimal predictive architecture\nfor specific PVPF tasks remains challenging, as it requires cross-domain\nknowledge and significant labor costs. To address this challenge, we introduce\nAutoPV, a novel framework for the automated search and construction of PVPF\nmodels based on neural architecture search (NAS) technology. We develop a brand\nnew NAS search space that incorporates various data processing techniques from\nstate-of-the-art (SOTA) TSF models and typical PVPF deep learning models. The\neffectiveness of AutoPV is evaluated on diverse PVPF tasks using a dataset from\nthe Daqing Photovoltaic Station in China. Experimental results demonstrate that\nAutoPV can complete the predictive architecture construction process in a\nrelatively short time, and the newly constructed architecture is superior to\nSOTA predefined models. This work bridges the gap in applying NAS to TSF\nproblems, assisting non-experts and industries in automatically designing\neffective PVPF models.\n","authors":["Dayin Chen","Xiaodan Shi","Mingkun Jiang","Haoran Zhang","Dongxiao Zhang","Yuntian Chen","Jinyue Yan"],"pdf_url":"https://arxiv.org/pdf/2408.00601v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00573v1","updated":"2024-08-01T14:06:34Z","published":"2024-08-01T14:06:34Z","title":"Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks","summary":"  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD) have been proven effective in training neural networks. In the\nsetting of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD in training two-layer neural networks has a\npoor dependence on the sample size and the Gram matrix, resulting in a slow\ntraining process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD enjoys a\nfaster convergence rate. Moreover, we further generalize the method for GD in\ntraining two-layer Physics-Informed Neural Networks (PINNs), showing a similar\nimprovement for the learning rate. Although the improved learning rate depends\nmildly on the Gram matrix, we still need to set it small enough in practice due\nto the agnostic eigenvalues of the Gram matrix. More importantly, the\nconvergence rate relies on the least eigenvalue of the Gram matrix, leading to\nslow convergence. In this work, we provide the convergence analysis of natural\ngradient descent (NGD) in training two-layer PINNs. We show that the learning\nrate can be $\\mathcal{O}(1)$ and at this time, the convergence rate is\nindependent of the Gram matrix.\n","authors":["Xianliang Xu","Ting Du","Wang Kong","Ye Li","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00570v1","updated":"2024-08-01T14:03:11Z","published":"2024-08-01T14:03:11Z","title":"Analyzing the Effectiveness of Quantum Annealing with Meta-Learning","summary":"  The field of Quantum Computing has gathered significant popularity in recent\nyears and a large number of papers have studied its effectiveness in tackling\nmany tasks. We focus in particular on Quantum Annealing (QA), a meta-heuristic\nsolver for Quadratic Unconstrained Binary Optimization (QUBO) problems. It is\nknown that the effectiveness of QA is dependent on the task itself, as is the\ncase for classical solvers, but there is not yet a clear understanding of which\nare the characteristics of a problem that makes it difficult to solve with QA.\nIn this work, we propose a new methodology to study the effectiveness of QA\nbased on meta-learning models. To do so, we first build a dataset composed of\nmore than five thousand instances of ten different optimization problems. We\ndefine a set of more than a hundred features to describe their characteristics,\nand solve them with both QA and three classical solvers. We publish this\ndataset online for future research. Then, we train multiple meta-models to\npredict whether QA would solve that instance effectively and use them to probe\nwhich are the features with the strongest impact on the effectiveness of QA.\nOur results indicate that it is possible to accurately predict the\neffectiveness of QA, validating our methodology. Furthermore, we observe that\nthe distribution of the problem coefficients representing the bias and coupling\nterms is very informative to identify the probability of finding good\nsolutions, while the density of these coefficients alone is not enough. The\nmethodology we propose allows to open new research directions to further our\nunderstanding of the effectiveness of QA, by probing specific dimensions or by\ndeveloping new QUBO formulations that are better suited for the particular\nnature of QA. Furthermore, the proposed methodology is flexible and can be\nextended or used to study other quantum or classical solvers.\n","authors":["Riccardo Pellini","Maurizio Ferrari Dacrema"],"pdf_url":"https://arxiv.org/pdf/2408.00570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00549v1","updated":"2024-08-01T13:34:19Z","published":"2024-08-01T13:34:19Z","title":"Learning to Embed Distributions via Maximum Kernel Entropy","summary":"  Empirical data can often be considered as samples from a set of probability\ndistributions. Kernel methods have emerged as a natural approach for learning\nto classify these distributions. Although numerous kernels between\ndistributions have been proposed, applying kernel methods to distribution\nregression tasks remains challenging, primarily because selecting a suitable\nkernel is not straightforward. Surprisingly, the question of learning a\ndata-dependent distribution kernel has received little attention. In this\npaper, we propose a novel objective for the unsupervised learning of\ndata-dependent distribution kernel, based on the principle of entropy\nmaximization in the space of probability measure embeddings. We examine the\ntheoretical properties of the latent embedding space induced by our objective,\ndemonstrating that its geometric structure is well-suited for solving\ndownstream discriminative tasks. Finally, we demonstrate the performance of the\nlearned kernel across different modalities.\n","authors":["Oleksii Kachaiev","Stefano Recanatesi"],"pdf_url":"https://arxiv.org/pdf/2408.00549v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00540v1","updated":"2024-08-01T13:23:15Z","published":"2024-08-01T13:23:15Z","title":"The Energy Cost of Artificial Intelligence of Things Lifecycle","summary":"  Artificial intelligence (AI)coupled with existing Internet of Things (IoT)\nenables more streamlined and autonomous operations across various economic\nsectors. Consequently, the paradigm of Artificial Intelligence of Things (AIoT)\nhaving AI techniques at its core implies additional energy and carbon costs\nthat may become significant with more complex neural architectures. To better\nunderstand the energy and Carbon Footprint (CF) of some AIoT components, very\nrecent studies employ conventional metrics. However, these metrics are not\ndesigned to capture energy efficiency aspects of inference. In this paper, we\npropose a new metric, the Energy Cost of AIoT Lifecycle (eCAL) to capture the\noverall energy cost of inference over the lifecycle of an AIoT system. We\ndevise a new methodology for determining eCAL of an AIoT system by analyzing\nthe complexity of data manipulation in individual components involved in the\nAIoT lifecycle and derive the overall and per bit energy consumption. With eCAL\nwe show that the better a model is and the more it is used, the more energy\nefficient an inference is. For an example AIoT configuration, eCAL for making\n$100$ inferences is $1.43$ times higher than for $1000$ inferences. We also\nevaluate the CF of the AIoT system by calculating the equivalent CO$_{2}$\nemissions based on the energy consumption and the Carbon Intensity (CI) across\ndifferent countries. Using 2023 renewable data, our analysis reveals that\ndeploying an AIoT system in Germany results in emitting $4.62$ times higher\nCO$_2$ than in Finland, due to latter using more low-CI energy sources.\n","authors":["Shih-Kai Chou","Jernej Hribar","Mihael Mohori","Carolina Fortuna"],"pdf_url":"https://arxiv.org/pdf/2408.00540v1.pdf","comment":"12 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.00531v1","updated":"2024-08-01T13:08:02Z","published":"2024-08-01T13:08:02Z","title":"ReSi: A Comprehensive Benchmark for Representational Similarity Measures","summary":"  Measuring the similarity of different representations of neural architectures\nis a fundamental task and an open research challenge for the machine learning\ncommunity. This paper presents the first comprehensive benchmark for evaluating\nrepresentational similarity measures based on well-defined groundings of\nsimilarity. The representational similarity (ReSi) benchmark consists of (i)\nsix carefully designed tests for similarity measures, (ii) 23 similarity\nmeasures, (iii) eleven neural network architectures, and (iv) six datasets,\nspanning over the graph, language, and vision domains. The benchmark opens up\nseveral important avenues of research on representational similarity that\nenable novel explorations and applications of neural architectures. We\ndemonstrate the utility of the ReSi benchmark by conducting experiments on\nvarious neural network architectures, real world datasets and similarity\nmeasures. All components of the benchmark are publicly available and thereby\nfacilitate systematic reproduction and production of research results. The\nbenchmark is extensible, future research can build on and further expand it. We\nbelieve that the ReSi benchmark can serve as a sound platform catalyzing future\nresearch that aims to systematically evaluate existing and explore novel ways\nof comparing representations of neural architectures.\n","authors":["Max Klabunde","Tassilo Wald","Tobias Schumacher","Klaus Maier-Hein","Markus Strohmaier","Florian Lemmerich"],"pdf_url":"https://arxiv.org/pdf/2408.00531v1.pdf","comment":"Feedback welcome! Code and data at https://github.com/mklabunde/resi"},{"id":"http://arxiv.org/abs/2408.00527v1","updated":"2024-08-01T12:58:19Z","published":"2024-08-01T12:58:19Z","title":"Contrastive Learning with Dynamic Localized Repulsion for Brain Age\n  Prediction on 3D Stiffness Maps","summary":"  In the field of neuroimaging, accurate brain age prediction is pivotal for\nuncovering the complexities of brain aging and pinpointing early indicators of\nneurodegenerative conditions. Recent advancements in self-supervised learning,\nparticularly in contrastive learning, have demonstrated greater robustness when\ndealing with complex datasets. However, current approaches often fall short in\ngeneralizing across non-uniformly distributed data, prevalent in medical\nimaging scenarios. To bridge this gap, we introduce a novel contrastive loss\nthat adapts dynamically during the training process, focusing on the localized\nneighborhoods of samples. Moreover, we expand beyond traditional structural\nfeatures by incorporating brain stiffness, a mechanical property previously\nunderexplored yet promising due to its sensitivity to age-related changes. This\nwork presents the first application of self-supervised learning to brain\nmechanical properties, using compiled stiffness maps from various clinical\nstudies to predict brain age. Our approach, featuring dynamic localized loss,\nconsistently outperforms existing state-of-the-art methods, demonstrating\nsuperior performance and laying the way for new directions in brain aging\nresearch.\n","authors":["Jakob Truble","Lucy Hiscox","Curtis Johnson","Carola-Bibiane Schnlieb","Gabriele Kaminski Schierle","Angelica Aviles-Rivero"],"pdf_url":"https://arxiv.org/pdf/2408.00527v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00526v1","updated":"2024-08-01T12:57:35Z","published":"2024-08-01T12:57:35Z","title":"Hilbert curves for efficient exploratory landscape analysis\n  neighbourhood sampling","summary":"  Landscape analysis aims to characterise optimisation problems based on their\nobjective (or fitness) function landscape properties. The problem search space\nis typically sampled, and various landscape features are estimated based on the\nsamples. One particularly salient set of features is information content, which\nrequires the samples to be sequences of neighbouring solutions, such that the\nlocal relationships between consecutive sample points are preserved. Generating\nsuch spatially correlated samples that also provide good search space coverage\nis challenging. It is therefore common to first obtain an unordered sample with\ngood search space coverage, and then apply an ordering algorithm such as the\nnearest neighbour to minimise the distance between consecutive points in the\nsample. However, the nearest neighbour algorithm becomes computationally\nprohibitive in higher dimensions, thus there is a need for more efficient\nalternatives. In this study, Hilbert space-filling curves are proposed as a\nmethod to efficiently obtain high-quality ordered samples. Hilbert curves are a\nspecial case of fractal curves, and guarantee uniform coverage of a bounded\nsearch space while providing a spatially correlated sample. We study the\neffectiveness of Hilbert curves as samplers, and discover that they are capable\nof extracting salient features at a fraction of the computational cost compared\nto Latin hypercube sampling with post-factum ordering. Further, we investigate\nthe use of Hilbert curves as an ordering strategy, and find that they order the\nsample significantly faster than the nearest neighbour ordering, without\nsacrificing the saliency of the extracted features.\n","authors":["Johannes J. Pienaar","Anna S. Bosman","Katherine M. Malan"],"pdf_url":"https://arxiv.org/pdf/2408.00526v1.pdf","comment":"A version of this paper is published as conference proceedings of\n  EvoApps 2024"},{"id":"http://arxiv.org/abs/2408.00525v1","updated":"2024-08-01T12:57:12Z","published":"2024-08-01T12:57:12Z","title":"Identifying the Hierarchical Emotional Areas in the Human Brain Through\n  Information Fusion","summary":"  The brain basis of emotion has consistently received widespread attention,\nattracting a large number of studies to explore this cutting-edge topic.\nHowever, the methods employed in these studies typically only model the\npairwise relationship between two brain regions, while neglecting the\ninteractions and information fusion among multiple brain\nregions$\\unicode{x2014}$one of the key ideas of the psychological\nconstructionist hypothesis. To overcome the limitations of traditional methods,\nthis study provides an in-depth theoretical analysis of how to maximize\ninteractions and information fusion among brain regions. Building on the\nresults of this analysis, we propose to identify the hierarchical emotional\nareas in the human brain through multi-source information fusion and graph\nmachine learning methods. Comprehensive experiments reveal that the identified\nhierarchical emotional areas, from lower to higher levels, primarily facilitate\nthe fundamental process of emotion perception, the construction of basic\npsychological operations, and the coordination and integration of these\noperations. Overall, our findings provide unique insights into the brain\nmechanisms underlying specific emotions based on the psychological\nconstructionist hypothesis.\n","authors":["Zhongyu Huang","Changde Du","Chaozhuo Li","Kaicheng Fu","Huiguang He"],"pdf_url":"https://arxiv.org/pdf/2408.00525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00523v1","updated":"2024-08-01T12:54:46Z","published":"2024-08-01T12:54:46Z","title":"Jailbreaking Text-to-Image Models with LLM-Based Agents","summary":"  Recent advancements have significantly improved automated task-solving\ncapabilities using autonomous agents powered by large language models (LLMs).\nHowever, most LLM-based agents focus on dialogue, programming, or specialized\ndomains, leaving gaps in addressing generative AI safety tasks. These gaps are\nprimarily due to the challenges posed by LLM hallucinations and the lack of\nclear guidelines. In this paper, we propose Atlas, an advanced LLM-based\nmulti-agent framework that integrates an efficient fuzzing workflow to target\ngenerative AI models, specifically focusing on jailbreak attacks against\ntext-to-image (T2I) models with safety filters. Atlas utilizes a\nvision-language model (VLM) to assess whether a prompt triggers the T2I model's\nsafety filter. It then iteratively collaborates with both LLM and VLM to\ngenerate an alternative prompt that bypasses the filter. Atlas also enhances\nthe reasoning abilities of LLMs in attack scenarios by leveraging multi-agent\ncommunication, in-context learning (ICL) memory mechanisms, and the\nchain-of-thought (COT) approach. Our evaluation demonstrates that Atlas\nsuccessfully jailbreaks several state-of-the-art T2I models in a black-box\nsetting, which are equipped with multi-modal safety filters. In addition, Atlas\noutperforms existing methods in both query efficiency and the quality of the\ngenerated images.\n","authors":["Yingkai Dong","Zheng Li","Xiangtao Meng","Ning Yu","Shanqing Guo"],"pdf_url":"https://arxiv.org/pdf/2408.00523v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00516v1","updated":"2024-08-01T12:46:37Z","published":"2024-08-01T12:46:37Z","title":"Low-Power Vibration-Based Predictive Maintenance for Industry 4.0 using\n  Neural Networks: A Survey","summary":"  The advancements in smart sensors for Industry 4.0 offer ample opportunities\nfor low-powered predictive maintenance and condition monitoring. However,\ntraditional approaches in this field rely on processing in the cloud, which\nincurs high costs in energy and storage. This paper investigates the potential\nof neural networks for low-power on-device computation of vibration sensor data\nfor predictive maintenance. We review the literature on Spiking Neural Networks\n(SNNs) and Artificial Neuronal Networks (ANNs) for vibration-based predictive\nmaintenance by analyzing datasets, data preprocessing, network architectures,\nand hardware implementations. Our findings suggest that no satisfactory\nstandard benchmark dataset exists for evaluating neural networks in predictive\nmaintenance tasks. Furthermore frequency domain transformations are commonly\nemployed for preprocessing. SNNs mainly use shallow feed forward architectures,\nwhereas ANNs explore a wider range of models and deeper networks. Finally, we\nhighlight the need for future research on hardware implementations of neural\nnetworks for low-power predictive maintenance applications and the development\nof a standardized benchmark dataset.\n","authors":["Alexandru Vasilache","Sven Nitzsche","Daniel Floegel","Tobias Schuermann","Stefan von Dosky","Thomas Bierweiler","Marvin Muler","Florian Klber","Soeren Hohmann","Juergen Becker"],"pdf_url":"https://arxiv.org/pdf/2408.00516v1.pdf","comment":"The final version will be published at the ECML-PKDD 2024 joint\n  post-workshop proceeding in Springer Communications in Computer and\n  Information Science"},{"id":"http://arxiv.org/abs/2408.00513v1","updated":"2024-08-01T12:39:27Z","published":"2024-08-01T12:39:27Z","title":"VecAug: Unveiling Camouflaged Frauds with Cohort Augmentation for\n  Enhanced Detection","summary":"  Fraud detection presents a challenging task characterized by ever-evolving\nfraud patterns and scarce labeled data. Existing methods predominantly rely on\ngraph-based or sequence-based approaches. While graph-based approaches connect\nusers through shared entities to capture structural information, they remain\nvulnerable to fraudsters who can disrupt or manipulate these connections. In\ncontrast, sequence-based approaches analyze users' behavioral patterns,\noffering robustness against tampering but overlooking the interactions between\nsimilar users. Inspired by cohort analysis in retention and healthcare, this\npaper introduces VecAug, a novel cohort-augmented learning framework that\naddresses these challenges by enhancing the representation learning of target\nusers with personalized cohort information. To this end, we first propose a\nvector burn-in technique for automatic cohort identification, which retrieves a\ntask-specific cohort for each target user. Then, to fully exploit the cohort\ninformation, we introduce an attentive cohort aggregation technique for\naugmenting target user representations. To improve the robustness of such\ncohort augmentation, we also propose a novel label-aware cohort neighbor\nseparation mechanism to distance negative cohort neighbors and calibrate the\naggregated cohort information. By integrating this cohort information with\ntarget user representations, VecAug enhances the modeling capacity and\ngeneralization capabilities of the model to be augmented. Our framework is\nflexible and can be seamlessly integrated with existing fraud detection models.\nWe deploy our framework on e-commerce platforms and evaluate it on three fraud\ndetection datasets, and results show that VecAug improves the detection\nperformance of base models by up to 2.48\\% in AUC and 22.5\\% in R@P$_{0.9}$,\noutperforming state-of-the-art methods significantly.\n","authors":["Fei Xiao","Shaofeng Cai","Gang Chen","H. V. Jagadish","Beng Chin Ooi","Meihui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00513v1.pdf","comment":"Accepted by KDD 2024"},{"id":"http://arxiv.org/abs/2408.00508v1","updated":"2024-08-01T12:28:22Z","published":"2024-08-01T12:28:22Z","title":"Block-Operations: Using Modular Routing to Improve Compositional\n  Generalization","summary":"  We explore the hypothesis that poor compositional generalization in neural\nnetworks is caused by difficulties with learning effective routing. To solve\nthis problem, we propose the concept of block-operations, which is based on\nsplitting all activation tensors in the network into uniformly sized blocks and\nusing an inductive bias to encourage modular routing and modification of these\nblocks. Based on this concept we introduce the Multiplexer, a new architectural\ncomponent that enhances the Feed Forward Neural Network (FNN). We\nexperimentally confirm that Multiplexers exhibit strong compositional\ngeneralization. On both a synthetic and a realistic task our model was able to\nlearn the underlying process behind the task, whereas both FNNs and\nTransformers were only able to learn heuristic approximations. We propose as\nfuture work to use the principles of block-operations to improve other existing\narchitectures.\n","authors":["Florian Dietz","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2408.00508v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00490v1","updated":"2024-08-01T11:51:52Z","published":"2024-08-01T11:51:52Z","title":"Graph Representation Learning via Causal Diffusion for\n  Out-of-Distribution Recommendation","summary":"  Graph Neural Networks (GNNs)-based recommendation algorithms typically assume\nthat training and testing data are drawn from independent and identically\ndistributed (IID) spaces. However, this assumption often fails in the presence\nof out-of-distribution (OOD) data, resulting in significant performance\ndegradation. In this study, we construct a Structural Causal Model (SCM) to\nanalyze interaction data, revealing that environmental confounders (e.g., the\nCOVID-19 pandemic) lead to unstable correlations in GNN-based models, thus\nimpairing their generalization to OOD data. To address this issue, we propose a\nnovel approach, graph representation learning via causal diffusion\n(CausalDiffRec) for OOD recommendation. This method enhances the model's\ngeneralization on OOD data by eliminating environmental confounding factors and\nlearning invariant graph representations. Specifically, we use backdoor\nadjustment and variational inference to infer the real environmental\ndistribution, thereby eliminating the impact of environmental confounders. This\ninferred distribution is then used as prior knowledge to guide the\nrepresentation learning in the reverse phase of the diffusion process to learn\nthe invariant representation. In addition, we provide a theoretical derivation\nthat proves optimizing the objective function of CausalDiffRec can encourage\nthe model to learn environment-invariant graph representations, thereby\nachieving excellent generalization performance in recommendations under\ndistribution shifts. Our extensive experiments validate the effectiveness of\nCausalDiffRec in improving the generalization of OOD data, and the average\nimprovement is up to 10.69% on Food, 18.83% on KuaiRec, 22.41% on Yelp2018, and\n11.65% on Douban datasets.\n","authors":["Chu Zhao","Enneng Yang","Yuliang Liang","Pengxiang Lan","Yuting Liu","Jianzhe Zhao","Guibing Guo","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00490v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.00483v1","updated":"2024-08-01T11:39:45Z","published":"2024-08-01T11:39:45Z","title":"A Systematic Review on Long-Tailed Learning","summary":"  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n","authors":["Chongsheng Zhang","George Almpanidis","Gaojuan Fan","Binquan Deng","Yanbo Zhang","Ji Liu","Aouaidjia Kamel","Paolo Soda","Joo Gama"],"pdf_url":"https://arxiv.org/pdf/2408.00483v1.pdf","comment":"Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]"},{"id":"http://arxiv.org/abs/2408.00465v1","updated":"2024-08-01T11:09:01Z","published":"2024-08-01T11:09:01Z","title":"Infrequent Resolving Algorithm for Online Linear Programming","summary":"  Online linear programming (OLP) has gained significant attention from both\nresearchers and practitioners due to its extensive applications, such as online\nauction, network revenue management and advertising. Existing OLP algorithms\nfall into two categories: LP-based algorithms and LP-free algorithms. The\nformer one typically guarantees better performance, even offering a constant\nregret, but requires solving a large number of LPs, which could be\ncomputationally expensive. In contrast, LP-free algorithm only requires\nfirst-order computations but induces a worse performance, lacking a constant\nregret bound. In this work, we bridge the gap between these two extremes by\nproposing an algorithm that achieves a constant regret while solving LPs only\n$O(\\log\\log T)$ times over the time horizon $T$. Moreover, when we are allowed\nto solve LPs only $M$ times, we propose an algorithm that can guarantee an\n$O\\left(T^{(1/2+\\epsilon)^{M-1}}\\right)$ regret. Furthermore, when the arrival\nprobabilities are known at the beginning, our algorithm can guarantee a\nconstant regret by solving LPs $O(\\log\\log T)$ times, and an\n$O\\left(T^{(1/2+\\epsilon)^{M}}\\right)$ regret by solving LPs only $M$ times.\nNumerical experiments are conducted to demonstrate the efficiency of the\nproposed algorithms.\n","authors":["Guokai Li","Zizhuo Wang","Jingwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00465v1.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00462v1","updated":"2024-08-01T11:06:05Z","published":"2024-08-01T11:06:05Z","title":"Designing Efficient LLM Accelerators for Edge Devices","summary":"  The increase in open-source availability of Large Language Models (LLMs) has\nenabled users to deploy them on more and more resource-constrained edge devices\nto reduce reliance on network connections and provide more privacy. However,\nthe high computation and memory demands of LLMs make their execution on\nresource-constrained edge devices challenging and inefficient. To address this\nissue, designing new and efficient edge accelerators for LLM inference is\ncrucial. FPGA-based accelerators are ideal for LLM acceleration due to their\nreconfigurability, as they enable model-specific optimizations and higher\nperformance per watt. However, creating and integrating FPGA-based accelerators\nfor LLMs (particularly on edge devices) has proven challenging, mainly due to\nthe limited hardware design flows for LLMs in existing FPGA platforms.\n  To tackle this issue, in this paper we first propose a new design platform,\nnamed SECDA-LLM, that utilizes the SECDA methodology to streamline the process\nof designing, integrating, and deploying efficient FPGA-based LLM accelerators\nfor the llama.cpp inference framework. We then demonstrate, through a case\nstudy, the potential benefits of SECDA-LLM by creating a new MatMul accelerator\nthat supports block floating point quantized operations for LLMs. Our initial\naccelerator design, deployed on the PYNQ-Z1 board, reduces latency 1.7 seconds\nper token or ~2 seconds per word) by 11x over the dual-core Arm NEON-based CPU\nexecution for the TinyLlama model.\n","authors":["Jude Haris","Rappy Saha","Wenhao Hu","Jos Cano"],"pdf_url":"https://arxiv.org/pdf/2408.00462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00458v1","updated":"2024-08-01T10:55:20Z","published":"2024-08-01T10:55:20Z","title":"Reenact Anything: Semantic Video Motion Transfer Using Motion-Textual\n  Inversion","summary":"  Recent years have seen a tremendous improvement in the quality of video\ngeneration and editing approaches. While several techniques focus on editing\nappearance, few address motion. Current approaches using text, trajectories, or\nbounding boxes are limited to simple motions, so we specify motions with a\nsingle motion reference video instead. We further propose to use a pre-trained\nimage-to-video model rather than a text-to-video model. This approach allows us\nto preserve the exact appearance and position of a target object or scene and\nhelps disentangle appearance from motion. Our method, called motion-textual\ninversion, leverages our observation that image-to-video models extract\nappearance mainly from the (latent) image input, while the text/image embedding\ninjected via cross-attention predominantly controls motion. We thus represent\nmotion using text/image embedding tokens. By operating on an inflated\nmotion-text embedding containing multiple text/image embedding tokens per\nframe, we achieve a high temporal motion granularity. Once optimized on the\nmotion reference video, this embedding can be applied to various target images\nto generate videos with semantically similar motions. Our approach does not\nrequire spatial alignment between the motion reference video and target image,\ngeneralizes across various domains, and can be applied to various tasks such as\nfull-body and face reenactment, as well as controlling the motion of inanimate\nobjects and the camera. We empirically demonstrate the effectiveness of our\nmethod in the semantic video motion transfer task, significantly outperforming\nexisting methods in this context.\n","authors":["Manuel Kansy","Jacek Naruniec","Christopher Schroers","Markus Gross","Romann M. Weber"],"pdf_url":"https://arxiv.org/pdf/2408.00458v1.pdf","comment":"Preprint. All videos in this paper are best viewed as animations with\n  Acrobat Reader by pressing the highlighted frame of each video"},{"id":"http://arxiv.org/abs/2408.00439v1","updated":"2024-08-01T10:19:25Z","published":"2024-08-01T10:19:25Z","title":"Rapid and Power-Aware Learned Optimization for Modular Receive\n  Beamforming","summary":"  Multiple-input multiple-output (MIMO) systems play a key role in wireless\ncommunication technologies. A widely considered approach to realize scalable\nMIMO systems involves architectures comprised of multiple separate modules,\neach with its own beamforming capability. Such models accommodate cell-free\nmassive MIMO and partially connected hybrid MIMO architectures. A core issue\nwith the implementation of modular MIMO arises from the need to rapidly set the\nbeampatterns of the modules, while maintaining their power efficiency. This\nleads to challenging constrained optimization that should be repeatedly solved\non each coherence duration. In this work, we propose a power-oriented\noptimization algorithm for beamforming in uplink modular hybrid MIMO systems,\nwhich learns from data to operate rapidly. We derive our learned optimizer by\ntackling the rate maximization objective using projected gradient ascent steps\nwith momentum. We then leverage data to tune the hyperparameters of the\noptimizer, allowing it to operate reliably in a fixed and small number of\niterations while completely preserving its interpretable operation. We show how\npower efficient beamforming can be encouraged by the learned optimizer, via\nboosting architectures with low-resolution phase shifts and with deactivated\nanalog components. Numerical results show that our learn-to-optimize method\nnotably reduces the number of iterations and computation latency required to\nreliably tune modular MIMO receivers, and that it allows obtaining desirable\nbalances between power efficient designs and throughput.\n","authors":["Ohad Levy","Nir Shlezinger"],"pdf_url":"https://arxiv.org/pdf/2408.00439v1.pdf","comment":"Under review for possible publication in the IEEE"},{"id":"http://arxiv.org/abs/2408.00437v1","updated":"2024-08-01T10:16:57Z","published":"2024-08-01T10:16:57Z","title":"Efficient Patient Fine-Tuned Seizure Detection with a Tensor Kernel\n  Machine","summary":"  Recent developments in wearable devices have made accurate and efficient\nseizure detection more important than ever. A challenge in seizure detection is\nthat patient-specific models typically outperform patient-independent models.\nHowever, in a wearable device one typically starts with a patient-independent\nmodel, until such patient-specific data is available. To avoid having to\nconstruct a new classifier with this data, as required in conventional kernel\nmachines, we propose a transfer learning approach with a tensor kernel machine.\nThis method learns the primal weights in a compressed form using the canonical\npolyadic decomposition, making it possible to efficiently update the weights of\nthe patient-independent model with patient-specific data. The results show that\nthis patient fine-tuned model reaches as high a performance as a\npatient-specific SVM model with a model size that is twice as small as the\npatient-specific model and ten times as small as the patient-independent model.\n","authors":["Seline J. S. de Rooij","Frederiek Wesel","Borbla Hunyadi"],"pdf_url":"https://arxiv.org/pdf/2408.00437v1.pdf","comment":"5 pages, to be published in the EUSIPCO2024 conference proceedings"},{"id":"http://arxiv.org/abs/2408.00426v1","updated":"2024-08-01T09:57:48Z","published":"2024-08-01T09:57:48Z","title":"A Cross-Domain Benchmark for Active Learning","summary":"  Active Learning (AL) deals with identifying the most informative samples for\nlabeling to reduce data annotation costs for supervised learning tasks. AL\nresearch suffers from the fact that lifts from literature generalize poorly and\nthat only a small number of repetitions of experiments are conducted. To\novercome these obstacles, we propose \\emph{CDALBench}, the first active\nlearning benchmark which includes tasks in computer vision, natural language\nprocessing and tabular learning. Furthermore, by providing an efficient, greedy\noracle, \\emph{CDALBench} can be evaluated with 50 runs for each experiment. We\nshow, that both the cross-domain character and a large amount of repetitions\nare crucial for sophisticated evaluation of AL research. Concretely, we show\nthat the superiority of specific methods varies over the different domains,\nmaking it important to evaluate Active Learning with a cross-domain benchmark.\nAdditionally, we show that having a large amount of runs is crucial. With only\nconducting three runs as often done in the literature, the superiority of\nspecific methods can strongly vary with the specific runs. This effect is so\nstrong, that, depending on the seed, even a well-established method's\nperformance can be significantly better and significantly worse than random for\nthe same dataset.\n","authors":["Thorben Werner","Johannes Burchert","Maximilian Stubbemann","Lars Schmidt-Thieme"],"pdf_url":"https://arxiv.org/pdf/2408.00426v1.pdf","comment":"Updated version of paper \"Toward Comparable Active Learning\"\n  (arXiv:2311.18356). \"Toward Comparable Active Learning\" is deprecated, please\n  use this version"},{"id":"http://arxiv.org/abs/2408.00421v1","updated":"2024-08-01T09:46:06Z","published":"2024-08-01T09:46:06Z","title":"Towards Evolutionary-based Automated Machine Learning for Small Molecule\n  Pharmacokinetic Prediction","summary":"  Machine learning (ML) is revolutionising drug discovery by expediting the\nprediction of small molecule properties essential for developing new drugs.\nThese properties -- including absorption, distribution, metabolism and\nexcretion (ADME)-- are crucial in the early stages of drug development since\nthey provide an understanding of the course of the drug in the organism, i.e.,\nthe drug's pharmacokinetics. However, existing methods lack personalisation and\nrely on manually crafted ML algorithms or pipelines, which can introduce\ninefficiencies and biases into the process. To address these challenges, we\npropose a novel evolutionary-based automated ML method (AutoML) specifically\ndesigned for predicting small molecule properties, with a particular focus on\npharmacokinetics. Leveraging the advantages of grammar-based genetic\nprogramming, our AutoML method streamlines the process by automatically\nselecting algorithms and designing predictive pipelines tailored to the\nparticular characteristics of input molecular data. Results demonstrate\nAutoML's effectiveness in selecting diverse ML algorithms, resulting in\ncomparable or even improved predictive performances compared to conventional\napproaches. By offering personalised ML-driven pipelines, our method promises\nto enhance small molecule research in drug discovery, providing researchers\nwith a valuable tool for accelerating the development of novel therapeutic\ndrugs.\n","authors":["Alex G. C. de S","David B. Ascher"],"pdf_url":"https://arxiv.org/pdf/2408.00421v1.pdf","comment":"Paper accepted and presented at the 14th Workshop on Evolutionary\n  Computation for the Automated Design of Algorithms (ECADA), which happened\n  during the Genetic and Evolutionary Computation Conference (GECCO)"},{"id":"http://arxiv.org/abs/2408.00399v1","updated":"2024-08-01T09:11:08Z","published":"2024-08-01T09:11:08Z","title":"Unsupervised Pairwise Causal Discovery on Heterogeneous Data using\n  Mutual Information Measures","summary":"  A fundamental task in science is to determine the underlying causal relations\nbecause it is the knowledge of this functional structure what leads to the\ncorrect interpretation of an effect given the apparent associations in the\nobserved data. In this sense, Causal Discovery is a technique that tackles this\nchallenge by analyzing the statistical properties of the constituent variables.\nIn this work, we target the generalizability of the discovery method by\nfollowing a reductionist approach that only involves two variables, i.e., the\npairwise or bi-variate setting. We question the current (possibly misleading)\nbaseline results on the basis that they were obtained through supervised\nlearning, which is arguably contrary to this genuinely exploratory endeavor. In\nconsequence, we approach this problem in an unsupervised way, using robust\nMutual Information measures, and observing the impact of the different variable\ntypes, which is oftentimes ignored in the design of solutions. Thus, we provide\na novel set of standard unbiased results that can serve as a reference to guide\nfuture discovery tasks in completely unknown environments.\n","authors":["Alexandre Trilla","Nenad Mijatovic"],"pdf_url":"https://arxiv.org/pdf/2408.00399v1.pdf","comment":"26th International Conference of the Catalan Association for\n  Artificial Intelligence"},{"id":"http://arxiv.org/abs/2408.00386v1","updated":"2024-08-01T08:50:25Z","published":"2024-08-01T08:50:25Z","title":"What comes after transformers? -- A selective survey connecting ideas in\n  deep learning","summary":"  Transformers have become the de-facto standard model in artificial\nintelligence since 2017 despite numerous shortcomings ranging from energy\ninefficiency to hallucinations. Research has made a lot of progress in\nimproving elements of transformers, and, more generally, deep learning\nmanifesting in many proposals for architectures, layers, optimization\nobjectives, and optimization techniques. For researchers it is difficult to\nkeep track of such developments on a broader level. We provide a comprehensive\noverview of the many important, recent works in these areas to those who\nalready have a basic understanding of deep learning. Our focus differs from\nother works, as we target specifically novel, alternative potentially\ndisruptive approaches to transformers as well as successful ideas of recent\ndeep learning. We hope that such a holistic and unified treatment of\ninfluential, recent works and novel ideas helps researchers to form new\nconnections between diverse areas of deep learning. We identify and discuss\nmultiple patterns that summarize the key strategies for successful innovations\nover the last decade as well as works that can be seen as rising stars.\nEspecially, we discuss attempts on how to improve on transformers covering\n(partially) proven methods such as state space models but also including\nfar-out ideas in deep learning that seem promising despite not achieving\nstate-of-the-art results. We also cover a discussion on recent state-of-the-art\nmodels such as OpenAI's GPT series and Meta's LLama models and, Google's Gemini\nmodel family.\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.00386v1.pdf","comment":"This is an extended version of the published paper by Johannes\n  Schneider and Michalis Vlachos titled \"A survey of deep learning: From\n  activations to transformers'' which appeared at the International Conference\n  on Agents and Artificial Intelligence(ICAART) in 2024. It was selected for\n  post-publication and has been submitted to the post-publication proceedings"},{"id":"http://arxiv.org/abs/2408.00380v1","updated":"2024-08-01T08:41:13Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that \\name{} achieves excellent performance relative to the number of WSIs\nused and the model's parameter count. This suggests that the application of\nstain normalization has substantially improved the model's efficiency and\ngeneralization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v1.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.00376v1","updated":"2024-08-01T08:35:40Z","published":"2024-08-01T08:35:40Z","title":"On the Limitations and Prospects of Machine Unlearning for Generative AI","summary":"  Generative AI (GenAI), which aims to synthesize realistic and diverse data\nsamples from latent variables or other data modalities, has achieved remarkable\nresults in various domains, such as natural language, images, audio, and\ngraphs. However, they also pose challenges and risks to data privacy, security,\nand ethics. Machine unlearning is the process of removing or weakening the\ninfluence of specific data samples or features from a trained model, without\naffecting its performance on other data or tasks. While machine unlearning has\nshown significant efficacy in traditional machine learning tasks, it is still\nunclear if it could help GenAI become safer and aligned with human desire. To\nthis end, this position paper provides an in-depth discussion of the machine\nunlearning approaches for GenAI. Firstly, we formulate the problem of machine\nunlearning tasks on GenAI and introduce the background. Subsequently, we\nsystematically examine the limitations of machine unlearning on GenAI models by\nfocusing on the two representative branches: LLMs and image generative\n(diffusion) models. Finally, we provide our prospects mainly from three\naspects: benchmark, evaluation metrics, and utility-unlearning trade-off, and\nconscientiously advocate for the future development of this field.\n","authors":["Shiji Zhou","Lianzhe Wang","Jiangnan Ye","Yongliang Wu","Heng Chang"],"pdf_url":"https://arxiv.org/pdf/2408.00376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00374v1","updated":"2024-08-01T08:32:03Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00359v1","updated":"2024-08-01T07:58:51Z","published":"2024-08-01T07:58:51Z","title":"Memorization Capacity for Additive Fine-Tuning with Small ReLU Networks","summary":"  Fine-tuning large pre-trained models is a common practice in machine learning\napplications, yet its mathematical analysis remains largely unexplored. In this\npaper, we study fine-tuning through the lens of memorization capacity. Our new\nmeasure, the Fine-Tuning Capacity (FTC), is defined as the maximum number of\nsamples a neural network can fine-tune, or equivalently, as the minimum number\nof neurons ($m$) needed to arbitrarily change $N$ labels among $K$ samples\nconsidered in the fine-tuning process. In essence, FTC extends the memorization\ncapacity concept to the fine-tuning scenario. We analyze FTC for the additive\nfine-tuning scenario where the fine-tuned network is defined as the summation\nof the frozen pre-trained network $f$ and a neural network $g$ (with $m$\nneurons) designed for fine-tuning. When $g$ is a ReLU network with either 2 or\n3 layers, we obtain tight upper and lower bounds on FTC; we show that $N$\nsamples can be fine-tuned with $m=\\Theta(N)$ neurons for 2-layer networks, and\nwith $m=\\Theta(\\sqrt{N})$ neurons for 3-layer networks, no matter how large $K$\nis. Our results recover the known memorization capacity results when $N = K$ as\na special case.\n","authors":["Jy-yong Sohn","Dohyun Kwon","Seoyeon An","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00359v1.pdf","comment":"10 pages, 9 figures, UAI 2024"},{"id":"http://arxiv.org/abs/2408.00346v1","updated":"2024-08-01T07:31:23Z","published":"2024-08-01T07:31:23Z","title":"Neural Graph Matching for Video Retrieval in Large-Scale Video-driven\n  E-commerce","summary":"  With the rapid development of the short video industry, traditional\ne-commerce has encountered a new paradigm, video-driven e-commerce, which\nleverages attractive videos for product showcases and provides both video and\nitem services for users. Benefitting from the dynamic and visualized\nintroduction of items,video-driven e-commerce has shown huge potential in\nstimulating consumer confidence and promoting sales. In this paper, we focus on\nthe video retrieval task, facing the following challenges: (1) Howto handle the\nheterogeneities among users, items, and videos? (2)How to mine the\ncomplementarity between items and videos for better user understanding? In this\npaper, we first leverage the dual graph to model the co-existing of user-video\nand user-item interactions in video-driven e-commerce and innovatively reduce\nuser preference understanding to a graph matching problem. To solve it, we\nfurther propose a novel bi-level Graph Matching Network(GMN), which mainly\nconsists of node- and preference-level graph matching. Given a user, node-level\ngraph matching aims to match videos and items, while preference-level graph\nmatching aims to match multiple user preferences extracted from both videos and\nitems. Then the proposed GMN can generate and improve user embedding by\naggregating matched nodes or preferences from the dual graph in a bi-level\nmanner. Comprehensive experiments show the superiority of the proposed GMN with\nsignificant improvements over state-of-the-art approaches (e.g., AUC+1.9% and\nCTR+7.15%). We have developed it on a well-known video-driven e-commerce\nplatform, serving hundreds of millions of users every day\n","authors":["Houye Ji","Ye Tang","Zhaoxin Chen","Lixi Deng","Jun Hu","Lei Su"],"pdf_url":"https://arxiv.org/pdf/2408.00346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00343v1","updated":"2024-08-01T07:27:54Z","published":"2024-08-01T07:27:54Z","title":"IN-Sight: Interactive Navigation through Sight","summary":"  Current visual navigation systems often treat the environment as static,\nlacking the ability to adaptively interact with obstacles. This limitation\nleads to navigation failure when encountering unavoidable obstructions. In\nresponse, we introduce IN-Sight, a novel approach to self-supervised path\nplanning, enabling more effective navigation strategies through interaction\nwith obstacles. Utilizing RGB-D observations, IN-Sight calculates\ntraversability scores and incorporates them into a semantic map, facilitating\nlong-range path planning in complex, maze-like environments. To precisely\nnavigate around obstacles, IN-Sight employs a local planner, trained\nimperatively on a differentiable costmap using representation learning\ntechniques. The entire framework undergoes end-to-end training within the\nstate-of-the-art photorealistic Intel SPEAR Simulator. We validate the\neffectiveness of IN-Sight through extensive benchmarking in a variety of\nsimulated scenarios and ablation studies. Moreover, we demonstrate the system's\nreal-world applicability with zero-shot sim-to-real transfer, deploying our\nplanner on the legged robot platform ANYmal, showcasing its practical potential\nfor interactive navigation in real environments.\n","authors":["Philipp Schoch","Fan Yang","Yuntao Ma","Stefan Leutenegger","Marco Hutter","Quentin Leboute"],"pdf_url":"https://arxiv.org/pdf/2408.00343v1.pdf","comment":"The 2024 IEEE/RSJ International Conference on Intelligent Robots and\n  Systems (IROS 2024)"},{"id":"http://arxiv.org/abs/2408.00342v1","updated":"2024-08-01T07:27:18Z","published":"2024-08-01T07:27:18Z","title":"MuJoCo MPC for Humanoid Control: Evaluation on HumanoidBench","summary":"  We tackle the recently introduced benchmark for whole-body humanoid control\nHumanoidBench using MuJoCo MPC. We find that sparse reward functions of\nHumanoidBench yield undesirable and unrealistic behaviors when optimized;\ntherefore, we propose a set of regularization terms that stabilize the robot\nbehavior across tasks. Current evaluations on a subset of tasks demonstrate\nthat our proposed reward function allows achieving the highest HumanoidBench\nscores while maintaining realistic posture and smooth control signals. Our code\nis publicly available and will become a part of MuJoCo MPC, enabling rapid\nprototyping of robot behaviors.\n","authors":["Moritz Meser","Aditya Bhatt","Boris Belousov","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2408.00342v1.pdf","comment":"3 pages, 3 figures, submitted to IEEE Conference on Robotics and\n  Automation (ICRA@40)"},{"id":"http://arxiv.org/abs/2408.00330v1","updated":"2024-08-01T07:06:30Z","published":"2024-08-01T07:06:30Z","title":"\"Patriarchy Hurts Men Too.\" Does Your Model Agree? A Discussion on\n  Fairness Assumptions","summary":"  The pipeline of a fair ML practitioner is generally divided into three\nphases: 1) Selecting a fairness measure. 2) Choosing a model that minimizes\nthis measure. 3) Maximizing the model's performance on the data. In the context\nof group fairness, this approach often obscures implicit assumptions about how\nbias is introduced into the data. For instance, in binary classification, it is\noften assumed that the best model, with equal fairness, is the one with better\nperformance. However, this belief already imposes specific properties on the\nprocess that introduced bias. More precisely, we are already assuming that the\nbiasing process is a monotonic function of the fair scores, dependent solely on\nthe sensitive attribute. We formally prove this claim regarding several\nimplicit fairness assumptions. This leads, in our view, to two possible\nconclusions: either the behavior of the biasing process is more complex than\nmere monotonicity, which means we need to identify and reject our implicit\nassumptions in order to develop models capable of tackling more complex\nsituations; or the bias introduced in the data behaves predictably, implying\nthat many of the developed models are superfluous.\n","authors":["Marco Favier","Toon Calders"],"pdf_url":"https://arxiv.org/pdf/2408.00330v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00329v1","updated":"2024-08-01T07:04:18Z","published":"2024-08-01T07:04:18Z","title":"OTAD: An Optimal Transport-Induced Robust Model for Agnostic Adversarial\n  Attack","summary":"  Deep neural networks (DNNs) are vulnerable to small adversarial perturbations\nof the inputs, posing a significant challenge to their reliability and\nrobustness. Empirical methods such as adversarial training can defend against\nparticular attacks but remain vulnerable to more powerful attacks.\nAlternatively, Lipschitz networks provide certified robustness to unseen\nperturbations but lack sufficient expressive power. To harness the advantages\nof both approaches, we design a novel two-step Optimal Transport induced\nAdversarial Defense (OTAD) model that can fit the training data accurately\nwhile preserving the local Lipschitz continuity. First, we train a DNN with a\nregularizer derived from optimal transport theory, yielding a discrete optimal\ntransport map linking data to its features. By leveraging the map's inherent\nregularity, we interpolate the map by solving the convex integration problem\n(CIP) to guarantee the local Lipschitz property. OTAD is extensible to diverse\narchitectures of ResNet and Transformer, making it suitable for complex data.\nFor efficient computation, the CIP can be solved through training neural\nnetworks. OTAD opens a novel avenue for developing reliable and secure deep\nlearning systems through the regularity of optimal transport maps. Empirical\nresults demonstrate that OTAD can outperform other robust models on diverse\ndatasets.\n","authors":["Kuo Gai","Sicong Wang","Shihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00329v1.pdf","comment":"14 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.00326v1","updated":"2024-08-01T06:55:19Z","published":"2024-08-01T06:55:19Z","title":"Exploiting Preferences in Loss Functions for Sequential Recommendation\n  via Weak Transitivity","summary":"  A choice of optimization objective is immensely pivotal in the design of a\nrecommender system as it affects the general modeling process of a user's\nintent from previous interactions. Existing approaches mainly adhere to three\ncategories of loss functions: pairwise, pointwise, and setwise loss functions.\nDespite their effectiveness, a critical and common drawback of such objectives\nis viewing the next observed item as a unique positive while considering all\nremaining items equally negative. Such a binary label assignment is generally\nlimited to assuring a higher recommendation score of the positive item,\nneglecting potential structures induced by varying preferences between other\nunobserved items. To alleviate this issue, we propose a novel method that\nextends original objectives to explicitly leverage the different levels of\npreferences as relative orders between their scores. Finally, we demonstrate\nthe superior performance of our method compared to baseline objectives.\n","authors":["Hyunsoo Chung","Jungtaek Kim","Hyungeun Jo","Hyungwon Choi"],"pdf_url":"https://arxiv.org/pdf/2408.00326v1.pdf","comment":"Accepted to CIKM 2024, Short Research Paper Track"},{"id":"http://arxiv.org/abs/2408.00315v1","updated":"2024-08-01T06:26:05Z","published":"2024-08-01T06:26:05Z","title":"ADBM: Adversarial diffusion bridge model for reliable adversarial\n  purification","summary":"  Recently Diffusion-based Purification (DiffPure) has been recognized as an\neffective defense method against adversarial examples. However, we find\nDiffPure which directly employs the original pre-trained diffusion models for\nadversarial purification, to be suboptimal. This is due to an inherent\ntrade-off between noise purification performance and data recovery quality.\nAdditionally, the reliability of existing evaluations for DiffPure is\nquestionable, as they rely on weak adaptive attacks. In this work, we propose a\nnovel Adversarial Diffusion Bridge Model, termed ADBM. ADBM directly constructs\na reverse bridge from the diffused adversarial data back to its original clean\nexamples, enhancing the purification capabilities of the original diffusion\nmodels. Through theoretical analysis and experimental validation across various\nscenarios, ADBM has proven to be a superior and robust defense mechanism,\noffering significant promise for practical applications.\n","authors":["Xiao Li","Wenxuan Sun","Huanran Chen","Qiongxiu Li","Yining Liu","Yingzhe He","Jie Shi","Xiaolin Hu"],"pdf_url":"https://arxiv.org/pdf/2408.00315v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.00312v1","updated":"2024-08-01T06:14:42Z","published":"2024-08-01T06:14:42Z","title":"Adversarial Text Rewriting for Text-aware Recommender Systems","summary":"  Text-aware recommender systems incorporate rich textual features, such as\ntitles and descriptions, to generate item recommendations for users. The use of\ntextual features helps mitigate cold-start problems, and thus, such recommender\nsystems have attracted increased attention. However, we argue that the\ndependency on item descriptions makes the recommender system vulnerable to\nmanipulation by adversarial sellers on e-commerce platforms. In this paper, we\nexplore the possibility of such manipulation by proposing a new text rewriting\nframework to attack text-aware recommender systems. We show that the rewriting\nattack can be exploited by sellers to unfairly uprank their products, even\nthough the adversarially rewritten descriptions are perceived as realistic by\nhuman evaluators. Methodologically, we investigate two different variations to\ncarry out text rewriting attacks: (1) two-phase fine-tuning for greater attack\nperformance, and (2) in-context learning for higher text rewriting quality.\nExperiments spanning 3 different datasets and 4 existing approaches demonstrate\nthat recommender systems exhibit vulnerability against the proposed text\nrewriting attack. Our work adds to the existing literature around the\nrobustness of recommender systems, while highlighting a new dimension of\nvulnerability in the age of large-scale automated text generation.\n","authors":["Sejoon Oh","Gaurav Verma","Srijan Kumar"],"pdf_url":"https://arxiv.org/pdf/2408.00312v1.pdf","comment":"Accepted for publication at: 33rd ACM International Conference on\n  Information and Knowledge Management (CIKM 2024). Code and data at:\n  https://github.com/sejoonoh/ATR"},{"id":"http://arxiv.org/abs/2408.00310v1","updated":"2024-08-01T06:13:24Z","published":"2024-08-01T06:13:24Z","title":"Online Linear Programming with Batching","summary":"  We study Online Linear Programming (OLP) with batching. The planning horizon\nis cut into $K$ batches, and the decisions on customers arriving within a batch\ncan be delayed to the end of their associated batch. Compared with OLP without\nbatching, the ability to delay decisions brings better operational performance,\nas measured by regret. Two research questions of interest are: (1) What is a\nlower bound of the regret as a function of $K$? (2) What algorithms can achieve\nthe regret lower bound? These questions have been analyzed in the literature\nwhen the distribution of the reward and the resource consumption of the\ncustomers have finite support. By contrast, this paper analyzes these questions\nwhen the conditional distribution of the reward given the resource consumption\nis continuous, and we show the answers are different under this setting. When\nthere is only a single type of resource and the decision maker knows the total\nnumber of customers, we propose an algorithm with a $O(\\log K)$ regret upper\nbound and provide a $\\Omega(\\log K)$ regret lower bound. We also propose\nalgorithms with $O(\\log K)$ regret upper bound for the setting in which there\nare multiple types of resource and the setting in which customers arrive\nfollowing a Poisson process. All these regret upper and lower bounds are\nindependent of the length of the planning horizon, and all the proposed\nalgorithms delay decisions on customers arriving in only the first and the last\nbatch. We also take customer impatience into consideration and establish a way\nof selecting an appropriate batch size.\n","authors":["Haoran Xu","Peter W. Glynn","Yinyu Ye"],"pdf_url":"https://arxiv.org/pdf/2408.00310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00309v1","updated":"2024-08-01T06:06:53Z","published":"2024-08-01T06:06:53Z","title":"Discretizing Continuous Action Space with Unimodal Probability\n  Distributions for On-Policy Reinforcement Learning","summary":"  For on-policy reinforcement learning, discretizing action space for\ncontinuous control can easily express multiple modes and is straightforward to\noptimize. However, without considering the inherent ordering between the\ndiscrete atomic actions, the explosion in the number of discrete actions can\npossess undesired properties and induce a higher variance for the policy\ngradient estimator. In this paper, we introduce a straightforward architecture\nthat addresses this issue by constraining the discrete policy to be unimodal\nusing Poisson probability distributions. This unimodal architecture can better\nleverage the continuity in the underlying continuous action space using\nexplicit unimodal probability distributions. We conduct extensive experiments\nto show that the discrete policy with the unimodal probability distribution\nprovides significantly faster convergence and higher performance for on-policy\nreinforcement learning algorithms in challenging control tasks, especially in\nhighly complex tasks such as Humanoid. We provide theoretical analysis on the\nvariance of the policy gradient estimator, which suggests that our attentively\ndesigned unimodal discrete policy can retain a lower variance and yield a\nstable learning process.\n","authors":["Yuanyang Zhu","Zhi Wang","Yuanheng Zhu","Chunlin Chen","Dongbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.00309v1.pdf","comment":"IEEE Transactions on Neural Networks and Learning Systems"},{"id":"http://arxiv.org/abs/2408.00307v1","updated":"2024-08-01T06:06:25Z","published":"2024-08-01T06:06:25Z","title":"ABC Align: Large Language Model Alignment for Safety & Accuracy","summary":"  Alignment of Large Language Models (LLMs) remains an unsolved problem. Human\npreferences are highly distributed and can be captured at multiple levels of\nabstraction, from the individual to diverse populations. Organisational\npreferences, represented by standards and principles, are defined to mitigate\nreputational risk or meet legislative obligations. In this paper, we present\nABC Align, a novel alignment methodology for LLMs that enables integration of\nthe standards and preferences of a large media organisation into the LLM\nitself. We combine a set of data and methods that build on recent breakthroughs\nin synthetic data generation, preference optimisation, and post-training model\nquantisation. Our unified approach mitigates bias and improves accuracy, while\npreserving reasoning capability, as measured against standard benchmarks.\n","authors":["Gareth Seneque","Lap-Hang Ho","Ariel Kuperman","Nafise Erfanian Saeedi","Jeffrey Molendijk"],"pdf_url":"https://arxiv.org/pdf/2408.00307v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00295v1","updated":"2024-08-01T05:45:21Z","published":"2024-08-01T05:45:21Z","title":"Contrastive Graph Representation Learning with Adversarial Cross-view\n  Reconstruction and Information Bottleneck","summary":"  Graph Neural Networks (GNNs) have received extensive research attention due\nto their powerful information aggregation capabilities. Despite the success of\nGNNs, most of them suffer from the popularity bias issue in a graph caused by a\nsmall number of popular categories. Additionally, real graph datasets always\ncontain incorrect node labels, which hinders GNNs from learning effective node\nrepresentations. Graph contrastive learning (GCL) has been shown to be\neffective in solving the above problems for node classification tasks. Most\nexisting GCL methods are implemented by randomly removing edges and nodes to\ncreate multiple contrasting views, and then maximizing the mutual information\n(MI) between these contrasting views to improve the node feature\nrepresentation. However, maximizing the mutual information between multiple\ncontrasting views may lead the model to learn some redundant information\nirrelevant to the node classification task. To tackle this issue, we propose an\neffective Contrastive Graph Representation Learning with Adversarial Cross-view\nReconstruction and Information Bottleneck (CGRL) for node classification, which\ncan adaptively learn to mask the nodes and edges in the graph to obtain the\noptimal graph structure representation. Furthermore, we innovatively introduce\nthe information bottleneck theory into GCLs to remove redundant information in\nmultiple contrasting views while retaining as much information as possible\nabout node classification. Moreover, we add noise perturbations to the original\nviews and reconstruct the augmented views by constructing adversarial views to\nimprove the robustness of node feature representation. Extensive experiments on\nreal-world public datasets demonstrate that our method significantly\noutperforms existing state-of-the-art algorithms.\n","authors":["Yuntao Shou","Haozhi Lan","Xiangyong Cao"],"pdf_url":"https://arxiv.org/pdf/2408.00295v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00288v1","updated":"2024-08-01T05:22:41Z","published":"2024-08-01T05:22:41Z","title":"Gradient Harmonization in Unsupervised Domain Adaptation","summary":"  Unsupervised domain adaptation (UDA) intends to transfer knowledge from a\nlabeled source domain to an unlabeled target domain. Many current methods focus\non learning feature representations that are both discriminative for\nclassification and invariant across domains by simultaneously optimizing domain\nalignment and classification tasks. However, these methods often overlook a\ncrucial challenge: the inherent conflict between these two tasks during\ngradient-based optimization. In this paper, we delve into this issue and\nintroduce two effective solutions known as Gradient Harmonization, including GH\nand GH++, to mitigate the conflict between domain alignment and classification\ntasks. GH operates by altering the gradient angle between different tasks from\nan obtuse angle to an acute angle, thus resolving the conflict and trade-offing\nthe two tasks in a coordinated manner. Yet, this would cause both tasks to\ndeviate from their original optimization directions. We thus further propose an\nimproved version, GH++, which adjusts the gradient angle between tasks from an\nobtuse angle to a vertical angle. This not only eliminates the conflict but\nalso minimizes deviation from the original gradient directions. Finally, for\noptimization convenience and efficiency, we evolve the gradient harmonization\nstrategies into a dynamically weighted loss function using an integral operator\non the harmonized gradient. Notably, GH/GH++ are orthogonal to UDA and can be\nseamlessly integrated into most existing UDA models. Theoretical insights and\nexperimental analyses demonstrate that the proposed approaches not only enhance\npopular UDA baselines but also improve recent state-of-the-art models.\n","authors":["Fuxiang Huang","Suqi Song","Lei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00288v1.pdf","comment":"IEEE TPAMI 2024"},{"id":"http://arxiv.org/abs/2408.00278v1","updated":"2024-08-01T04:37:03Z","published":"2024-08-01T04:37:03Z","title":"High Performance Im2win and Direct Convolutions using Three Tensor\n  Layouts on SIMD Architectures","summary":"  Convolution is the core component within deep neural networks and it is\ncomputationally intensive and time consuming. Tensor data layouts significantly\nimpact convolution operations in terms of memory access and computational\nefficiency. Yet, there is still a lack of comprehensive performance\ncharacterization on data layouts on SIMD architectures concerning convolution\nmethods. This paper proposes three novel data layouts for im2win convolution:\nNHWC, CHWN, and CHWN8, and introduces a set of general optimization techniques\nfor both direct and im2win convolutions. We compare the optimized im2win\nconvolution with the direct convolution and PyTorch's im2col-based convolution\nacross the aforementioned layouts on SIMD machines. The experiments\ndemonstrated that the im2win convolution with the new NHWC layout achieved up\nto 355% performance speedup over NCHW layout. Our optimizations also\nsignificantly improve the performance of both im2win and direct convolutions.\nOur optimized im2win and direct convolutions achieved up to 95% and 94% of\nmachine's theoretical peak performance, respectively.\n","authors":["Xiang Fu","Xinpeng Zhang","Jixiang Ma","Peng Zhao","Shuai Lu","Xu T. Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00264v1","updated":"2024-08-01T03:43:32Z","published":"2024-08-01T03:43:32Z","title":"Clover-2: Accurate Inference for Regressive Lightweight Speculative\n  Decoding","summary":"  Large Language Models (LLMs) frequently suffer from inefficiencies, largely\nattributable to the discord between the requirements of auto-regressive\ndecoding and the architecture of contemporary GPUs. Recently, regressive\nlightweight speculative decoding has garnered attention for its notable\nefficiency improvements in text generation tasks. This approach utilizes a\nlightweight regressive draft model, like a Recurrent Neural Network (RNN) or a\nsingle transformer decoder layer, leveraging sequential information to\niteratively predict potential tokens. Specifically, RNN draft models are\ncomputationally economical but tend to deliver lower accuracy, while attention\ndecoder layer models exhibit the opposite traits. This paper presents Clover-2,\nan advanced iteration of Clover, an RNN-based draft model designed to achieve\ncomparable accuracy to that of attention decoder layer models while maintaining\nminimal computational overhead. Clover-2 enhances the model architecture and\nincorporates knowledge distillation to increase Clover's accuracy and improve\noverall efficiency. We conducted experiments using the open-source Vicuna 7B\nand LLaMA3-Instruct 8B models. The results demonstrate that Clover-2 surpasses\nexisting methods across various model architectures, showcasing its efficacy\nand robustness.\n","authors":["Bin Xiao","Lujun Gui","Lei Su","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.00264v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00256v1","updated":"2024-08-01T03:28:10Z","published":"2024-08-01T03:28:10Z","title":"Mobility-Aware Federated Self-supervised Learning in Vehicular Network","summary":"  Federated Learning (FL) is an advanced distributed machine learning approach,\nthat protects the privacy of each vehicle by allowing the model to be trained\non multiple devices simultaneously without the need to upload all data to a\nroad side unit (RSU). This enables FL to handle scenarios with sensitive or\nwidely distributed data. However, in these fields, it is well known that the\nlabeling costs can be a significant expense, and models relying on labels are\nnot suitable for these rapidly evolving fields especially in vehicular\nnetworks, or mobile internet of things (MIoT), where new data emerges\nconstantly. To handle this issue, the self-supervised learning paves the way\nfor training without labels. Additionally, for vehicles with high velocity,\nowing to blurred images, simple aggregation not only impacts the accuracy of\nthe aggregated model but also reduces the convergence speed of FL. This paper\nproposes a FL algorithm based on image blur level to aggregation, called\nFLSimCo, which does not require labels and serves as a pre-training stage for\nself-supervised learning in the vehicular environment. Simulation results\ndemonstrate that the proposed algorithm exhibits fast and stable convergence.\n","authors":["Xueying Gu","Qiong Wu","Pingyi Fan","Qiang Fan"],"pdf_url":"https://arxiv.org/pdf/2408.00256v1.pdf","comment":"This paper has been submitted to urban lifeline. The source code has\n  been released at: The source code has been released at:\n  https://github.com/qiongwu86/FLSimCo"},{"id":"http://arxiv.org/abs/2408.00251v1","updated":"2024-08-01T03:15:08Z","published":"2024-08-01T03:15:08Z","title":"Discovering Car-following Dynamics from Trajectory Data through Deep\n  Learning","summary":"  This study aims to discover the governing mathematical expressions of\ncar-following dynamics from trajectory data directly using deep learning\ntechniques. We propose an expression exploration framework based on deep\nsymbolic regression (DSR) integrated with a variable intersection selection\n(VIS) method to find variable combinations that encourage interpretable and\nparsimonious mathematical expressions. In the exploration learning process, two\npenalty terms are added to improve the reward function: (i) a complexity\npenalty to regulate the complexity of the explored expressions to be\nparsimonious, and (ii) a variable interaction penalty to encourage the\nexpression exploration to focus on variable combinations that can best describe\nthe data. We show the performance of the proposed method to learn several\ncar-following dynamics models and discuss its limitations and future research\ndirections.\n","authors":["Ohay Angah","James Enouen"," Xuegang"," Ban","Yan Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00244v1","updated":"2024-08-01T02:49:58Z","published":"2024-08-01T02:49:58Z","title":"Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms","summary":"  Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.\n","authors":["Tian Meng","Yang Tao","Wuliang Yin"],"pdf_url":"https://arxiv.org/pdf/2408.00244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00237v1","updated":"2024-08-01T02:13:11Z","published":"2024-08-01T02:13:11Z","title":"Empirical Bayes Linked Matrix Decomposition","summary":"  Data for several applications in diverse fields can be represented as\nmultiple matrices that are linked across rows or columns. This is particularly\ncommon in molecular biomedical research, in which multiple molecular \"omics\"\ntechnologies may capture different feature sets (e.g., corresponding to rows in\na matrix) and/or different sample populations (corresponding to columns). This\nhas motivated a large body of work on integrative matrix factorization\napproaches that identify and decompose low-dimensional signal that is shared\nacross multiple matrices or specific to a given matrix. We propose an empirical\nvariational Bayesian approach to this problem that has several advantages over\nexisting techniques, including the flexibility to accommodate shared signal\nover any number of row or column sets (i.e., bidimensional integration), an\nintuitive model-based objective function that yields appropriate shrinkage for\nthe inferred signals, and a relatively efficient estimation algorithm with no\ntuning parameters. A general result establishes conditions for the uniqueness\nof the underlying decomposition for a broad family of methods that includes the\nproposed approach. For scenarios with missing data, we describe an associated\niterative imputation approach that is novel for the single-matrix context and a\npowerful approach for \"blockwise\" imputation (in which an entire row or column\nis missing) in various linked matrix contexts. Extensive simulations show that\nthe method performs very well under different scenarios with respect to\nrecovering underlying low-rank signal, accurately decomposing shared and\nspecific signals, and accurately imputing missing data. The approach is applied\nto gene expression and miRNA data from breast cancer tissue and normal breast\ntissue, for which it gives an informative decomposition of variation and\noutperforms alternative strategies for missing data imputation.\n","authors":["Eric F. Lock"],"pdf_url":"https://arxiv.org/pdf/2408.00237v1.pdf","comment":"29 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.00232v1","updated":"2024-08-01T01:57:09Z","published":"2024-08-01T01:57:09Z","title":"CDFGNN: a Systematic Design of Cache-based Distributed Full-Batch Graph\n  Neural Network Training with Communication Reduction","summary":"  Graph neural network training is mainly categorized into mini-batch and\nfull-batch training methods. The mini-batch training method samples subgraphs\nfrom the original graph in each iteration. This sampling operation introduces\nextra computation overhead and reduces the training accuracy. Meanwhile, the\nfull-batch training method calculates the features and corresponding gradients\nof all vertices in each iteration, and therefore has higher convergence\naccuracy. However, in the distributed cluster, frequent remote accesses of\nvertex features and gradients lead to huge communication overhead, thus\nrestricting the overall training efficiency.\n  In this paper, we introduce the cached-based distributed full-batch graph\nneural network training framework (CDFGNN). We propose the adaptive cache\nmechanism to reduce the remote vertex access by caching the historical features\nand gradients of neighbor vertices. Besides, we further optimize the\ncommunication overhead by quantifying the messages and designing the graph\npartition algorithm for the hierarchical communication architecture.\nExperiments show that the adaptive cache mechanism reduces remote vertex\naccesses by 63.14% on average. Combined with communication quantization and\nhierarchical GP algorithm, CDFGNN outperforms the state-of-the-art distributed\nfull-batch training frameworks by 30.39% in our experiments. Our results\nindicate that CDFGNN has great potential in accelerating distributed full-batch\nGNN training tasks.\n","authors":["Shuai Zhang","Zite Jiang","Haihang You"],"pdf_url":"https://arxiv.org/pdf/2408.00232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00229v1","updated":"2024-08-01T01:48:46Z","published":"2024-08-01T01:48:46Z","title":"Invariant Discovery of Features Across Multiple Length Scales:\n  Applications in Microscopy and Autonomous Materials Characterization","summary":"  Physical imaging is a foundational characterization method in areas from\ncondensed matter physics and chemistry to astronomy and spans length scales\nfrom atomic to universe. Images encapsulate crucial data regarding atomic\nbonding, materials microstructures, and dynamic phenomena such as\nmicrostructural evolution and turbulence, among other phenomena. The challenge\nlies in effectively extracting and interpreting this information. Variational\nAutoencoders (VAEs) have emerged as powerful tools for identifying underlying\nfactors of variation in image data, providing a systematic approach to\ndistilling meaningful patterns from complex datasets. However, a significant\nhurdle in their application is the definition and selection of appropriate\ndescriptors reflecting local structure. Here we introduce the scale-invariant\nVAE approach (SI-VAE) based on the progressive training of the VAE with the\ndescriptors sampled at different length scales. The SI-VAE allows the discovery\nof the length scale dependent factors of variation in the system. Here, we\nillustrate this approach using the ferroelectric domain images and generalize\nit to the movies of the electron-beam induced phenomena in graphene and\ntopography evolution across combinatorial libraries. This approach can further\nbe used to initialize the decision making in automated experiments including\nstructure-property discovery and can be applied across a broad range of imaging\nmethods. This approach is universal and can be applied to any spatially\nresolved data including both experimental imaging studies and simulations, and\ncan be particularly useful for exploration of phenomena such as turbulence,\nscale-invariant transformation fronts, etc.\n","authors":["Aditya Raghavan","Utkarsh Pratiush","Mani Valleti","Richard Liu","Reece Emery","Hiroshi Funakubo","Yongtao Liu","Philip Rack","Sergei Kalinin"],"pdf_url":"https://arxiv.org/pdf/2408.00229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00220v1","updated":"2024-08-01T01:15:52Z","published":"2024-08-01T01:15:52Z","title":"Persistent de Rham-Hodge Laplacians in the Eulerian representation","summary":"  Recently, topological data analysis (TDA) has become a trending topic in data\nscience and engineering. However, the key technique of TDA, i.e., persistent\nhomology, is defined on point cloud data, which restricts its scope. In this\nwork, we propose persistent de Rham-Hodge Laplacian, or persistent Hodge\nLaplacian (PHL) for abbreviation, for the TDA on manifolds with boundaries, or\nvolumetric data. Specifically, we extended the evolutionary de Rham-Hodge\ntheory from the Lagrangian formulation to the Eulerian formulation via\nstructure-persevering Cartesian grids, and extended the persistent Laplacian on\npoint clouds to persistent (de Rham-)Hodge Laplacian on nested families of\nmanifolds with appropriate boundary conditions. The proposed PHL facilitates\nthe machine learning and deep learning prediction of volumetric data. For a\nproof-of-principle application of the proposed PHL, we propose a persistent\nHodge Laplacian learning (PHLL) algorithm for data on manifolds or volumetric\ndata. To this end, we showcase the PHLL prediction of protein-ligand binding\naffinities in two benchmark datasets. Our numerical experiments highlight the\npower and promise of PHLL.\n","authors":["Zhe Su","Yiying Tong","Guo-Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2408.00220v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00217v1","updated":"2024-08-01T00:56:36Z","published":"2024-08-01T00:56:36Z","title":"Load Balancing in Federated Learning","summary":"  Federated Learning (FL) is a decentralized machine learning framework that\nenables learning from data distributed across multiple remote devices,\nenhancing communication efficiency and data privacy. Due to limited\ncommunication resources, a scheduling policy is often applied to select a\nsubset of devices for participation in each FL round. The scheduling process\nconfronts significant challenges due to the need for fair workload\ndistribution, efficient resource utilization, scalability in environments with\nnumerous edge devices, and statistically heterogeneous data across devices.\nThis paper proposes a load metric for scheduling policies based on the Age of\nInformation and addresses the above challenges by minimizing the load metric\nvariance across the clients. Furthermore, a decentralized Markov scheduling\npolicy is presented, that ensures a balanced workload distribution while\neliminating the management overhead irrespective of the network size due to\nindependent client decision-making. We establish the optimal parameters of the\nMarkov chain model and validate our approach through simulations. The results\ndemonstrate that reducing the load metric variance not only promotes fairness\nand improves operational efficiency, but also enhances the convergence rate of\nthe learning models.\n","authors":["Alireza Javani","Zhiying Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00217v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00211v1","updated":"2024-08-01T00:45:37Z","published":"2024-08-01T00:45:37Z","title":"Penzai + Treescope: A Toolkit for Interpreting, Visualizing, and Editing\n  Models As Data","summary":"  Much of today's machine learning research involves interpreting, modifying or\nvisualizing models after they are trained. I present Penzai, a neural network\nlibrary designed to simplify model manipulation by representing models as\nsimple data structures, and Treescope, an interactive pretty-printer and array\nvisualizer that can visualize both model inputs/outputs and the models\nthemselves. Penzai models are built using declarative combinators that expose\nthe model forward pass in the structure of the model object itself, and use\nnamed axes to ensure each operation is semantically meaningful. With Penzai's\ntree-editing selector system, users can both insert and replace model\ncomponents, allowing them to intervene on intermediate values or make other\nedits to the model structure. Users can then get immediate feedback by\nvisualizing the modified model with Treescope. I describe the motivation and\nmain features of Penzai and Treescope, and discuss how treating the model as\ndata enables a variety of analyses and interventions to be implemented as\ndata-structure transformations, without requiring model designers to add\nexplicit hooks.\n","authors":["Daniel D. Johnson"],"pdf_url":"https://arxiv.org/pdf/2408.00211v1.pdf","comment":"Presented at the ICML 2024 Mechanistic Interpretability workshop\n  (Spotlight). 5 pages"},{"id":"http://arxiv.org/abs/2408.00208v1","updated":"2024-08-01T00:33:32Z","published":"2024-08-01T00:33:32Z","title":"Prognosis of COVID-19 using Artificial Intelligence: A Systematic Review\n  and Meta-analysis","summary":"  Purpose: Artificial intelligence (AI) techniques have been extensively\nutilized for diagnosing and prognosis of several diseases in recent years. This\nstudy identifies, appraises and synthesizes published studies on the use of AI\nfor the prognosis of COVID-19. Method: Electronic search was performed using\nMedline, Google Scholar, Scopus, Embase, Cochrane and ProQuest. Studies that\nexamined machine learning or deep learning methods to determine the prognosis\nof COVID-19 using CT or chest X-ray images were included. Polled sensitivity,\nspecificity area under the curve and diagnostic odds ratio were calculated.\nResult: A total of 36 articles were included; various prognosis-related issues,\nincluding disease severity, mechanical ventilation or admission to the\nintensive care unit and mortality, were investigated. Several AI models and\narchitectures were employed, such as the Siamense model, support vector\nmachine, Random Forest , eXtreme Gradient Boosting, and convolutional neural\nnetworks. The models achieved 71%, 88% and 67% sensitivity for mortality,\nseverity assessment and need for ventilation, respectively. The specificity of\n69%, 89% and 89% were reported for the aforementioned variables. Conclusion:\nBased on the included articles, machine learning and deep learning methods used\nfor the prognosis of COVID-19 patients using radiomic features from CT or CXR\nimages can help clinicians manage patients and allocate resources more\neffectively. These studies also demonstrate that combining patient demographic,\nclinical data, laboratory tests and radiomic features improves model\nperformances.\n","authors":["SaeedReza Motamedian","Sadra Mohaghegh","Elham Babadi Oregani","Mahrsa Amjadi","Parnian Shobeiri","Negin Cheraghi","Niusha Solouki","Nikoo Ahmadi","Hossein Mohammad-Rahimi","Yassine Bouchareb","Arman Rahmim"],"pdf_url":"https://arxiv.org/pdf/2408.00208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00203v1","updated":"2024-08-01T00:00:43Z","published":"2024-08-01T00:00:43Z","title":"OmniParser for Pure Vision Based GUI Agent","summary":"  The recent success of large vision language models shows great potential in\ndriving the agent system operating on user interfaces. However, we argue that\nthe power multimodal models like GPT-4V as a general agent on multiple\noperating systems across different applications is largely underestimated due\nto the lack of a robust screen parsing technique capable of: 1) reliably\nidentifying interactable icons within the user interface, and 2) understanding\nthe semantics of various elements in a screenshot and accurately associate the\nintended action with the corresponding region on the screen. To fill these\ngaps, we introduce \\textsc{OmniParser}, a comprehensive method for parsing user\ninterface screenshots into structured elements, which significantly enhances\nthe ability of GPT-4V to generate actions that can be accurately grounded in\nthe corresponding regions of the interface. We first curated an interactable\nicon detection dataset using popular webpages and an icon description dataset.\nThese datasets were utilized to fine-tune specialized models: a detection model\nto parse interactable regions on the screen and a caption model to extract the\nfunctional semantics of the detected elements. \\textsc{OmniParser}\nsignificantly improves GPT-4V's performance on ScreenSpot benchmark. And on\nMind2Web and AITW benchmark, \\textsc{OmniParser} with screenshot only input\noutperforms the GPT-4V baselines requiring additional information outside of\nscreenshot.\n","authors":["Yadong Lu","Jianwei Yang","Yelong Shen","Ahmed Awadallah"],"pdf_url":"https://arxiv.org/pdf/2408.00203v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00955v1","updated":"2024-08-01T23:32:14Z","published":"2024-08-01T23:32:14Z","title":"Aggregation Models with Optimal Weights for Distributed Gaussian\n  Processes","summary":"  Gaussian process (GP) models have received increasingly attentions in recent\nyears due to their superb prediction accuracy and modeling flexibility. To\naddress the computational burdens of GP models for large-scale datasets,\ndistributed learning for GPs are often adopted. Current aggregation models for\ndistributed GPs are not time-efficient when incorporating correlations between\nGP experts. In this work, we propose a novel approach for aggregated prediction\nin distributed GPs. The technique is suitable for both the exact and sparse\nvariational GPs. The proposed method incorporates correlations among experts,\nleading to better prediction accuracy with manageable computational\nrequirements. As demonstrated by empirical studies, the proposed approach\nresults in more stable predictions in less time than state-of-the-art\nconsistent aggregation models.\n","authors":["Haoyuan Chen","Rui Tuo"],"pdf_url":"https://arxiv.org/pdf/2408.00955v1.pdf","comment":"25 pages, 12 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.19092v2","updated":"2024-08-01T23:12:50Z","published":"2024-07-26T21:18:26Z","title":"Boosted generalized normal distributions: Integrating machine learning\n  with operations knowledge","summary":"  Applications of machine learning (ML) techniques to operational settings\noften face two challenges: i) ML methods mostly provide point predictions\nwhereas many operational problems require distributional information; and ii)\nThey typically do not incorporate the extensive body of knowledge in the\noperations literature, particularly the theoretical and empirical findings that\ncharacterize specific distributions. We introduce a novel and rigorous\nmethodology, the Boosted Generalized Normal Distribution ($b$GND), to address\nthese challenges. The Generalized Normal Distribution (GND) encompasses a wide\nrange of parametric distributions commonly encountered in operations, and\n$b$GND leverages gradient boosting with tree learners to flexibly estimate the\nparameters of the GND as functions of covariates. We establish $b$GND's\nstatistical consistency, thereby extending this key property to special cases\nstudied in the ML literature that lacked such guarantees. Using data from a\nlarge academic emergency department in the United States, we show that the\ndistributional forecasting of patient wait and service times can be\nmeaningfully improved by leveraging findings from the healthcare operations\nliterature. Specifically, $b$GND performs 6% and 9% better than the\ndistribution-agnostic ML benchmark used to forecast wait and service times\nrespectively. Further analysis suggests that these improvements translate into\na 9% increase in patient satisfaction and a 4% reduction in mortality for\nmyocardial infarction patients. Our work underscores the importance of\nintegrating ML with operations knowledge to enhance distributional forecasts.\n","authors":["Ragip Gurlek","Francis de Vericourt","Donald K. K. Lee"],"pdf_url":"https://arxiv.org/pdf/2407.19092v2.pdf","comment":"28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.00949v1","updated":"2024-08-01T23:08:37Z","published":"2024-08-01T23:08:37Z","title":"Equivariant neural networks and piecewise linear representation theory","summary":"  Equivariant neural networks are neural networks with symmetry. Motivated by\nthe theory of group representations, we decompose the layers of an equivariant\nneural network into simple representations. The nonlinear activation functions\nlead to interesting nonlinear equivariant maps between simple representations.\nFor example, the rectified linear unit (ReLU) gives rise to piecewise linear\nmaps. We show that these considerations lead to a filtration of equivariant\nneural networks, generalizing Fourier series. This observation might provide a\nuseful tool for interpreting equivariant neural networks.\n","authors":["Joel Gibson","Daniel Tubbenhauer","Geordie Williamson"],"pdf_url":"https://arxiv.org/pdf/2408.00949v1.pdf","comment":"24 pages, many figures, comments welcome"},{"id":"http://arxiv.org/abs/2405.05480v4","updated":"2024-08-01T22:57:53Z","published":"2024-05-09T00:37:56Z","title":"FloorSet -- a VLSI Floorplanning Dataset with Design Constraints of\n  Real-World SoCs","summary":"  Floorplanning for systems-on-a-chip (SoCs) and its sub-systems is a crucial\nand non-trivial step of the physical design flow. It represents a difficult\ncombinatorial optimization problem. A typical large scale SoC with 120\npartitions generates a search-space of nearly 10E250. As novel machine learning\n(ML) approaches emerge to tackle such problems, there is a growing need for a\nmodern benchmark that comprises a large training dataset and performance\nmetrics that better reflect real-world constraints and objectives compared to\nexisting benchmarks. To address this need, we present FloorSet -- two\ncomprehensive datasets of synthetic fixed-outline floorplan layouts that\nreflect the distribution of real SoCs. Each dataset has 1M training samples and\n100 test samples where each sample is a synthetic floor-plan. FloorSet-Prime\ncomprises fully-abutted rectilinear partitions and near-optimal wire-length. A\nsimplified dataset that reflects early design phases, FloorSet-Lite comprises\nrectangular partitions, with under 5 percent white-space and near-optimal\nwire-length. Both datasets define hard constraints seen in modern design flows\nsuch as shape constraints, edge-affinity, grouping constraints, and\npre-placement constraints. FloorSet is intended to spur fundamental research on\nlarge-scale constrained optimization problems. Crucially, FloorSet alleviates\nthe core issue of reproducibility in modern ML driven solutions to such\nproblems. FloorSet is available as an open-source repository for the research\ncommunity.\n","authors":["Uday Mallappa","Hesham Mostafa","Mikhail Galkin","Mariano Phielipp","Somdeb Majumdar"],"pdf_url":"https://arxiv.org/pdf/2405.05480v4.pdf","comment":"10 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.00946v1","updated":"2024-08-01T22:55:40Z","published":"2024-08-01T22:55:40Z","title":"Generalisation of Total Uncertainty in AI: A Theoretical Study","summary":"  AI has been dealing with uncertainty to have highly accurate results. This\nbecomes even worse with reasonably small data sets or a variation in the data\nsets. This has far-reaching effects on decision-making, forecasting and\nlearning mechanisms. This study seeks to unpack the nature of uncertainty that\nexists within AI by drawing ideas from established works, the latest\ndevelopments and practical applications and provide a novel total uncertainty\ndefinition in AI.\n  From inception theories up to current methodologies, this paper provides an\nintegrated view of dealing with better total uncertainty as well as\ncomplexities of uncertainty in AI that help us understand its meaning and value\nacross different domains.\n","authors":["Keivan Shariatmadar"],"pdf_url":"https://arxiv.org/pdf/2408.00946v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2304.11880v2","updated":"2024-08-01T22:49:23Z","published":"2023-04-24T07:50:35Z","title":"The State of the Art in transformer fault diagnosis with artificial\n  intelligence and Dissolved Gas Analysis: A Review of the Literature","summary":"  Transformer fault diagnosis (TFD) is a critical aspect of power system\nmaintenance and management. This review paper provides a comprehensive overview\nof the current state of the art in TFD using artificial intelligence (AI) and\ndissolved gas analysis (DGA). The paper presents an analysis of recent\nadvancements in this field, including the use of deep learning algorithms and\nadvanced data analytics techniques, and their potential impact on TFD and the\npower industry as a whole. The review also highlights the benefits and\nlimitations of different approaches to transformer fault diagnosis, including\nrule-based systems, expert systems, neural networks, and machine learning\nalgorithms. Overall, this review aims to provide valuable insights into the\nimportance of TFD and the role of AI in ensuring the reliable operation of\npower systems.\n","authors":["Yuyan Li"],"pdf_url":"https://arxiv.org/pdf/2304.11880v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19024v3","updated":"2024-08-01T22:45:45Z","published":"2024-05-29T12:07:17Z","title":"Inverse Concave-Utility Reinforcement Learning is Inverse Game Theory","summary":"  We consider inverse reinforcement learning problems with concave utilities.\nConcave Utility Reinforcement Learning (CURL) is a generalisation of the\nstandard RL objective, which employs a concave function of the state occupancy\nmeasure, rather than a linear function. CURL has garnered recent attention for\nits ability to represent instances of many important applications including the\nstandard RL such as imitation learning, pure exploration, constrained MDPs,\noffline RL, human-regularized RL, and others. Inverse reinforcement learning is\na powerful paradigm that focuses on recovering an unknown reward function that\ncan rationalize the observed behaviour of an agent. There has been recent\ntheoretical advances in inverse RL where the problem is formulated as\nidentifying the set of feasible reward functions. However, inverse RL for CURL\nproblems has not been considered previously. In this paper we show that most of\nthe standard IRL results do not apply to CURL in general, since CURL\ninvalidates the classical Bellman equations. This calls for a new theoretical\nframework for the inverse CURL problem. Using a recent equivalence result\nbetween CURL and Mean-field Games, we propose a new definition for the feasible\nrewards for I-CURL by proving that this problem is equivalent to an inverse\ngame theory problem in a subclass of mean-field games. We outline future\ndirections and applications in human--AI collaboration enabled by our results.\n","authors":["Mustafa Mert elikok","Frans A. Oliehoek","Jan-Willem van de Meent"],"pdf_url":"https://arxiv.org/pdf/2405.19024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.02652v2","updated":"2024-08-01T22:39:20Z","published":"2024-06-04T16:14:19Z","title":"RepCNN: Micro-sized, Mighty Models for Wakeword Detection","summary":"  Always-on machine learning models require a very low memory and compute\nfootprint. Their restricted parameter count limits the model's capacity to\nlearn, and the effectiveness of the usual training algorithms to find the best\nparameters. Here we show that a small convolutional model can be better trained\nby first refactoring its computation into a larger redundant multi-branched\narchitecture. Then, for inference, we algebraically re-parameterize the trained\nmodel into the single-branched form with fewer parameters for a lower memory\nfootprint and compute cost. Using this technique, we show that our always-on\nwake-word detector model, RepCNN, provides a good trade-off between latency and\naccuracy during inference. RepCNN re-parameterized models are 43% more accurate\nthan a uni-branch convolutional model while having the same runtime. RepCNN\nalso meets the accuracy of complex architectures like BC-ResNet, while having\n2x lesser peak memory usage and 10x faster runtime.\n","authors":["Arnav Kundu","Prateeth Nayak","Priyanka Padmanabhan","Devang Naik"],"pdf_url":"https://arxiv.org/pdf/2406.02652v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07292v4","updated":"2024-08-01T21:51:58Z","published":"2023-06-09T21:01:29Z","title":"SARN: Structurally-Aware Recurrent Network for Spatio-Temporal\n  Disaggregation","summary":"  Open data is frequently released spatially aggregated, usually to comply with\nprivacy policies. But coarse, heterogeneous aggregations complicate learning\nand integration for downstream AI/ML systems. In this work, we consider models\nto disaggregate spatio-temporal data from a low-resolution, irregular partition\n(e.g., census tract) to a high-resolution, irregular partition (e.g., city\nblock). We propose an overarching model named the Structurally-Aware Recurrent\nNetwork (SARN), which integrates structurally-aware spatial attention (SASA)\nlayers into the Gated Recurrent Unit (GRU) model. The spatial attention layers\ncapture spatial interactions among regions, while the gated recurrent module\ncaptures the temporal dependencies. Each SASA layer calculates both global and\nstructural attention -- global attention facilitates comprehensive interactions\nbetween different geographic levels, while structural attention leverages the\ncontainment relationship between different geographic levels (e.g., a city\nblock being wholly contained within a census tract) to ensure coherent and\nconsistent results. For scenarios with limited historical training data, we\nexplore transfer learning and show that a model pre-trained on one city\nvariable can be fine-tuned for another city variable using only a few hundred\nsamples. Evaluating these techniques on two mobility datasets, we find that on\nboth datasets, SARN significantly outperforms other neural models (5% and 1%)\nand typical heuristic methods (40% and 14%), enabling us to generate realistic,\nhigh-quality fine-grained data for downstream applications.\n","authors":["Bin Han","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2306.07292v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01651v3","updated":"2024-08-01T21:41:44Z","published":"2023-10-02T21:27:57Z","title":"Fool Your (Vision and) Language Model With Embarrassingly Simple\n  Permutations","summary":"  Large language and vision-language models are rapidly being deployed in\npractice thanks to their impressive capabilities in instruction following,\nin-context learning, and so on. This raises an urgent need to carefully analyse\ntheir robustness so that stakeholders can understand if and when such models\nare trustworthy enough to be relied upon in any given application. In this\npaper, we highlight a specific vulnerability in popular models, namely\npermutation sensitivity in multiple-choice question answering (MCQA).\nSpecifically, we show empirically that popular models are vulnerable to\nadversarial permutation in answer sets for multiple-choice prompting, which is\nsurprising as models should ideally be as invariant to prompt permutation as\nhumans are. These vulnerabilities persist across various model sizes, and exist\nin very recent language and vision-language models. Code is available at\nhttps://github.com/ys-zong/FoolyourVLLMs.\n","authors":["Yongshuo Zong","Tingyang Yu","Ruchika Chavhan","Bingchen Zhao","Timothy Hospedales"],"pdf_url":"https://arxiv.org/pdf/2310.01651v3.pdf","comment":"ICML 2024; v3 fix typo"},{"id":"http://arxiv.org/abs/2407.00730v2","updated":"2024-08-01T21:39:05Z","published":"2024-06-30T15:38:38Z","title":"D-CDLF: Decomposition of Common and Distinctive Latent Factors for\n  Multi-view High-dimensional Data","summary":"  A typical approach to the joint analysis of multiple high-dimensional data\nviews is to decompose each view's data matrix into three parts: a low-rank\ncommon-source matrix generated by common latent factors of all data views, a\nlow-rank distinctive-source matrix generated by distinctive latent factors of\nthe corresponding data view, and an additive noise matrix. Existing\ndecomposition methods often focus on the uncorrelatedness between the common\nlatent factors and distinctive latent factors, but inadequately address the\nequally necessary uncorrelatedness between distinctive latent factors from\ndifferent data views. We propose a novel decomposition method, called\nDecomposition of Common and Distinctive Latent Factors (D-CDLF), to effectively\nachieve both types of uncorrelatedness for two-view data. We also discuss the\nestimation of the D-CDLF under high-dimensional settings.\n","authors":["Hai Shu"],"pdf_url":"https://arxiv.org/pdf/2407.00730v2.pdf","comment":"This revision updates only Paragraph 1 of Section 2.1 and Remark 2 of\n  Section 3.2 from version 1"},{"id":"http://arxiv.org/abs/2408.00930v1","updated":"2024-08-01T21:38:09Z","published":"2024-08-01T21:38:09Z","title":"Enabling High Data Throughput Reinforcement Learning on GPUs: A Domain\n  Agnostic Framework for Data-Driven Scientific Research","summary":"  We introduce WarpSci, a domain agnostic framework designed to overcome\ncrucial system bottlenecks encountered in the application of reinforcement\nlearning to intricate environments with vast datasets featuring\nhigh-dimensional observation or action spaces. Notably, our framework\neliminates the need for data transfer between the CPU and GPU, enabling the\nconcurrent execution of thousands of simulations on a single or multiple GPUs.\nThis high data throughput architecture proves particularly advantageous for\ndata-driven scientific research, where intricate environment models are\ncommonly essential.\n","authors":["Tian Lan","Huan Wang","Caiming Xiong","Silvio Savarese"],"pdf_url":"https://arxiv.org/pdf/2408.00930v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00929v1","updated":"2024-08-01T21:37:10Z","published":"2024-08-01T21:37:10Z","title":"Verification of Machine Unlearning is Fragile","summary":"  As privacy concerns escalate in the realm of machine learning, data owners\nnow have the option to utilize machine unlearning to remove their data from\nmachine learning models, following recent legislation. To enhance transparency\nin machine unlearning and avoid potential dishonesty by model providers,\nvarious verification strategies have been proposed. These strategies enable\ndata owners to ascertain whether their target data has been effectively\nunlearned from the model. However, our understanding of the safety issues of\nmachine unlearning verification remains nascent. In this paper, we explore the\nnovel research question of whether model providers can circumvent verification\nstrategies while retaining the information of data supposedly unlearned. Our\ninvestigation leads to a pessimistic answer: \\textit{the verification of\nmachine unlearning is fragile}. Specifically, we categorize the current\nverification strategies regarding potential dishonesty among model providers\ninto two types. Subsequently, we introduce two novel adversarial unlearning\nprocesses capable of circumventing both types. We validate the efficacy of our\nmethods through theoretical analysis and empirical experiments using real-world\ndatasets. This study highlights the vulnerabilities and limitations in machine\nunlearning verification, paving the way for further research into the safety of\nmachine unlearning.\n","authors":["Binchi Zhang","Zihan Chen","Cong Shen","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00929v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.00921v1","updated":"2024-08-01T21:22:16Z","published":"2024-08-01T21:22:16Z","title":"Automatic Pull Request Description Generation Using LLMs: A T5 Model\n  Approach","summary":"  Developers create pull request (PR) descriptions to provide an overview of\ntheir changes and explain the motivations behind them. These descriptions help\nreviewers and fellow developers quickly understand the updates. Despite their\nimportance, some developers omit these descriptions. To tackle this problem, we\npropose an automated method for generating PR descriptions based on commit\nmessages and source code comments. This method frames the task as a text\nsummarization problem, for which we utilized the T5 text-to-text transfer\nmodel. We fine-tuned a pre-trained T5 model using a dataset containing 33,466\nPRs. The model's effectiveness was assessed using ROUGE metrics, which are\nrecognized for their strong alignment with human evaluations. Our findings\nreveal that the T5 model significantly outperforms LexRank, which served as our\nbaseline for comparison.\n","authors":["Md Nazmus Sakib","Md Athikul Islam","Md Mashrur Arifin"],"pdf_url":"https://arxiv.org/pdf/2408.00921v1.pdf","comment":"Accepted to 2nd International Conference on Artificial Intelligence,\n  Blockchain, and Internet of Things (AIBThings-2024), September 07-08, 2024,\n  Michigan, USA"},{"id":"http://arxiv.org/abs/2408.00920v1","updated":"2024-08-01T21:22:10Z","published":"2024-08-01T21:22:10Z","title":"Towards Certified Unlearning for Deep Neural Networks","summary":"  In the field of machine unlearning, certified unlearning has been extensively\nstudied in convex machine learning models due to its high efficiency and strong\ntheoretical guarantees. However, its application to deep neural networks\n(DNNs), known for their highly nonconvex nature, still poses challenges. To\nbridge the gap between certified unlearning and DNNs, we propose several simple\ntechniques to extend certified unlearning methods to nonconvex objectives. To\nreduce the time complexity, we develop an efficient computation method by\ninverse Hessian approximation without compromising certification guarantees. In\naddition, we extend our discussion of certification to nonconvergence training\nand sequential unlearning, considering that real-world users can send\nunlearning requests at different time points. Extensive experiments on three\nreal-world datasets demonstrate the efficacy of our method and the advantages\nof certified unlearning in DNNs.\n","authors":["Binchi Zhang","Yushun Dong","Tianhao Wang","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00920v1.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2404.00859v2","updated":"2024-08-01T21:21:28Z","published":"2024-04-01T02:01:28Z","title":"Do language models plan ahead for future tokens?","summary":"  Do transformers \"think ahead\" during inference at a given position? It is\nknown transformers prepare information in the hidden states of the forward pass\nat time step $t$ that is then used in future forward passes $t+\\tau$. We posit\ntwo explanations for this phenomenon: pre-caching, in which off-diagonal\ngradient terms present during training result in the model computing features\nat $t$ irrelevant to the present inference task but useful for the future, and\nbreadcrumbs, in which features most relevant to time step $t$ are already the\nsame as those that would most benefit inference at time $t+\\tau$. We test these\nhypotheses by training language models without propagating gradients to past\ntimesteps, a scheme we formalize as myopic training. In a constructed synthetic\ndata setting, we find clear evidence for pre-caching. In the autoregressive\nlanguage modeling setting, our experiments are more suggestive of the\nbreadcrumbs hypothesis, though pre-caching increases with model scale.\n","authors":["Wilson Wu","John X. Morris","Lionel Levine"],"pdf_url":"https://arxiv.org/pdf/2404.00859v2.pdf","comment":"24 pages, 11 figures. Camera-ready for COLM 2024"},{"id":"http://arxiv.org/abs/2408.00911v1","updated":"2024-08-01T21:04:27Z","published":"2024-08-01T21:04:27Z","title":"Distance-Preserving Generative Modeling of Spatial Transcriptomics","summary":"  Spatial transcriptomics data is invaluable for understanding the spatial\norganization of gene expression in tissues. There have been consistent efforts\nin studying how to effectively utilize the associated spatial information for\nrefining gene expression modeling. We introduce a class of distance-preserving\ngenerative models for spatial transcriptomics, which utilizes the provided\nspatial information to regularize the learned representation space of gene\nexpressions to have a similar pair-wise distance structure. This helps the\nlatent space to capture meaningful encodings of genes in spatial proximity. We\ncarry out theoretical analysis over a tractable loss function for this purpose\nand formalize the overall learning objective as a regularized evidence lower\nbound. Our framework grants compatibility with any variational-inference-based\ngenerative models for gene expression modeling. Empirically, we validate our\nproposed method on the mouse brain tissues Visium dataset and observe improved\nperformance with variational autoencoders and scVI used as backbone models.\n","authors":["Wenbin Zhou","Jin-Hong Du"],"pdf_url":"https://arxiv.org/pdf/2408.00911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00908v1","updated":"2024-08-01T20:58:07Z","published":"2024-08-01T20:58:07Z","title":"Early Stopping Based on Repeated Significance","summary":"  For a bucket test with a single criterion for success and a fixed number of\nsamples or testing period, requiring a $p$-value less than a specified value of\n$\\alpha$ for the success criterion produces statistical confidence at level $1\n- \\alpha$. For multiple criteria, a Bonferroni correction that partitions\n$\\alpha$ among the criteria produces statistical confidence, at the cost of\nrequiring lower $p$-values for each criterion. The same concept can be applied\nto decisions about early stopping, but that can lead to strict requirements for\n$p$-values. We show how to address that challenge by requiring criteria to be\nsuccessful at multiple decision points.\n","authors":["Eric Bax","Arundhyoti Sarkar","Alex Shtoff"],"pdf_url":"https://arxiv.org/pdf/2408.00908v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15847v2","updated":"2024-08-01T20:54:46Z","published":"2024-06-22T13:26:14Z","title":"Enhancing Solar Driver Forecasting with Multivariate Transformers","summary":"  In this work, we develop a comprehensive framework for F10.7, S10.7, M10.7,\nand Y10.7 solar driver forecasting with a time series Transformer (PatchTST).\nTo ensure an equal representation of high and low levels of solar activity, we\nconstruct a custom loss function to weight samples based on the distance\nbetween the solar driver's historical distribution and the training set. The\nsolar driver forecasting framework includes an 18-day lookback window and\nforecasts 6 days into the future. When benchmarked against the Space\nEnvironment Technologies (SET) dataset, our model consistently produces\nforecasts with a lower standard mean error in nearly all cases, with improved\nprediction accuracy during periods of high solar activity. All the code is\navailable on Github https://github.com/ARCLab-MIT/sw-driver-forecaster.\n","authors":["Sergio Sanchez-Hurtado","Victor Rodriguez-Fernandez","Julia Briden","Peng Mun Siew","Richard Linares"],"pdf_url":"https://arxiv.org/pdf/2406.15847v2.pdf","comment":"Short paper accepted for oral presentation at the SPAICE Conference\n  2024 (https://spaice.esa.int/)"},{"id":"http://arxiv.org/abs/2408.00906v1","updated":"2024-08-01T20:54:33Z","published":"2024-08-01T20:54:33Z","title":"Parkinson's Disease Detection from Resting State EEG using Multi-Head\n  Graph Structure Learning with Gradient Weighted Graph Attention Explanations","summary":"  Parkinson's disease (PD) is a debilitating neurodegenerative disease that has\nsevere impacts on an individual's quality of life. Compared with structural and\nfunctional MRI-based biomarkers for the disease, electroencephalography (EEG)\ncan provide more accessible alternatives for clinical insights. While deep\nlearning (DL) techniques have provided excellent outcomes, many techniques fail\nto model spatial information and dynamic brain connectivity, and face\nchallenges in robust feature learning, limited data sizes, and poor\nexplainability. To address these issues, we proposed a novel graph neural\nnetwork (GNN) technique for explainable PD detection using resting state EEG.\nSpecifically, we employ structured global convolutions with contrastive\nlearning to better model complex features with limited data, a novel multi-head\ngraph structure learner to capture the non-Euclidean structure of EEG data, and\na head-wise gradient-weighted graph attention explainer to offer neural\nconnectivity insights. We developed and evaluated our method using the UC San\nDiego Parkinson's disease EEG dataset, and achieved 69.40% detection accuracy\nin subject-wise leave-one-out cross-validation while generating intuitive\nexplanations for the learnt graph topology.\n","authors":["Christopher Neves","Yong Zeng","Yiming Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.00906v1.pdf","comment":"Accepted at MLCN 2024"},{"id":"http://arxiv.org/abs/2406.02529v2","updated":"2024-08-01T20:53:09Z","published":"2024-06-04T17:51:08Z","title":"ReLUs Are Sufficient for Learning Implicit Neural Representations","summary":"  Motivated by the growing theoretical understanding of neural networks that\nemploy the Rectified Linear Unit (ReLU) as their activation function, we\nrevisit the use of ReLU activation functions for learning implicit neural\nrepresentations (INRs). Inspired by second order B-spline wavelets, we\nincorporate a set of simple constraints to the ReLU neurons in each layer of a\ndeep neural network (DNN) to remedy the spectral bias. This in turn enables its\nuse for various INR tasks. Empirically, we demonstrate that, contrary to\npopular belief, one can learn state-of-the-art INRs based on a DNN composed of\nonly ReLU neurons. Next, by leveraging recent theoretical works which\ncharacterize the kinds of functions ReLU neural networks learn, we provide a\nway to quantify the regularity of the learned function. This offers a\nprincipled approach to selecting the hyperparameters in INR architectures. We\nsubstantiate our claims through experiments in signal representation, super\nresolution, and computed tomography, demonstrating the versatility and\neffectiveness of our method. The code for all experiments can be found at\nhttps://github.com/joeshenouda/relu-inrs.\n","authors":["Joseph Shenouda","Yamin Zhou","Robert D. Nowak"],"pdf_url":"https://arxiv.org/pdf/2406.02529v2.pdf","comment":"Accepted to ICML 2024"},{"id":"http://arxiv.org/abs/2407.10886v2","updated":"2024-08-01T20:34:18Z","published":"2024-07-15T16:37:55Z","title":"SLIP: Securing LLMs IP Using Weights Decomposition","summary":"  Large language models (LLMs) have recently seen widespread adoption, in both\nacademia and industry. As these models grow, they become valuable intellectual\nproperty (IP), reflecting enormous investments by their owners. Moreover, the\nhigh cost of cloud-based deployment has driven interest towards deployment to\nedge devices, yet this risks exposing valuable parameters to theft and\nunauthorized use. Current methods to protect models' IP on the edge have\nlimitations in terms of practicality, loss in accuracy, or suitability to\nrequirements. In this paper, we introduce a novel hybrid inference algorithm,\nnamed SLIP, designed to protect edge-deployed models from theft. SLIP is the\nfirst hybrid protocol that is both practical for real-world applications and\nprovably secure, while having zero accuracy degradation and minimal impact on\nlatency. It involves partitioning the model between two computing resources,\none secure but expensive, and another cost-effective but vulnerable. This is\nachieved through matrix decomposition, ensuring that the secure resource\nretains a maximally sensitive portion of the model's IP while performing a\nminimal amount of computations, and vice versa for the vulnerable resource.\nImportantly, the protocol includes security guarantees that prevent attackers\nfrom exploiting the partition to infer the secured information. Finally, we\npresent experimental results that show the robustness and effectiveness of our\nmethod, positioning it as a compelling solution for protecting LLMs.\n","authors":["Yehonathan Refael","Adam Hakim","Lev Greenberg","Tal Aviv","Satya Lokam","Ben Fishman","Shachar Seidman"],"pdf_url":"https://arxiv.org/pdf/2407.10886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00895v1","updated":"2024-08-01T20:21:52Z","published":"2024-08-01T20:21:52Z","title":"Discrete Randomized Smoothing Meets Quantum Computing","summary":"  Breakthroughs in machine learning (ML) and advances in quantum computing (QC)\ndrive the interdisciplinary field of quantum machine learning to new levels.\nHowever, due to the susceptibility of ML models to adversarial attacks,\npractical use raises safety-critical concerns. Existing Randomized Smoothing\n(RS) certification methods for classical machine learning models are\ncomputationally intensive. In this paper, we propose the combination of QC and\nthe concept of discrete randomized smoothing to speed up the stochastic\ncertification of ML models for discrete data. We show how to encode all the\nperturbations of the input binary data in superposition and use Quantum\nAmplitude Estimation (QAE) to obtain a quadratic reduction in the number of\ncalls to the model that are required compared to traditional randomized\nsmoothing techniques. In addition, we propose a new binary threat model to\nallow for an extensive evaluation of our approach on images, graphs, and text.\n","authors":["Tom Wollschlger","Aman Saxena","Nicola Franco","Jeanette Miriam Lorenz","Stephan Gnnemann"],"pdf_url":"https://arxiv.org/pdf/2408.00895v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09436v2","updated":"2024-08-01T20:19:29Z","published":"2023-11-27T21:18:06Z","title":"Temporal Transfer Learning for Traffic Optimization with Coarse-grained\n  Advisory Autonomy","summary":"  The recent development of connected and automated vehicle (CAV) technologies\nhas spurred investigations to optimize dense urban traffic to maximize vehicle\nspeed and throughput. This paper explores advisory autonomy, in which real-time\ndriving advisories are issued to the human drivers, thus achieving near-term\nperformance of automated vehicles. Due to the complexity of traffic systems,\nrecent studies of coordinating CAVs have resorted to leveraging deep\nreinforcement learning (RL). Coarse-grained advisory is formalized as\nzero-order holds, and we consider a range of hold duration from 0.1 to 40\nseconds. However, despite the similarity of the higher frequency tasks on CAVs,\na direct application of deep RL fails to be generalized to advisory autonomy\ntasks. To overcome this, we utilize zero-shot transfer, training policies on a\nset of source tasks--specific traffic scenarios with designated hold\ndurations--and then evaluating the efficacy of these policies on different\ntarget tasks. We introduce Temporal Transfer Learning (TTL) algorithms to\nselect source tasks for zero-shot transfer, systematically leveraging the\ntemporal structure to solve the full range of tasks. TTL selects the most\nsuitable source tasks to maximize the performance of the range of tasks. We\nvalidate our algorithms on diverse mixed-traffic scenarios, demonstrating that\nTTL more reliably solves the tasks than baselines. This paper underscores the\npotential of coarse-grained advisory autonomy with TTL in traffic flow\noptimization.\n","authors":["Jung-Hoon Cho","Sirui Li","Jeongyun Kim","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2312.09436v2.pdf","comment":"18 pages, 12 figures"},{"id":"http://arxiv.org/abs/2407.00741v5","updated":"2024-08-01T20:15:37Z","published":"2024-06-30T16:05:31Z","title":"Diffusion Models for Offline Multi-agent Reinforcement Learning with\n  Safety Constraints","summary":"  In recent advancements in Multi-agent Reinforcement Learning (MARL), its\napplication has extended to various safety-critical scenarios. However, most\nmethods focus on online learning, which presents substantial risks when\ndeployed in real-world settings. Addressing this challenge, we introduce an\ninnovative framework integrating diffusion models within the MARL paradigm.\nThis approach notably enhances the safety of actions taken by multiple agents\nthrough risk mitigation while modeling coordinated action. Our framework is\ngrounded in the Centralized Training with Decentralized Execution (CTDE)\narchitecture, augmented by a Diffusion Model for prediction trajectory\ngeneration. Additionally, we incorporate a specialized algorithm to further\nensure operational safety. We evaluate our model against baselines on the DSRL\nbenchmark. Experiment results demonstrate that our model not only adheres to\nstringent safety constraints but also achieves superior performance compared to\nexisting methodologies. This underscores the potential of our approach in\nadvancing the safety and efficacy of MARL in real-world applications.\n","authors":["Jianuo Huang"],"pdf_url":"https://arxiv.org/pdf/2407.00741v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2101.05436 by other authors"},{"id":"http://arxiv.org/abs/2408.00892v1","updated":"2024-08-01T20:12:49Z","published":"2024-08-01T20:12:49Z","title":"Peptide Sequencing Via Protein Language Models","summary":"  We introduce a protein language model for determining the complete sequence\nof a peptide based on measurement of a limited set of amino acids. To date,\nprotein sequencing relies on mass spectrometry, with some novel edman\ndegregation based platforms able to sequence non-native peptides. Current\nprotein sequencing techniques face limitations in accurately identifying all\namino acids, hindering comprehensive proteome analysis. Our method simulates\npartial sequencing data by selectively masking amino acids that are\nexperimentally difficult to identify in protein sequences from the UniRef\ndatabase. This targeted masking mimics real-world sequencing limitations. We\nthen modify and finetune a ProtBert derived transformer-based model, for a new\ndownstream task predicting these masked residues, providing an approximation of\nthe complete sequence. Evaluating on three bacterial Escherichia species, we\nachieve per-amino-acid accuracy up to 90.5% when only four amino acids ([KCYM])\nare known. Structural assessment using AlphaFold and TM-score validates the\nbiological relevance of our predictions. The model also demonstrates potential\nfor evolutionary analysis through cross-species performance. This integration\nof simulated experimental constraints with computational predictions offers a\npromising avenue for enhancing protein sequence analysis, potentially\naccelerating advancements in proteomics and structural biology by providing a\nprobabilistic reconstruction of the complete protein sequence from limited\nexperimental data.\n","authors":["Thuong Le Hoai Pham","Jillur Rahman Saurav","Aisosa A. Omere","Calvin J. Heyl","Mohammad Sadegh Nasr","Cody Tyler Reynolds","Jai Prakash Yadav Veerla","Helen H Shang","Justyn Jaworski","Alison Ravenscraft","Joseph Anthony Buonomo","Jacob M. Luber"],"pdf_url":"https://arxiv.org/pdf/2408.00892v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.17701v4","updated":"2024-08-01T19:41:07Z","published":"2024-04-26T20:59:23Z","title":"Embedded FPGA Developments in 130nm and 28nm CMOS for Machine Learning\n  in Particle Detector Readout","summary":"  Embedded field programmable gate array (eFPGA) technology allows the\nimplementation of reconfigurable logic within the design of an\napplication-specific integrated circuit (ASIC). This approach offers the low\npower and efficiency of an ASIC along with the ease of FPGA configuration,\nparticularly beneficial for the use case of machine learning in the data\npipeline of next-generation collider experiments. An open-source framework\ncalled \"FABulous\" was used to design eFPGAs using 130 nm and 28 nm CMOS\ntechnology nodes, which were subsequently fabricated and verified through\ntesting. The capability of an eFPGA to act as a front-end readout chip was\nassessed using simulation of high energy particles passing through a silicon\npixel sensor. A machine learning-based classifier, designed for reduction of\nsensor data at the source, was synthesized and configured onto the eFPGA. A\nsuccessful proof-of-concept was demonstrated through reproduction of the\nexpected algorithm result on the eFPGA with perfect accuracy. Further\ndevelopment of the eFPGA technology and its application to collider detector\nreadout is discussed.\n","authors":["Julia Gonski","Aseem Gupta","Haoyi Jia","Hyunjoon Kim","Lorenzo Rota","Larry Ruckman","Angelo Dragone","Ryan Herbst"],"pdf_url":"https://arxiv.org/pdf/2404.17701v4.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.00876v1","updated":"2024-08-01T18:56:08Z","published":"2024-08-01T18:56:08Z","title":"On the Relationship Between Monotone and Squared Probabilistic Circuits","summary":"  Probabilistic circuits are a unifying representation of functions as\ncomputation graphs of weighted sums and products. Their primary application is\nin probabilistic modeling, where circuits with non-negative weights (monotone\ncircuits) can be used to represent and learn density/mass functions, with\ntractable marginal inference. Recently, it was proposed to instead represent\ndensities as the square of the circuit function (squared circuits); this allows\nthe use of negative weights while retaining tractability, and can be\nexponentially more compact than monotone circuits. Unfortunately, we show the\nreverse also holds, meaning that monotone circuits and squared circuits are\nincomparable in general. This raises the question of whether we can reconcile,\nand indeed improve upon the two modeling approaches. We answer in the positive\nby proposing InceptionPCs, a novel type of circuit that naturally encompasses\nboth monotone circuits and squared circuits as special cases, and employs\ncomplex parameters. Empirically, we validate that InceptionPCs can outperform\nboth monotone and squared circuits on image datasets.\n","authors":["Benjie Wang","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2408.00876v1.pdf","comment":"7th Workshop on Tractable Probabilistic Modeling"},{"id":"http://arxiv.org/abs/2408.00872v1","updated":"2024-08-01T18:46:05Z","published":"2024-08-01T18:46:05Z","title":"Online Detection of Anomalies in Temporal Knowledge Graphs with\n  Interpretability","summary":"  Temporal knowledge graphs (TKGs) are valuable resources for capturing\nevolving relationships among entities, yet they are often plagued by noise,\nnecessitating robust anomaly detection mechanisms. Existing dynamic graph\nanomaly detection approaches struggle to capture the rich semantics introduced\nby node and edge categories within TKGs, while TKG embedding methods lack\ninterpretability, undermining the credibility of anomaly detection. Moreover,\nthese methods falter in adapting to pattern changes and semantic drifts\nresulting from knowledge updates. To tackle these challenges, we introduce\nAnoT, an efficient TKG summarization method tailored for interpretable online\nanomaly detection in TKGs. AnoT begins by summarizing a TKG into a novel rule\ngraph, enabling flexible inference of complex patterns in TKGs. When new\nknowledge emerges, AnoT maps it onto a node in the rule graph and traverses the\nrule graph recursively to derive the anomaly score of the knowledge. The\ntraversal yields reachable nodes that furnish interpretable evidence for the\nvalidity or the anomalous of the new knowledge. Overall, AnoT embodies a\ndetector-updater-monitor architecture, encompassing a detector for offline TKG\nsummarization and online scoring, an updater for real-time rule graph updates\nbased on emerging knowledge, and a monitor for estimating the approximation\nerror of the rule graph. Experimental results on four real-world datasets\ndemonstrate that AnoT surpasses existing methods significantly in terms of\naccuracy and interoperability. All of the raw datasets and the implementation\nof AnoT are provided in https://github.com/zjs123/ANoT.\n","authors":["Jiasheng Zhang","Jie Shao","Rex Ying"],"pdf_url":"https://arxiv.org/pdf/2408.00872v1.pdf","comment":"15 pages, 8 figures. Accepted by SIGMOD 2025 Round 2"},{"id":"http://arxiv.org/abs/2408.00863v1","updated":"2024-08-01T18:31:31Z","published":"2024-08-01T18:31:31Z","title":"UniMoT: Unified Molecule-Text Language Model with Discrete Token\n  Representation","summary":"  The remarkable success of Large Language Models (LLMs) across diverse tasks\nhas driven the research community to extend their capabilities to molecular\napplications. However, most molecular LLMs employ adapter-based architectures\nthat do not treat molecule and text modalities equally and lack a supervision\nsignal for the molecule modality. To address these issues, we introduce UniMoT,\na Unified Molecule-Text LLM adopting a tokenizer-based architecture that\nexpands the vocabulary of LLM with molecule tokens. Specifically, we introduce\na Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge\nthe modality gap between molecule and text. This tokenizer transforms molecules\ninto sequences of molecule tokens with causal dependency, encapsulating\nhigh-level molecular and textual information. Equipped with this tokenizer,\nUniMoT can unify molecule and text modalities under a shared token\nrepresentation and an autoregressive training paradigm, enabling it to\ninterpret molecules as a foreign language and generate them as text. Following\na four-stage training scheme, UniMoT emerges as a multi-modal generalist\ncapable of performing both molecule-to-text and text-to-molecule tasks.\nExtensive experiments demonstrate that UniMoT achieves state-of-the-art\nperformance across a wide range of molecule comprehension and generation tasks.\n","authors":["Juzheng Zhang","Yatao Bian","Yongqiang Chen","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2408.00863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00856v1","updated":"2024-08-01T18:10:05Z","published":"2024-08-01T18:10:05Z","title":"Deep Learning Approach for Changepoint Detection: Penalty Parameter\n  Optimization","summary":"  Changepoint detection, a technique for identifying significant shifts within\ndata sequences, is crucial in various fields such as finance, genomics,\nmedicine, etc. Dynamic programming changepoint detection algorithms are\nemployed to identify the locations of changepoints within a sequence, which\nrely on a penalty parameter to regulate the number of changepoints. To estimate\nthis penalty parameter, previous work uses simple models such as linear models\nor decision trees. This study introduces a novel deep learning method for\npredicting penalty parameters, leading to demonstrably improved changepoint\ndetection accuracy on large benchmark supervised labeled datasets compared to\nprevious methods.\n","authors":["Tung L Nguyen","Toby Dylan Hocking"],"pdf_url":"https://arxiv.org/pdf/2408.00856v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00845v1","updated":"2024-08-01T18:01:14Z","published":"2024-08-01T18:01:14Z","title":"A Novel Use of Pseudospectra in Mathematical Biology: Understanding HPA\n  Axis Sensitivity","summary":"  The Hypothalamic-Pituitary-Adrenal (HPA) axis is a major neuroendocrine\nsystem, and its dysregulation is implicated in various diseases. This system\nalso presents interesting mathematical challenges for modeling. We consider a\nnonlinear delay differential equation model and calculate pseudospectra of\nthree different linearizations: a time-dependent Jacobian, linearization around\nthe limit cycle, and dynamic mode decomposition (DMD) analysis of Koopman\noperators (global linearization). The time-dependent Jacobian provided insight\ninto experimental phenomena, explaining why rats respond differently to\nperturbations during corticosterone secretion's upward versus downward slopes.\nWe developed new mathematical techniques for the other two linearizations to\ncalculate pseudospectra on Banach spaces and apply DMD to delay differential\nequations, respectively. These methods helped establish local and global limit\ncycle stability and study transients. Additionally, we discuss using\npseudospectra to substantiate the model in experimental contexts and establish\nbio-variability via data-driven methods. This work is the first to utilize\npseudospectra to explore the HPA axis.\n","authors":["Catherine Drysdale","Matthew J. Colbrook"],"pdf_url":"https://arxiv.org/pdf/2408.00845v1.pdf","comment":"15 pages, keywords: HPA axis, pseudospectra, nonlinear delay\n  differential equations, dynamic mode decomposition (DMD)"},{"id":"http://arxiv.org/abs/2408.00838v1","updated":"2024-08-01T18:00:05Z","published":"2024-08-01T18:00:05Z","title":"Calibrating Bayesian Generative Machine Learning for Bayesiamplification","summary":"  Recently, combinations of generative and Bayesian machine learning have been\nintroduced in particle physics for both fast detector simulation and inference\ntasks. These neural networks aim to quantify the uncertainty on the generated\ndistribution originating from limited training statistics. The interpretation\nof a distribution-wide uncertainty however remains ill-defined. We show a clear\nscheme for quantifying the calibration of Bayesian generative machine learning\nmodels. For a Continuous Normalizing Flow applied to a low-dimensional toy\nexample, we evaluate the calibration of Bayesian uncertainties from either a\nmean-field Gaussian weight posterior, or Monte Carlo sampling network weights,\nto gauge their behaviour on unsteady distribution edges. Well calibrated\nuncertainties can then be used to roughly estimate the number of uncorrelated\ntruth samples that are equivalent to the generated sample and clearly indicate\ndata amplification for smooth features of the distribution.\n","authors":["Sebastian Bieringer","Sascha Diefenbacher","Gregor Kasieczka","Mathias Trabs"],"pdf_url":"https://arxiv.org/pdf/2408.00838v1.pdf","comment":"15 pages, 6 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.14364v2","updated":"2024-08-01T11:16:30Z","published":"2024-07-19T14:52:11Z","title":"Towards Assessing Data Replication in Music Generation with Music\n  Similarity Metrics on Raw Audio","summary":"  Recent advancements in music generation are raising multiple concerns about\nthe implications of AI in creative music processes, current business models and\nimpacts related to intellectual property management. A relevant discussion and\nrelated technical challenge is the potential replication and plagiarism of the\ntraining set in AI-generated music, which could lead to misuse of data and\nintellectual property rights violations. To tackle this issue, we present the\nMusic Replication Assessment (MiRA) tool: a model-independent open evaluation\nmethod based on diverse audio music similarity metrics to assess data\nreplication. We evaluate the ability of five metrics to identify exact\nreplication by conducting a controlled replication experiment in different\nmusic genres using synthetic samples. Our results show that the proposed\nmethodology can estimate exact data replication with a proportion higher than\n10%. By introducing the MiRA tool, we intend to encourage the open evaluation\nof music-generative models by researchers, developers, and users concerning\ndata replication, highlighting the importance of the ethical, social, legal,\nand economic consequences. Code and examples are available for reproducibility\npurposes.\n","authors":["Roser Batlle-Roca","Wei-Hisang Liao","Xavier Serra","Yuki Mitsufuji","Emilia Gmez"],"pdf_url":"https://arxiv.org/pdf/2407.14364v2.pdf","comment":"Accepted at ISMIR 2024"},{"id":"http://arxiv.org/abs/2407.03104v2","updated":"2024-08-01T08:08:43Z","published":"2024-07-03T13:41:44Z","title":"KeyVideoLLM: Towards Large-scale Video Keyframe Selection","summary":"  Recently, with the rise of web videos, managing and understanding large-scale\nvideo datasets has become increasingly important. Video Large Language Models\n(VideoLLMs) have emerged in recent years due to their strong video\nunderstanding capabilities. However, training and inference processes for\nVideoLLMs demand vast amounts of data, presenting significant challenges to\ndata management, particularly regarding efficiency, robustness, and\neffectiveness. In this work, we present KeyVideoLLM, a text-video frame\nsimilarity-based keyframe selection method designed to manage VideoLLM data\nefficiently, robustly, and effectively. Specifically, KeyVideoLLM achieves a\nremarkable data compression rate of up to 60.9 times, substantially lowering\ndisk space requirements, which proves its high efficiency. Additionally, it\nmaintains a 100% selection success rate across all video formats and scales,\nenhances processing speed by up to 200 times compared to existing keyframe\nselection methods, and does not require hyperparameter tuning. Beyond its\noutstanding efficiency and robustness, KeyVideoLLM further improves model\nperformance in video question-answering tasks during both training and\ninference stages. Notably, it consistently achieved the state-of-the-art (SoTA)\nexperimental results on diverse datasets.\n","authors":["Hao Liang","Jiapeng Li","Tianyi Bai","Xijie Huang","Linzhuang Sun","Zhengren Wang","Conghui He","Bin Cui","Chong Chen","Wentao Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.03104v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00674v1","updated":"2024-08-01T16:16:29Z","published":"2024-08-01T16:16:29Z","title":"ChordSync: Conformer-Based Alignment of Chord Annotations to Music Audio","summary":"  In the Western music tradition, chords are the main constituent components of\nharmony, a fundamental dimension of music. Despite its relevance for several\nMusic Information Retrieval (MIR) tasks, chord-annotated audio datasets are\nlimited and need more diversity. One way to improve those resources is to\nleverage the large number of chord annotations available online, but this\nrequires aligning them with music audio. However, existing audio-to-score\nalignment techniques, which typically rely on Dynamic Time Warping (DTW), fail\nto address this challenge, as they require weakly aligned data for precise\nsynchronisation. In this paper, we introduce ChordSync, a novel conformer-based\nmodel designed to seamlessly align chord annotations with audio, eliminating\nthe need for weak alignment. We also provide a pre-trained model and a\nuser-friendly library, enabling users to synchronise chord annotations with\naudio tracks effortlessly. In this way, ChordSync creates opportunities for\nharnessing crowd-sourced chord data for MIR, especially in audio chord\nestimation, thereby facilitating the generation of novel datasets.\nAdditionally, our system extends its utility to music education, enhancing\nmusic learning experiences by providing accurately aligned annotations, thus\nenabling learners to engage in synchronised musical practices.\n","authors":["Andrea Poltronieri","Valentina Presutti","Martn Rocamora"],"pdf_url":"https://arxiv.org/pdf/2408.00674v1.pdf","comment":"8 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.00599v1","updated":"2024-08-01T14:31:06Z","published":"2024-08-01T14:31:06Z","title":"Learned Compression of Point Cloud Geometry and Attributes in a Single\n  Model through Multimodal Rate-Control","summary":"  Point cloud compression is essential to experience volumetric multimedia as\nit drastically reduces the required streaming data rates. Point attributes,\nspecifically colors, extend the challenge of lossy compression beyond geometric\nrepresentation to achieving joint reconstruction of texture and geometry.\nState-of-the-art methods separate geometry and attributes to compress them\nindividually. This comes at a computational cost, requiring an encoder and a\ndecoder for each modality. Additionally, as attribute compression methods\nrequire the same geometry for encoding and decoding, the encoder emulates the\ndecoder-side geometry reconstruction as an input step to project and compress\nthe attributes. In this work, we propose to learn joint compression of geometry\nand attributes using a single, adaptive autoencoder model, embedding both\nmodalities into a unified latent space which is then entropy encoded. Key to\nthe technique is to replace the search for trade-offs between rate, attribute\nquality and geometry quality, through conditioning the model on the desired\nqualities of both modalities, bypassing the need for training model ensembles.\nTo differentiate important point cloud regions during encoding or to allow\nview-dependent compression for user-centered streaming, conditioning is\npointwise, which allows for local quality and rate variation. Our evaluation\nshows comparable performance to state-of-the-art compression methods for\ngeometry and attributes, while reducing complexity compared to related\ncompression methods.\n","authors":["Michael Rudolph","Aron Riemenschneider","Amr Rizk"],"pdf_url":"https://arxiv.org/pdf/2408.00599v1.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2408.00491v1","updated":"2024-08-01T11:52:56Z","published":"2024-08-01T11:52:56Z","title":"GalleryGPT: Analyzing Paintings with Large Multimodal Models","summary":"  Artwork analysis is important and fundamental skill for art appreciation,\nwhich could enrich personal aesthetic sensibility and facilitate the critical\nthinking ability. Understanding artworks is challenging due to its subjective\nnature, diverse interpretations, and complex visual elements, requiring\nexpertise in art history, cultural background, and aesthetic theory. However,\nlimited by the data collection and model ability, previous works for\nautomatically analyzing artworks mainly focus on classification, retrieval, and\nother simple tasks, which is far from the goal of AI. To facilitate the\nresearch progress, in this paper, we step further to compose comprehensive\nanalysis inspired by the remarkable perception and generation ability of large\nmultimodal models. Specifically, we first propose a task of composing paragraph\nanalysis for artworks, i.e., painting in this paper, only focusing on visual\ncharacteristics to formulate more comprehensive understanding of artworks. To\nsupport the research on formal analysis, we collect a large dataset\nPaintingForm, with about 19k painting images and 50k analysis paragraphs. We\nfurther introduce a superior large multimodal model for painting analysis\ncomposing, dubbed GalleryGPT, which is slightly modified and fine-tuned based\non LLaVA architecture leveraging our collected data. We conduct formal analysis\ngeneration and zero-shot experiments across several datasets to assess the\ncapacity of our model. The results show remarkable performance improvements\ncomparing with powerful baseline LMMs, demonstrating its superb ability of art\nanalysis and generalization. \\textcolor{blue}{The codes and model are available\nat: https://github.com/steven640pixel/GalleryGPT.\n","authors":["Yi Bin","Wenhao Shi","Yujuan Ding","Zhiqiang Hu","Zheng Wang","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00491v1.pdf","comment":"Accepted as Oral Presentation at ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.00483v1","updated":"2024-08-01T11:39:45Z","published":"2024-08-01T11:39:45Z","title":"A Systematic Review on Long-Tailed Learning","summary":"  Long-tailed data is a special type of multi-class imbalanced data with a very\nlarge amount of minority/tail classes that have a very significant combined\ninfluence. Long-tailed learning aims to build high-performance models on\ndatasets with long-tailed distributions, which can identify all the classes\nwith high accuracy, in particular the minority/tail classes. It is a\ncutting-edge research direction that has attracted a remarkable amount of\nresearch effort in the past few years. In this paper, we present a\ncomprehensive survey of latest advances in long-tailed visual learning. We\nfirst propose a new taxonomy for long-tailed learning, which consists of eight\ndifferent dimensions, including data balancing, neural architecture, feature\nenrichment, logits adjustment, loss function, bells and whistles, network\noptimization, and post hoc processing techniques. Based on our proposed\ntaxonomy, we present a systematic review of long-tailed learning methods,\ndiscussing their commonalities and alignable differences. We also analyze the\ndifferences between imbalance learning and long-tailed learning approaches.\nFinally, we discuss prospects and future directions in this field.\n","authors":["Chongsheng Zhang","George Almpanidis","Gaojuan Fan","Binquan Deng","Yanbo Zhang","Ji Liu","Aouaidjia Kamel","Paolo Soda","Joo Gama"],"pdf_url":"https://arxiv.org/pdf/2408.00483v1.pdf","comment":"Current Under Revision at IEEE TNNLS. [This is the long/Full-length\n  version of our Long-Tailed Learning Survey paper]"},{"id":"http://arxiv.org/abs/2408.00305v1","updated":"2024-08-01T06:04:44Z","published":"2024-08-01T06:04:44Z","title":"Leveraging Weak Cross-Modal Guidance for Coherence Modelling via\n  Iterative Learning","summary":"  Cross-modal coherence modeling is essential for intelligent systems to help\nthem organize and structure information, thereby understanding and creating\ncontent of the physical world coherently like human-beings. Previous work on\ncross-modal coherence modeling attempted to leverage the order information from\nanother modality to assist the coherence recovering of the target modality.\nDespite of the effectiveness, labeled associated coherency information is not\nalways available and might be costly to acquire, making the cross-modal\nguidance hard to leverage. To tackle this challenge, this paper explores a new\nway to take advantage of cross-modal guidance without gold labels on coherency,\nand proposes the Weak Cross-Modal Guided Ordering (WeGO) model. More\nspecifically, it leverages high-confidence predicted pairwise order in one\nmodality as reference information to guide the coherence modeling in another.\nAn iterative learning paradigm is further designed to jointly optimize the\ncoherence modeling in two modalities with selected guidance from each other.\nThe iterative cross-modal boosting also functions in inference to further\nenhance coherence prediction in each modality. Experimental results on two\npublic datasets have demonstrated that the proposed method outperforms existing\nmethods for cross-modal coherence modeling tasks. Major technical modules have\nbeen evaluated effective through ablation studies. Codes are available at:\n\\url{https://github.com/scvready123/IterWeGO}.\n","authors":["Yi Bin","Junrong Liao","Yujuan Ding","Haoxuan Li","Yang Yang","See-Kiong Ng","Heng Tao Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00305v1.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.00300v1","updated":"2024-08-01T05:56:34Z","published":"2024-08-01T05:56:34Z","title":"Towards Flexible Evaluation for Generative Visual Question Answering","summary":"  Throughout rapid development of multimodal large language models, a crucial\ningredient is a fair and accurate evaluation of their multimodal comprehension\nabilities. Although Visual Question Answering (VQA) could serve as a developed\ntest field, limitations of VQA evaluation, like the inflexible pattern of Exact\nMatch, have hindered MLLMs from demonstrating their real capability and\ndiscourage rich responses. Therefore, this paper proposes the use of\nsemantics-based evaluators for assessing unconstrained open-ended responses on\nVQA datasets. As characteristics of VQA have made such evaluation significantly\ndifferent than the traditional Semantic Textual Similarity (STS) task, to\nsystematically analyze the behaviour and compare the performance of various\nevaluators including LLM-based ones, we proposes three key properties, i.e.,\nAlignment, Consistency and Generalization, and a corresponding dataset\nAssessing VQA Evaluators (AVE) to facilitate analysis. In addition, this paper\nproposes a Semantically Flexible VQA Evaluator (SFVE) with meticulous design\nbased on the unique features of VQA evaluation. Experimental results verify the\nfeasibility of model-based VQA evaluation and effectiveness of the proposed\nevaluator that surpasses existing semantic evaluators by a large margin. The\nproposed training scheme generalizes to both the BERT-like encoders and\ndecoder-only LLM.\n","authors":["Huishan Ji","Qingyi Si","Zheng Lin","Weiping Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00300v1.pdf","comment":null}]},"2024-08-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.01423v1","updated":"2024-08-02T17:59:42Z","published":"2024-08-02T17:59:42Z","title":"Prompt Recursive Search: A Living Framework with Adaptive Growth in LLM\n  Auto-Prompting","summary":"  Large Language Models (LLMs) exhibit remarkable proficiency in addressing a\ndiverse array of tasks within the Natural Language Processing (NLP) domain,\nwith various prompt design strategies significantly augmenting their\ncapabilities. However, these prompts, while beneficial, each possess inherent\nlimitations. The primary prompt design methodologies are twofold: The first,\nexemplified by the Chain of Thought (CoT), involves manually crafting prompts\nspecific to individual datasets, hence termed Expert-Designed Prompts (EDPs).\nOnce these prompts are established, they are unalterable, and their\neffectiveness is capped by the expertise of the human designers. When applied\nto LLMs, the static nature of EDPs results in a uniform approach to both simple\nand complex problems within the same dataset, leading to the inefficient use of\ntokens for straightforward issues. The second method involves prompts\nautonomously generated by the LLM, known as LLM-Derived Prompts (LDPs), which\nprovide tailored solutions to specific problems, mitigating the limitations of\nEDPs. However, LDPs may encounter a decline in performance when tackling\ncomplex problems due to the potential for error accumulation during the\nsolution planning process. To address these challenges, we have conceived a\nnovel Prompt Recursive Search (PRS) framework that leverages the LLM to\ngenerate solutions specific to the problem, thereby conserving tokens. The\nframework incorporates an assessment of problem complexity and an adjustable\nstructure, ensuring a reduction in the likelihood of errors. We have\nsubstantiated the efficacy of PRS framework through extensive experiments using\nLLMs with different numbers of parameters across a spectrum of datasets in\nvarious domains. Compared to the CoT method, the PRS method has increased the\naccuracy on the BBH dataset by 8% using Llama3-7B model, achieving a 22%\nimprovement.\n","authors":["Xiangyu Zhao","Chengqian Ma"],"pdf_url":"https://arxiv.org/pdf/2408.01423v1.pdf","comment":"8 pages,4 figures"},{"id":"http://arxiv.org/abs/2402.16827v3","updated":"2024-08-02T17:59:31Z","published":"2024-02-26T18:54:35Z","title":"A Survey on Data Selection for Language Models","summary":"  A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.\n","authors":["Alon Albalak","Yanai Elazar","Sang Michael Xie","Shayne Longpre","Nathan Lambert","Xinyi Wang","Niklas Muennighoff","Bairu Hou","Liangming Pan","Haewon Jeong","Colin Raffel","Shiyu Chang","Tatsunori Hashimoto","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16827v3.pdf","comment":"Paper list available at\n  https://github.com/alon-albalak/data-selection-survey"},{"id":"http://arxiv.org/abs/2408.01420v1","updated":"2024-08-02T17:55:50Z","published":"2024-08-02T17:55:50Z","title":"Mission Impossible: A Statistical Perspective on Jailbreaking LLMs","summary":"  Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.\n","authors":["Jingtong Su","Julia Kempe","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2408.01420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01419v1","updated":"2024-08-02T17:54:34Z","published":"2024-08-02T17:54:34Z","title":"DebateQA: Evaluating Question Answering on Debatable Knowledge","summary":"  The rise of large language models (LLMs) has enabled us to seek answers to\ninherently debatable questions on LLM chatbots, necessitating a reliable way to\nevaluate their ability. However, traditional QA benchmarks assume fixed answers\nare inadequate for this purpose. To address this, we introduce DebateQA, a\ndataset of 2,941 debatable questions, each accompanied by multiple\nhuman-annotated partial answers that capture a variety of perspectives. We\ndevelop two metrics: Perspective Diversity, which evaluates the\ncomprehensiveness of perspectives, and Dispute Awareness, which assesses if the\nLLM acknowledges the question's debatable nature. Experiments demonstrate that\nboth metrics align with human preferences and are stable across different\nunderlying models. Using DebateQA with two metrics, we assess 12 popular LLMs\nand retrieval-augmented generation methods. Our findings reveal that while LLMs\ngenerally excel at recognizing debatable issues, their ability to provide\ncomprehensive answers encompassing diverse perspectives varies considerably.\n","authors":["Rongwu Xu","Xuan Qi","Zehan Qi","Wei Xu","Zhijiang Guo"],"pdf_url":"https://arxiv.org/pdf/2408.01419v1.pdf","comment":"Dataset and scripts for evaluation are available at\n  https://github.com/pillowsofwind/DebateQA"},{"id":"http://arxiv.org/abs/2408.00118v2","updated":"2024-08-02T17:52:12Z","published":"2024-07-31T19:13:07Z","title":"Gemma 2: Improving Open Language Models at a Practical Size","summary":"  In this work, we introduce Gemma 2, a new addition to the Gemma family of\nlightweight, state-of-the-art open models, ranging in scale from 2 billion to\n27 billion parameters. In this new version, we apply several known technical\nmodifications to the Transformer architecture, such as interleaving\nlocal-global attentions (Beltagy et al., 2020a) and group-query attention\n(Ainslie et al., 2023). We also train the 2B and 9B models with knowledge\ndistillation (Hinton et al., 2015) instead of next token prediction. The\nresulting models deliver the best performance for their size, and even offer\ncompetitive alternatives to models that are 2-3 times bigger. We release all\nour models to the community.\n","authors":[" Gemma Team","Morgane Riviere","Shreya Pathak","Pier Giuseppe Sessa","Cassidy Hardin","Surya Bhupatiraju","Lonard Hussenot","Thomas Mesnard","Bobak Shahriari","Alexandre Ram","Johan Ferret","Peter Liu","Pouya Tafti","Abe Friesen","Michelle Casbon","Sabela Ramos","Ravin Kumar","Charline Le Lan","Sammy Jerome","Anton Tsitsulin","Nino Vieillard","Piotr Stanczyk","Sertan Girgin","Nikola Momchev","Matt Hoffman","Shantanu Thakoor","Jean-Bastien Grill","Behnam Neyshabur","Olivier Bachem","Alanna Walton","Aliaksei Severyn","Alicia Parrish","Aliya Ahmad","Allen Hutchison","Alvin Abdagic","Amanda Carl","Amy Shen","Andy Brock","Andy Coenen","Anthony Laforge","Antonia Paterson","Ben Bastian","Bilal Piot","Bo Wu","Brandon Royal","Charlie Chen","Chintu Kumar","Chris Perry","Chris Welty","Christopher A. Choquette-Choo","Danila Sinopalnikov","David Weinberger","Dimple Vijaykumar","Dominika Rogoziska","Dustin Herbison","Elisa Bandy","Emma Wang","Eric Noland","Erica Moreira","Evan Senter","Evgenii Eltyshev","Francesco Visin","Gabriel Rasskin","Gary Wei","Glenn Cameron","Gus Martins","Hadi Hashemi","Hanna Klimczak-Pluciska","Harleen Batra","Harsh Dhand","Ivan Nardini","Jacinda Mein","Jack Zhou","James Svensson","Jeff Stanway","Jetha Chan","Jin Peng Zhou","Joana Carrasqueira","Joana Iljazi","Jocelyn Becker","Joe Fernandez","Joost van Amersfoort","Josh Gordon","Josh Lipschultz","Josh Newlan","Ju-yeong Ji","Kareem Mohamed","Kartikeya Badola","Kat Black","Katie Millican","Keelin McDonell","Kelvin Nguyen","Kiranbir Sodhia","Kish Greene","Lars Lowe Sjoesund","Lauren Usui","Laurent Sifre","Lena Heuermann","Leticia Lago","Lilly McNealus","Livio Baldini Soares","Logan Kilpatrick","Lucas Dixon","Luciano Martins","Machel Reid","Manvinder Singh","Mark Iverson","Martin Grner","Mat Velloso","Mateo Wirth","Matt Davidow","Matt Miller","Matthew Rahtz","Matthew Watson","Meg Risdal","Mehran Kazemi","Michael Moynihan","Ming Zhang","Minsuk Kahng","Minwoo Park","Mofi Rahman","Mohit Khatwani","Natalie Dao","Nenshad Bardoliwalla","Nesh Devanathan","Neta Dumai","Nilay Chauhan","Oscar Wahltinez","Pankil Botarda","Parker Barnes","Paul Barham","Paul Michel","Pengchong Jin","Petko Georgiev","Phil Culliton","Pradeep Kuppala","Ramona Comanescu","Ramona Merhej","Reena Jana","Reza Ardeshir Rokni","Rishabh Agarwal","Ryan Mullins","Samaneh Saadat","Sara Mc Carthy","Sarah Perrin","Sbastien M. R. Arnold","Sebastian Krause","Shengyang Dai","Shruti Garg","Shruti Sheth","Sue Ronstrom","Susan Chan","Timothy Jordan","Ting Yu","Tom Eccles","Tom Hennigan","Tomas Kocisky","Tulsee Doshi","Vihan Jain","Vikas Yadav","Vilobh Meshram","Vishal Dharmadhikari","Warren Barkley","Wei Wei","Wenming Ye","Woohyun Han","Woosuk Kwon","Xiang Xu","Zhe Shen","Zhitao Gong","Zichuan Wei","Victor Cotruta","Phoebe Kirk","Anand Rao","Minh Giang","Ludovic Peran","Tris Warkentin","Eli Collins","Joelle Barral","Zoubin Ghahramani","Raia Hadsell","D. Sculley","Jeanine Banks","Anca Dragan","Slav Petrov","Oriol Vinyals","Jeff Dean","Demis Hassabis","Koray Kavukcuoglu","Clement Farabet","Elena Buchatskaya","Sebastian Borgeaud","Noah Fiedel","Armand Joulin","Kathleen Kenealy","Robert Dadashi","Alek Andreev"],"pdf_url":"https://arxiv.org/pdf/2408.00118v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2404.17513v2","updated":"2024-08-02T17:39:32Z","published":"2024-04-26T16:28:34Z","title":"A Comprehensive Evaluation on Event Reasoning of Large Language Models","summary":"  Event reasoning is a fundamental ability that underlies many applications. It\nrequires event schema knowledge to perform global reasoning and needs to deal\nwith the diversity of the inter-event relations and the reasoning paradigms.\nHow well LLMs accomplish event reasoning on various relations and reasoning\nparadigms remains unknown. To mitigate this disparity, we comprehensively\nevaluate the abilities of event reasoning of LLMs. We introduce a novel\nbenchmark EV2 for EValuation of EVent reasoning. EV2 consists of two levels of\nevaluation of schema and instance and is comprehensive in relations and\nreasoning paradigms. We conduct extensive experiments on EV2. We find that LLMs\nhave abilities to accomplish event reasoning but their performances are far\nfrom satisfactory. We also notice the imbalance of event reasoning abilities in\nLLMs. Besides, LLMs have event schema knowledge, however, they're not aligned\nwith humans on how to utilize the knowledge. Based on these findings, we guide\nthe LLMs in utilizing the event schema knowledge as memory leading to\nimprovements on event reasoning.\n","authors":["Zhengwei Tao","Zhi Jin","Yifan Zhang","Xiancai Chen","Haiyan Zhao","Jia Li","Bing Liang","Chongyang Tao","Qun Liu","Kam-Fai Wong"],"pdf_url":"https://arxiv.org/pdf/2404.17513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01402v1","updated":"2024-08-02T17:25:34Z","published":"2024-08-02T17:25:34Z","title":"Pre-trained Language Models Improve the Few-shot Prompt Ability of\n  Decision Transformer","summary":"  Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.\n","authors":["Yu Yang","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01402v1.pdf","comment":"2 figures, 8 tables. Accepted by the Training Agents with Foundation\n  Models Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2408.01394v1","updated":"2024-08-02T17:10:12Z","published":"2024-08-02T17:10:12Z","title":"Improving Multilingual Neural Machine Translation by Utilizing Semantic\n  and Linguistic Features","summary":"  The many-to-many multilingual neural machine translation can be regarded as\nthe process of integrating semantic features from the source sentences and\nlinguistic features from the target sentences. To enhance zero-shot\ntranslation, models need to share knowledge across languages, which can be\nachieved through auxiliary tasks for learning a universal representation or\ncross-lingual mapping. To this end, we propose to exploit both semantic and\nlinguistic features between multiple languages to enhance multilingual\ntranslation. On the encoder side, we introduce a disentangling learning task\nthat aligns encoder representations by disentangling semantic and linguistic\nfeatures, thus facilitating knowledge transfer while preserving complete\ninformation. On the decoder side, we leverage a linguistic encoder to integrate\nlow-level linguistic features to assist in the target language generation.\nExperimental results on multilingual datasets demonstrate significant\nimprovement in zero-shot translation compared to the baseline system, while\nmaintaining performance in supervised translation. Further analysis validates\nthe effectiveness of our method in leveraging both semantic and linguistic\nfeatures. The code is available at https://github.com/ictnlp/SemLing-MNMT.\n","authors":["Mengyu Bu","Shuhao Gu","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2408.01394v1.pdf","comment":"Accepted by ACL2024 Findings"},{"id":"http://arxiv.org/abs/2408.01380v1","updated":"2024-08-02T16:37:44Z","published":"2024-08-02T16:37:44Z","title":"Coalitions of Large Language Models Increase the Robustness of AI Agents","summary":"  The emergence of Large Language Models (LLMs) have fundamentally altered the\nway we interact with digital systems and have led to the pursuit of LLM powered\nAI agents to assist in daily workflows. LLMs, whilst powerful and capable of\ndemonstrating some emergent properties, are not logical reasoners and often\nstruggle to perform well at all sub-tasks carried out by an AI agent to plan\nand execute a workflow. While existing studies tackle this lack of proficiency\nby generalised pretraining at a huge scale or by specialised fine-tuning for\ntool use, we assess if a system comprising of a coalition of pretrained LLMs,\neach exhibiting specialised performance at individual sub-tasks, can match the\nperformance of single model agents. The coalition of models approach showcases\nits potential for building robustness and reducing the operational costs of\nthese AI agents by leveraging traits exhibited by specific models. Our findings\ndemonstrate that fine-tuning can be mitigated by considering a coalition of\npretrained models and believe that this approach can be applied to other\nnon-agentic systems which utilise LLMs.\n","authors":["Prattyush Mangal","Carol Mak","Theo Kanakis","Timothy Donovan","Dave Braines","Edward Pyzer-Knapp"],"pdf_url":"https://arxiv.org/pdf/2408.01380v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01367v1","updated":"2024-08-02T16:21:48Z","published":"2024-08-02T16:21:48Z","title":"Transformers are Universal In-context Learners","summary":"  Transformers are deep architectures that define \"in-context mappings\" which\nenable predicting new tokens based on a given set of tokens (such as a prompt\nin NLP applications or a set of patches for vision transformers). This work\nstudies in particular the ability of these architectures to handle an\narbitrarily large number of context tokens. To mathematically and uniformly\naddress the expressivity of these architectures, we consider the case that the\nmappings are conditioned on a context represented by a probability distribution\nof tokens (discrete for a finite number of tokens). The related notion of\nsmoothness corresponds to continuity in terms of the Wasserstein distance\nbetween these contexts. We demonstrate that deep transformers are universal and\ncan approximate continuous in-context mappings to arbitrary precision,\nuniformly over compact token domains. A key aspect of our results, compared to\nexisting findings, is that for a fixed precision, a single transformer can\noperate on an arbitrary (even infinite) number of tokens. Additionally, it\noperates with a fixed embedding dimension of tokens (this dimension does not\nincrease with precision) and a fixed number of heads (proportional to the\ndimension). The use of MLP layers between multi-head attention layers is also\nexplicitly controlled.\n","authors":["Takashi Furuya","Maarten V. de Hoop","Gabriel Peyr"],"pdf_url":"https://arxiv.org/pdf/2408.01367v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.01363v1","updated":"2024-08-02T16:15:25Z","published":"2024-08-02T16:15:25Z","title":"Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation","summary":"  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n","authors":["Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01363v1.pdf","comment":"Accepted by ACM SIGIR 2024 LLM4Eval Workshop:\n  https://llm4eval.github.io/papers"},{"id":"http://arxiv.org/abs/2302.05737v3","updated":"2024-08-02T16:09:14Z","published":"2023-02-11T16:26:57Z","title":"A Reparameterized Discrete Diffusion Model for Text Generation","summary":"  This work studies discrete diffusion probabilistic models with applications\nto natural language generation. We derive an alternative yet equivalent\nformulation of the sampling from discrete diffusion processes and leverage this\ninsight to develop a family of reparameterized discrete diffusion models. The\nderived generic framework is highly flexible, offers a fresh perspective of the\ngeneration process in discrete diffusion models, and features more effective\ntraining and decoding techniques. We conduct extensive experiments to evaluate\nthe text generation capability of our model, demonstrating significant\nimprovements over existing diffusion models.\n","authors":["Lin Zheng","Jianbo Yuan","Lei Yu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2302.05737v3.pdf","comment":"COLM 2024; Code available at\n  https://github.com/hkunlp/reparam-discrete-diffusion"},{"id":"http://arxiv.org/abs/2408.01346v1","updated":"2024-08-02T15:46:36Z","published":"2024-08-02T15:46:36Z","title":"Prompt Refinement or Fine-tuning? Best Practices for using LLMs in\n  Computational Social Science Tasks","summary":"  Large Language Models are expressive tools that enable complex tasks of text\nunderstanding within Computational Social Science. Their versatility, while\nbeneficial, poses a barrier for establishing standardized best practices within\nthe field. To bring clarity on the values of different strategies, we present\nan overview of the performance of modern LLM-based classification methods on a\nbenchmark of 23 social knowledge tasks. Our results point to three best\npractices: select models with larger vocabulary and pre-training corpora; avoid\nsimple zero-shot in favor of AI-enhanced prompting; fine-tune on task-specific\ndata, and consider more complex forms instruction-tuning on multiple datasets\nonly when only training data is more abundant.\n","authors":["Anders Giovanni Mller","Luca Maria Aiello"],"pdf_url":"https://arxiv.org/pdf/2408.01346v1.pdf","comment":"5 pages, 1 table"},{"id":"http://arxiv.org/abs/2408.01337v1","updated":"2024-08-02T15:34:05Z","published":"2024-08-02T15:34:05Z","title":"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language\n  Models","summary":"  Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.\n","authors":["Benno Weck","Ilaria Manco","Emmanouil Benetos","Elio Quinton","George Fazekas","Dmitry Bogdanov"],"pdf_url":"https://arxiv.org/pdf/2408.01337v1.pdf","comment":"Accepted at ISMIR 2024. Data: https://doi.org/10.5281/zenodo.12709974\n  Code: https://github.com/mulab-mir/muchomusic Supplementary material:\n  https://mulab-mir.github.io/muchomusic"},{"id":"http://arxiv.org/abs/2408.01323v1","updated":"2024-08-02T15:21:20Z","published":"2024-08-02T15:21:20Z","title":"FANNO: Augmenting High-Quality Instruction Data with Open-Sourced LLMs\n  Only","summary":"  Instruction fine-tuning stands as a crucial advancement in leveraging large\nlanguage models (LLMs) for enhanced task performance. However, the annotation\nof instruction datasets has traditionally been expensive and laborious, often\nrelying on manual annotations or costly API calls of proprietary LLMs. To\naddress these challenges, we introduce FANNO, a fully autonomous, open-sourced\nframework that revolutionizes the annotation process without the need for\npre-existing annotated data. Utilizing a Mistral-7b-instruct model, FANNO\nefficiently produces diverse and high-quality datasets through a structured\nprocess involving document pre-screening, instruction generation, and response\ngeneration. Experiments on Open LLM Leaderboard and AlpacaEval benchmark show\nthat the FANNO can generate high-quality data with diversity and complexity for\nfree, comparable to human-annotated or cleaned datasets like\nAlpaca-GPT4-Cleaned.\n","authors":["He Zhu","Junyou Su","Tianle Lun","Yicheng Tao","Wenjia Zhang","Zipei Fan","Guanhua Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01323v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02659v2","updated":"2024-08-02T15:13:26Z","published":"2024-07-02T20:49:21Z","title":"LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model\n  Training Data Through Knowledge Graph Comparison","summary":"  In light of recent legal allegations brought by publishers, newspapers, and\nother creators of copyrighted corpora against large language model developers\nwho use their copyrighted materials for training or fine-tuning purposes, we\npropose a novel system, a variant of a plagiarism detection system, that\nassesses whether a knowledge source has been used in the training or\nfine-tuning of a large language model. Unlike current methods, we utilize an\napproach that uses Resource Description Framework (RDF) triples to create\nknowledge graphs from both a source document and an LLM continuation of that\ndocument. These graphs are then analyzed with respect to content using cosine\nsimilarity and with respect to structure using a normalized version of graph\nedit distance that shows the degree of isomorphism. Unlike traditional\nplagiarism systems that focus on content matching and keyword identification\nbetween a source and a target corpus, our approach enables a broader and more\naccurate evaluation of similarity between a source document and LLM\ncontinuation by focusing on relationships between ideas and their organization\nwith regards to others. Additionally, our approach does not require access to\nLLM metrics like perplexity that may be unavailable in closed large language\nmodel \"black-box\" systems, as well as the training corpus. We thus assess\nwhether an LLM has \"plagiarized\" a corpus in its continuation through\nsimilarity measures. A prototype of our system will be found on a hyperlinked\nGitHub repository.\n","authors":["Devam Mondal","Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2407.02659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01308v1","updated":"2024-08-02T15:00:05Z","published":"2024-08-02T15:00:05Z","title":"Reconsidering Token Embeddings with the Definitions for Pre-trained\n  Language Models","summary":"  Learning token embeddings based on token co-occurrence statistics has proven\neffective for both pre-training and fine-tuning in natural language processing.\nHowever, recent studies have pointed out the distribution of learned embeddings\ndegenerates into anisotropy, and even pre-trained language models (PLMs) suffer\nfrom a loss of semantics-related information in embeddings for low-frequency\ntokens. This study first analyzes fine-tuning dynamics of a PLM, BART-large,\nand demonstrates its robustness against degeneration. On the basis of this\nfinding, we propose DefinitionEMB, a method that utilizes definitions to\nconstruct isotropically distributed and semantics-related token embeddings for\nPLMs while maintaining original robustness during fine-tuning. Our experiments\ndemonstrate the effectiveness of leveraging definitions from Wiktionary to\nconstruct such embeddings for RoBERTa-base and BART-large. Furthermore, the\nconstructed embeddings for low-frequency tokens improve the performance of\nthese models across various GLUE and four text summarization datasets.\n","authors":["Ying Zhang","Dongyuan Li","Manabu Okumura"],"pdf_url":"https://arxiv.org/pdf/2408.01308v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21043v2","updated":"2024-08-02T14:58:54Z","published":"2024-07-22T04:07:12Z","title":"CP-Prompt: Composition-Based Cross-modal Prompting for\n  Domain-Incremental Continual Learning","summary":"  The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.\n","authors":["Yu Feng","Zhen Tian","Yifan Zhu","Zongfu Han","Haoran Luo","Guangwei Zhang","Meina Song"],"pdf_url":"https://arxiv.org/pdf/2407.21043v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2401.13979v2","updated":"2024-08-02T14:50:05Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  Developing foundational large language models (LLMs) is becoming increasingly\ncostly and inefficient. Also, closed-source and larger open-source models\ngenerally offer better response quality but come with higher inference costs\nthan smaller models. In this paper, we introduce Routoo, an architecture\ndesigned to optimize the selection of LLMs for specific prompts based on\nperformance, cost, and efficiency. Routoo consists of two key components: a\nperformance predictor and a cost-aware decoding. The performance predictor is a\nlightweight LLM that estimates the performance of various underlying LLMs\nwithout needing to execute and evaluate them. The cost-aware decoding then\nselects the most suitable model based on these predictions and other\nconstraints like cost and latency. We evaluated Routoo using the MMLU benchmark\nacross 57 domains employing open-source models. Our results show that Routoo\nmatches the performance of the Mixtral 8x7b model while reducing inference\ncosts by one-third. Additionally, by allowing increased costs, Routoo surpasses\nMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of\n75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's\nperformance at half the cost and exceeds it with a 25% cost reduction. These\noutcomes highlight Routoo's potential to create new SOTA in a cost-effective\nmanner by leveraging the collective knowledge of multiple LLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00690v2","updated":"2024-08-02T14:36:05Z","published":"2024-08-01T16:31:35Z","title":"Improving Text Embeddings for Smaller Language Models Using Contrastive\n  Fine-tuning","summary":"  While Large Language Models show remarkable performance in natural language\nunderstanding, their resource-intensive nature makes them less accessible. In\ncontrast, smaller language models such as MiniCPM offer more sustainable\nscalability, but often underperform without specialized optimization. In this\npaper, we explore the enhancement of smaller language models through the\nimprovement of their text embeddings. We select three language models, MiniCPM,\nPhi-2, and Gemma, to conduct contrastive fine-tuning on the NLI dataset. Our\nresults demonstrate that this fine-tuning method enhances the quality of text\nembeddings for all three models across various benchmarks, with MiniCPM showing\nthe most significant improvements of an average 56.33% performance gain. The\ncontrastive fine-tuning code is publicly available at\nhttps://github.com/trapoom555/Language-Model-STS-CFT.\n","authors":["Trapoom Ukarapol","Zhicheng Lee","Amy Xin"],"pdf_url":"https://arxiv.org/pdf/2408.00690v2.pdf","comment":"Code: https://github.com/trapoom555/Language-Model-STS-CFT,\n  Huggingface:\n  https://huggingface.co/collections/trapoom555/small-lms-text-embedding-663b3ec87527788a577f6852"},{"id":"http://arxiv.org/abs/2309.10931v4","updated":"2024-08-02T14:27:46Z","published":"2023-09-19T21:07:52Z","title":"A Family of Pretrained Transformer Language Models for Russian","summary":"  Transformer language models (LMs) are fundamental to NLP research\nmethodologies and applications in various languages. However, developing such\nmodels specifically for the Russian language has received little attention.\nThis paper introduces a collection of 13 Russian Transformer LMs, which spans\nencoder (ruBERT, ruRoBERTa, ruELECTRA), decoder (ruGPT-3), and encoder-decoder\n(ruT5, FRED-T5) architectures. We provide a report on the model architecture\ndesign and pretraining, and the results of evaluating their generalization\nabilities on Russian language understanding and generation datasets and\nbenchmarks. By pretraining and releasing these specialized Transformer LMs, we\naim to broaden the scope of the NLP research directions and enable the\ndevelopment of industrial solutions for the Russian language.\n","authors":["Dmitry Zmitrovich","Alexander Abramov","Andrey Kalmykov","Maria Tikhonova","Ekaterina Taktasheva","Danil Astafurov","Mark Baushenko","Artem Snegirev","Vitalii Kadulin","Sergey Markov","Tatiana Shavrina","Vladislav Mikhailov","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2309.10931v4.pdf","comment":"LREC-COLING-2024"},{"id":"http://arxiv.org/abs/2407.14962v3","updated":"2024-08-02T14:26:55Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v3.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on Artificial Intelligence (TAI)"},{"id":"http://arxiv.org/abs/2112.06311v4","updated":"2024-08-02T14:21:43Z","published":"2021-12-12T20:02:42Z","title":"Weakly Supervised Text-to-SQL Parsing through Question Decomposition","summary":"  Text-to-SQL parsers are crucial in enabling non-experts to effortlessly query\nrelational data. Training such parsers, by contrast, generally requires\nexpertise in annotating natural language (NL) utterances with corresponding SQL\nqueries. In this work, we propose a weak supervision approach for training\ntext-to-SQL parsers. We take advantage of the recently proposed question\nmeaning representation called QDMR, an intermediate between NL and formal query\nlanguages. Given questions, their QDMR structures (annotated by non-experts or\nautomatically predicted), and the answers, we are able to automatically\nsynthesize SQL queries that are used to train text-to-SQL models. We test our\napproach by experimenting on five benchmark datasets. Our results show that the\nweakly supervised models perform competitively with those trained on annotated\nNL-SQL data. Overall, we effectively train text-to-SQL parsers, while using\nzero SQL annotations.\n","authors":["Tomer Wolfson","Daniel Deutch","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2112.06311v4.pdf","comment":"Accepted for publication in Findings of NAACL 2022. Author's final\n  version"},{"id":"http://arxiv.org/abs/2408.01287v1","updated":"2024-08-02T14:19:34Z","published":"2024-08-02T14:19:34Z","title":"Deep Learning based Visually Rich Document Content Understanding: A\n  Survey","summary":"  Visually Rich Documents (VRDs) are essential in academia, finance, medical\nfields, and marketing due to their multimodal information content. Traditional\nmethods for extracting information from VRDs depend on expert knowledge and\nmanual labor, making them costly and inefficient. The advent of deep learning\nhas revolutionized this process, introducing models that leverage multimodal\ninformation vision, text, and layout along with pretraining tasks to develop\ncomprehensive document representations. These models have achieved\nstate-of-the-art performance across various downstream tasks, significantly\nenhancing the efficiency and accuracy of information extraction from VRDs. In\nresponse to the growing demands and rapid developments in Visually Rich\nDocument Understanding (VRDU), this paper provides a comprehensive review of\ndeep learning-based VRDU frameworks. We systematically survey and analyze\nexisting methods and benchmark datasets, categorizing them based on adopted\nstrategies and downstream tasks. Furthermore, we compare different techniques\nused in VRDU models, focusing on feature representation and fusion, model\narchitecture, and pretraining methods, while highlighting their strengths,\nlimitations, and appropriate scenarios. Finally, we identify emerging trends\nand challenges in VRDU, offering insights into future research directions and\npractical applications. This survey aims to provide a thorough understanding of\nVRDU advancements, benefiting both academic and industrial sectors.\n","authors":["Yihao Ding","Jean Lee","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2408.01287v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2304.13007v4","updated":"2024-08-02T14:18:51Z","published":"2023-04-25T17:27:37Z","title":"Answering Questions by Meta-Reasoning over Multiple Chains of Thought","summary":"  Modern systems for multi-hop question answering (QA) typically break\nquestions into a sequence of reasoning steps, termed chain-of-thought (CoT),\nbefore arriving at a final answer. Often, multiple chains are sampled and\naggregated through a voting mechanism over the final answers, but the\nintermediate steps themselves are discarded. While such approaches improve\nperformance, they do not consider the relations between intermediate steps\nacross chains and do not provide a unified explanation for the predicted\nanswer. We introduce Multi-Chain Reasoning (MCR), an approach which prompts\nlarge language models to meta-reason over multiple chains of thought, rather\nthan aggregating their answers. MCR examines different reasoning chains, mixes\ninformation between them and selects the most relevant facts in generating an\nexplanation and predicting the answer. MCR outperforms strong baselines on 7\nmulti-hop QA datasets. Moreover, our analysis reveals that MCR explanations\nexhibit high quality, enabling humans to verify its answers.\n","authors":["Ori Yoran","Tomer Wolfson","Ben Bogin","Uri Katz","Daniel Deutch","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2304.13007v4.pdf","comment":"Accepted for publication in The 2023 Conference on Empirical Methods\n  in Natural Language Processing (EMNLP 2023). Author's final version"},{"id":"http://arxiv.org/abs/2408.01285v1","updated":"2024-08-02T14:13:06Z","published":"2024-08-02T14:13:06Z","title":"The Mismeasure of Man and Models: Evaluating Allocational Harms in Large\n  Language Models","summary":"  Large language models (LLMs) are now being considered and even deployed for\napplications that support high-stakes decision-making, such as recruitment and\nclinical decisions. While several methods have been proposed for measuring\nbias, there remains a gap between predictions, which are what the proposed\nmethods consider, and how they are used to make decisions. In this work, we\nintroduce Rank-Allocational-Based Bias Index (RABBI), a model-agnostic bias\nmeasure that assesses potential allocational harms arising from biases in LLM\npredictions. We compare RABBI and current bias metrics on two allocation\ndecision tasks. We evaluate their predictive validity across ten LLMs and\nutility for model selection. Our results reveal that commonly-used bias metrics\nbased on average performance gap and distribution distance fail to reliably\ncapture group disparities in allocation outcomes, whereas RABBI exhibits a\nstrong correlation with allocation disparities. Our work highlights the need to\naccount for how models are used in contexts with limited resource constraints.\n","authors":["Hannah Chen","Yangfeng Ji","David Evans"],"pdf_url":"https://arxiv.org/pdf/2408.01285v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02604v2","updated":"2024-08-02T13:45:53Z","published":"2024-07-02T18:43:10Z","title":"D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data\n  and eXpert model predictions","summary":"  Large vision language models (VLMs) have progressed incredibly from research\nto applicability for general-purpose use cases. LLaVA-Med, a pioneering large\nlanguage and vision assistant for biomedicine, can perform multi-modal\nbiomedical image and data analysis to provide a natural language interface for\nradiologists. While it is highly generalizable and works with multi-modal data,\nit is currently limited by well-known challenges that exist in the large\nlanguage model space. Hallucinations and imprecision in responses can lead to\nmisdiagnosis which currently hinder the clinical adaptability of VLMs. To\ncreate precise, user-friendly models in healthcare, we propose D-Rax -- a\ndomain-specific, conversational, radiologic assistance tool that can be used to\ngain insights about a particular radiologic image. In this study, we enhance\nthe conversational analysis of chest X-ray (CXR) images to support radiological\nreporting, offering comprehensive insights from medical imaging and aiding in\nthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the\nLLaVA-Med architecture on our curated enhanced instruction-following data,\ncomprising of images, instructions, as well as disease diagnosis and\ndemographic predictions derived from MIMIC-CXR imaging data, CXR-related visual\nquestion answer (VQA) pairs, and predictive outcomes from multiple expert AI\nmodels. We observe statistically significant improvement in responses when\nevaluated for both open and close-ended conversations. Leveraging the power of\nstate-of-the-art diagnostic models combined with VLMs, D-Rax empowers\nclinicians to interact with medical images using natural language, which could\npotentially streamline their decision-making process, enhance diagnostic\naccuracy, and conserve their time.\n","authors":["Hareem Nisar","Syed Muhammad Anwar","Zhifan Jiang","Abhijeet Parida","Ramon Sanchez-Jacob","Vishwesh Nath","Holger R. Roth","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2407.02604v2.pdf","comment":"accepted to the MICCAI 2024 Second International Workshop on\n  Foundation Models for General Medical AI"},{"id":"http://arxiv.org/abs/2408.01262v1","updated":"2024-08-02T13:35:11Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Ruobing Wang","Shi Yu","Shuo Wang","Yukun Yan","Zhenghao Liu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04531v3","updated":"2024-08-02T13:23:18Z","published":"2024-01-09T12:55:21Z","title":"MERA: A Comprehensive LLM Evaluation in Russian","summary":"  Over the past few years, one of the most notable advancements in AI research\nhas been in foundation models (FMs), headlined by the rise of language models\n(LMs). As the models' size increases, LMs demonstrate enhancements in\nmeasurable aspects and the development of new qualitative features. However,\ndespite researchers' attention and the rapid growth in LM application, the\ncapabilities, limitations, and associated risks still need to be better\nunderstood. To address these issues, we introduce an open Multimodal Evaluation\nof Russian-language Architectures (MERA), a new instruction benchmark for\nevaluating foundation models oriented towards the Russian language. The\nbenchmark encompasses 21 evaluation tasks for generative models in 11 skill\ndomains and is designed as a black-box test to ensure the exclusion of data\nleakage. The paper introduces a methodology to evaluate FMs and LMs in zero-\nand few-shot fixed instruction settings that can be extended to other\nmodalities. We propose an evaluation methodology, an open-source code base for\nthe MERA assessment, and a leaderboard with a submission system. We evaluate\nopen LMs as baselines and find that they are still far behind the human level.\nWe publicly release MERA to guide forthcoming research, anticipate\ngroundbreaking model features, standardize the evaluation procedure, and\naddress potential societal drawbacks.\n","authors":["Alena Fenogenova","Artem Chervyakov","Nikita Martynov","Anastasia Kozlova","Maria Tikhonova","Albina Akhmetgareeva","Anton Emelyanov","Denis Shevelev","Pavel Lebedev","Leonid Sinev","Ulyana Isaeva","Katerina Kolomeytseva","Daniil Moskovskiy","Elizaveta Goncharova","Nikita Savushkin","Polina Mikhailova","Denis Dimitrov","Alexander Panchenko","Sergei Markov"],"pdf_url":"https://arxiv.org/pdf/2401.04531v3.pdf","comment":"The paper version comparable with the release code v.1.1.0 of the\n  benchmark MERA. ACL-2024 main track camera ready version"},{"id":"http://arxiv.org/abs/2308.01154v4","updated":"2024-08-02T12:39:17Z","published":"2023-08-02T13:58:37Z","title":"Arithmetic with Language Models: from Memorization to Computation","summary":"  A better understanding of the emergent computation and problem-solving\ncapabilities of recent large language models is of paramount importance to\nfurther improve them and broaden their applicability. This work investigates\nhow a language model, trained to predict the next token, can perform arithmetic\ncomputations generalizing beyond training data. Binary addition and\nmultiplication constitute a good testbed for this purpose, since they require a\nvery small vocabulary and exhibit relevant input/output discontinuities making\nsmooth input interpolation ineffective for novel data. We successfully trained\na light language model to learn these tasks and ran a number of experiments to\ninvestigate the extrapolation capabilities and internal information processing.\nOur findings support the hypothesis that the language model works as an\nEncoding-Regression-Decoding machine where the computation takes place in the\nvalue space once the input token representation is mapped to an appropriate\ninternal representation.\n","authors":["Davide Maltoni","Matteo Ferrara"],"pdf_url":"https://arxiv.org/pdf/2308.01154v4.pdf","comment":"The article has been accepted for publication in Elsevier Neural\n  Networks journal. The final version is available on the Elsevier\n  ScienceDirect platform"},{"id":"http://arxiv.org/abs/2407.19813v2","updated":"2024-08-02T12:11:17Z","published":"2024-07-29T09:05:10Z","title":"Improving Retrieval Augmented Language Model with Self-Reasoning","summary":"  The Retrieval-Augmented Language Model (RALM) has shown remarkable\nperformance on knowledge-intensive tasks by incorporating external knowledge\nduring inference, which mitigates the factual hallucinations inherited in large\nlanguage models (LLMs). Despite these advancements, challenges persist in the\nimplementation of RALMs, particularly concerning their reliability and\ntraceability. To be specific, the irrelevant document retrieval may result in\nunhelpful response generation or even deteriorate the performance of LLMs,\nwhile the lack of proper citations in generated outputs complicates efforts to\nverify the trustworthiness of the models. To this end, we propose a novel\nself-reasoning framework aimed at improving the reliability and traceability of\nRALMs, whose core idea is to leverage reasoning trajectories generated by the\nLLM itself. The framework involves constructing self-reason trajectories with\nthree processes: a relevance-aware process, an evidence-aware selective\nprocess, and a trajectory analysis process. We have evaluated our framework\nacross four public datasets (two short-form QA datasets, one long-form QA\ndataset, and one fact verification dataset) to demonstrate the superiority of\nour method, which can outperform existing state-of-art models and can achieve\ncomparable performance with GPT-4, while only using 2,000 training samples.\n","authors":["Yuan Xia","Jingbo Zhou","Zhenhui Shi","Jun Chen","Haifeng Huang"],"pdf_url":"https://arxiv.org/pdf/2407.19813v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01214v1","updated":"2024-08-02T12:00:00Z","published":"2024-08-02T12:00:00Z","title":"High-Throughput Phenotyping of Clinical Text Using Large Language Models","summary":"  High-throughput phenotyping automates the mapping of patient signs to\nstandardized ontology concepts and is essential for precision medicine. This\nstudy evaluates the automation of phenotyping of clinical summaries from the\nOnline Mendelian Inheritance in Man (OMIM) database using large language\nmodels. Due to their rich phenotype data, these summaries can be surrogates for\nphysician notes. We conduct a performance comparison of GPT-4 and\nGPT-3.5-Turbo. Our results indicate that GPT-4 surpasses GPT-3.5-Turbo in\nidentifying, categorizing, and normalizing signs, achieving concordance with\nmanual annotators comparable to inter-rater agreement. Despite some limitations\nin sign normalization, the extensive pre-training of GPT-4 results in high\nperformance and generalizability across several phenotyping tasks while\nobviating the need for manually annotated training data. Large language models\nare expected to be the dominant method for automating high-throughput\nphenotyping of clinical text.\n","authors":["Daniel B. Hier","S. Ilyas Munzir","Anne Stahlfeld","Tayo Obafemi-Ajayi","Michael D. Carrithers"],"pdf_url":"https://arxiv.org/pdf/2408.01214v1.pdf","comment":"Submitted to IEEE-EMBS International Conference on Biomedical and\n  Health Informatics (BHI), Houston TX"},{"id":"http://arxiv.org/abs/2406.04988v2","updated":"2024-08-02T11:49:54Z","published":"2024-06-07T14:54:56Z","title":"Language models emulate certain cognitive profiles: An investigation of\n  how predictability measures interact with individual differences","summary":"  To date, most investigations on surprisal and entropy effects in reading have\nbeen conducted on the group level, disregarding individual differences. In this\nwork, we revisit the predictive power of surprisal and entropy measures\nestimated from a range of language models (LMs) on data of human reading times\nas a measure of processing effort by incorporating information of language\nusers' cognitive capacities. To do so, we assess the predictive power of\nsurprisal and entropy estimated from generative LMs on reading data obtained\nfrom individuals who also completed a wide range of psychometric tests.\nSpecifically, we investigate if modulating surprisal and entropy relative to\ncognitive scores increases prediction accuracy of reading times, and we examine\nwhether LMs exhibit systematic biases in the prediction of reading times for\ncognitively high- or low-performing groups, revealing what type of\npsycholinguistic subject a given LM emulates. Our study finds that in most\ncases, incorporating cognitive capacities increases predictive power of\nsurprisal and entropy on reading times, and that generally, high performance in\nthe psychometric tests is associated with lower sensitivity to predictability\neffects. Finally, our results suggest that the analyzed LMs emulate readers\nwith lower verbal intelligence, suggesting that for a given target group (i.e.,\nindividuals with high verbal intelligence), these LMs provide less accurate\npredictability estimates.\n","authors":["Patrick Haller","Lena S. Bolliger","Lena A. Jger"],"pdf_url":"https://arxiv.org/pdf/2406.04988v2.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.01168v1","updated":"2024-08-02T10:35:49Z","published":"2024-08-02T10:35:49Z","title":"Misinforming LLMs: vulnerabilities, challenges and opportunities","summary":"  Large Language Models (LLMs) have made significant advances in natural\nlanguage processing, but their underlying mechanisms are often misunderstood.\nDespite exhibiting coherent answers and apparent reasoning behaviors, LLMs rely\non statistical patterns in word embeddings rather than true cognitive\nprocesses. This leads to vulnerabilities such as \"hallucination\" and\nmisinformation. The paper argues that current LLM architectures are inherently\nuntrustworthy due to their reliance on correlations of sequential patterns of\nword embedding vectors. However, ongoing research into combining generative\ntransformer-based models with fact bases and logic programming languages may\nlead to the development of trustworthy LLMs capable of generating statements\nbased on given truth and explaining their self-reasoning process.\n","authors":["Bo Zhou","Daniel Geiler","Paul Lukowicz"],"pdf_url":"https://arxiv.org/pdf/2408.01168v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01154v1","updated":"2024-08-02T10:12:42Z","published":"2024-08-02T10:12:42Z","title":"DERA: Dense Entity Retrieval for Entity Alignment in Knowledge Graphs","summary":"  Entity Alignment (EA) aims to match equivalent entities in different\nKnowledge Graphs (KGs), which is essential for knowledge fusion and\nintegration. Recently, embedding-based EA has attracted significant attention\nand many approaches have been proposed. Early approaches primarily focus on\nlearning entity embeddings from the structural features of KGs, defined by\nrelation triples. Later methods incorporated entities' names and attributes as\nauxiliary information to enhance embeddings for EA. However, these approaches\noften used different techniques to encode structural and attribute information,\nlimiting their interaction and mutual enhancement. In this work, we propose a\ndense entity retrieval framework for EA, leveraging language models to\nuniformly encode various features of entities and facilitate nearest entity\nsearch across KGs. Alignment candidates are first generated through entity\nretrieval, which are subsequently reranked to determine the final alignments.\nWe conduct comprehensive experiments on both cross-lingual and monolingual EA\ndatasets, demonstrating that our approach achieves state-of-the-art performance\ncompared to existing EA methods.\n","authors":["Zhichun Wang","Xuan Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01154v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13071v2","updated":"2024-08-02T09:30:03Z","published":"2024-05-20T20:55:07Z","title":"A Novel Method for News Article Event-Based Embedding","summary":"  Embedding news articles is a crucial tool for multiple fields, such as media\nbias detection, identifying fake news, and making news recommendations.\nHowever, existing news embedding methods are not optimized to capture the\nlatent context of news events. Most embedding methods rely on full-text\ninformation and neglect time-relevant embedding generation. In this paper, we\npropose a novel lightweight method that optimizes news embedding generation by\nfocusing on entities and themes mentioned in articles and their historical\nconnections to specific events. We suggest a method composed of three stages.\nFirst, we process and extract events, entities, and themes from the given news\narticles. Second, we generate periodic time embeddings for themes and entities\nby training time-separated GloVe models on current and historical data. Lastly,\nwe concatenate the news embeddings generated by two distinct approaches: Smooth\nInverse Frequency (SIF) for article-level vectors and Siamese Neural Networks\nfor embeddings with nuanced event-related information. We leveraged over\n850,000 news articles and 1,000,000 events from the GDELT project to test and\nevaluate our method. We conducted a comparative analysis of different news\nembedding generation methods for validation. Our experiments demonstrate that\nour approach can both improve and outperform state-of-the-art methods on shared\nevent detection tasks.\n","authors":["Koren Ishlach","Itzhak Ben-David","Michael Fire","Lior Rokach"],"pdf_url":"https://arxiv.org/pdf/2405.13071v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01122v1","updated":"2024-08-02T09:03:48Z","published":"2024-08-02T09:03:48Z","title":"CFBench: A Comprehensive Constraints-Following Benchmark for LLMs","summary":"  The adeptness of Large Language Models (LLMs) in comprehending and following\nnatural language instructions is critical for their deployment in sophisticated\nreal-world applications. Existing evaluations mainly focus on fragmented\nconstraints or narrow scenarios, but they overlook the comprehensiveness and\nauthenticity of constraints from the user's perspective. To bridge this gap, we\npropose CFBench, a large-scale Comprehensive Constraints Following Benchmark\nfor LLMs, featuring 1,000 curated samples that cover more than 200 real-life\nscenarios and over 50 NLP tasks. CFBench meticulously compiles constraints from\nreal-world instructions and constructs an innovative systematic framework for\nconstraint types, which includes 10 primary categories and over 25\nsubcategories, and ensures each constraint is seamlessly integrated within the\ninstructions. To make certain that the evaluation of LLM outputs aligns with\nuser perceptions, we propose an advanced methodology that integrates\nmulti-dimensional assessment criteria with requirement prioritization, covering\nvarious perspectives of constraints, instructions, and requirement fulfillment.\nEvaluating current leading LLMs on CFBench reveals substantial room for\nimprovement in constraints following, and we further investigate influencing\nfactors and enhancement strategies. The data and code are publicly available at\nhttps://github.com/PKU-Baichuan-MLSystemLab/CFBench\n","authors":["Tao Zhang","Yanjun Shen","Wenjing Luo","Yan Zhang","Hao Liang","Tao Zhang","Fan Yang","Mingan Lin","Yujing Qiao","Weipeng Chen","Bin Cui","Wentao Zhang","Zenan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.01122v1.pdf","comment":"15 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.01119v1","updated":"2024-08-02T09:00:03Z","published":"2024-08-02T09:00:03Z","title":"Task Prompt Vectors: Effective Initialization through Multi-Task\n  Soft-Prompt Transfer","summary":"  Prompt tuning is a modular and efficient solution for training large language\nmodels (LLMs). One of its main advantages is task modularity, making it\nsuitable for multi-task problems. However, current soft-prompt-based methods\noften sacrifice multi-task modularity, requiring the training process to be\nfully or partially repeated for each newly added task. While recent work on\ntask vectors applied arithmetic operations on full model weights to achieve the\ndesired multi-task performance, a similar approach for soft-prompts is still\nmissing. To this end, we introduce Task Prompt Vectors, created by element-wise\ndifference between weights of tuned soft-prompts and their random\ninitialization. Experimental results on 12 NLU datasets show that task prompt\nvectors can be used in low-resource settings to effectively initialize prompt\ntuning on similar tasks. In addition, we show that task prompt vectors are\nindependent of the random initialization of prompt tuning. This allows prompt\narithmetics with the pre-trained vectors from different tasks. In this way, by\narithmetic addition of task prompt vectors from multiple tasks, we are able to\noutperform a state-of-the-art baseline in some cases.\n","authors":["Robert Belanec","Simon Ostermann","Ivan Srba","Maria Bielikova"],"pdf_url":"https://arxiv.org/pdf/2408.01119v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01118v1","updated":"2024-08-02T08:59:09Z","published":"2024-08-02T08:59:09Z","title":"IAI Group at CheckThat! 2024: Transformer Models and Data Augmentation\n  for Checkworthy Claim Detection","summary":"  This paper describes IAI group's participation for automated check-worthiness\nestimation for claims, within the framework of the 2024 CheckThat! Lab \"Task 1:\nCheck-Worthiness Estimation\". The task involves the automated detection of\ncheck-worthy claims in English, Dutch, and Arabic political debates and Twitter\ndata. We utilized various pre-trained generative decoder and encoder\ntransformer models, employing methods such as few-shot chain-of-thought\nreasoning, fine-tuning, data augmentation, and transfer learning from one\nlanguage to another. Despite variable success in terms of performance, our\nmodels achieved notable placements on the organizer's leaderboard: ninth-best\nin English, third-best in Dutch, and the top placement in Arabic, utilizing\nmultilingual datasets for enhancing the generalizability of check-worthiness\ndetection. Despite a significant drop in performance on the unlabeled test\ndataset compared to the development test dataset, our findings contribute to\nthe ongoing efforts in claim detection research, highlighting the challenges\nand potential of language-specific adaptations in claim verification systems.\n","authors":["Peter Rysland Aarnes","Vinay Setty","Petra Galukov"],"pdf_url":"https://arxiv.org/pdf/2408.01118v1.pdf","comment":"Accepted to CLEF2024 CheckThat!"},{"id":"http://arxiv.org/abs/2407.17900v3","updated":"2024-08-02T08:55:52Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.765 and an AP value of 0.415 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15132v2","updated":"2024-08-02T08:49:14Z","published":"2024-02-23T06:33:51Z","title":"Improving Sentence Embeddings with Automatic Generation of Training Data\n  Using Few-shot Examples","summary":"  Decoder-based large language models (LLMs) have shown high performance on\nmany tasks in natural language processing. This is also true for sentence\nembedding learning, where a decoder-based model, PromptEOL, has achieved the\nbest performance on semantic textual similarity (STS) tasks. However, PromptEOL\nrequires a manually annotated natural language inference (NLI) dataset for\nfine-tuning. We aim to improve sentence embeddings without using large manually\nannotated datasets by automatically generating an NLI dataset with an LLM and\nusing it for fine-tuning of PromptEOL. To achieve this, we explore methods of\ndata generation suitable for sentence embedding learning in this study.\nSpecifically, we will focus on automatic dataset generation through few-shot\nlearning and explore the appropriate methods to leverage few-shot examples.\nExperimental results on the STS tasks demonstrate that our approach outperforms\nexisting models in settings without large manually annotated datasets.\n","authors":["Soma Sato","Hayato Tsukagoshi","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2402.15132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01107v1","updated":"2024-08-02T08:37:03Z","published":"2024-08-02T08:37:03Z","title":"BioRAG: A RAG-LLM Framework for Biological Question Reasoning","summary":"  The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks.\n","authors":["Chengrui Wang","Qingqing Long","Xiao Meng","Xunxin Cai","Chengjun Wu","Zhen Meng","Xuezhi Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.01107v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00655v2","updated":"2024-08-02T08:27:08Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Faster, Longer and More Accurate Inference with\n  Next-sentence Prediction for Large Language Models","summary":"  Contemporary large language models (LLMs) primarily rely on next-token\nprediction method for inference, which significantly impedes their processing\nspeed. In this paper, we introduce a novel inference methodology termed\nnext-sentence prediction, aimed at enhancing the inference efficiency of LLMs.\nWe present Sentence Variational Autoencoder (SentenceVAE), a tiny model\nconsisting of a Sentence Encoder and a Sentence Decoder. The encoder\neffectively condenses the information within a sentence into a singular token,\nwhile the decoder reconstructs this compressed data back into its original\nsentential form. By integrating SentenceVAE into the input and output layers of\nLLMs, we develop Sentence-level LLMs (SLLMs) that employ a sentence-by-sentence\ninference approach, markedly accelerating inference speeds. SentenceVAE also\nmaintains the integrity of the original semantic content by segmenting the text\ninto sentences, thereby improving accuracy while boosting inference speeds.\nCompared to published LLMs, SLLMs process fewer tokens over equivalent context\nlengths, significantly reducing memory demands for self-attention computations\nand facilitating the handling of longer contexts. Our experimental findings\nreveal that this method can accelerate inference speeds by 204~365%, reduce\nperplexity (PPL) to 46~75% of its original metric, and decrease memory overhead\nby 86~91% for the same context length, compared to the token-by-token method.\nMoreover, the benefits of this approach become even more pronounced as model\nparameters increase.\n","authors":["Hongjun An","Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v2.pdf","comment":"Modified some of the expression details and optimized the charts"},{"id":"http://arxiv.org/abs/2408.01090v1","updated":"2024-08-02T08:09:13Z","published":"2024-08-02T08:09:13Z","title":"General-purpose Dataflow Model with Neuromorphic Primitives","summary":"  Neuromorphic computing exhibits great potential to provide high-performance\nbenefits in various applications beyond neural networks. However, a\ngeneral-purpose program execution model that aligns with the features of\nneuromorphic computing is required to bridge the gap between program\nversatility and neuromorphic hardware efficiency. The dataflow model offers a\npotential solution, but it faces high graph complexity and incompatibility with\nneuromorphic hardware when dealing with control flow programs, which decreases\nthe programmability and performance. Here, we present a dataflow model tailored\nfor neuromorphic hardware, called neuromorphic dataflow, which provides a\ncompact, concise, and neuromorphic-compatible program representation for\ncontrol logic. The neuromorphic dataflow introduces \"when\" and \"where\"\nprimitives, which restructure the view of control. The neuromorphic dataflow\nembeds these primitives in the dataflow schema with the plasticity inherited\nfrom the spiking algorithms. Our method enables the deployment of\ngeneral-purpose programs on neuromorphic hardware with both programmability and\nplasticity, while fully utilizing the hardware's potential.\n","authors":["Weihao Zhang","Yu Du","Hongyi Li","Songchen Ma","Rong Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.01090v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01088v1","updated":"2024-08-02T08:07:15Z","published":"2024-08-02T08:07:15Z","title":"Bridging Information Gaps in Dialogues With Grounded Exchanges Using\n  Knowledge Graphs","summary":"  Knowledge models are fundamental to dialogue systems for enabling\nconversational interactions, which require handling domain-specific knowledge.\nEnsuring effective communication in information-providing conversations entails\naligning user understanding with the knowledge available to the system.\nHowever, dialogue systems often face challenges arising from semantic\ninconsistencies in how information is expressed in natural language compared to\nhow it is represented within the system's internal knowledge. To address this\nproblem, we study the potential of large language models for conversational\ngrounding, a mechanism to bridge information gaps by establishing shared\nknowledge between dialogue participants. Our approach involves annotating human\nconversations across five knowledge domains to create a new dialogue corpus\ncalled BridgeKG. Through a series of experiments on this dataset, we\nempirically evaluate the capabilities of large language models in classifying\ngrounding acts and identifying grounded information items within a knowledge\ngraph structure. Our findings offer insights into how these models use\nin-context learning for conversational grounding tasks and common prediction\nerrors, which we illustrate with examples from challenging dialogues. We\ndiscuss how the models handle knowledge graphs as a semantic layer between\nunstructured dialogue utterances and structured information items.\n","authors":["Phillip Schneider","Nektarios Machner","Kristiina Jokinen","Florian Matthes"],"pdf_url":"https://arxiv.org/pdf/2408.01088v1.pdf","comment":"Accepted to SIGDIAL 2024"},{"id":"http://arxiv.org/abs/2408.01084v1","updated":"2024-08-02T08:03:38Z","published":"2024-08-02T08:03:38Z","title":"Adaptive Contrastive Decoding in Retrieval-Augmented Generation for\n  Handling Noisy Contexts","summary":"  When using large language models (LLMs) in knowledge-intensive tasks, such as\nopen-domain question answering, external context can bridge a gap between\nexternal knowledge and LLM's parametric knowledge. Recent research has been\ndeveloped to amplify contextual knowledge over the parametric knowledge of LLM\nwith contrastive decoding approaches. While these approaches could yield\ntruthful responses when relevant context is provided, they are prone to\nvulnerabilities when faced with noisy contexts. We extend the scope of previous\nstudies to encompass noisy contexts and propose adaptive contrastive decoding\n(ACD) to leverage contextual influence effectively. ACD demonstrates\nimprovements in open-domain question answering tasks compared to baselines,\nespecially in robustness by remaining undistracted by noisy contexts in\nretrieval-augmented generation.\n","authors":["Youna Kim","Hyuhng Joon Kim","Cheonbok Park","Choonghyun Park","Hyunsoo Cho","Junyeob Kim","Kang Min Yoo","Sang-goo Lee","Taeuk Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01063v1","updated":"2024-08-02T07:31:57Z","published":"2024-08-02T07:31:57Z","title":"Leveraging Large Language Models for Mobile App Review Feature\n  Extraction","summary":"  Mobile app review analysis presents unique challenges due to the low quality,\nsubjective bias, and noisy content of user-generated documents. Extracting\nfeatures from these reviews is essential for tasks such as feature\nprioritization and sentiment analysis, but it remains a challenging task.\nMeanwhile, encoder-only models based on the Transformer architecture have shown\npromising results for classification and information extraction tasks for\nmultiple software engineering processes. This study explores the hypothesis\nthat encoder-only large language models can enhance feature extraction from\nmobile app reviews. By leveraging crowdsourced annotations from an industrial\ncontext, we redefine feature extraction as a supervised token classification\ntask. Our approach includes extending the pre-training of these models with a\nlarge corpus of user reviews to improve contextual understanding and employing\ninstance selection techniques to optimize model fine-tuning. Empirical\nevaluations demonstrate that this method improves the precision and recall of\nextracted features and enhances performance efficiency. Key contributions\ninclude a novel approach to feature extraction, annotated datasets, extended\npre-trained models, and an instance selection mechanism for cost-effective\nfine-tuning. This research provides practical methods and empirical evidence in\napplying large language models to natural language processing tasks within\nmobile app reviews, offering improved performance in feature extraction.\n","authors":["Quim Motger","Alessio Miaschi","Felice Dell'Orletta","Xavier Franch","Jordi Marco"],"pdf_url":"https://arxiv.org/pdf/2408.01063v1.pdf","comment":"46 pages, 8 tables, 11 figures"},{"id":"http://arxiv.org/abs/2408.01050v1","updated":"2024-08-02T06:56:59Z","published":"2024-08-02T06:56:59Z","title":"The Impact of Hyperparameters on Large Language Model Inference\n  Performance: An Evaluation of vLLM and HuggingFace Pipelines","summary":"  The recent surge of open-source large language models (LLMs) enables\ndevelopers to create AI-based solutions while maintaining control over aspects\nsuch as privacy and compliance, thereby providing governance and ownership of\nthe model deployment process. To utilize these LLMs, inference engines are\nneeded. These engines load the model's weights onto available resources, such\nas GPUs, and process queries to generate responses. The speed of inference, or\nperformance, of the LLM, is critical for real-time applications, as it computes\nmillions or billions of floating point operations per inference. Recently,\nadvanced inference engines such as vLLM have emerged, incorporating novel\nmechanisms such as efficient memory management to achieve state-of-the-art\nperformance. In this paper, we analyze the performance, particularly the\nthroughput (tokens generated per unit of time), of 20 LLMs using two inference\nlibraries: vLLM and HuggingFace's pipelines. We investigate how various\nhyperparameters, which developers must configure, influence inference\nperformance. Our results reveal that throughput landscapes are irregular, with\ndistinct peaks, highlighting the importance of hyperparameter optimization to\nachieve maximum performance. We also show that applying hyperparameter\noptimization when upgrading or downgrading the GPU model used for inference can\nimprove throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,\nrespectively.\n","authors":["Matias Martinez"],"pdf_url":"https://arxiv.org/pdf/2408.01050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01046v1","updated":"2024-08-02T06:46:08Z","published":"2024-08-02T06:46:08Z","title":"QUDSELECT: Selective Decoding for Questions Under Discussion Parsing","summary":"  Question Under Discussion (QUD) is a discourse framework that uses implicit\nquestions to reveal discourse relationships between sentences. In QUD parsing,\neach sentence is viewed as an answer to a question triggered by an anchor\nsentence in prior context. The resulting QUD structure is required to conform\nto several theoretical criteria like answer compatibility (how well the\nquestion is answered), making QUD parsing a challenging task. Previous works\nconstruct QUD parsers in a pipelined manner (i.e. detect the trigger sentence\nin context and then generate the question). However, these parsers lack a\nholistic view of the task and can hardly satisfy all the criteria. In this\nwork, we introduce QUDSELECT, a joint-training framework that selectively\ndecodes the QUD dependency structures considering the QUD criteria. Using\ninstruction-tuning, we train models to simultaneously predict the anchor\nsentence and generate the associated question. To explicitly incorporate the\ncriteria, we adopt a selective decoding strategy of sampling multiple QUD\ncandidates during inference, followed by selecting the best one with criteria\nscorers. Our method outperforms the state-of-the-art baseline models by 9% in\nhuman evaluation and 4% in automatic evaluation, demonstrating the\neffectiveness of our framework.\n","authors":["Ashima Suvarna","Xiao Liu","Tanmay Parekh","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2408.01046v1.pdf","comment":"11 Pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01038v1","updated":"2024-08-02T06:21:36Z","published":"2024-08-02T06:21:36Z","title":"UNER: A Unified Prediction Head for Named Entity Recognition in\n  Visually-rich Documents","summary":"  The recognition of named entities in visually-rich documents (VrD-NER) plays\na critical role in various real-world scenarios and applications. However, the\nresearch in VrD-NER faces three major challenges: complex document layouts,\nincorrect reading orders, and unsuitable task formulations. To address these\nchallenges, we propose a query-aware entity extraction head, namely UNER, to\ncollaborate with existing multi-modal document transformers to develop more\nrobust VrD-NER models. The UNER head considers the VrD-NER task as a\ncombination of sequence labeling and reading order prediction, effectively\naddressing the issues of discontinuous entities in documents. Experimental\nevaluations on diverse datasets demonstrate the effectiveness of UNER in\nimproving entity extraction performance. Moreover, the UNER head enables a\nsupervised pre-training stage on various VrD-NER datasets to enhance the\ndocument transformer backbones and exhibits substantial knowledge transfer from\nthe pre-training stage to the fine-tuning stage. By incorporating universal\nlayout understanding, a pre-trained UNER-based model demonstrates significant\nadvantages in few-shot and cross-linguistic scenarios and exhibits zero-shot\nentity extraction abilities.\n","authors":["Yi Tu","Chong Zhang","Ya Guo","Huan Chen","Jinyang Tang","Huijia Zhu","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.01038v1.pdf","comment":"accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2403.02504v3","updated":"2024-08-02T04:44:29Z","published":"2024-03-04T21:51:11Z","title":"A Tutorial on the Pretrain-Finetune Paradigm for Natural Language\n  Processing","summary":"  Given that natural language serves as the primary conduit for expressing\nthoughts and emotions, text analysis has become a key technique in\npsychological research. It enables the extraction of valuable insights from\nnatural language, facilitating endeavors like personality traits assessment,\nmental health monitoring, and sentiment analysis in interpersonal\ncommunications. In text analysis, existing studies often resort to either human\ncoding, which is time-consuming, using pre-built dictionaries, which often\nfails to cover all possible scenarios, or training models from scratch, which\nrequires large amounts of labeled data. In this tutorial, we introduce the\npretrain-finetune paradigm. The pretrain-finetune paradigm represents a\ntransformative approach in text analysis and natural language processing. This\nparadigm distinguishes itself through the use of large pretrained language\nmodels, demonstrating remarkable efficiency in finetuning tasks, even with\nlimited training data. This efficiency is especially beneficial for research in\nsocial sciences, where the number of annotated samples is often quite limited.\nOur tutorial offers a comprehensive introduction to the pretrain-finetune\nparadigm. We first delve into the fundamental concepts of pretraining and\nfinetuning, followed by practical exercises using real-world applications. We\ndemonstrate the application of the paradigm across various tasks, including\nmulti-class classification and regression. Emphasizing its efficacy and\nuser-friendliness, the tutorial aims to encourage broader adoption of this\nparadigm. To this end, we have provided open access to all our code and\ndatasets. The tutorial is highly beneficial across various psychology\ndisciplines, providing a comprehensive guide to employing text analysis in\ndiverse research settings.\n","authors":["Yu Wang","Wen Qu"],"pdf_url":"https://arxiv.org/pdf/2403.02504v3.pdf","comment":"29 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.01005v1","updated":"2024-08-02T04:40:15Z","published":"2024-08-02T04:40:15Z","title":"Enhancing Financial Market Predictions: Causality-Driven Feature\n  Selection","summary":"  This paper introduces the FinSen dataset that revolutionizes financial market\nanalysis by integrating economic and financial news articles from 197 countries\nwith stock market data. The dataset's extensive coverage spans 15 years from\n2007 to 2023 with temporal information, offering a rich, global perspective\nwith 160,000 records on financial market news. Our study leverages causally\nvalidated sentiment scores and LSTM models to enhance market forecast accuracy\nand reliability. Utilizing the FinSen dataset, we introduce an innovative Focal\nCalibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent\nwith the DAN 3 model. This not only improves prediction accuracy but also\naligns probabilistic forecasts closely with real outcomes, crucial for the\nfinancial sector where predicted probability is paramount. Our approach\ndemonstrates the effectiveness of combining sentiment analysis with precise\ncalibration techniques for trustworthy financial forecasting where the cost of\nmisinterpretation can be high. Finsen Data can be found at [this github\nURL](https://github.com/EagleAdelaide/FinSen_Dataset.git).\n","authors":["Wenhao Liang","Zhengyang Li","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01005v1.pdf","comment":"Accepted by The 20th International Conference Advanced Data Mining\n  and Applications 2024 (ADMA 2024)"},{"id":"http://arxiv.org/abs/2402.12806v2","updated":"2024-08-02T04:18:32Z","published":"2024-02-20T08:27:05Z","title":"SymBa: Symbolic Backward Chaining for Structured Natural Language\n  Reasoning","summary":"  While Large Language Models (LLMs) have demonstrated remarkable reasoning\nability, providing a structured, explainable proof to ensure explainability,\ni.e. structured reasoning, still remains challenging. Among two directions of\nstructured reasoning, we specifically focus on backward chaining, where the\nquery is recursively decomposed to subgoals by applying inference rules. We\npoint out that current popular backward chaining implementations (Least-to-most\nprompting and LAMBADA) fail to implement the necessary features of backward\nchaining, such as arbitrary-depth recursion and binding propagation. To this\nend, we propose a novel backward chaining framework, SymBa (Symbolic Backward\nChaining). In SymBA, a symbolic solver controls the whole proof process, and an\nLLM searches for the relevant natural language premises and translates them\ninto a symbolic form for the solver. By this LLM-solver integration, while\nproducing a completely structured proof that is symbolically verified, SymBa\nachieves significant improvement in performance, proof accuracy, and efficiency\nin diverse structured reasoning benchmarks compared to baselines.\n","authors":["Jinu Lee","Wonseok Hwang"],"pdf_url":"https://arxiv.org/pdf/2402.12806v2.pdf","comment":"16 pages (8 pages for main text),9 figures"},{"id":"http://arxiv.org/abs/2312.01700v3","updated":"2024-08-02T03:56:35Z","published":"2023-12-04T07:42:16Z","title":"Data Management For Training Large Language Models: A Survey","summary":"  Data plays a fundamental role in training Large Language Models (LLMs).\nEfficient data management, particularly in formulating a well-suited training\ndataset, is significant for enhancing model performance and improving training\nefficiency during pretraining and supervised fine-tuning stages. Despite the\nconsiderable importance of data management, the underlying mechanism of current\nprominent practices are still unknown. Consequently, the exploration of data\nmanagement has attracted more and more attention among the research community.\nThis survey aims to provide a comprehensive overview of current research in\ndata management within both the pretraining and supervised fine-tuning stages\nof LLMs, covering various aspects of data management strategy design. Looking\ninto the future, we extrapolate existing challenges and outline promising\ndirections for development in this field. Therefore, this survey serves as a\nguiding resource for practitioners aspiring to construct powerful LLMs through\nefficient data management practices. The collection of the latest papers is\navailable at https://github.com/ZigeW/data_management_LLM.\n","authors":["Zige Wang","Wanjun Zhong","Yufei Wang","Qi Zhu","Fei Mi","Baojun Wang","Lifeng Shang","Xin Jiang","Qun Liu"],"pdf_url":"https://arxiv.org/pdf/2312.01700v3.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.00994v1","updated":"2024-08-02T03:54:36Z","published":"2024-08-02T03:54:36Z","title":"ArchCode: Incorporating Software Requirements in Code Generation with\n  Large Language Models","summary":"  This paper aims to extend the code generation capability of large language\nmodels (LLMs) to automatically manage comprehensive software requirements from\ngiven textual descriptions. Such requirements include both functional (i.e.\nachieving expected behavior for inputs) and non-functional (e.g., time/space\nperformance, robustness, maintainability) requirements. However, textual\ndescriptions can either express requirements verbosely or may even omit some of\nthem. We introduce ARCHCODE, a novel framework that leverages in-context\nlearning to organize requirements observed in descriptions and to extrapolate\nunexpressed requirements from them. ARCHCODE generates requirements from given\ndescriptions, conditioning them to produce code snippets and test cases. Each\ntest case is tailored to one of the requirements, allowing for the ranking of\ncode snippets based on the compliance of their execution results with the\nrequirements. Public benchmarks show that ARCHCODE enhances to satisfy\nfunctional requirements, significantly improving Pass@k scores. Furthermore, we\nintroduce HumanEval-NFR, the first evaluation of LLMs' non-functional\nrequirements in code generation, demonstrating ARCHCODE's superiority over\nbaseline methods. The implementation of ARCHCODE and the HumanEval-NFR\nbenchmark are both publicly accessible.\n","authors":["Hojae Han","Jaejin Kim","Jaeseok Yoo","Youngwon Lee","Seung-won Hwang"],"pdf_url":"https://arxiv.org/pdf/2408.00994v1.pdf","comment":"Accepted by ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2408.00992v1","updated":"2024-08-02T03:44:14Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15077v3","updated":"2024-08-02T03:38:58Z","published":"2024-05-23T21:56:12Z","title":"Eliciting Informative Text Evaluations with Large Language Models","summary":"  Peer prediction mechanisms motivate high-quality feedback with provable\nguarantees. However, current methods only apply to rather simple reports, like\nmultiple-choice or scalar numbers. We aim to broaden these techniques to the\nlarger domain of text-based reports, drawing on the recent developments in\nlarge language models. This vastly increases the applicability of peer\nprediction mechanisms as textual feedback is the norm in a large variety of\nfeedback channels: peer reviews, e-commerce customer reviews, and comments on\nsocial media.\n  We introduce two mechanisms, the Generative Peer Prediction Mechanism (GPPM)\nand the Generative Synopsis Peer Prediction Mechanism (GSPPM). These mechanisms\nutilize LLMs as predictors, mapping from one agent's report to a prediction of\nher peer's report. Theoretically, we show that when the LLM prediction is\nsufficiently accurate, our mechanisms can incentivize high effort and\ntruth-telling as an (approximate) Bayesian Nash equilibrium. Empirically, we\nconfirm the efficacy of our mechanisms through experiments conducted on two\nreal datasets: the Yelp review dataset and the ICLR OpenReview dataset. We\nhighlight the results that on the ICLR dataset, our mechanisms can\ndifferentiate three quality levels -- human-written reviews, GPT-4-generated\nreviews, and GPT-3.5-generated reviews in terms of expected scores.\nAdditionally, GSPPM penalizes LLM-generated reviews more effectively than GPPM.\n","authors":["Yuxuan Lu","Shengwei Xu","Yichi Zhang","Yuqing Kong","Grant Schoenebeck"],"pdf_url":"https://arxiv.org/pdf/2405.15077v3.pdf","comment":"Accepted by the Twenty-Fifth ACM Conference on Economics and\n  Computation (EC'24)"},{"id":"http://arxiv.org/abs/2407.11046v2","updated":"2024-08-02T03:22:22Z","published":"2024-07-08T12:32:10Z","title":"A Survey on LoRA of Large Language Models","summary":"  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github page\n(https://github.com/ZJU-LLMs/Awesome-LoRAs.git) for readers to check the\nupdates and initiate discussions on this survey paper.\n","authors":["Yuren Mao","Yuhang Ge","Yijiang Fan","Wenyi Xu","Yu Mi","Zhonghao Hu","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00981v1","updated":"2024-08-02T02:31:54Z","published":"2024-08-02T02:31:54Z","title":"Cross-domain Named Entity Recognition via Graph Matching","summary":"  Cross-domain NER is a practical yet challenging problem since the data\nscarcity in the real-world scenario. A common practice is first to learn a NER\nmodel in a rich-resource general domain and then adapt the model to specific\ndomains. Due to the mismatch problem between entity types across domains, the\nwide knowledge in the general domain can not effectively transfer to the target\ndomain NER model. To this end, we model the label relationship as a probability\ndistribution and construct label graphs in both source and target label spaces.\nTo enhance the contextual representation with label structures, we fuse the\nlabel graph into the word embedding output by BERT. By representing label\nrelationships as graphs, we formulate cross-domain NER as a graph matching\nproblem. Furthermore, the proposed method has good applicability with\npre-training methods and is potentially capable of other cross-domain\nprediction tasks. Empirical results on four datasets show that our method\noutperforms a series of transfer learning, multi-task learning, and few-shot\nlearning methods.\n","authors":["Junhao Zheng","Haibin Chen","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2408.00981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.16340v3","updated":"2024-08-02T01:41:38Z","published":"2023-10-25T03:53:31Z","title":"RCAgent: Cloud Root Cause Analysis by Autonomous Agents with\n  Tool-Augmented Large Language Models","summary":"  Large language model (LLM) applications in cloud root cause analysis (RCA)\nhave been actively explored recently. However, current methods are still\nreliant on manual workflow settings and do not unleash LLMs' decision-making\nand environment interaction capabilities. We present RCAgent, a tool-augmented\nLLM autonomous agent framework for practical and privacy-aware industrial RCA\nusage. Running on an internally deployed model rather than GPT families,\nRCAgent is capable of free-form data collection and comprehensive analysis with\ntools. Our framework combines a variety of enhancements, including a unique\nSelf-Consistency for action trajectories, and a suite of methods for context\nmanagement, stabilization, and importing domain knowledge. Our experiments show\nRCAgent's evident and consistent superiority over ReAct across all aspects of\nRCA -- predicting root causes, solutions, evidence, and responsibilities -- and\ntasks covered or uncovered by current rules, as validated by both automated\nmetrics and human evaluations. Furthermore, RCAgent has already been integrated\ninto the diagnosis and issue discovery workflow of the Real-time Compute\nPlatform for Apache Flink of Alibaba Cloud.\n","authors":["Zefan Wang","Zichuan Liu","Yingying Zhang","Aoxiao Zhong","Jihong Wang","Fengbin Yin","Lunting Fan","Lingfei Wu","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2310.16340v3.pdf","comment":"Accepted by the 33rd ACM International Conference on Information and\n  Knowledge Management (CIKM 2024)"},{"id":"http://arxiv.org/abs/2408.00966v1","updated":"2024-08-02T01:22:46Z","published":"2024-08-02T01:22:46Z","title":"Automatic Extraction of Relationships among Motivations, Emotions and\n  Actions from Natural Language Texts","summary":"  We propose a new graph-based framework to reveal relationships among\nmotivations, emotions and actions explicitly given natural language texts. A\ndirected acyclic graph is designed to describe human's nature. Nurture beliefs\nare incorporated to connect outside events and the human's nature graph. No\nannotation resources are required due to the power of large language models.\nAmazon Fine Foods Reviews dataset is used as corpus and food-related\nmotivations are focused. Totally 92,990 relationship graphs are generated, of\nwhich 63% make logical sense. We make further analysis to investigate error\ntypes for optimization direction in future research.\n","authors":["Fei Yang"],"pdf_url":"https://arxiv.org/pdf/2408.00966v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00960v1","updated":"2024-08-02T00:24:22Z","published":"2024-08-02T00:24:22Z","title":"PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized\n  Language Prompting","summary":"  Understanding the nuances of a user's extensive interaction history is key to\nbuilding accurate and personalized natural language systems that can adapt to\nevolving user preferences. To address this, we introduce PERSOMA, Personalized\nSoft Prompt Adapter architecture. Unlike previous personalized prompting\nmethods for large language models, PERSOMA offers a novel approach to\nefficiently capture user history. It achieves this by resampling and\ncompressing interactions as free form text into expressive soft prompt\nembeddings, building upon recent research utilizing embedding representations\nas input for LLMs. We rigorously validate our approach by evaluating various\nadapter architectures, first-stage sampling strategies, parameter-efficient\ntuning techniques like LoRA, and other personalization methods. Our results\ndemonstrate PERSOMA's superior ability to handle large and complex user\nhistories compared to existing embedding-based and text-prompt-based\ntechniques.\n","authors":["Liam Hebert","Krishna Sayana","Ambarish Jash","Alexandros Karatzoglou","Sukhdeep Sodhi","Sumanth Doddapaneni","Yanli Cai","Dima Kuzmin"],"pdf_url":"https://arxiv.org/pdf/2408.00960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.11169v3","updated":"2024-08-02T23:09:32Z","published":"2023-05-18T17:58:08Z","title":"Emergent Representations of Program Semantics in Language Models Trained\n  on Programs","summary":"  We present evidence that language models (LMs) of code can learn to represent\nthe formal semantics of programs, despite being trained only to perform\nnext-token prediction. Specifically, we train a Transformer model on a\nsynthetic corpus of programs written in a domain-specific language for\nnavigating 2D grid world environments. Each program in the corpus is preceded\nby a (partial) specification in the form of several input-output grid world\nstates. Despite providing no further inductive biases, we find that a probing\nclassifier is able to extract increasingly accurate representations of the\nunobserved, intermediate grid world states from the LM hidden states over the\ncourse of training, suggesting the LM acquires an emergent ability to interpret\nprograms in the formal sense. We also develop a novel interventional baseline\nthat enables us to disambiguate what is represented by the LM as opposed to\nlearned by the probe. We anticipate that this technique may be generally\napplicable to a broad range of semantic probing experiments. In summary, this\npaper does not propose any new techniques for training LMs of code, but\ndevelops an experimental framework for and provides insights into the\nacquisition and representation of formal semantics in statistical models of\ncode. Our code is available at\nhttps://github.com/charlesjin/emergent-semantics.\n","authors":["Charles Jin","Martin Rinard"],"pdf_url":"https://arxiv.org/pdf/2305.11169v3.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2311.09675v3","updated":"2024-08-02T22:47:24Z","published":"2023-11-16T08:42:26Z","title":"Where Do People Tell Stories Online? Story Detection Across Online\n  Communities","summary":"  Story detection in online communities is a challenging task as stories are\nscattered across communities and interwoven with non-storytelling spans within\na single text. We address this challenge by building and releasing the\nStorySeeker toolkit, including a richly annotated dataset of 502 Reddit posts\nand comments, a detailed codebook adapted to the social media context, and\nmodels to predict storytelling at the document and span levels. Our dataset is\nsampled from hundreds of popular English-language Reddit communities ranging\nacross 33 topic categories, and it contains fine-grained expert annotations,\nincluding binary story labels, story spans, and event spans. We evaluate a\nrange of detection methods using our data, and we identify the distinctive\ntextual features of online storytelling, focusing on storytelling spans. We\nilluminate distributional characteristics of storytelling on a large\ncommunity-centric social media platform, and we also conduct a case study on\nr/ChangeMyView, where storytelling is used as one of many persuasive\nstrategies, illustrating that our data and models can be used for both inter-\nand intra-community research. Finally, we discuss implications of our tools and\nanalyses for narratology and the study of online communities.\n","authors":["Maria Antoniak","Joel Mire","Maarten Sap","Elliott Ash","Andrew Piper"],"pdf_url":"https://arxiv.org/pdf/2311.09675v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06416v2","updated":"2024-08-02T21:59:03Z","published":"2024-01-12T07:24:26Z","title":"Mission: Impossible Language Models","summary":"  Chomsky and others have very directly claimed that large language models\n(LLMs) are equally capable of learning languages that are possible and\nimpossible for humans to learn. However, there is very little published\nexperimental evidence to support such a claim. Here, we develop a set of\nsynthetic impossible languages of differing complexity, each designed by\nsystematically altering English data with unnatural word orders and grammar\nrules. These languages lie on an impossibility continuum: at one end are\nlanguages that are inherently impossible, such as random and irreversible\nshuffles of English words, and on the other, languages that may not be\nintuitively impossible but are often considered so in linguistics, particularly\nthose with rules based on counting word positions. We report on a wide range of\nevaluations to assess the capacity of GPT-2 small models to learn these\nuncontroversially impossible languages, and crucially, we perform these\nassessments at various stages throughout training to compare the learning\nprocess for each language. Our core finding is that GPT-2 struggles to learn\nimpossible languages when compared to English as a control, challenging the\ncore claim. More importantly, we hope our approach opens up a productive line\nof inquiry in which different LLM architectures are tested on a variety of\nimpossible languages in an effort to learn more about how LLMs can be used as\ntools for these cognitive and typological investigations.\n","authors":["Julie Kallini","Isabel Papadimitriou","Richard Futrell","Kyle Mahowald","Christopher Potts"],"pdf_url":"https://arxiv.org/pdf/2401.06416v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03646v2","updated":"2024-08-02T21:29:25Z","published":"2024-04-04T17:58:31Z","title":"Locating and Editing Factual Associations in Mamba","summary":"  We investigate the mechanisms of factual recall in the Mamba state space\nmodel. Our work is inspired by previous findings in autoregressive transformer\nlanguage models suggesting that their knowledge recall is localized to\nparticular modules at specific token locations; we therefore ask whether\nfactual recall in Mamba can be similarly localized. To investigate this, we\nconduct four lines of experiments on Mamba. First, we apply causal tracing or\ninterchange interventions to localize key components inside Mamba that are\nresponsible for recalling facts, revealing that specific components within\nmiddle layers show strong causal effects at the last token of the subject,\nwhile the causal effect of intervening on later layers is most pronounced at\nthe last token of the prompt, matching previous findings on autoregressive\ntransformers. Second, we show that rank-one model editing methods can\nsuccessfully insert facts at specific locations, again resembling findings on\ntransformer LMs. Third, we examine the linearity of Mamba's representations of\nfactual relations. Finally we adapt attention-knockout techniques to Mamba in\norder to dissect information flow during factual recall. We compare Mamba\ndirectly to a similar-sized autoregressive transformer LM and conclude that\ndespite significant differences in architectural approach, when it comes to\nfactual recall, the two architectures share many similarities.\n","authors":["Arnab Sen Sharma","David Atkinson","David Bau"],"pdf_url":"https://arxiv.org/pdf/2404.03646v2.pdf","comment":"18 pages, COLM-2024"},{"id":"http://arxiv.org/abs/2305.13088v2","updated":"2024-08-02T19:20:25Z","published":"2023-05-22T14:54:21Z","title":"Should We Attend More or Less? Modulating Attention for Fairness","summary":"  The advances in natural language processing (NLP) pose both opportunities and\nchallenges. While recent progress enables the development of high-performing\nmodels for a variety of tasks, it also poses the risk of models learning\nharmful biases from the data, such as gender stereotypes. In this work, we\ninvestigate the role of attention, a widely-used technique in current\nstate-of-the-art NLP models, in the propagation of social biases. Specifically,\nwe study the relationship between the entropy of the attention distribution and\nthe model's performance and fairness. We then propose a novel method for\nmodulating attention weights to improve model fairness after training. Since\nour method is only applied post-training and pre-inference, it is an\nintra-processing method and is, therefore, less computationally expensive than\nexisting in-processing and pre-processing approaches. Our results show an\nincrease in fairness and minimal performance loss on different text\nclassification and generation tasks using language models of varying sizes.\nWARNING: This work uses language that is offensive.\n","authors":["Abdelrahman Zayed","Goncalo Mordido","Samira Shabanian","Sarath Chandar"],"pdf_url":"https://arxiv.org/pdf/2305.13088v2.pdf","comment":"In Proceedings of Conference on Language Modeling (COLM 2024)"},{"id":"http://arxiv.org/abs/2408.01527v1","updated":"2024-08-02T18:40:10Z","published":"2024-08-02T18:40:10Z","title":"Analyzing LLMs' Capabilities to Establish Implicit User Sentiment of\n  Software Desirability","summary":"  This study explores the use of several LLMs for providing quantitative\nzero-shot sentiment analysis of implicit software desirability expressed by\nusers. The study provides scaled numerical sentiment analysis unlike other\nmethods that simply classify sentiment as positive, neutral, or negative.\nNumerical analysis provides deeper insights into the magnitude of sentiment, to\ndrive better decisions regarding product desirability.\n  Data is collected through the use of the Microsoft Product Desirability\nToolkit (PDT), a well-known qualitative user experience analysis tool. For\ninitial exploration, the PDT metric was given to users of ZORQ, a gamification\nsystem used in undergraduate computer science education. The PDT data collected\nwas fed through several LLMs (Claude Sonnet 3 and 3.5, GPT4, and GPT4o) and\nthrough a leading transfer learning technique, Twitter-Roberta-Base-Sentiment\n(TRBS), and through Vader, a leading sentiment analysis tool, for quantitative\nsentiment analysis. Each system was asked to evaluate the data in two ways,\nfirst by looking at the sentiment expressed in the PDT word/explanation pairs;\nand by looking at the sentiment expressed by the users in their grouped\nselection of five words and explanations, as a whole. Each LLM was also asked\nto provide its confidence (low, medium, high) in its sentiment score, along\nwith an explanation of why it selected the sentiment value.\n  All LLMs tested were able to statistically detect user sentiment from the\nusers' grouped data, whereas TRBS and Vader were not. The confidence and\nexplanation of confidence provided by the LLMs assisted in understanding the\nuser sentiment. This study adds to a deeper understanding of evaluating user\nexperiences, toward the goal of creating a universal tool that quantifies\nimplicit sentiment expressed.\n","authors":["Sherri Weitl-Harms","John D. Hastings","Jonah Lum"],"pdf_url":"https://arxiv.org/pdf/2408.01527v1.pdf","comment":"6 pages, 2 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.01505v1","updated":"2024-08-02T18:05:10Z","published":"2024-08-02T18:05:10Z","title":"MoDE: Effective Multi-task Parameter Efficient Fine-Tuning with a\n  Mixture of Dyadic Experts","summary":"  Parameter-efficient fine-tuning techniques like Low-Rank Adaptation (LoRA)\nhave revolutionized the adaptation of large language models (LLMs) to diverse\ntasks. Recent efforts have explored mixtures of LoRA modules for multi-task\nsettings. However, our analysis reveals redundancy in the down-projection\nmatrices of these architectures. This observation motivates our proposed\nmethod, Mixture of Dyadic Experts (MoDE), which introduces a novel design for\nefficient multi-task adaptation. This is done by sharing the down-projection\nmatrix across tasks and employing atomic rank-one adapters, coupled with\nrouters that allow more sophisticated task-level specialization. Our design\nallows for more fine-grained mixing, thereby increasing the model's ability to\njointly handle multiple tasks. We evaluate MoDE on the Supernatural\nInstructions (SNI) benchmark consisting of a diverse set of 700+ tasks and\ndemonstrate that it outperforms state-of-the-art multi-task parameter-efficient\nfine-tuning (PEFT) methods, without introducing additional parameters. Our\nfindings contribute to a deeper understanding of parameter efficiency in\nmulti-task LLM adaptation and provide a practical solution for deploying\nhigh-performing, lightweight models.\n","authors":["Lin Ning","Harsh Lara","Meiqi Guo","Abhinav Rastogi"],"pdf_url":"https://arxiv.org/pdf/2408.01505v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01384v1","updated":"2024-08-02T16:41:34Z","published":"2024-08-02T16:41:34Z","title":"NOLO: Navigate Only Look Once","summary":"  The in-context learning ability of Transformer models has brought new\npossibilities to visual navigation. In this paper, we focus on the video\nnavigation setting, where an in-context navigation policy needs to be learned\npurely from videos in an offline manner, without access to the actual\nenvironment. For this setting, we propose Navigate Only Look Once (NOLO), a\nmethod for learning a navigation policy that possesses the in-context ability\nand adapts to new scenes by taking corresponding context videos as input\nwithout finetuning or re-training. To enable learning from videos, we first\npropose a pseudo action labeling procedure using optical flow to recover the\naction label from egocentric videos. Then, offline reinforcement learning is\napplied to learn the navigation policy. Through extensive experiments on\ndifferent scenes, we show that our algorithm outperforms baselines by a large\nmargin, which demonstrates the in-context learning ability of the learned\npolicy.\n","authors":["Bohan Zhou","Jiangxing Wang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01372v1","updated":"2024-08-02T16:28:51Z","published":"2024-08-02T16:28:51Z","title":"Spatial-Spectral Morphological Mamba for Hyperspectral Image\n  Classification","summary":"  In recent years, Transformers have garnered significant attention for\nHyperspectral Image Classification (HSIC) due to their self-attention\nmechanism, which provides strong classification performance. However, these\nmodels face major challenges in computational efficiency, as their complexity\nincreases quadratically with the sequence length. The Mamba architecture,\nleveraging a State Space Model, offers a more efficient alternative to\nTransformers. This paper introduces the Spatial-Spectral Morphological Mamba\n(MorpMamba) model. In the MorpMamba model, a token generation module first\nconverts the Hyperspectral Image (HSI) patch into spatial-spectral tokens.\nThese tokens are then processed by a morphology block, which computes\nstructural and shape information using depthwise separable convolutional\noperations. The extracted information is enhanced in a feature enhancement\nmodule that adjusts the spatial and spectral tokens based on the center region\nof the HSI sample, allowing for effective information fusion within each block.\nSubsequently, the tokens are refined in a multi-head self-attention block to\nfurther improve the feature space. Finally, the combined information is fed\ninto the state space block for classification and the creation of the ground\ntruth map. Experiments on widely used Hyperspectral (HS) datasets demonstrate\nthat the MorpMamba model outperforms (parametric efficiency) both CNN and\nTransformer models.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Adil Mehmood Khan","Manual Mazzara","Salvatore Distenano"],"pdf_url":"https://arxiv.org/pdf/2408.01372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01370v1","updated":"2024-08-02T16:24:55Z","published":"2024-08-02T16:24:55Z","title":"EVIT: Event-based Visual-Inertial Tracking in Semi-Dense Maps Using\n  Windowed Nonlinear Optimization","summary":"  Event cameras are an interesting visual exteroceptive sensor that reacts to\nbrightness changes rather than integrating absolute image intensities. Owing to\nthis design, the sensor exhibits strong performance in situations of\nchallenging dynamics and illumination conditions. While event-based\nsimultaneous tracking and mapping remains a challenging problem, a number of\nrecent works have pointed out the sensor's suitability for prior map-based\ntracking. By making use of cross-modal registration paradigms, the camera's\nego-motion can be tracked across a large spectrum of illumination and dynamics\nconditions on top of accurate maps that have been created a priori by more\ntraditional sensors. The present paper follows up on a recently introduced\nevent-based geometric semi-dense tracking paradigm, and proposes the addition\nof inertial signals in order to robustify the estimation. More specifically,\nthe added signals provide strong cues for pose initialization as well as\nregularization during windowed, multi-frame tracking. As a result, the proposed\nframework achieves increased performance under challenging illumination\nconditions as well as a reduction of the rate at which intermediate event\nrepresentations need to be registered in order to maintain stable tracking\nacross highly dynamic sequences. Our evaluation focuses on a diverse set of\nreal world sequences and comprises a comparison of our proposed method against\na purely event-based alternative running at different rates.\n","authors":["Runze Yuan","Tao Liu","Zijia Dai","Yi-Fan Zuo","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.01370v1.pdf","comment":"8 pages, 5 figures, 3 tables, International Conference on Intelligent\n  Robots and Systems 2024"},{"id":"http://arxiv.org/abs/2408.01366v1","updated":"2024-08-02T16:20:56Z","published":"2024-08-02T16:20:56Z","title":"Play to the Score: Stage-Guided Dynamic Multi-Sensory Fusion for Robotic\n  Manipulation","summary":"  Humans possess a remarkable talent for flexibly alternating to different\nsenses when interacting with the environment. Picture a chef skillfully gauging\nthe timing of ingredient additions and controlling the heat according to the\ncolors, sounds, and aromas, seamlessly navigating through every stage of the\ncomplex cooking process. This ability is founded upon a thorough comprehension\nof task stages, as achieving the sub-goal within each stage can necessitate the\nutilization of different senses. In order to endow robots with similar ability,\nwe incorporate the task stages divided by sub-goals into the imitation learning\nprocess to accordingly guide dynamic multi-sensory fusion. We propose MS-Bot, a\nstage-guided dynamic multi-sensory fusion method with coarse-to-fine stage\nunderstanding, which dynamically adjusts the priority of modalities based on\nthe fine-grained state within the predicted current stage. We train a robot\nsystem equipped with visual, auditory, and tactile sensors to accomplish\nchallenging robotic manipulation tasks: pouring and peg insertion with keyway.\nExperimental results indicate that our approach enables more effective and\nexplainable dynamic fusion, aligning more closely with the human fusion process\nthan existing methods.\n","authors":["Ruoxuan Feng","Di Hu","Wenke Ma","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01366v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.17804v3","updated":"2024-08-02T16:17:35Z","published":"2023-11-29T16:54:25Z","title":"The Importance of Downstream Networks in Digital Pathology Foundation\n  Models","summary":"  Digital pathology has significantly advanced disease detection and\npathologist efficiency through the analysis of gigapixel whole-slide images\n(WSI). In this process, WSIs are first divided into patches, for which a\nfeature extractor model is applied to obtain feature vectors, which are\nsubsequently processed by an aggregation model to predict the respective WSI\nlabel. With the rapid evolution of representation learning, numerous new\nfeature extractor models, often termed foundational models, have emerged.\nTraditional evaluation methods rely on a static downstream aggregation model\nsetup, encompassing a fixed architecture and hyperparameters, a practice we\nidentify as potentially biasing the results. Our study uncovers a sensitivity\nof feature extractor models towards aggregation model configurations,\nindicating that performance comparability can be skewed based on the chosen\nconfigurations. By accounting for this sensitivity, we find that the\nperformance of many current feature extractor models is notably similar. We\nsupport this insight by evaluating seven feature extractor models across three\ndifferent datasets with 162 different aggregation model configurations. This\ncomprehensive approach provides a more nuanced understanding of the feature\nextractors' sensitivity to various aggregation model configurations, leading to\na fairer and more accurate assessment of new foundation models in digital\npathology.\n","authors":["Gustav Bredell","Marcel Fischer","Przemyslaw Szostak","Samaneh Abbasi-Sureshjani","Alvaro Gomariz"],"pdf_url":"https://arxiv.org/pdf/2311.17804v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01363v1","updated":"2024-08-02T16:15:25Z","published":"2024-08-02T16:15:25Z","title":"Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation","summary":"  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n","authors":["Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01363v1.pdf","comment":"Accepted by ACM SIGIR 2024 LLM4Eval Workshop:\n  https://llm4eval.github.io/papers"},{"id":"http://arxiv.org/abs/2404.13534v3","updated":"2024-08-02T16:14:46Z","published":"2024-04-21T05:09:56Z","title":"Motion-aware Latent Diffusion Models for Video Frame Interpolation","summary":"  With the advancement of AIGC, video frame interpolation (VFI) has become a\ncrucial component in existing video generation frameworks, attracting\nwidespread research interest. For the VFI task, the motion estimation between\nneighboring frames plays a crucial role in avoiding motion ambiguity. However,\nexisting VFI methods always struggle to accurately predict the motion\ninformation between consecutive frames, and this imprecise estimation leads to\nblurred and visually incoherent interpolated frames. In this paper, we propose\na novel diffusion framework, motion-aware latent diffusion models (MADiff),\nwhich is specifically designed for the VFI task. By incorporating motion priors\nbetween the conditional neighboring frames with the target interpolated frame\npredicted throughout the diffusion sampling procedure, MADiff progressively\nrefines the intermediate outcomes, culminating in generating both visually\nsmooth and realistic results. Extensive experiments conducted on benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance\nsignificantly outperforming existing approaches, especially under challenging\nscenarios involving dynamic textures with complex motion.\n","authors":["Zhilin Huang","Yijie Yu","Ling Yang","Chujun Qin","Bing Zheng","Xiawu Zheng","Zikun Zhou","Yaowei Wang","Wenming Yang"],"pdf_url":"https://arxiv.org/pdf/2404.13534v3.pdf","comment":"17 pages, 4 figures"},{"id":"http://arxiv.org/abs/2404.18849v2","updated":"2024-08-02T16:13:40Z","published":"2024-04-29T16:42:58Z","title":"MiPa: Mixed Patch Infrared-Visible Modality Agnostic Object Detection","summary":"  In real-world scenarios, using multiple modalities like visible (RGB) and\ninfrared (IR) can greatly improve the performance of a predictive task such as\nobject detection (OD). Multimodal learning is a common way to leverage these\nmodalities, where multiple modality-specific encoders and a fusion module are\nused to improve performance. In this paper, we tackle a different way to employ\nRGB and IR modalities, where only one modality or the other is observed by a\nsingle shared vision encoder. This realistic setting requires a lower memory\nfootprint and is more suitable for applications such as autonomous driving and\nsurveillance, which commonly rely on RGB and IR data. However, when learning a\nsingle encoder on multiple modalities, one modality can dominate the other,\nproducing uneven recognition results. This work investigates how to efficiently\nleverage RGB and IR modalities to train a common transformer-based OD vision\nencoder, while countering the effects of modality imbalance. For this, we\nintroduce a novel training technique to Mix Patches (MiPa) from the two\nmodalities, in conjunction with a patch-wise modality agnostic module, for\nlearning a common representation of both modalities. Our experiments show that\nMiPa can learn a representation to reach competitive results on traditional\nRGB/IR benchmarks while only requiring a single modality during inference. Our\ncode is available at: https://github.com/heitorrapela/MiPa.\n","authors":["Heitor R. Medeiros","David Latortue","Eric Granger","Marco Pedersoli"],"pdf_url":"https://arxiv.org/pdf/2404.18849v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01356v1","updated":"2024-08-02T16:09:06Z","published":"2024-08-02T16:09:06Z","title":"Balanced Residual Distillation Learning for 3D Point Cloud\n  Class-Incremental Semantic Segmentation","summary":"  Class-incremental learning (CIL) thrives due to its success in processing the\ninflux of information by learning from continuously added new classes while\npreventing catastrophic forgetting about the old ones. It is essential for the\nperformance breakthrough of CIL to effectively refine past knowledge from the\nbase model and balance it with new learning. However, such an issue has not yet\nbeen considered in current research. In this work, we explore the potential of\nCIL from these perspectives and propose a novel balanced residual distillation\nframework (BRD-CIL) to push the performance bar of CIL to a new higher level.\nSpecifically, BRD-CIL designs a residual distillation learning strategy, which\ncan dynamically expand the network structure to capture the residuals between\nthe base and target models, effectively refining the past knowledge.\nFurthermore, BRD-CIL designs a balanced pseudo-label learning strategy by\ngenerating a guidance mask to reduce the preference for old classes, ensuring\nbalanced learning from new and old classes. We apply the proposed BRD-CIL to a\nchallenging 3D point cloud semantic segmentation task where the data are\nunordered and unstructured. Extensive experimental results demonstrate that\nBRD-CIL sets a new benchmark with an outstanding balance capability in\nclass-biased scenarios.\n","authors":["Yuanzhi Su","Siyuan Chen","Yuan-Gen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01356v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01355v1","updated":"2024-08-02T16:07:15Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v1.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2304.11857v3","updated":"2024-08-02T15:43:33Z","published":"2023-04-24T07:12:50Z","title":"Accurate and Efficient Event-based Semantic Segmentation Using Adaptive\n  Spiking Encoder-Decoder Network","summary":"  Spiking neural networks (SNNs), known for their low-power, event-driven\ncomputation and intrinsic temporal dynamics, are emerging as promising\nsolutions for processing dynamic, asynchronous signals from event-based\nsensors. Despite their potential, SNNs face challenges in training and\narchitectural design, resulting in limited performance in challenging\nevent-based dense prediction tasks compared to artificial neural networks\n(ANNs). In this work, we develop an efficient spiking encoder-decoder network\n(SpikingEDN) for large-scale event-based semantic segmentation tasks. To\nenhance the learning efficiency from dynamic event streams, we harness the\nadaptive threshold which improves network accuracy, sparsity and robustness in\nstreaming inference. Moreover, we develop a dual-path Spiking\nSpatially-Adaptive Modulation module, which is specifically tailored to enhance\nthe representation of sparse events and multi-modal inputs, thereby\nconsiderably improving network performance. Our SpikingEDN attains a mean\nintersection over union (MIoU) of 72.57\\% on the DDD17 dataset and 58.32\\% on\nthe larger DSEC-Semantic dataset, showing competitive results to the\nstate-of-the-art ANNs while requiring substantially fewer computational\nresources. Our results shed light on the untapped potential of SNNs in\nevent-based vision applications. The source code will be made publicly\navailable.\n","authors":["Rui Zhang","Luziwei Leng","Kaiwei Che","Hu Zhang","Jie Cheng","Qinghai Guo","Jiangxing Liao","Ran Cheng"],"pdf_url":"https://arxiv.org/pdf/2304.11857v3.pdf","comment":"Accepted for publication in IEEE Transactions on Neural Networks and\n  Learning Systems"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v1","updated":"2024-08-02T15:32:42Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v1.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2404.12501v2","updated":"2024-08-02T15:28:19Z","published":"2024-04-18T20:43:33Z","title":"SPIdepth: Strengthened Pose Information for Self-supervised Monocular\n  Depth Estimation","summary":"  Self-supervised monocular depth estimation has garnered considerable\nattention for its applications in autonomous driving and robotics. While recent\nmethods have made strides in leveraging techniques like the Self Query Layer\n(SQL) to infer depth from motion, they often overlook the potential of\nstrengthening pose information. In this paper, we introduce SPIdepth, a novel\napproach that prioritizes enhancing the pose network for improved depth\nestimation. Building upon the foundation laid by SQL, SPIdepth emphasizes the\nimportance of pose information in capturing fine-grained scene structures. By\nenhancing the pose network's capabilities, SPIdepth achieves remarkable\nadvancements in scene understanding and depth estimation. Experimental results\non benchmark datasets such as KITTI, Cityscapes, and Make3D showcase SPIdepth's\nstate-of-the-art performance, surpassing previous methods by significant\nmargins. Specifically, SPIdepth tops the self-supervised KITTI benchmark.\nAdditionally, SPIdepth achieves the lowest AbsRel (0.029), SqRel (0.069), and\nRMSE (1.394) on KITTI, establishing new state-of-the-art results. On\nCityscapes, SPIdepth shows improvements over SQLdepth of 21.7% in AbsRel, 36.8%\nin SqRel, and 16.5% in RMSE, even without using motion masks. On Make3D,\nSPIdepth in zero-shot outperforms all other models. Remarkably, SPIdepth\nachieves these results using only a single image for inference, surpassing even\nmethods that utilize video sequences for inference, thus demonstrating its\nefficacy and efficiency in real-world applications. Our approach represents a\nsignificant leap forward in self-supervised monocular depth estimation,\nunderscoring the importance of strengthening pose information for advancing\nscene understanding in real-world applications.\n","authors":["Mykola Lavreniuk"],"pdf_url":"https://arxiv.org/pdf/2404.12501v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01322v1","updated":"2024-08-02T15:20:34Z","published":"2024-08-02T15:20:34Z","title":"A Robotics-Inspired Scanpath Model Reveals the Importance of Uncertainty\n  and Semantic Object Cues for Gaze Guidance in Dynamic Scenes","summary":"  How we perceive objects around us depends on what we actively attend to, yet\nour eye movements depend on the perceived objects. Still, object segmentation\nand gaze behavior are typically treated as two independent processes. Drawing\non an information processing pattern from robotics, we present a mechanistic\nmodel that simulates these processes for dynamic real-world scenes. Our\nimage-computable model uses the current scene segmentation for object-based\nsaccadic decision-making while using the foveated object to refine its scene\nsegmentation recursively. To model this refinement, we use a Bayesian filter,\nwhich also provides an uncertainty estimate for the segmentation that we use to\nguide active scene exploration. We demonstrate that this model closely\nresembles observers' free viewing behavior, measured by scanpath statistics,\nincluding foveation duration and saccade amplitude distributions used for\nparameter fitting and higher-level statistics not used for fitting. These\ninclude how object detections, inspections, and returns are balanced and a\ndelay of returning saccades without an explicit implementation of such temporal\ninhibition of return. Extensive simulations and ablation studies show that\nuncertainty promotes balanced exploration and that semantic object cues are\ncrucial to form the perceptual units used in object-based attention. Moreover,\nwe show how our model's modular design allows for extensions, such as\nincorporating saccadic momentum or pre-saccadic attention, to further align its\noutput with human scanpaths.\n","authors":["Vito Mengers","Nicolas Roth","Oliver Brock","Klaus Obermayer","Martin Rolfs"],"pdf_url":"https://arxiv.org/pdf/2408.01322v1.pdf","comment":"35+16 pages, 8+4 figures"},{"id":"http://arxiv.org/abs/2406.07867v2","updated":"2024-08-02T15:05:47Z","published":"2024-06-12T04:48:36Z","title":"Let's Go Real Talk: Spoken Dialogue Model for Face-to-Face Conversation","summary":"  In this paper, we introduce a novel Face-to-Face spoken dialogue model. It\nprocesses audio-visual speech from user input and generates audio-visual speech\nas the response, marking the initial step towards creating an avatar chatbot\nsystem without relying on intermediate text. To this end, we newly introduce\nMultiDialog, the first large-scale multimodal (i.e., audio and visual) spoken\ndialogue corpus containing 340 hours of approximately 9,000 dialogues, recorded\nbased on the open domain dialogue dataset, TopicalChat. The MultiDialog\ncontains parallel audio-visual recordings of conversation partners acting\naccording to the given script with emotion annotations, which we expect to open\nup research opportunities in multimodal synthesis. Our Face-to-Face spoken\ndialogue model incorporates a textually pretrained large language model and\nadapts it into the audio-visual spoken dialogue domain by incorporating\nspeech-text joint pretraining. Through extensive experiments, we validate the\neffectiveness of our model in facilitating a face-to-face conversation. Demo\nand data are available at https://multidialog.github.io and\nhttps://huggingface.co/datasets/IVLLab/MultiDialog, respectively.\n","authors":["Se Jin Park","Chae Won Kim","Hyeongseop Rha","Minsu Kim","Joanna Hong","Jeong Hun Yeo","Yong Man Ro"],"pdf_url":"https://arxiv.org/pdf/2406.07867v2.pdf","comment":"Accepted to ACL 2024 (Oral)"},{"id":"http://arxiv.org/abs/2408.01311v1","updated":"2024-08-02T15:01:29Z","published":"2024-08-02T15:01:29Z","title":"TopoNAS: Boosting Search Efficiency of Gradient-based NAS via\n  Topological Simplification","summary":"  Improving search efficiency serves as one of the crucial objectives of Neural\nArchitecture Search (NAS). However, many current approaches ignore the\nuniversality of the search strategy and fail to reduce the computational\nredundancy during the search process, especially in one-shot NAS architectures.\nBesides, current NAS methods show invalid reparameterization in non-linear\nsearch space, leading to poor efficiency in common search spaces like DARTS. In\nthis paper, we propose TopoNAS, a model-agnostic approach for gradient-based\none-shot NAS that significantly reduces searching time and memory usage by\ntopological simplification of searchable paths. Firstly, we model the\nnon-linearity in search spaces to reveal the parameterization difficulties. To\nimprove the search efficiency, we present a topological simplification method\nand iteratively apply module-sharing strategies to simplify the topological\nstructure of searchable paths. In addition, a kernel normalization technique is\nalso proposed to preserve the search accuracy. Experimental results on the\nNASBench201 benchmark with various search spaces demonstrate the effectiveness\nof our method. It proves the proposed TopoNAS enhances the performance of\nvarious architectures in terms of search efficiency while maintaining a high\nlevel of accuracy. The project page is available at\nhttps://xdedss.github.io/topo_simplification.\n","authors":["Danpei Zhao","Zhuoran Liu","Bo Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.01311v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01293v1","updated":"2024-08-02T14:28:49Z","published":"2024-08-02T14:28:49Z","title":"Underwater Object Detection Enhancement via Channel Stabilization","summary":"  The complex marine environment exacerbates the challenges of object detection\nmanifold. Marine trash endangers the aquatic ecosystem, presenting a persistent\nchallenge. Accurate detection of marine deposits is crucial for mitigating this\nharm. Our work addresses underwater object detection by enhancing image quality\nand evaluating detection methods. We use Detectron2's backbone with various\nbase models and configurations for this task.\n  We propose a novel channel stabilization technique alongside a simplified\nimage enhancement model to reduce haze and color cast in training images,\nimproving multi-scale object detection. Following image processing, we test\ndifferent Detectron2 backbones for optimal detection accuracy. Additionally, we\napply a sharpening filter with augmentation techniques to highlight object\nprofiles for easier recognition.\n  Results are demonstrated on the TrashCan Dataset, both instance and material\nversions. The best-performing backbone method incorporates our channel\nstabilization and augmentation techniques. We also compare our Detectron2\ndetection results with the Deformable Transformer. In the instance version of\nTrashCan 1.0, our method achieves a 9.53% absolute increase in average\nprecision for small objects and a 7% absolute gain in bounding box detection\ncompared to the baseline. The code will be available on Code:\nhttps://github.com/aliman80/Underwater-\nObject-Detection-via-Channel-Stablization\n","authors":["Muhammad Ali","Salman Khan"],"pdf_url":"https://arxiv.org/pdf/2408.01293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01292v1","updated":"2024-08-02T14:28:10Z","published":"2024-08-02T14:28:10Z","title":"3DPX: Progressive 2D-to-3D Oral Image Reconstruction with Hybrid MLP-CNN\n  Networks","summary":"  Panoramic X-ray (PX) is a prevalent modality in dental practice for its wide\navailability and low cost. However, as a 2D projection image, PX does not\ncontain 3D anatomical information, and therefore has limited use in dental\napplications that can benefit from 3D information, e.g., tooth angular\nmisa-lignment detection and classification. Reconstructing 3D structures\ndirectly from 2D PX has recently been explored to address limitations with\nexisting methods primarily reliant on Convolutional Neural Networks (CNNs) for\ndirect 2D-to-3D mapping. These methods, however, are unable to correctly infer\ndepth-axis spatial information. In addition, they are limited by the in-trinsic\nlocality of convolution operations, as the convolution kernels only capture the\ninformation of immediate neighborhood pixels. In this study, we propose a\nprogressive hybrid Multilayer Perceptron (MLP)-CNN pyra-mid network (3DPX) for\n2D-to-3D oral PX reconstruction. We introduce a progressive reconstruction\nstrategy, where 3D images are progressively re-constructed in the 3DPX with\nguidance imposed on the intermediate recon-struction result at each pyramid\nlevel. Further, motivated by the recent ad-vancement of MLPs that show promise\nin capturing fine-grained long-range dependency, our 3DPX integrates MLPs and\nCNNs to improve the semantic understanding during reconstruction. Extensive\nexperiments on two large datasets involving 464 studies demonstrate that our\n3DPX outperforms state-of-the-art 2D-to-3D oral reconstruction methods,\nincluding standalone MLP and transformers, in reconstruction quality, and also\nim-proves the performance of downstream angular misalignment classification\ntasks.\n","authors":["Xiaoshuang Li","Mingyuan Meng","Zimo Huang","Lei Bi","Eduardo Delamare","Dagan Feng","Bin Sheng","Jinman Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01292v1.pdf","comment":"accepted by MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.01291v1","updated":"2024-08-02T14:24:40Z","published":"2024-08-02T14:24:40Z","title":"TexGen: Text-Guided 3D Texture Generation with Multi-view Sampling and\n  Resampling","summary":"  Given a 3D mesh, we aim to synthesize 3D textures that correspond to\narbitrary textual descriptions. Current methods for generating and assembling\ntextures from sampled views often result in prominent seams or excessive\nsmoothing. To tackle these issues, we present TexGen, a novel multi-view\nsampling and resampling framework for texture generation leveraging a\npre-trained text-to-image diffusion model. For view consistent sampling, first\nof all we maintain a texture map in RGB space that is parameterized by the\ndenoising step and updated after each sampling step of the diffusion model to\nprogressively reduce the view discrepancy. An attention-guided multi-view\nsampling strategy is exploited to broadcast the appearance information across\nviews. To preserve texture details, we develop a noise resampling technique\nthat aids in the estimation of noise, generating inputs for subsequent\ndenoising steps, as directed by the text prompt and current texture map.\nThrough an extensive amount of qualitative and quantitative evaluations, we\ndemonstrate that our proposed method produces significantly better texture\nquality for diverse 3D objects with a high degree of view consistency and rich\nappearance details, outperforming current state-of-the-art methods.\nFurthermore, our proposed texture generation technique can also be applied to\ntexture editing while preserving the original identity. More experimental\nresults are available at https://dong-huo.github.io/TexGen/\n","authors":["Dong Huo","Zixin Guo","Xinxin Zuo","Zhihao Shi","Juwei Lu","Peng Dai","Songcen Xu","Li Cheng","Yee-Hong Yang"],"pdf_url":"https://arxiv.org/pdf/2408.01291v1.pdf","comment":"European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.01287v1","updated":"2024-08-02T14:19:34Z","published":"2024-08-02T14:19:34Z","title":"Deep Learning based Visually Rich Document Content Understanding: A\n  Survey","summary":"  Visually Rich Documents (VRDs) are essential in academia, finance, medical\nfields, and marketing due to their multimodal information content. Traditional\nmethods for extracting information from VRDs depend on expert knowledge and\nmanual labor, making them costly and inefficient. The advent of deep learning\nhas revolutionized this process, introducing models that leverage multimodal\ninformation vision, text, and layout along with pretraining tasks to develop\ncomprehensive document representations. These models have achieved\nstate-of-the-art performance across various downstream tasks, significantly\nenhancing the efficiency and accuracy of information extraction from VRDs. In\nresponse to the growing demands and rapid developments in Visually Rich\nDocument Understanding (VRDU), this paper provides a comprehensive review of\ndeep learning-based VRDU frameworks. We systematically survey and analyze\nexisting methods and benchmark datasets, categorizing them based on adopted\nstrategies and downstream tasks. Furthermore, we compare different techniques\nused in VRDU models, focusing on feature representation and fusion, model\narchitecture, and pretraining methods, while highlighting their strengths,\nlimitations, and appropriate scenarios. Finally, we identify emerging trends\nand challenges in VRDU, offering insights into future research directions and\npractical applications. This survey aims to provide a thorough understanding of\nVRDU advancements, benefiting both academic and industrial sectors.\n","authors":["Yihao Ding","Jean Lee","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2408.01287v1.pdf","comment":"Work in Progress"},{"id":"http://arxiv.org/abs/2408.01284v1","updated":"2024-08-02T14:10:20Z","published":"2024-08-02T14:10:20Z","title":"Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot\n  Learning: A General Framework","summary":"  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring\naccurate classification of both seen and unseen classes. Within this domain,\nAudio-visual GZSL emerges as an extremely exciting yet difficult task, given\nthe inclusion of both visual and acoustic features as multi-modal inputs.\nExisting efforts in this field mostly utilize either embedding-based or\ngenerative-based methods. However, generative training is difficult and\nunstable, while embedding-based methods often encounter domain shift problem.\nThus, we find it promising to integrate both methods into a unified framework\nto leverage their advantages while mitigating their respective disadvantages.\nOur study introduces a general framework employing out-of-distribution (OOD)\ndetection, aiming to harness the strengths of both approaches. We first employ\ngenerative adversarial networks to synthesize unseen features, enabling the\ntraining of an OOD detector alongside classifiers for seen and unseen classes.\nThis detector determines whether a test feature belongs to seen or unseen\nclasses, followed by classification utilizing separate classifiers for each\nfeature type. We test our framework on three popular audio-visual datasets and\nobserve a significant improvement comparing to existing state-of-the-art works.\nCodes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.\n","authors":["Liuyuan Wen"],"pdf_url":"https://arxiv.org/pdf/2408.01284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01276v1","updated":"2024-08-02T14:01:34Z","published":"2024-08-02T14:01:34Z","title":"Wave-Mamba: Wavelet State Space Model for Ultra-High-Definition\n  Low-Light Image Enhancement","summary":"  Ultra-high-definition (UHD) technology has attracted widespread attention due\nto its exceptional visual quality, but it also poses new challenges for\nlow-light image enhancement (LLIE) techniques. UHD images inherently possess\nhigh computational complexity, leading existing UHD LLIE methods to employ\nhigh-magnification downsampling to reduce computational costs, which in turn\nresults in information loss. The wavelet transform not only allows downsampling\nwithout loss of information, but also separates the image content from the\nnoise. It enables state space models (SSMs) to avoid being affected by noise\nwhen modeling long sequences, thus making full use of the long-sequence\nmodeling capability of SSMs. On this basis, we propose Wave-Mamba, a novel\napproach based on two pivotal insights derived from the wavelet domain: 1) most\nof the content information of an image exists in the low-frequency component,\nless in the high-frequency component. 2) The high-frequency component exerts a\nminimal influence on the outcomes of low-light enhancement. Specifically, to\nefficiently model global content information on UHD images, we proposed a\nlow-frequency state space block (LFSSBlock) by improving SSMs to focus on\nrestoring the information of low-frequency sub-bands. Moreover, we propose a\nhigh-frequency enhance block (HFEBlock) for high-frequency sub-band\ninformation, which uses the enhanced low-frequency information to correct the\nhigh-frequency information and effectively restore the correct high-frequency\ndetails. Through comprehensive evaluation, our method has demonstrated superior\nperformance, significantly outshining current leading techniques while\nmaintaining a more streamlined architecture. The code is available at\nhttps://github.com/AlexZou14/Wave-Mamba.\n","authors":["Wenbin Zou","Hongxia Gao","Weipeng Yang","Tongtong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01276v1.pdf","comment":"10 pages, 8 figures, ACMMM2024 accepted"},{"id":"http://arxiv.org/abs/2408.01269v1","updated":"2024-08-02T13:46:15Z","published":"2024-08-02T13:46:15Z","title":"A General Framework to Boost 3D GS Initialization for Text-to-3D\n  Generation by Lexical Richness","summary":"  Text-to-3D content creation has recently received much attention, especially\nwith the prevalence of 3D Gaussians Splatting. In general, GS-based methods\ncomprise two key stages: initialization and rendering optimization. To achieve\ninitialization, existing works directly apply random sphere initialization or\n3D diffusion models, e.g., Point-E, to derive the initial shapes. However, such\nstrategies suffer from two critical yet challenging problems: 1) the final\nshapes are still similar to the initial ones even after training; 2) shapes can\nbe produced only from simple texts, e.g., \"a dog\", not for lexically richer\ntexts, e.g., \"a dog is sitting on the top of the airplane\". To address these\nproblems, this paper proposes a novel general framework to boost the 3D GS\nInitialization for text-to-3D generation upon the lexical richness. Our key\nidea is to aggregate 3D Gaussians into spatially uniform voxels to represent\ncomplex shapes while enabling the spatial interaction among the 3D Gaussians\nand semantic interaction between Gaussians and texts. Specifically, we first\nconstruct a voxelized representation, where each voxel holds a 3D Gaussian with\nits position, scale, and rotation fixed while setting opacity as the sole\nfactor to determine a position's occupancy. We then design an initialization\nnetwork mainly consisting of two novel components: 1) Global Information\nPerception (GIP) block and 2) Gaussians-Text Fusion (GTF) block. Such a design\nenables each 3D Gaussian to assimilate the spatial information from other areas\nand semantic information from texts. Extensive experiments show the superiority\nof our framework of high-quality 3D GS initialization against the existing\nmethods, e.g., Shap-E, by taking lexically simple, medium, and hard texts.\nAlso, our framework can be seamlessly plugged into SoTA training frameworks,\ne.g., LucidDreamer, for semantically consistent text-to-3D generation.\n","authors":["Lutao Jiang","Hangyu Li","Lin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01269v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09004v2","updated":"2024-08-02T13:27:09Z","published":"2023-11-15T14:46:20Z","title":"Incremental Object-Based Novelty Detection with Feedback Loop","summary":"  Object-based Novelty Detection (ND) aims to identify unknown objects that do\nnot belong to classes seen during training by an object detection model. The\ntask is particularly crucial in real-world applications, as it allows to avoid\npotentially harmful behaviours, e.g. as in the case of object detection models\nadopted in a self-driving car or in an autonomous robot. Traditional approaches\nto ND focus on one time offline post processing of the pretrained object\ndetection output, leaving no possibility to improve the model robustness after\ntraining and discarding the abundant amount of out-of-distribution data\nencountered during deployment. In this work, we propose a novel framework for\nobject-based ND, assuming that human feedback can be requested on the predicted\noutput and later incorporated to refine the ND model without negatively\naffecting the main object detection performance. This refinement operation is\nrepeated whenever new feedback is available. To tackle this new formulation of\nthe problem for object detection, we propose a lightweight ND module attached\non top of a pre-trained object detection model, which is incrementally updated\nthrough a feedback loop. We also propose a new benchmark to evaluate methods on\nthis new setting and test extensively our ND approach against baselines,\nshowing increased robustness and a successful incorporation of the received\nfeedback.\n","authors":["Simone Caldarella","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2311.09004v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2207.09339v4","updated":"2024-08-02T13:13:47Z","published":"2022-07-19T15:49:35Z","title":"Vision Transformers: From Semantic Segmentation to Dense Prediction","summary":"  The emergence of vision transformers (ViTs) in image classification has\nshifted the methodologies for visual representation learning. In particular,\nViTs learn visual representation at full receptive field per layer across all\nthe image patches, in comparison to the increasing receptive fields of CNNs\nacross layers and other alternatives (e.g., large kernels and atrous\nconvolution). In this work, for the first time we explore the global context\nlearning potentials of ViTs for dense visual prediction (e.g., semantic\nsegmentation). Our motivation is that through learning global context at full\nreceptive field layer by layer, ViTs may capture stronger long-range dependency\ninformation, critical for dense prediction tasks. We first demonstrate that\nencoding an image as a sequence of patches, a vanilla ViT without local\nconvolution and resolution reduction can yield stronger visual representation\nfor semantic segmentation. For example, our model, termed as SEgmentation\nTRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the\ntest leaderboard on the day of submission) and performs competitively on\nCityscapes. However, the basic ViT architecture falls short in broader dense\nprediction applications, such as object detection and instance segmentation,\ndue to its lack of a pyramidal structure, high computational demand, and\ninsufficient local context. For tackling general dense visual prediction tasks\nin a cost-effective manner, we further formulate a family of Hierarchical\nLocal-Global (HLG) Transformers, characterized by local attention within\nwindows and global-attention across windows in a pyramidal architecture.\nExtensive experiments show that our methods achieve appealing performance on a\nvariety of dense prediction tasks (e.g., object detection and instance\nsegmentation and semantic segmentation) as well as image classification.\n","authors":["Li Zhang","Jiachen Lu","Sixiao Zheng","Xinxuan Zhao","Xiatian Zhu","Yanwei Fu","Tao Xiang","Jianfeng Feng","Philip H. S. Torr"],"pdf_url":"https://arxiv.org/pdf/2207.09339v4.pdf","comment":"Extended version of CVPR 2021 paper arXiv:2012.15840 Published on\n  International Journal of Computer Vision (2024)"},{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01233v1","updated":"2024-08-02T12:48:36Z","published":"2024-08-02T12:48:36Z","title":"CLIP4Sketch: Enhancing Sketch to Mugshot Matching through Dataset\n  Augmentation using Diffusion Models","summary":"  Forensic sketch-to-mugshot matching is a challenging task in face\nrecognition, primarily hindered by the scarcity of annotated forensic sketches\nand the modality gap between sketches and photographs. To address this, we\npropose CLIP4Sketch, a novel approach that leverages diffusion models to\ngenerate a large and diverse set of sketch images, which helps in enhancing the\nperformance of face recognition systems in sketch-to-mugshot matching. Our\nmethod utilizes Denoising Diffusion Probabilistic Models (DDPMs) to generate\nsketches with explicit control over identity and style. We combine CLIP and\nAdaface embeddings of a reference mugshot, along with textual descriptions of\nstyle, as the conditions to the diffusion model. We demonstrate the efficacy of\nour approach by generating a comprehensive dataset of sketches corresponding to\nmugshots and training a face recognition model on our synthetic data. Our\nresults show significant improvements in sketch-to-mugshot matching accuracy\nover training on an existing, limited amount of real face sketch data,\nvalidating the potential of diffusion models in enhancing the performance of\nface recognition systems across modalities. We also compare our dataset with\ndatasets generated using GAN-based methods to show its superiority.\n","authors":["Kushal Kumar Jain","Steve Grosz","Anoop M. Namboodiri","Anil K. Jain"],"pdf_url":"https://arxiv.org/pdf/2408.01233v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01231v1","updated":"2024-08-02T12:44:07Z","published":"2024-08-02T12:44:07Z","title":"WaveMamba: Spatial-Spectral Wavelet Mamba for Hyperspectral Image\n  Classification","summary":"  Hyperspectral Imaging (HSI) has proven to be a powerful tool for capturing\ndetailed spectral and spatial information across diverse applications. Despite\nthe advancements in Deep Learning (DL) and Transformer architectures for HSI\nClassification (HSIC), challenges such as computational efficiency and the need\nfor extensive labeled data persist. This paper introduces WaveMamba, a novel\napproach that integrates wavelet transformation with the Spatial-Spectral Mamba\narchitecture to enhance HSIC. WaveMamba captures both local texture patterns\nand global contextual relationships in an end-to-end trainable model. The\nWavelet-based enhanced features are then processed through the state-space\narchitecture to model spatial-spectral relationships and temporal dependencies.\nThe experimental results indicate that WaveMamba surpasses existing models,\nachieving an accuracy improvement of 4.5\\% on the University of Houston dataset\nand a 2.0\\% increase on the Pavia University dataset. These findings validate\nits effectiveness in addressing the complex data interactions inherent in HSIs.\n","authors":["Muhammad Ahmad","Muhammad Usama","Manual Mazzara"],"pdf_url":"https://arxiv.org/pdf/2408.01231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01228v1","updated":"2024-08-02T12:36:13Z","published":"2024-08-02T12:36:13Z","title":"The Phantom Menace: Unmasking Privacy Leakages in Vision-Language Models","summary":"  Vision-Language Models (VLMs) combine visual and textual understanding,\nrendering them well-suited for diverse tasks like generating image captions and\nanswering visual questions across various domains. However, these capabilities\nare built upon training on large amount of uncurated data crawled from the web.\nThe latter may include sensitive information that VLMs could memorize and leak,\nraising significant privacy concerns. In this paper, we assess whether these\nvulnerabilities exist, focusing on identity leakage. Our study leads to three\nkey findings: (i) VLMs leak identity information, even when the vision-language\nalignment and the fine-tuning use anonymized data; (ii) context has little\ninfluence on identity leakage; (iii) simple, widely used anonymization\ntechniques, like blurring, are not sufficient to address the problem. These\nfindings underscore the urgent need for robust privacy protection strategies\nwhen deploying VLMs. Ethical awareness and responsible development practices\nare essential to mitigate these risks.\n","authors":["Simone Caldarella","Massimiliano Mancini","Elisa Ricci","Rahaf Aljundi"],"pdf_url":"https://arxiv.org/pdf/2408.01228v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01224v1","updated":"2024-08-02T12:27:15Z","published":"2024-08-02T12:27:15Z","title":"Multi-head Spatial-Spectral Mamba for Hyperspectral Image Classification","summary":"  Spatial-Spectral Mamba (SSM) improves computational efficiency and captures\nlong-range dependencies, addressing Transformer limitations. However,\ntraditional Mamba models overlook rich spectral information in HSIs and\nstruggle with high dimensionality and sequential data. To address these issues,\nwe propose the SSM with multi-head self-attention and token enhancement\n(MHSSMamba). This model integrates spectral and spatial information by\nenhancing spectral tokens and using multi-head attention to capture complex\nrelationships between spectral bands and spatial locations. It also manages\nlong-range dependencies and the sequential nature of HSI data, preserving\ncontextual information across spectral bands. MHSSMamba achieved remarkable\nclassification accuracies of 97.62\\% on Pavia University, 96.92\\% on the\nUniversity of Houston, 96.85\\% on Salinas, and 99.49\\% on Wuhan-longKou\ndatasets.\n","authors":["Muhammad Ahmad","Muhammad Hassaan Farooq Butt","Muhammad Usama","Hamad Ahmed Altuwaijri","Manual Mazzara","Salvatore Distenano"],"pdf_url":"https://arxiv.org/pdf/2408.01224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01218v1","updated":"2024-08-02T12:16:07Z","published":"2024-08-02T12:16:07Z","title":"S2TD-Face: Reconstruct a Detailed 3D Face with Controllable Texture from\n  a Single Sketch","summary":"  3D textured face reconstruction from sketches applicable in many scenarios\nsuch as animation, 3D avatars, artistic design, missing people search, etc., is\na highly promising but underdeveloped research topic. On the one hand, the\nstylistic diversity of sketches leads to existing sketch-to-3D-face methods\nonly being able to handle pose-limited and realistically shaded sketches. On\nthe other hand, texture plays a vital role in representing facial appearance,\nyet sketches lack this information, necessitating additional texture control in\nthe reconstruction process. This paper proposes a novel method for\nreconstructing controllable textured and detailed 3D faces from sketches, named\nS2TD-Face. S2TD-Face introduces a two-stage geometry reconstruction framework\nthat directly reconstructs detailed geometry from the input sketch. To keep\ngeometry consistent with the delicate strokes of the sketch, we propose a novel\nsketch-to-geometry loss that ensures the reconstruction accurately fits the\ninput features like dimples and wrinkles. Our training strategies do not rely\non hard-to-obtain 3D face scanning data or labor-intensive hand-drawn sketches.\nFurthermore, S2TD-Face introduces a texture control module utilizing text\nprompts to select the most suitable textures from a library and seamlessly\nintegrate them into the geometry, resulting in a 3D detailed face with\ncontrollable texture. S2TD-Face surpasses existing state-of-the-art methods in\nextensive quantitative and qualitative experiments. Our project is available at\nhttps://github.com/wang-zidu/S2TD-Face .\n","authors":["Zidu Wang","Xiangyu Zhu","Jiang Yu","Tianshuo Zhang","Zhen Lei"],"pdf_url":"https://arxiv.org/pdf/2408.01218v1.pdf","comment":"ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.01191v1","updated":"2024-08-02T11:18:32Z","published":"2024-08-02T11:18:32Z","title":"A Weakly Supervised and Globally Explainable Learning Framework for\n  Brain Tumor Segmentation","summary":"  Machine-based brain tumor segmentation can help doctors make better\ndiagnoses. However, the complex structure of brain tumors and expensive\npixel-level annotations present challenges for automatic tumor segmentation. In\nthis paper, we propose a counterfactual generation framework that not only\nachieves exceptional brain tumor segmentation performance without the need for\npixel-level annotations, but also provides explainability. Our framework\neffectively separates class-related features from class-unrelated features of\nthe samples, and generate new samples that preserve identity features while\naltering class attributes by embedding different class-related features. We\nperform topological data analysis on the extracted class-related features and\nobtain a globally explainable manifold, and for each abnormal sample to be\nsegmented, a meaningful normal sample could be effectively generated with the\nguidance of the rule-based paths designed within the manifold for comparison\nfor identifying the tumor regions. We evaluate our proposed method on two\ndatasets, which demonstrates superior performance of brain tumor segmentation.\nThe code is available at https://github.com/xrt11/tumor-segmentation.\n","authors":["Ruitao Xie","Limai Jiang","Xiaoxi He","Yi Pan","Yunpeng Cai"],"pdf_url":"https://arxiv.org/pdf/2408.01191v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo"},{"id":"http://arxiv.org/abs/2408.01181v1","updated":"2024-08-02T11:03:22Z","published":"2024-08-02T11:03:22Z","title":"VAR-CLIP: Text-to-Image Generator with Visual Auto-Regressive Modeling","summary":"  VAR is a new generation paradigm that employs 'next-scale prediction' as\nopposed to 'next-token prediction'. This innovative transformation enables\nauto-regressive (AR) transformers to rapidly learn visual distributions and\nachieve robust generalization. However, the original VAR model is constrained\nto class-conditioned synthesis, relying solely on textual captions for\nguidance. In this paper, we introduce VAR-CLIP, a novel text-to-image model\nthat integrates Visual Auto-Regressive techniques with the capabilities of\nCLIP. The VAR-CLIP framework encodes captions into text embeddings, which are\nthen utilized as textual conditions for image generation. To facilitate\ntraining on extensive datasets, such as ImageNet, we have constructed a\nsubstantial image-text dataset leveraging BLIP2. Furthermore, we delve into the\nsignificance of word positioning within CLIP for the purpose of caption\nguidance. Extensive experiments confirm VAR-CLIP's proficiency in generating\nfantasy images with high fidelity, textual congruence, and aesthetic\nexcellence. Our project page are https://github.com/daixiangzi/VAR-CLIP\n","authors":["Qian Zhang","Xiangzi Dai","Ninghua Yang","Xiang An","Ziyong Feng","Xingyu Ren"],"pdf_url":"https://arxiv.org/pdf/2408.01181v1.pdf","comment":"total 10 pages, code:https://github.com/daixiangzi/VAR-CLIP"},{"id":"http://arxiv.org/abs/2407.19546v2","updated":"2024-08-02T10:53:37Z","published":"2024-07-28T17:38:21Z","title":"XLIP: Cross-modal Attention Masked Modelling for Medical Language-Image\n  Pre-Training","summary":"  Vision-and-language pretraining (VLP) in the medical field utilizes\ncontrastive learning on image-text pairs to achieve effective transfer across\ntasks. Yet, current VLP approaches with the masked modelling strategy face two\nchallenges when applied to the medical domain. First, current models struggle\nto accurately reconstruct key pathological features due to the scarcity of\nmedical data. Second, most methods only adopt either paired image-text or\nimage-only data, failing to exploit the combination of both paired and unpaired\ndata. To this end, this paper proposes a XLIP (Masked modelling for medical\nLanguage-Image Pre-training) framework to enhance pathological learning and\nfeature learning via unpaired data. First, we introduce the attention-masked\nimage modelling (AttMIM) and entity-driven masked language modelling module\n(EntMLM), which learns to reconstruct pathological visual and textual tokens\nvia multi-modal feature interaction, thus improving medical-enhanced features.\nThe AttMIM module masks a portion of the image features that are highly\nresponsive to textual features. This allows XLIP to improve the reconstruction\nof highly similar image data in medicine efficiency. Second, our XLIP\ncapitalizes unpaired data to enhance multimodal learning by introducing\ndisease-kind prompts. The experimental results show that XLIP achieves SOTA for\nzero-shot and fine-tuning classification performance on five datasets. Our code\nwill be available at https://github.com/White65534/XLIP\n","authors":["Biao Wu","Yutong Xie","Zeyu Zhang","Minh Hieu Phan","Qi Chen","Ling Chen","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2407.19546v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01167v1","updated":"2024-08-02T10:34:23Z","published":"2024-08-02T10:34:23Z","title":"Rethinking Pre-trained Feature Extractor Selection in Multiple Instance\n  Learning for Whole Slide Image Classification","summary":"  Multiple instance learning (MIL) has become a preferred method for\nclassifying gigapixel whole slide images (WSIs), without requiring patch label\nannotation. The focus of the current MIL research stream is on the\nembedding-based MIL approach, which involves extracting feature vectors from\npatches using a pre-trained feature extractor. These feature vectors are then\nfed into an MIL aggregator for slide-level prediction. Despite prior research\nsuggestions on enhancing the most commonly used ResNet50 supervised model\npre-trained on ImageNet-1K, there remains a lack of clear guidance on selecting\nthe optimal feature extractor to maximize WSI performance. This study aims at\naddressing this gap by examining MIL feature extractors across three\ndimensions: pre-training dataset, backbone model, and pre-training method.\nExtensive experiments were carried out on the two public WSI datasets\n(TCGA-NSCLC and Camelyon16) using four SOTA MIL models. The main findings\nindicate the following: 1) Performance significantly improves with larger and\nmore varied pre-training datasets in both CNN and Transformer backbones. 2)\n`Modern and deeper' backbones greatly outperform `standard' backbones (ResNet\nand ViT), with performance improvements more guaranteed in Transformer-based\nbackbones. 3) The choice of self-supervised learning (SSL) method is crucial,\nwith the most significant benefits observed when applied to the Transformer\n(ViT) backbone. The study findings have practical implications, including\ndesigning more effective pathological foundation models. Our code is available\nat: https://anonymous.4open.science/r/MIL-Feature-Extractor-Selection\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01167v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.01162v1","updated":"2024-08-02T10:24:35Z","published":"2024-08-02T10:24:35Z","title":"PreMix: Boosting Multiple Instance Learning in Digital Histopathology\n  through Pre-training with Intra-Batch Slide Mixing","summary":"  The classification of gigapixel-sized whole slide images (WSIs), digital\nrepresentations of histological slides obtained via a high-resolution scanner,\nfaces significant challenges associated with the meticulous and time-consuming\nnature of fine-grained labeling. While weakly-supervised multiple instance\nlearning (MIL) has emerged as a promising approach, current MIL methods are\nconstrained by their limited ability to leverage the wealth of information\nembedded within unlabeled WSIs. This limitation often necessitates training MIL\nfeature aggregators from scratch after the feature extraction process,\nhindering efficiency and accuracy. PreMix extends the general MIL framework by\npre-training the MIL aggregator with an intra-batch slide mixing approach.\nSpecifically, PreMix incorporates Barlow Twins Slide Mixing during\npre-training, enhancing its ability to handle diverse WSI sizes and maximizing\nthe utility of unlabeled WSIs. Combined with Mixup and Manifold Mixup during\nfine-tuning, PreMix achieves a mean of 4.7% performance improvement over the\nbaseline MIL framework, the hierarchical image pyramid transformer (HIPT), on\nthe Camelyon16 dataset. The observed improvement across a range of active\nlearning acquisition functions and WSI-labeled training budgets highlights the\nframework's adaptability to diverse datasets and varying resource constraints.\nUltimately, PreMix paves the way for more efficient and accurate WSI\nclassification under limited WSI-labeled datasets, encouraging the broader\nadoption of unlabeled WSI data in histopathological research. The code is\navailable at https://anonymous.4open.science/r/PreMix\n","authors":["Bryan Wong","Mun Yong Yi"],"pdf_url":"https://arxiv.org/pdf/2408.01162v1.pdf","comment":"15 pages"},{"id":"http://arxiv.org/abs/2408.01159v1","updated":"2024-08-02T10:21:10Z","published":"2024-08-02T10:21:10Z","title":"Robust Curve Detection in Volumetric Medical Imaging via Attraction\n  Field","summary":"  Understanding body part geometry is crucial for precise medical diagnostics.\nCurves effectively describe anatomical structures and are widely used in\nmedical imaging applications related to cardiovascular, respiratory, and\nskeletal diseases. Traditional curve detection methods are often task-specific,\nrelying heavily on domain-specific features, limiting their broader\napplicability. This paper introduces a novel approach for detecting\nnon-branching curves, which does not require prior knowledge of the object's\norientation, shape, or position. Our method uses neural networks to predict (1)\nan attraction field, which offers subpixel accuracy, and (2) a closeness map,\nwhich limits the region of interest and essentially eliminates outliers far\nfrom the desired curve. We tested our curve detector on several clinically\nrelevant tasks with diverse morphologies and achieved impressive subpixel-level\naccuracy results that surpass existing methods, highlighting its versatility\nand robustness. Additionally, to support further advancements in this field, we\nprovide our private annotations of aortic centerlines and masks, which can\nserve as a benchmark for future research. The dataset can be found at\nhttps://github.com/neuro-ml/curve-detection.\n","authors":["Farukh Yaushev","Daria Nogina","Valentin Samokhin","Mariya Dugova","Ekaterina Petrash","Dmitry Sevryukov","Mikhail Belyaev","Maxim Pisov"],"pdf_url":"https://arxiv.org/pdf/2408.01159v1.pdf","comment":"Accepted to ShapeMI MICCAI 2024"},{"id":"http://arxiv.org/abs/2403.15559v2","updated":"2024-08-02T10:19:34Z","published":"2024-03-22T18:28:04Z","title":"An Optimization Framework to Enforce Multi-View Consistency for\n  Texturing 3D Meshes","summary":"  A fundamental problem in the texturing of 3D meshes using pre-trained\ntext-to-image models is to ensure multi-view consistency. State-of-the-art\napproaches typically use diffusion models to aggregate multi-view inputs, where\ncommon issues are the blurriness caused by the averaging operation in the\naggregation step or inconsistencies in local features. This paper introduces an\noptimization framework that proceeds in four stages to achieve multi-view\nconsistency. Specifically, the first stage generates an over-complete set of 2D\ntextures from a predefined set of viewpoints using an MV-consistent diffusion\nprocess. The second stage selects a subset of views that are mutually\nconsistent while covering the underlying 3D model. We show how to achieve this\ngoal by solving semi-definite programs. The third stage performs non-rigid\nalignment to align the selected views across overlapping regions. The fourth\nstage solves an MRF problem to associate each mesh face with a selected view.\nIn particular, the third and fourth stages are iterated, with the cuts obtained\nin the fourth stage encouraging non-rigid alignment in the third stage to focus\non regions close to the cuts. Experimental results show that our approach\nsignificantly outperforms baseline approaches both qualitatively and\nquantitatively. Project page: https://aigc3d.github.io/ConsistenTex.\n","authors":["Zhengyi Zhao","Chen Song","Xiaodong Gu","Yuan Dong","Qi Zuo","Weihao Yuan","Liefeng Bo","Zilong Dong","Qixing Huang"],"pdf_url":"https://arxiv.org/pdf/2403.15559v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19394v3","updated":"2024-08-02T10:05:03Z","published":"2024-07-28T04:23:40Z","title":"Depth-Wise Convolutions in Vision Transformers for Efficient Training on\n  Small Datasets","summary":"  The Vision Transformer (ViT) leverages the Transformer's encoder to capture\nglobal information by dividing images into patches and achieves superior\nperformance across various computer vision tasks. However, the self-attention\nmechanism of ViT captures the global context from the outset, overlooking the\ninherent relationships between neighboring pixels in images or videos.\nTransformers mainly focus on global information while ignoring the fine-grained\nlocal details. Consequently, ViT lacks inductive bias during image or video\ndataset training. In contrast, convolutional neural networks (CNNs), with their\nreliance on local filters, possess an inherent inductive bias, making them more\nefficient and quicker to converge than ViT with less data. In this paper, we\npresent a lightweight Depth-Wise Convolution module as a shortcut in ViT\nmodels, bypassing entire Transformer blocks to ensure the models capture both\nlocal and global information with minimal overhead. Additionally, we introduce\ntwo architecture variants, allowing the Depth-Wise Convolution modules to be\napplied to multiple Transformer blocks for parameter savings, and incorporating\nindependent parallel Depth-Wise Convolution modules with different kernels to\nenhance the acquisition of local information. The proposed approach\nsignificantly boosts the performance of ViT models on image classification,\nobject detection and instance segmentation by a large margin, especially on\nsmall datasets, as evaluated on CIFAR-10, CIFAR-100, Tiny-ImageNet and ImageNet\nfor image classification, and COCO for object detection and instance\nsegmentation. The source code can be accessed at\nhttps://github.com/ZTX-100/Efficient_ViT_with_DW.\n","authors":["Tianxiao Zhang","Wenju Xu","Bo Luo","Guanghui Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19394v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.06542v2","updated":"2024-08-02T10:04:09Z","published":"2024-01-12T12:35:45Z","title":"Robustness-Aware 3D Object Detection in Autonomous Driving: A Review and\n  Outlook","summary":"  In the realm of modern autonomous driving, the perception system is\nindispensable for accurately assessing the state of the surrounding\nenvironment, thereby enabling informed prediction and planning. The key step to\nthis system is related to 3D object detection that utilizes vehicle-mounted\nsensors such as LiDAR and cameras to identify the size, the category, and the\nlocation of nearby objects. Despite the surge in 3D object detection methods\naimed at enhancing detection precision and efficiency, there is a gap in the\nliterature that systematically examines their resilience against environmental\nvariations, noise, and weather changes. This study emphasizes the importance of\nrobustness, alongside accuracy and latency, in evaluating perception systems\nunder practical scenarios. Our work presents an extensive survey of\ncamera-only, LiDAR-only, and multi-modal 3D object detection algorithms,\nthoroughly evaluating their trade-off between accuracy, latency, and\nrobustness, particularly on datasets like KITTI-C and nuScenes-C to ensure fair\ncomparisons. Among these, multi-modal 3D detection approaches exhibit superior\nrobustness, and a novel taxonomy is introduced to reorganize the literature for\nenhanced clarity. This survey aims to offer a more practical perspective on the\ncurrent capabilities and the constraints of 3D object detection algorithms in\nreal-world applications, thus steering future research towards\nrobustness-centric advancements.\n","authors":["Ziying Song","Lin Liu","Feiyang Jia","Yadan Luo","Guoxin Zhang","Lei Yang","Li Wang","Caiyan Jia"],"pdf_url":"https://arxiv.org/pdf/2401.06542v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.08372v3","updated":"2024-08-02T09:56:14Z","published":"2023-12-13T18:59:58Z","title":"SAM-guided Graph Cut for 3D Instance Segmentation","summary":"  This paper addresses the challenge of 3D instance segmentation by\nsimultaneously leveraging 3D geometric and multi-view image information. Many\nprevious works have applied deep learning techniques to 3D point clouds for\ninstance segmentation. However, these methods often failed to generalize to\nvarious types of scenes due to the scarcity and low-diversity of labeled 3D\npoint cloud data. Some recent works have attempted to lift 2D instance\nsegmentations to 3D within a bottom-up framework. The inconsistency in 2D\ninstance segmentations among views can substantially degrade the performance of\n3D segmentation. In this work, we introduce a novel 3D-to-2D query framework to\neffectively exploit 2D segmentation models for 3D instance segmentation.\nSpecifically, we pre-segment the scene into several superpoints in 3D,\nformulating the task into a graph cut problem. The superpoint graph is\nconstructed based on 2D segmentation models, where node features are obtained\nfrom multi-view image features and edge weights are computed based on\nmulti-view segmentation results, enabling the better generalization ability. To\nprocess the graph, we train a graph neural network using pseudo 3D labels from\n2D segmentation models. Experimental results on the ScanNet, ScanNet++ and\nKITTI-360 datasets demonstrate that our method achieves robust segmentation\nperformance and can generalize across different types of scenes. Our project\npage is available at https://zju3dv.github.io/sam_graph.\n","authors":["Haoyu Guo","He Zhu","Sida Peng","Yuang Wang","Yujun Shen","Ruizhen Hu","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2312.08372v3.pdf","comment":"Project page: https://zju3dv.github.io/sam_graph"},{"id":"http://arxiv.org/abs/2408.01139v1","updated":"2024-08-02T09:35:06Z","published":"2024-08-02T09:35:06Z","title":"Interpreting Global Perturbation Robustness of Image Models using\n  Axiomatic Spectral Importance Decomposition","summary":"  Perturbation robustness evaluates the vulnerabilities of models, arising from\na variety of perturbations, such as data corruptions and adversarial attacks.\nUnderstanding the mechanisms of perturbation robustness is critical for global\ninterpretability. We present a model-agnostic, global mechanistic\ninterpretability method to interpret the perturbation robustness of image\nmodels. This research is motivated by two key aspects. First, previous global\ninterpretability works, in tandem with robustness benchmarks, e.g. mean\ncorruption error (mCE), are not designed to directly interpret the mechanisms\nof perturbation robustness within image models. Second, we notice that the\nspectral signal-to-noise ratios (SNR) of perturbed natural images exponentially\ndecay over the frequency. This power-law-like decay implies that: Low-frequency\nsignals are generally more robust than high-frequency signals -- yet high\nclassification accuracy can not be achieved by low-frequency signals alone. By\napplying Shapley value theory, our method axiomatically quantifies the\npredictive powers of robust features and non-robust features within an\ninformation theory framework. Our method, dubbed as \\textbf{I-ASIDE}\n(\\textbf{I}mage \\textbf{A}xiomatic \\textbf{S}pectral \\textbf{I}mportance\n\\textbf{D}ecomposition \\textbf{E}xplanation), provides a unique insight into\nmodel robustness mechanisms. We conduct extensive experiments over a variety of\nvision models pre-trained on ImageNet to show that \\textbf{I-ASIDE} can not\nonly \\textbf{measure} the perturbation robustness but also \\textbf{provide\ninterpretations} of its mechanisms.\n","authors":["Risn Luo","James McDermott","Colm O'Riordan"],"pdf_url":"https://arxiv.org/pdf/2408.01139v1.pdf","comment":"Accepted by Transactions on Machine Learning Research (TMLR 2024)"},{"id":"http://arxiv.org/abs/2408.01137v1","updated":"2024-08-02T09:31:21Z","published":"2024-08-02T09:31:21Z","title":"PGNeXt: High-Resolution Salient Object Detection via Pyramid Grafting\n  Network","summary":"  We present an advanced study on more challenging high-resolution salient\nobject detection (HRSOD) from both dataset and network framework perspectives.\nTo compensate for the lack of HRSOD dataset, we thoughtfully collect a\nlarge-scale high resolution salient object detection dataset, called UHRSD,\ncontaining 5,920 images from real-world complex scenarios at 4K-8K resolutions.\nAll the images are finely annotated in pixel-level, far exceeding previous\nlow-resolution SOD datasets. Aiming at overcoming the contradiction between the\nsampling depth and the receptive field size in the past methods, we propose a\nnovel one-stage framework for HR-SOD task using pyramid grafting mechanism. In\ngeneral, transformer-based and CNN-based backbones are adopted to extract\nfeatures from different resolution images independently and then these features\nare grafted from transformer branch to CNN branch. An attention-based\nCross-Model Grafting Module (CMGM) is proposed to enable CNN branch to combine\nbroken detailed information more holistically, guided by different source\nfeature during decoding process. Moreover, we design an Attention Guided Loss\n(AGL) to explicitly supervise the attention matrix generated by CMGM to help\nthe network better interact with the attention from different branches.\nComprehensive experiments on UHRSD and widely-used SOD datasets demonstrate\nthat our method can simultaneously locate salient object and preserve rich\ndetails, outperforming state-of-the-art methods. To verify the generalization\nability of the proposed framework, we apply it to the camouflaged object\ndetection (COD) task. Notably, our method performs superior to most\nstate-of-the-art COD methods without bells and whistles.\n","authors":["Changqun Xia","Chenxi Xie","Zhentao He","Tianshu Yu","Jia Li"],"pdf_url":"https://arxiv.org/pdf/2408.01137v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01126v1","updated":"2024-08-02T09:07:31Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["Furkan Aykut Sarikamis","Abdullah Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v1.pdf","comment":"8 pages, 3 page ref, 5 figures, 3DV submission"},{"id":"http://arxiv.org/abs/2408.01120v1","updated":"2024-08-02T09:01:05Z","published":"2024-08-02T09:01:05Z","title":"An Efficient and Effective Transformer Decoder-Based Framework for\n  Multi-Task Visual Grounding","summary":"  Most advanced visual grounding methods rely on Transformers for\nvisual-linguistic feature fusion. However, these Transformer-based approaches\nencounter a significant drawback: the computational costs escalate\nquadratically due to the self-attention mechanism in the Transformer Encoder,\nparticularly when dealing with high-resolution images or long context\nsentences. This quadratic increase in computational burden restricts the\napplicability of visual grounding to more intricate scenes, such as\nconversation-based reasoning segmentation, which involves lengthy language\nexpressions. In this paper, we propose an efficient and effective multi-task\nvisual grounding (EEVG) framework based on Transformer Decoder to address this\nissue, which reduces the cost in both language and visual aspects. In the\nlanguage aspect, we employ the Transformer Decoder to fuse visual and\nlinguistic features, where linguistic features are input as memory and visual\nfeatures as queries. This allows fusion to scale linearly with language\nexpression length. In the visual aspect, we introduce a parameter-free approach\nto reduce computation by eliminating background visual tokens based on\nattention scores. We then design a light mask head to directly predict\nsegmentation masks from the remaining sparse feature maps. Extensive results\nand ablation studies on benchmarks demonstrate the efficiency and effectiveness\nof our approach. Code is available in https://github.com/chenwei746/EEVG.\n","authors":["Wei Chen","Long Chen","Yu Wu"],"pdf_url":"https://arxiv.org/pdf/2408.01120v1.pdf","comment":"21pages, 10 figures, 9 tables. Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.01099v1","updated":"2024-08-02T08:24:05Z","published":"2024-08-02T08:24:05Z","title":"Contribution-based Low-Rank Adaptation with Pre-training Model for Real\n  Image Restoration","summary":"  Recently, pre-trained model and efficient parameter tuning have achieved\nremarkable success in natural language processing and high-level computer\nvision with the aid of masked modeling and prompt tuning. In low-level computer\nvision, however, there have been limited investigations on pre-trained models\nand even efficient fine-tuning strategy has not yet been explored despite its\nimportance and benefit in various real-world tasks such as alleviating memory\ninflation issue when integrating new tasks on AI edge devices. Here, we propose\na novel efficient parameter tuning approach dubbed contribution-based low-rank\nadaptation (CoLoRA) for multiple image restorations along with effective\npre-training method with random order degradations (PROD). Unlike prior arts\nthat tune all network parameters, our CoLoRA effectively fine-tunes small\namount of parameters by leveraging LoRA (low-rank adaptation) for each new\nvision task with our contribution-based method to adaptively determine layer by\nlayer capacity for that task to yield comparable performance to full tuning.\nFurthermore, our PROD strategy allows to extend the capability of pre-trained\nmodels with improved performance as well as robustness to bridge synthetic\npre-training and real-world fine-tuning. Our CoLoRA with PROD has demonstrated\nits superior performance in various image restoration tasks across diverse\ndegradation types on both synthetic and real-world datasets for known and novel\ntasks.\n","authors":["Donwon Park","Hayeon Kim","Se Young Chun"],"pdf_url":"https://arxiv.org/pdf/2408.01099v1.pdf","comment":"33 pages, 15 figures, for homepage see this url :\n  https://janeyeon.github.io/colora/"},{"id":"http://arxiv.org/abs/2405.04788v3","updated":"2024-08-02T08:22:12Z","published":"2024-05-08T03:43:58Z","title":"SemiCD-VL: Visual-Language Model Guidance Makes Better Semi-supervised\n  Change Detector","summary":"  Change Detection (CD) aims to identify pixels with semantic changes between\nimages. However, annotating massive numbers of pixel-level images is\nlabor-intensive and costly, especially for multi-temporal images, which require\npixel-wise comparisons by human experts. Considering the excellent performance\nof visual language models (VLMs) for zero-shot, open-vocabulary, etc. with\nprompt-based reasoning, it is promising to utilize VLMs to make better CD under\nlimited labeled data. In this paper, we propose a VLM guidance-based\nsemi-supervised CD method, namely SemiCD-VL. The insight of SemiCD-VL is to\nsynthesize free change labels using VLMs to provide additional supervision\nsignals for unlabeled data. However, almost all current VLMs are designed for\nsingle-temporal images and cannot be directly applied to bi- or multi-temporal\nimages. Motivated by this, we first propose a VLM-based mixed change event\ngeneration (CEG) strategy to yield pseudo labels for unlabeled CD data. Since\nthe additional supervised signals provided by these VLM-driven pseudo labels\nmay conflict with the pseudo labels from the consistency regularization\nparadigm (e.g. FixMatch), we propose the dual projection head for de-entangling\ndifferent signal sources. Further, we explicitly decouple the bi-temporal\nimages semantic representation through two auxiliary segmentation decoders,\nwhich are also guided by VLM. Finally, to make the model more adequately\ncapture change representations, we introduce metric-aware supervision by\nfeature-level contrastive loss in auxiliary branches. Extensive experiments\nshow the advantage of SemiCD-VL. For instance, SemiCD-VL improves the FixMatch\nbaseline by +5.3 IoU on WHU-CD and by +2.4 IoU on LEVIR-CD with 5% labels. In\naddition, our CEG strategy, in an un-supervised manner, can achieve performance\nfar superior to state-of-the-art un-supervised CD methods.\n","authors":["Kaiyu Li","Xiangyong Cao","Yupeng Deng","Junmin Liu","Deyu Meng","Zhi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.04788v3.pdf","comment":"13 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01089v1","updated":"2024-08-02T08:08:56Z","published":"2024-08-02T08:08:56Z","title":"Prototypical Partial Optimal Transport for Universal Domain Adaptation","summary":"  Universal domain adaptation (UniDA) aims to transfer knowledge from a labeled\nsource domain to an unlabeled target domain without requiring the same label\nsets of both domains. The existence of domain and category shift makes the task\nchallenging and requires us to distinguish \"known\" samples (i.e., samples whose\nlabels exist in both domains) and \"unknown\" samples (i.e., samples whose labels\nexist in only one domain) in both domains before reducing the domain gap. In\nthis paper, we consider the problem from the point of view of distribution\nmatching which we only need to align two distributions partially. A novel\napproach, dubbed mini-batch Prototypical Partial Optimal Transport (m-PPOT), is\nproposed to conduct partial distribution alignment for UniDA. In training\nphase, besides minimizing m-PPOT, we also leverage the transport plan of m-PPOT\nto reweight source prototypes and target samples, and design reweighted entropy\nloss and reweighted cross-entropy loss to distinguish \"known\" and \"unknown\"\nsamples. Experiments on four benchmarks show that our method outperforms the\nprevious state-of-the-art UniDA methods.\n","authors":["Yucheng Yang","Xiang Gu","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01085v1","updated":"2024-08-02T08:06:12Z","published":"2024-08-02T08:06:12Z","title":"Effect of Fog Particle Size Distribution on 3D Object Detection Under\n  Adverse Weather Conditions","summary":"  LiDAR-based sensors employing optical spectrum signals play a vital role in\nproviding significant information about the target objects in autonomous\ndriving vehicle systems. However, the presence of fog in the atmosphere\nseverely degrades the overall system's performance. This manuscript analyzes\nthe role of fog particle size distributions in 3D object detection under\nadverse weather conditions. We utilise Mie theory and meteorological optical\nrange (MOR) to calculate the attenuation and backscattering coefficient values\nfor point cloud generation and analyze the overall system's accuracy in Car,\nCyclist, and Pedestrian case scenarios under easy, medium and hard detection\ndifficulties. Gamma and Junge (Power-Law) distributions are employed to\nmathematically model the fog particle size distribution under strong and\nmoderate advection fog environments. Subsequently, we modified the KITTI\ndataset based on the backscattering coefficient values and trained it on the\nPV-RCNN++ deep neural network model for Car, Cyclist, and Pedestrian cases\nunder different detection difficulties. The result analysis shows a significant\nvariation in the system's accuracy concerning the changes in target object\ndimensionality, the nature of the fog environment and increasing detection\ndifficulties, with the Car exhibiting the highest accuracy of around 99% and\nthe Pedestrian showing the lowest accuracy of around 73%.\n","authors":["Ajinkya Shinde","Gaurav Sharma","Manisha Pattanaik","Sri Niwas Singh"],"pdf_url":"https://arxiv.org/pdf/2408.01085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01080v1","updated":"2024-08-02T07:57:06Z","published":"2024-08-02T07:57:06Z","title":"FCDFusion: a Fast, Low Color Deviation Method for Fusing Visible and\n  Infrared Image Pairs","summary":"  Visible and infrared image fusion (VIF) aims to combine information from\nvisible and infrared images into a single fused image. Previous VIF methods\nusually employ a color space transformation to keep the hue and saturation from\nthe original visible image. However, for fast VIF methods, this operation\naccounts for the majority of the calculation and is the bottleneck preventing\nfaster processing. In this paper, we propose a fast fusion method, FCDFusion,\nwith little color deviation. It preserves color information without color space\ntransformations, by directly operating in RGB color space. It incorporates\ngamma correction at little extra cost, allowing color and contrast to be\nrapidly improved. We regard the fusion process as a scaling operation on 3D\ncolor vectors, greatly simplifying the calculations. A theoretical analysis and\nexperiments show that our method can achieve satisfactory results in only 7\nFLOPs per pixel. Compared to state-of-the-art fast, color-preserving methods\nusing HSV color space, our method provides higher contrast at only half of the\ncomputational cost. We further propose a new metric, color deviation, to\nmeasure the ability of a VIF method to preserve color. It is specifically\ndesigned for VIF tasks with color visible-light images, and overcomes\ndeficiencies of existing VIF metrics used for this purpose. Our code is\navailable at https://github.com/HeasonLee/FCDFusion.\n","authors":["Hesong Li","Ying Fu"],"pdf_url":"https://arxiv.org/pdf/2408.01080v1.pdf","comment":"This article has been accepted by Computational Visual Media"},{"id":"http://arxiv.org/abs/2408.01077v1","updated":"2024-08-02T07:52:28Z","published":"2024-08-02T07:52:28Z","title":"PhysMamba: Leveraging Dual-Stream Cross-Attention SSD for Remote\n  Physiological Measurement","summary":"  Remote Photoplethysmography (rPPG) is a non-contact technique for extracting\nphysiological signals from facial videos, used in applications like emotion\nmonitoring, medical assistance, and anti-face spoofing. Unlike controlled\nlaboratory settings, real-world environments often contain motion artifacts and\nnoise, affecting the performance of existing methods. To address this, we\npropose PhysMamba, a dual-stream time-frequency interactive model based on\nMamba. PhysMamba integrates the state-of-the-art Mamba-2 model and employs a\ndual-stream architecture to learn diverse rPPG features, enhancing robustness\nin noisy conditions. Additionally, we designed the Cross-Attention State Space\nDuality (CASSD) module to improve information exchange and feature\ncomplementarity between the two streams. We validated PhysMamba using PURE,\nUBFC-rPPG and MMPD. Experimental results show that PhysMamba achieves\nstate-of-the-art performance across various scenarios, particularly in complex\nenvironments, demonstrating its potential in practical remote heart rate\nmonitoring applications.\n","authors":["Zhixin Yan","Yan Zhong","Wenjun Zhang","Lin Shu","Hongbin Xu","Wenxiong Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01077v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01076v1","updated":"2024-08-02T07:51:44Z","published":"2024-08-02T07:51:44Z","title":"Exploiting the Semantic Knowledge of Pre-trained Text-Encoders for\n  Continual Learning","summary":"  Deep neural networks (DNNs) excel on fixed datasets but struggle with\nincremental and shifting data in real-world scenarios. Continual learning\naddresses this challenge by allowing models to learn from new data while\nretaining previously learned knowledge. Existing methods mainly rely on visual\nfeatures, often neglecting the rich semantic information encoded in text. The\nsemantic knowledge available in the label information of the images, offers\nimportant semantic information that can be related with previously acquired\nknowledge of semantic classes. Consequently, effectively leveraging this\ninformation throughout continual learning is expected to be beneficial. To\naddress this, we propose integrating semantic guidance within and across tasks\nby capturing semantic similarity using text embeddings. We start from a\npre-trained CLIP model, employ the \\emph{Semantically-guided Representation\nLearning (SG-RL)} module for a soft-assignment towards all current task\nclasses, and use the Semantically-guided Knowledge Distillation (SG-KD) module\nfor enhanced knowledge transfer. Experimental results demonstrate the\nsuperiority of our method on general and fine-grained datasets. Our code can be\nfound in\nhttps://github.com/aprilsveryown/semantically-guided-continual-learning.\n","authors":["Lu Yu","Zhe Tao","Hantao Yao","Joost Van de Weijer","Changsheng Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01067v1","updated":"2024-08-02T07:40:34Z","published":"2024-08-02T07:40:34Z","title":"Amodal Segmentation for Laparoscopic Surgery Video Instruments","summary":"  Segmentation of surgical instruments is crucial for enhancing surgeon\nperformance and ensuring patient safety. Conventional techniques such as\nbinary, semantic, and instance segmentation share a common drawback: they do\nnot accommodate the parts of instruments obscured by tissues or other\ninstruments. Precisely predicting the full extent of these occluded instruments\ncan significantly improve laparoscopic surgeries by providing critical guidance\nduring operations and assisting in the analysis of potential surgical errors,\nas well as serving educational purposes. In this paper, we introduce Amodal\nSegmentation to the realm of surgical instruments in the medical field. This\ntechnique identifies both the visible and occluded parts of an object. To\nachieve this, we introduce a new Amoal Instruments Segmentation (AIS) dataset,\nwhich was developed by reannotating each instrument with its complete mask,\nutilizing the 2017 MICCAI EndoVis Robotic Instrument Segmentation Challenge\ndataset. Additionally, we evaluate several leading amodal segmentation methods\nto establish a benchmark for this new dataset.\n","authors":["Ruohua Shi","Zhaochen Liu","Lingyu Duan","Tingting Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.01067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02371v2","updated":"2024-08-02T07:14:59Z","published":"2024-07-02T15:40:29Z","title":"OpenVid-1M: A Large-Scale High-Quality Dataset for Text-to-video\n  Generation","summary":"  Text-to-video (T2V) generation has recently garnered significant attention\nthanks to the large multi-modality model Sora. However, T2V generation still\nfaces two important challenges: 1) Lacking a precise open sourced high-quality\ndataset. The previous popular video datasets, e.g. WebVid-10M and Panda-70M,\nare either with low quality or too large for most research institutions.\nTherefore, it is challenging but crucial to collect a precise high-quality\ntext-video pairs for T2V generation. 2) Ignoring to fully utilize textual\ninformation. Recent T2V methods have focused on vision transformers, using a\nsimple cross attention module for video generation, which falls short of\nthoroughly extracting semantic information from text prompt. To address these\nissues, we introduce OpenVid-1M, a precise high-quality dataset with expressive\ncaptions. This open-scenario dataset contains over 1 million text-video pairs,\nfacilitating research on T2V generation. Furthermore, we curate 433K 1080p\nvideos from OpenVid-1M to create OpenVidHD-0.4M, advancing high-definition\nvideo generation. Additionally, we propose a novel Multi-modal Video Diffusion\nTransformer (MVDiT) capable of mining both structure information from visual\ntokens and semantic information from text tokens. Extensive experiments and\nablation studies verify the superiority of OpenVid-1M over previous datasets\nand the effectiveness of our MVDiT.\n","authors":["Kepan Nan","Rui Xie","Penghao Zhou","Tiehan Fan","Zhenheng Yang","Zhijie Chen","Xiang Li","Jian Yang","Ying Tai"],"pdf_url":"https://arxiv.org/pdf/2407.02371v2.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.19397v2","updated":"2024-08-02T06:53:08Z","published":"2024-07-28T04:46:55Z","title":"Domain Adaptive Lung Nodule Detection in X-ray Image","summary":"  Medical images from different healthcare centers exhibit varied data\ndistributions, posing significant challenges for adapting lung nodule detection\ndue to the domain shift between training and application phases. Traditional\nunsupervised domain adaptive detection methods often struggle with this shift,\nleading to suboptimal outcomes. To overcome these challenges, we introduce a\nnovel domain adaptive approach for lung nodule detection that leverages mean\nteacher self-training and contrastive learning. First, we propose a\nhierarchical contrastive learning strategy to refine nodule representations and\nenhance the distinction between nodules and background. Second, we introduce a\nnodule-level domain-invariant feature learning (NDL) module to capture\ndomain-invariant features through adversarial learning across different\ndomains. Additionally, we propose a new annotated dataset of X-ray images to\naid in advancing lung nodule detection research. Extensive experiments\nconducted on multiple X-ray datasets demonstrate the efficacy of our approach\nin mitigating domain shift impacts.\n","authors":["Haifeng Zhao","Lixiang Jiang","Leilei Ma","Dengdi Sun","Yanping Fu"],"pdf_url":"https://arxiv.org/pdf/2407.19397v2.pdf","comment":"This paper will submit to IEEE SMC 2024"},{"id":"http://arxiv.org/abs/2408.01044v1","updated":"2024-08-02T06:32:45Z","published":"2024-08-02T06:32:45Z","title":"Boosting Gaze Object Prediction via Pixel-level Supervision from Vision\n  Foundation Model","summary":"  Gaze object prediction (GOP) aims to predict the category and location of the\nobject that a human is looking at. Previous methods utilized box-level\nsupervision to identify the object that a person is looking at, but struggled\nwith semantic ambiguity, ie, a single box may contain several items since\nobjects are close together. The Vision foundation model (VFM) has improved in\nobject segmentation using box prompts, which can reduce confusion by more\nprecisely locating objects, offering advantages for fine-grained prediction of\ngaze objects. This paper presents a more challenging gaze object segmentation\n(GOS) task, which involves inferring the pixel-level mask corresponding to the\nobject captured by human gaze behavior. In particular, we propose that the\npixel-level supervision provided by VFM can be integrated into gaze object\nprediction to mitigate semantic ambiguity. This leads to our gaze object\ndetection and segmentation framework that enables accurate pixel-level\npredictions. Different from previous methods that require additional head input\nor ignore head features, we propose to automatically obtain head features from\nscene features to ensure the model's inference efficiency and flexibility in\nthe real world. Moreover, rather than directly fuse features to predict gaze\nheatmap as in existing methods, which may overlook spatial location and subtle\ndetails of the object, we develop a space-to-object gaze regression method to\nfacilitate human-object gaze interaction. Specifically, it first constructs an\ninitial human-object spatial connection, then refines this connection by\ninteracting with semantically clear features in the segmentation branch,\nultimately predicting a gaze heatmap for precise localization. Extensive\nexperiments on GOO-Synth and GOO-Real datasets demonstrate the effectiveness of\nour method.\n","authors":["Yang Jin","Lei Zhang","Shi Yan","Bin Fan","Binglu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01044v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.01040v1","updated":"2024-08-02T06:24:39Z","published":"2024-08-02T06:24:39Z","title":"Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix","summary":"  In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.\n","authors":["Seungeun Oh","Sihun Baek","Jihong Park","Hyelin Nam","Praneeth Vepakomma","Ramesh Raskar","Mehdi Bennis","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01040v1.pdf","comment":"23 pages, 11 figures, 8 tables, to be published in Transactions on\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.01037v1","updated":"2024-08-02T06:20:48Z","published":"2024-08-02T06:20:48Z","title":"MambaST: A Plug-and-Play Cross-Spectral Spatial-Temporal Fuser for\n  Efficient Pedestrian Detection","summary":"  This paper proposes MambaST, a plug-and-play cross-spectral spatial-temporal\nfusion pipeline for efficient pedestrian detection. Several challenges exist\nfor pedestrian detection in autonomous driving applications. First, it is\ndifficult to perform accurate detection using RGB cameras under dark or\nlow-light conditions. Cross-spectral systems must be developed to integrate\ncomplementary information from multiple sensor modalities, such as thermal and\nvisible cameras, to improve the robustness of the detections. Second,\npedestrian detection models are latency-sensitive. Efficient and easy-to-scale\ndetection models with fewer parameters are highly desirable for real-time\napplications such as autonomous driving. Third, pedestrian video data provides\nspatial-temporal correlations of pedestrian movement. It is beneficial to\nincorporate temporal as well as spatial information to enhance pedestrian\ndetection. This work leverages recent advances in the state space model (Mamba)\nand proposes a novel Multi-head Hierarchical Patching and Aggregation (MHHPA)\nstructure to extract both fine-grained and coarse-grained information from both\nRGB and thermal imagery. Experimental results show that the proposed MHHPA is\nan effective and efficient alternative to a Transformer model for\ncross-spectral pedestrian detection. Our proposed model also achieves superior\nperformance on small-scale pedestrian detection. The code is available at\nhttps://github.com/XiangboGaoBarry/MambaST}{https://github.com/XiangboGaoBarry/MambaST.\n","authors":["Xiangbo Gao","Asiegbu Miracle Kanu-Asiegbu","Xiaoxiao Du"],"pdf_url":"https://arxiv.org/pdf/2408.01037v1.pdf","comment":"ITSC 2024 Accepted"},{"id":"http://arxiv.org/abs/2408.01035v1","updated":"2024-08-02T06:18:39Z","published":"2024-08-02T06:18:39Z","title":"Structure from Motion-based Motion Estimation and 3D Reconstruction of\n  Unknown Shaped Space Debris","summary":"  With the boost in the number of spacecraft launches in the current decades,\nthe space debris problem is daily becoming significantly crucial. For\nsustainable space utilization, the continuous removal of space debris is the\nmost severe problem for humanity. To maximize the reliability of the debris\ncapture mission in orbit, accurate motion estimation of the target is\nessential. Space debris has lost its attitude and orbit control capabilities,\nand its shape is unknown due to the break. This paper proposes the Structure\nfrom Motion-based algorithm to perform unknown shaped space debris motion\nestimation with limited resources, where only 2D images are required as input.\nThe method then outputs the reconstructed shape of the unknown object and the\nrelative pose trajectory between the target and the camera simultaneously,\nwhich are exploited to estimate the target's motion. The method is\nquantitatively validated with the realistic image dataset generated by the\nmicrogravity experiment in a 2D air-floating testbed and 3D kinematic\nsimulation.\n","authors":["Kentaro Uno","Takehiro Matsuoka","Akiyoshi Uchida","Kazuya Yoshida"],"pdf_url":"https://arxiv.org/pdf/2408.01035v1.pdf","comment":"6 pages, 10 figures. Manuscript accepted at the 2024 IEEE 20th\n  International Conference on Automation Science and Engineerin (CASE 2024)"},{"id":"http://arxiv.org/abs/2408.01031v1","updated":"2024-08-02T06:13:29Z","published":"2024-08-02T06:13:29Z","title":"POA: Pre-training Once for Models of All Sizes","summary":"  Large-scale self-supervised pre-training has paved the way for one foundation\nmodel to handle many different vision tasks. Most pre-training methodologies\ntrain a single model of a certain size at one time. Nevertheless, various\ncomputation or storage constraints in real-world scenarios require substantial\nefforts to develop a series of models with different sizes to deploy. Thus, in\nthis study, we propose a novel tri-branch self-supervised training framework,\ntermed as POA (Pre-training Once for All), to tackle this aforementioned issue.\nOur approach introduces an innovative elastic student branch into a modern\nself-distillation paradigm. At each pre-training step, we randomly sample a\nsub-network from the original student to form the elastic student and train all\nbranches in a self-distilling fashion. Once pre-trained, POA allows the\nextraction of pre-trained models of diverse sizes for downstream tasks.\nRemarkably, the elastic student facilitates the simultaneous pre-training of\nmultiple models with different sizes, which also acts as an additional ensemble\nof models of various sizes to enhance representation learning. Extensive\nexperiments, including k-nearest neighbors, linear probing evaluation and\nassessments on multiple downstream tasks demonstrate the effectiveness and\nadvantages of our POA. It achieves state-of-the-art performance using ViT, Swin\nTransformer and ResNet backbones, producing around a hundred models with\ndifferent sizes through a single pre-training session. The code is available\nat: https://github.com/Qichuzyy/POA.\n","authors":["Yingying Zhang","Xin Guo","Jiangwei Lao","Lei Yu","Lixiang Ru","Jian Wang","Guo Ye","Huimei He","Jingdong Chen","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.01031v1.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.01026v1","updated":"2024-08-02T05:50:49Z","published":"2024-08-02T05:50:49Z","title":"PINNs for Medical Image Analysis: A Survey","summary":"  The incorporation of physical information in machine learning frameworks is\ntransforming medical image analysis (MIA). By integrating fundamental knowledge\nand governing physical laws, these models achieve enhanced robustness and\ninterpretability. In this work, we explore the utility of physics-informed\napproaches for MIA (PIMIA) tasks such as registration, generation,\nclassification, and reconstruction. We present a systematic literature review\nof over 80 papers on physics-informed methods dedicated to MIA. We propose a\nunified taxonomy to investigate what physics knowledge and processes are\nmodelled, how they are represented, and the strategies to incorporate them into\nMIA models. We delve deep into a wide range of image analysis tasks, from\nimaging, generation, prediction, inverse imaging (super-resolution and\nreconstruction), registration, and image analysis (segmentation and\nclassification). For each task, we thoroughly examine and present in a tabular\nformat the central physics-guided operation, the region of interest (with\nrespect to human anatomy), the corresponding imaging modality, the dataset used\nfor model training, the deep network architecture employed, and the primary\nphysical process, equation, or principle utilized. Additionally, we also\nintroduce a novel metric to compare the performance of PIMIA methods across\ndifferent tasks and datasets. Based on this review, we summarize and distil our\nperspectives on the challenges, open research questions, and directions for\nfuture research. We highlight key open challenges in PIMIA, including selecting\nsuitable physics priors and establishing a standardized benchmarking platform.\n","authors":["Chayan Banerjee","Kien Nguyen","Olivier Salvado","Truyen Tran","Clinton Fookes"],"pdf_url":"https://arxiv.org/pdf/2408.01026v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.03298v2","updated":"2024-08-02T05:26:14Z","published":"2024-06-05T14:08:13Z","title":"L-PR: Exploiting LiDAR Fiducial Marker for Unordered Low Overlap\n  Multiview Point Cloud Registration","summary":"  Point cloud registration is a prerequisite for many applications in computer\nvision and robotics. Most existing methods focus on pairwise registration of\ntwo point clouds with high overlap. Although there have been some methods for\nlow overlap cases, they struggle in degraded scenarios. This paper introduces a\nnovel framework dubbed L-PR, designed to register unordered low overlap\nmultiview point clouds leveraging LiDAR fiducial markers. We refer to them as\nLiDAR fiducial markers, but they are the same as the popular AprilTag and ArUco\nmarkers, thin sheets of paper that do not affect the 3D geometry of the\nenvironment. We first propose an improved adaptive threshold marker detection\nmethod to provide robust detection results when the viewpoints among point\nclouds change dramatically. Then, we formulate the unordered multiview point\ncloud registration problem as a maximum a-posteriori (MAP) problem and develop\na framework consisting of two levels of graphs to address it. The first-level\ngraph, constructed as a weighted graph, is designed to efficiently and\noptimally infer initial values of scan poses from the unordered set. The\nsecond-level graph is constructed as a factor graph. By globally optimizing the\nvariables on the graph, including scan poses, marker poses, and marker corner\npositions, we tackle the MAP problem. We conduct both qualitative and\nquantitative experiments to demonstrate that the proposed method surpasses\nprevious state-of-the-art (SOTA) methods and to showcase that L-PR can serve as\na low-cost and efficient tool for 3D asset collection and training data\ncollection. In particular, we collect a new dataset named Livox-3DMatch using\nL-PR and incorporate it into the training of the SOTA learning-based method,\nSGHR, which brings evident improvements for SGHR on various benchmarks.\n","authors":["Yibo Liu","Jinjun Shan","Amaldev Haridevan","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.03298v2.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.01014v1","updated":"2024-08-02T05:17:14Z","published":"2024-08-02T05:17:14Z","title":"EIUP: A Training-Free Approach to Erase Non-Compliant Concepts\n  Conditioned on Implicit Unsafe Prompts","summary":"  Text-to-image diffusion models have shown the ability to learn a diverse\nrange of concepts. However, it is worth noting that they may also generate\nundesirable outputs, consequently giving rise to significant security concerns.\nSpecifically, issues such as Not Safe for Work (NSFW) content and potential\nviolations of style copyright may be encountered. Since image generation is\nconditioned on text, prompt purification serves as a straightforward solution\nfor content safety. Similar to the approach taken by LLM, some efforts have\nbeen made to control the generation of safe outputs by purifying prompts.\nHowever, it is also important to note that even with these efforts, non-toxic\ntext still carries a risk of generating non-compliant images, which is referred\nto as implicit unsafe prompts. Furthermore, some existing works fine-tune the\nmodels to erase undesired concepts from model weights. This type of method\nnecessitates multiple training iterations whenever the concept is updated,\nwhich can be time-consuming and may potentially lead to catastrophic\nforgetting. To address these challenges, we propose a simple yet effective\napproach that incorporates non-compliant concepts into an erasure prompt. This\nerasure prompt proactively participates in the fusion of image spatial features\nand text embeddings. Through attention mechanisms, our method is capable of\nidentifying feature representations of non-compliant concepts in the image\nspace. We re-weight these features to effectively suppress the generation of\nunsafe images conditioned on original implicit unsafe prompts. Our method\nexhibits superior erasure effectiveness while achieving high scores in image\nfidelity compared to the state-of-the-art baselines. WARNING: This paper\ncontains model outputs that may be offensive.\n","authors":["Die Chen","Zhiwen Li","Mingyuan Fan","Cen Chen","Wenmeng Zhou","Yaliang Li"],"pdf_url":"https://arxiv.org/pdf/2408.01014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00228v3","updated":"2024-08-02T05:17:10Z","published":"2024-03-01T02:19:40Z","title":"DISORF: A Distributed Online 3D Reconstruction Framework for Mobile\n  Robots","summary":"  We present a framework, DISORF, to enable online 3D reconstruction and\nvisualization of scenes captured by resource-constrained mobile robots and edge\ndevices. To address the limited computing capabilities of edge devices and\npotentially limited network availability, we design a framework that\nefficiently distributes computation between the edge device and the remote\nserver. We leverage on-device SLAM systems to generate posed keyframes and\ntransmit them to remote servers that can perform high-quality 3D reconstruction\nand visualization at runtime by leveraging recent advances in neural 3D\nmethods. We identify a key challenge with online training where naive image\nsampling strategies can lead to significant degradation in rendering quality.\nWe propose a novel shifted exponential frame sampling method that addresses\nthis challenge for online training. We demonstrate the effectiveness of our\nframework in enabling high-quality real-time reconstruction and visualization\nof unknown scenes as they are captured and streamed from cameras in mobile\nrobots and edge devices.\n","authors":["Chunlin Li","Hanrui Fan","Xiaorui Huang","Ruofan Liang","Sankeerth Durvasula","Nandita Vijaykumar"],"pdf_url":"https://arxiv.org/pdf/2403.00228v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21450v2","updated":"2024-08-02T05:01:38Z","published":"2024-07-31T08:54:50Z","title":"Forecasting Future Videos from Novel Views via Disentangled 3D Scene\n  Representation","summary":"  Video extrapolation in space and time (VEST) enables viewers to forecast a 3D\nscene into the future and view it from novel viewpoints. Recent methods propose\nto learn an entangled representation, aiming to model layered scene geometry,\nmotion forecasting and novel view synthesis together, while assuming simplified\naffine motion and homography-based warping at each scene layer, leading to\ninaccurate video extrapolation. Instead of entangled scene representation and\nrendering, our approach chooses to disentangle scene geometry from scene\nmotion, via lifting the 2D scene to 3D point clouds, which enables high quality\nrendering of future videos from novel views. To model future 3D scene motion,\nwe propose a disentangled two-stage approach that initially forecasts\nego-motion and subsequently the residual motion of dynamic objects (e.g., cars,\npeople). This approach ensures more precise motion predictions by reducing\ninaccuracies from entanglement of ego-motion with dynamic object motion, where\nbetter ego-motion forecasting could significantly enhance the visual outcomes.\nExtensive experimental analysis on two urban scene datasets demonstrate\nsuperior performance of our proposed method in comparison to strong baselines.\n","authors":["Sudhir Yarram","Junsong Yuan"],"pdf_url":"https://arxiv.org/pdf/2407.21450v2.pdf","comment":"Accepted to ECCV 2024. Project Page:\n  https://skrya.github.io/projects/ffn-dsr/"},{"id":"http://arxiv.org/abs/2408.00998v1","updated":"2024-08-02T04:13:38Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nextraordinary image generation based on natural-language text prompts. However,\nthe issue of lacking controllability of such models restricts their practical\napplicability for real-life content creation, for which attention has been\nfocused on leveraging a reference image to control text-to-image synthesis. Due\nto the close correlation between the reference image and the generated image,\nthis problem can also be regarded as the task of manipulating (or editing) the\nreference image as per the text, namely text-driven image-to-image translation.\nThis paper contributes a novel, concise, and efficient approach that adapts the\npre-trained large-scale text-to-image (T2I) diffusion model to the\nimage-to-image (I2I) paradigm in a plug-and-play manner, realizing high-quality\nand versatile text-driven I2I translation without any model training, model\nfine-tuning, or online optimization process. To guide T2I generation with a\nreference image, we propose to model diverse guiding factors with\ncorrespondingly different frequency bands of diffusion features in the DCT\nspectral space, and accordingly devise a novel frequency band substitution\nlayer that dynamically substitutes a certain DCT frequency band of the\ndiffusion features with the corresponding counterpart of the reference image\nalong the reverse sampling process. We demonstrate that our method flexibly\nenables highly controllable text-driven I2I translation both in the guiding\nfactor and guiding intensity of the reference image, simply by tuning the type\nand bandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify the superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v1.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2407.12939v3","updated":"2024-08-02T03:33:17Z","published":"2024-07-17T18:10:40Z","title":"GenRC: Generative 3D Room Completion from Sparse Image Collections","summary":"  Sparse RGBD scene completion is a challenging task especially when\nconsidering consistent textures and geometries throughout the entire scene.\nDifferent from existing solutions that rely on human-designed text prompts or\npredefined camera trajectories, we propose GenRC, an automated training-free\npipeline to complete a room-scale 3D mesh with high-fidelity textures. To\nachieve this, we first project the sparse RGBD images to a highly incomplete 3D\nmesh. Instead of iteratively generating novel views to fill in the void, we\nutilized our proposed E-Diffusion to generate a view-consistent panoramic RGBD\nimage which ensures global geometry and appearance consistency. Furthermore, we\nmaintain the input-output scene stylistic consistency through textual inversion\nto replace human-designed text prompts. To bridge the domain gap among\ndatasets, E-Diffusion leverages models trained on large-scale datasets to\ngenerate diverse appearances. GenRC outperforms state-of-the-art methods under\nmost appearance and geometric metrics on ScanNet and ARKitScenes datasets, even\nthough GenRC is not trained on these datasets nor using predefined camera\ntrajectories. Project page: https://minfenli.github.io/GenRC\n","authors":["Ming-Feng Li","Yueh-Feng Ku","Hong-Xuan Yen","Chi Liu","Yu-Lun Liu","Albert Y. C. Chen","Cheng-Hao Kuo","Min Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12939v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2406.08782v2","updated":"2024-08-02T01:56:17Z","published":"2024-06-13T03:27:01Z","title":"Hybrid Spatial-spectral Neural Network for Hyperspectral Image Denoising","summary":"  Hyperspectral image (HSI) denoising is an essential procedure for HSI\napplications. Unfortunately, the existing Transformer-based methods mainly\nfocus on non-local modeling, neglecting the importance of locality in image\ndenoising. Moreover, deep learning methods employ complex spectral learning\nmechanisms, thus introducing large computation costs.\n  To address these problems, we propose a hybrid spatial-spectral denoising\nnetwork (HSSD), in which we design a novel hybrid dual-path network inspired by\nCNN and Transformer characteristics, leading to capturing both local and\nnon-local spatial details while suppressing noise efficiently. Furthermore, to\nreduce computational complexity, we adopt a simple but effective decoupling\nstrategy that disentangles the learning of space and spectral channels, where\nmultilayer perception with few parameters is utilized to learn the global\ncorrelations among spectra. The synthetic and real experiments demonstrate that\nour proposed method outperforms state-of-the-art methods on spatial and\nspectral reconstruction. The code and details are available on\nhttps://github.com/HLImg/HSSD.\n","authors":["Hao Liang"," Chengjie","Kun Li","Xin Tian"],"pdf_url":"https://arxiv.org/pdf/2406.08782v2.pdf","comment":"There are some errors in professional theory"},{"id":"http://arxiv.org/abs/2407.11590v3","updated":"2024-08-02T01:36:59Z","published":"2024-07-16T10:50:10Z","title":"Rethinking Learned Image Compression: Context is All You Need","summary":"  Since LIC has made rapid progress recently compared to traditional methods,\nthis paper attempts to discuss the question about 'Where is the boundary of\nLearned Image Compression(LIC)?'. Thus this paper splits the above problem into\ntwo sub-problems:1)Where is the boundary of rate-distortion performance of\nPSNR? 2)How to further improve the compression gain and achieve the boundary?\nTherefore this paper analyzes the effectiveness of scaling parameters for\nencoder, decoder and context model, which are the three components of LIC. Then\nwe conclude that scaling for LIC is to scale for context model and decoder\nwithin LIC. Extensive experiments demonstrate that overfitting can actually\nserve as an effective context. By optimizing the context, this paper further\nimproves PSNR and achieves state-of-the-art performance, showing a performance\ngain of 14.39% with BD-RATE over VVC.\n","authors":["Jixiang Luo"],"pdf_url":"https://arxiv.org/pdf/2407.11590v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03452v2","updated":"2024-08-02T01:33:25Z","published":"2023-09-07T02:26:55Z","title":"Multimodal Guidance Network for Missing-Modality Inference in Content\n  Moderation","summary":"  Multimodal deep learning, especially vision-language models, have gained\nsignificant traction in recent years, greatly improving performance on many\ndownstream tasks, including content moderation and violence detection. However,\nstandard multimodal approaches often assume consistent modalities between\ntraining and inference, limiting applications in many real-world use cases, as\nsome modalities may not be available during inference. While existing research\nmitigates this problem through reconstructing the missing modalities, they\nunavoidably increase unnecessary computational cost, which could be just as\ncritical, especially for large, deployed infrastructures in industry. To this\nend, we propose a novel guidance network that promotes knowledge sharing during\ntraining, taking advantage of the multimodal representations to train better\nsingle-modality models to be used for inference. Real-world experiments in\nviolence detection shows that our proposed framework trains single-modality\nmodels that significantly outperform traditionally trained counterparts, while\navoiding increases in computational cost for inference.\n","authors":["Zhuokai Zhao","Harish Palani","Tianyi Liu","Lena Evans","Ruth Toner"],"pdf_url":"https://arxiv.org/pdf/2309.03452v2.pdf","comment":"ICME 2024 Camera Ready. Code is available at\n  https://github.com/zhuokaizhao/multimodal-guidance-network"},{"id":"http://arxiv.org/abs/2408.00969v1","updated":"2024-08-02T01:29:43Z","published":"2024-08-02T01:29:43Z","title":"Visible-Thermal Multiple Object Tracking: Large-scale Video Dataset and\n  Progressive Fusion Approach","summary":"  The complementary benefits from visible and thermal infrared data are widely\nutilized in various computer vision task, such as visual tracking, semantic\nsegmentation and object detection, but rarely explored in Multiple Object\nTracking (MOT). In this work, we contribute a large-scale Visible-Thermal video\nbenchmark for MOT, called VT-MOT. VT-MOT has the following main advantages. 1)\nThe data is large scale and high diversity. VT-MOT includes 582 video sequence\npairs, 401k frame pairs from surveillance, drone, and handheld platforms. 2)\nThe cross-modal alignment is highly accurate. We invite several professionals\nto perform both spatial and temporal alignment frame by frame. 3) The\nannotation is dense and high-quality. VT-MOT has 3.99 million annotation boxes\nannotated and double-checked by professionals, including heavy occlusion and\nobject re-acquisition (object disappear and reappear) challenges. To provide a\nstrong baseline, we design a simple yet effective tracking framework, which\neffectively fuses temporal information and complementary information of two\nmodalities in a progressive manner, for robust visible-thermal MOT. A\ncomprehensive experiment are conducted on VT-MOT and the results prove the\nsuperiority and effectiveness of the proposed method compared with\nstate-of-the-art methods. From the evaluation results and analysis, we specify\nseveral potential future directions for visible-thermal MOT. The project is\nreleased in https://github.com/wqw123wqw/PFTrack.\n","authors":["Yabin Zhu","Qianwu Wang","Chenglong Li","Jin Tang","Zhixiang Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00967v1","updated":"2024-08-02T01:24:01Z","published":"2024-08-02T01:24:01Z","title":"Extracting Object Heights From LiDAR & Aerial Imagery","summary":"  This work shows a procedural method for extracting object heights from LiDAR\nand aerial imagery. We discuss how to get heights and the future of LiDAR and\nimagery processing. SOTA object segmentation allows us to take get object\nheights with no deep learning background. Engineers will be keeping track of\nworld data across generations and reprocessing them. They will be using older\nprocedural methods like this paper and newer ones discussed here. SOTA methods\nare going beyond analysis and into generative AI. We cover both a procedural\nmethodology and the newer ones performed with language models. These include\npoint cloud, imagery and text encoding allowing for spatially aware AI.\n","authors":["Jesus Guerrero"],"pdf_url":"https://arxiv.org/pdf/2408.00967v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09786v5","updated":"2024-08-02T01:22:46Z","published":"2024-01-18T08:10:34Z","title":"Adaptive Self-training Framework for Fine-grained Scene Graph Generation","summary":"  Scene graph generation (SGG) models have suffered from inherent problems\nregarding the benchmark datasets such as the long-tailed predicate distribution\nand missing annotation problems. In this work, we aim to alleviate the\nlong-tailed problem of SGG by utilizing unannotated triplets. To this end, we\nintroduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels\nfor unannotated triplets based on which the SGG models are trained. While there\nhas been significant progress in self-training for image recognition, designing\na self-training framework for the SGG task is more challenging due to its\ninherent nature such as the semantic ambiguity and the long-tailed distribution\nof predicate classes. Hence, we propose a novel pseudo-labeling technique for\nSGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is\na model-agnostic framework that can be applied to any existing SGG models.\nFurthermore, we devise a graph structure learner (GSL) that is beneficial when\nadopting our proposed self-training framework to the state-of-the-art\nmessage-passing neural network (MPNN)-based SGG models. Our extensive\nexperiments verify the effectiveness of ST-SGG on various SGG models,\nparticularly in enhancing the performance on fine-grained predicate classes.\n","authors":["Kibum Kim","Kanghoon Yoon","Yeonjun In","Jinyoung Moon","Donghyun Kim","Chanyoung Park"],"pdf_url":"https://arxiv.org/pdf/2401.09786v5.pdf","comment":"9 pages; ICLR 2024"},{"id":"http://arxiv.org/abs/2311.13186v2","updated":"2024-08-02T00:56:39Z","published":"2023-11-22T06:26:24Z","title":"Applications of Spiking Neural Networks in Visual Place Recognition","summary":"  In robotics, Spiking Neural Networks (SNNs) are increasingly recognized for\ntheir largely-unrealized potential energy efficiency and low latency\nparticularly when implemented on neuromorphic hardware. Our paper highlights\nthree advancements for SNNs in Visual Place Recognition (VPR). Firstly, we\npropose Modular SNNs, where each SNN represents a set of non-overlapping\ngeographically distinct places, enabling scalable networks for large\nenvironments. Secondly, we present Ensembles of Modular SNNs, where multiple\nnetworks represent the same place, significantly enhancing accuracy compared to\nsingle-network models. Each of our Modular SNN modules is compact, comprising\nonly 1500 neurons and 474k synapses, making them ideally suited for ensembling\ndue to their small size. Lastly, we investigate the role of sequence matching\nin SNN-based VPR, a technique where consecutive images are used to refine place\nrecognition. We analyze the responsiveness of SNNs to ensembling and sequence\nmatching compared to other VPR techniques. Our contributions highlight the\nviability of SNNs for VPR, offering scalable and robust solutions, and paving\nthe way for their application in various energy-sensitive robotic tasks.\n","authors":["Somayeh Hussaini","Michael Milford","Tobias Fischer"],"pdf_url":"https://arxiv.org/pdf/2311.13186v2.pdf","comment":"20 pages, 10 figures, under review"},{"id":"http://arxiv.org/abs/2408.00963v1","updated":"2024-08-02T00:35:18Z","published":"2024-08-02T00:35:18Z","title":"MIS-ME: A Multi-modal Framework for Soil Moisture Estimation","summary":"  Soil moisture estimation is an important task to enable precision agriculture\nin creating optimal plans for irrigation, fertilization, and harvest. It is\ncommon to utilize statistical and machine learning models to estimate soil\nmoisture from traditional data sources such as weather forecasts, soil\nproperties, and crop properties. However, there is a growing interest in\nutilizing aerial and geospatial imagery to estimate soil moisture. Although\nthese images capture high-resolution crop details, they are expensive to curate\nand challenging to interpret. Imagine, an AI-enhanced software tool that\npredicts soil moisture using visual cues captured by smartphones and\nstatistical data given by weather forecasts. This work is a first step towards\nthat goal of developing a multi-modal approach for soil moisture estimation. In\nparticular, we curate a dataset consisting of real-world images taken from\nground stations and their corresponding weather data. We also propose MIS-ME -\nMeteorological & Image based Soil Moisture Estimator, a multi-modal framework\nfor soil moisture estimation. Our extensive analysis shows that MIS-ME achieves\na MAPE of 10.79%, outperforming traditional unimodal approaches with a\nreduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image\ndata, highlighting the effectiveness of tailored multi-modal approaches.\n","authors":["Mohammed Rakib","Adil Aman Mohammed","Cole Diggins","Sumit Sharma","Jeff Michael Sadler","Tyson Ochsner","Arun Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2408.00963v1.pdf","comment":"Accepted by DSAA2024"},{"id":"http://arxiv.org/abs/2404.07514v2","updated":"2024-08-02T00:06:55Z","published":"2024-04-11T07:11:43Z","title":"Generalization Gap in Data Augmentation: Insights from Illumination","summary":"  In the field of computer vision, data augmentation is widely used to enrich\nthe feature complexity of training datasets with deep learning techniques.\nHowever, regarding the generalization capabilities of models, the difference in\nartificial features generated by data augmentation and natural visual features\nhas not been fully revealed. This study introduces the concept of \"visual\nrepresentation variables\" to define the possible visual variations in a task as\na joint distribution of these variables. We focus on the visual representation\nvariable \"illumination\", by simulating its distribution degradation and\nexamining how data augmentation techniques enhance model performance on a\nclassification task. Our goal is to investigate the differences in\ngeneralization between models trained with augmented data and those trained\nunder real-world illumination conditions. Results indicate that after applying\nvarious data augmentation methods, model performance has significantly\nimproved. Yet, a noticeable generalization gap still exists after utilizing\nvarious data augmentation methods, emphasizing the critical role of feature\ndiversity in the training set for enhancing model generalization.\n","authors":["Jianqiang Xiao","Weiwen Guo","Junfeng Liu","Mengze Li"],"pdf_url":"https://arxiv.org/pdf/2404.07514v2.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.01363v1","updated":"2024-08-02T16:15:25Z","published":"2024-08-02T16:15:25Z","title":"Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation","summary":"  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n","authors":["Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01363v1.pdf","comment":"Accepted by ACM SIGIR 2024 LLM4Eval Workshop:\n  https://llm4eval.github.io/papers"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.01342v1","updated":"2024-08-02T15:38:55Z","published":"2024-08-02T15:38:55Z","title":"Leveraging Knowledge Graph Embedding for Effective Conversational\n  Recommendation","summary":"  Conversational recommender system (CRS), which combines the techniques of\ndialogue system and recommender system, has obtained increasing interest\nrecently. In contrast to traditional recommender system, it learns the user\npreference better through interactions (i.e. conversations), and then further\nboosts the recommendation performance. However, existing studies on CRS ignore\nto address the relationship among attributes, users, and items effectively,\nwhich might lead to inappropriate questions and inaccurate recommendations. In\nthis view, we propose a knowledge graph based conversational recommender system\n(referred as KG-CRS). Specifically, we first integrate the user-item graph and\nitem-attribute graph into a dynamic graph, i.e., dynamically changing during\nthe dialogue process by removing negative items or attributes. We then learn\ninformative embedding of users, items, and attributes by also considering\npropagation through neighbors on the graph. Extensive experiments on three real\ndatasets validate the superiority of our method over the state-of-the-art\napproaches in terms of both the recommendation and conversation tasks.\n","authors":["Yunwen Xia","Hui Fang","Jie Zhang","Chong Long"],"pdf_url":"https://arxiv.org/pdf/2408.01342v1.pdf","comment":"26pages, 15figures"},{"id":"http://arxiv.org/abs/2408.01262v1","updated":"2024-08-02T13:35:11Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) systems have demonstrated their\nadvantages in alleviating the hallucination of Large Language Models (LLMs).\nExisting RAG benchmarks mainly focus on evaluating whether LLMs can correctly\nanswer the general knowledge. However, they are unable to evaluate the\neffectiveness of the RAG system in dealing with the data from different\nvertical domains. This paper introduces RAGEval, a framework for automatically\ngenerating evaluation datasets to evaluate the knowledge usage ability of\ndifferent LLMs in different scenarios. Specifically, RAGEval summarizes a\nschema from seed documents, applies the configurations to generate diverse\ndocuments, and constructs question-answering pairs according to both articles\nand configurations. We propose three novel metrics, Completeness,\nHallucination, and Irrelevance, to carefully evaluate the responses generated\nby LLMs. By benchmarking RAG models in vertical domains, RAGEval has the\nability to better evaluate the knowledge usage ability of LLMs, which avoids\nthe confusion regarding the source of knowledge in answering question in\nexisting QA datasets--whether it comes from parameterized memory or retrieval.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Ruobing Wang","Shi Yu","Shuo Wang","Yukun Yan","Zhenghao Liu","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11053v3","updated":"2024-08-02T13:26:44Z","published":"2024-05-17T19:06:06Z","title":"The MovieLens Beliefs Dataset: Collecting Pre-Choice Data for Online\n  Recommender Systems","summary":"  An increasingly important aspect of designing recommender systems involves\nconsidering how recommendations will influence consumer choices. This paper\naddresses this issue by introducing a method for collecting user beliefs about\nun-experienced items - a critical predictor of choice behavior. We implemented\nthis method on the MovieLens platform, resulting in a rich dataset that\ncombines user ratings, beliefs, and observed recommendations. We document\nchallenges to such data collection, including selection bias in response and\nlimited coverage of the product space. This unique resource empowers\nresearchers to delve deeper into user behavior and analyze user choices absent\nrecommendations, measure the effectiveness of recommendations, and prototype\nalgorithms that leverage user belief data, ultimately leading to more impactful\nrecommender systems. The dataset can be found at\nhttps://grouplens.org/datasets/movielens/ml_belief_2024/.\n","authors":["Guy Aridor","Duarte Goncalves","Ruoyan Kong","Daniel Kluver","Joseph Konstan"],"pdf_url":"https://arxiv.org/pdf/2405.11053v3.pdf","comment":"To Appear in RecSys 2024"},{"id":"http://arxiv.org/abs/2408.01180v1","updated":"2024-08-02T11:02:38Z","published":"2024-08-02T11:02:38Z","title":"Nested Music Transformer: Sequentially Decoding Compound Tokens in\n  Symbolic Music and Audio Generation","summary":"  Representing symbolic music with compound tokens, where each token consists\nof several different sub-tokens representing a distinct musical feature or\nattribute, offers the advantage of reducing sequence length. While previous\nresearch has validated the efficacy of compound tokens in music sequence\nmodeling, predicting all sub-tokens simultaneously can lead to suboptimal\nresults as it may not fully capture the interdependencies between them. We\nintroduce the Nested Music Transformer (NMT), an architecture tailored for\ndecoding compound tokens autoregressively, similar to processing flattened\ntokens, but with low memory usage. The NMT consists of two transformers: the\nmain decoder that models a sequence of compound tokens and the sub-decoder for\nmodeling sub-tokens of each compound token. The experiment results showed that\napplying the NMT to compound tokens can enhance the performance in terms of\nbetter perplexity in processing various symbolic music datasets and discrete\naudio tokens from the MAESTRO dataset.\n","authors":["Jiwoo Ryu","Hao-Wen Dong","Jongmin Jung","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2408.01180v1.pdf","comment":"Accepted at 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2408.01107v1","updated":"2024-08-02T08:37:03Z","published":"2024-08-02T08:37:03Z","title":"BioRAG: A RAG-LLM Framework for Biological Question Reasoning","summary":"  The question-answering system for Life science research, which is\ncharacterized by the rapid pace of discovery, evolving insights, and complex\ninteractions among knowledge entities, presents unique challenges in\nmaintaining a comprehensive knowledge warehouse and accurate information\nretrieval. To address these issues, we introduce BioRAG, a novel\nRetrieval-Augmented Generation (RAG) with the Large Language Models (LLMs)\nframework. Our approach starts with parsing, indexing, and segmenting an\nextensive collection of 22 million scientific papers as the basic knowledge,\nfollowed by training a specialized embedding model tailored to this domain.\nAdditionally, we enhance the vector retrieval process by incorporating a\ndomain-specific knowledge hierarchy, which aids in modeling the intricate\ninterrelationships among each query and context. For queries requiring the most\ncurrent information, BioRAG deconstructs the question and employs an iterative\nretrieval process incorporated with the search engine for step-by-step\nreasoning. Rigorous experiments have demonstrated that our model outperforms\nfine-tuned LLM, LLM with search engines, and other scientific RAG frameworks\nacross multiple life science question-answering tasks.\n","authors":["Chengrui Wang","Qingqing Long","Xiao Meng","Xunxin Cai","Chengjun Wu","Zhen Meng","Xuezhi Wang","Yuanchun Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.01107v1.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.01094v1","updated":"2024-08-02T08:13:18Z","published":"2024-08-02T08:13:18Z","title":"An Encoding--Searching Separation Perspective on Bi-Encoder Neural\n  Search","summary":"  This paper reviews, analyzes, and proposes a new perspective on the\nbi-encoder architecture for neural search. While the bi-encoder architecture is\nwidely used due to its simplicity and scalability at test time, it has some\nnotable issues such as low performance on seen datasets and weak zero-shot\nperformance on new datasets. In this paper, we analyze these issues and\nsummarize two main critiques: the encoding information bottleneck problem and\nlimitations of the basic assumption of embedding search. We then construct a\nthought experiment to logically analyze the encoding and searching operations\nand challenge the basic assumption of embedding search. Building on these\nobservations, we propose a new perspective on the bi-encoder architecture\ncalled the \\textit{encoding--searching separation} perspective, which\nconceptually and practically separates the encoding and searching operations.\nThis new perspective is applied to explain the root cause of the identified\nissues and discuss ways to mitigate the problems. Finally, we discuss the\nimplications of the ideas underlying the new perspective, the design surface\nthat it exposes and the potential research directions arising from it.\n","authors":["Hung-Nghiep Tran","Akiko Aizawa","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2408.01094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00960v1","updated":"2024-08-02T00:24:22Z","published":"2024-08-02T00:24:22Z","title":"PERSOMA: PERsonalized SOft ProMpt Adapter Architecture for Personalized\n  Language Prompting","summary":"  Understanding the nuances of a user's extensive interaction history is key to\nbuilding accurate and personalized natural language systems that can adapt to\nevolving user preferences. To address this, we introduce PERSOMA, Personalized\nSoft Prompt Adapter architecture. Unlike previous personalized prompting\nmethods for large language models, PERSOMA offers a novel approach to\nefficiently capture user history. It achieves this by resampling and\ncompressing interactions as free form text into expressive soft prompt\nembeddings, building upon recent research utilizing embedding representations\nas input for LLMs. We rigorously validate our approach by evaluating various\nadapter architectures, first-stage sampling strategies, parameter-efficient\ntuning techniques like LoRA, and other personalization methods. Our results\ndemonstrate PERSOMA's superior ability to handle large and complex user\nhistories compared to existing embedding-based and text-prompt-based\ntechniques.\n","authors":["Liam Hebert","Krishna Sayana","Ambarish Jash","Alexandros Karatzoglou","Sukhdeep Sodhi","Sumanth Doddapaneni","Yanli Cai","Dima Kuzmin"],"pdf_url":"https://arxiv.org/pdf/2408.00960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01570v1","updated":"2024-08-02T20:55:45Z","published":"2024-08-02T20:55:45Z","title":"On Validation of Search & Retrieval of Tissue Images in Digital\n  Pathology","summary":"  Medical images play a crucial role in modern healthcare by providing vital\ninformation for diagnosis, treatment planning, and disease monitoring. Fields\nsuch as radiology and pathology rely heavily on accurate image interpretation,\nwith radiologists examining X-rays, CT scans, and MRIs to diagnose conditions\nfrom fractures to cancer, while pathologists use microscopy and digital images\nto detect cellular abnormalities for diagnosing cancers and infections. The\ntechnological advancements have exponentially increased the volume and\ncomplexity of medical images, necessitating efficient tools for management and\nretrieval. Content-Based Image Retrieval (CBIR) systems address this need by\nsearching and retrieving images based on visual content, enhancing diagnostic\naccuracy by allowing clinicians to find similar cases and compare pathological\npatterns. Comprehensive validation of image search engines in medical\napplications involves evaluating performance metrics like accuracy, indexing,\nand search times, and storage overhead, ensuring reliable and efficient\nretrieval of accurate results, as demonstrated by recent validations in\nhistopathology.\n","authors":["H. R. Tizhoosh"],"pdf_url":"https://arxiv.org/pdf/2408.01570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01556v1","updated":"2024-08-02T20:05:24Z","published":"2024-08-02T20:05:24Z","title":"pathfinder: A Semantic Framework for Literature Review and Knowledge\n  Discovery in Astronomy","summary":"  The exponential growth of astronomical literature poses significant\nchallenges for researchers navigating and synthesizing general insights or even\ndomain-specific knowledge. We present Pathfinder, a machine learning framework\ndesigned to enable literature review and knowledge discovery in astronomy,\nfocusing on semantic searching with natural language instead of syntactic\nsearches with keywords. Utilizing state-of-the-art large language models (LLMs)\nand a corpus of 350,000 peer-reviewed papers from the Astrophysics Data System\n(ADS), Pathfinder offers an innovative approach to scientific inquiry and\nliterature exploration. Our framework couples advanced retrieval techniques\nwith LLM-based synthesis to search astronomical literature by semantic context\nas a complement to currently existing methods that use keywords or citation\ngraphs. It addresses complexities of jargon, named entities, and temporal\naspects through time-based and citation-based weighting schemes. We demonstrate\nthe tool's versatility through case studies, showcasing its application in\nvarious research scenarios. The system's performance is evaluated using custom\nbenchmarks, including single-paper and multi-paper tasks. Beyond literature\nreview, Pathfinder offers unique capabilities for reformatting answers in ways\nthat are accessible to various audiences (e.g. in a different language or as\nsimplified text), visualizing research landscapes, and tracking the impact of\nobservatories and methodologies. This tool represents a significant advancement\nin applying AI to astronomical research, aiding researchers at all career\nstages in navigating modern astronomy literature.\n","authors":["Kartheik G. Iyer","Mikaeel Yunus","Charles O'Neill","Christine Ye","Alina Hyk","Kiera McCormick","Ioana Ciuca","John F. Wu","Alberto Accomazzi","Simone Astarita","Rishabh Chakrabarty","Jesse Cranney","Anjalie Field","Tirthankar Ghosal","Michele Ginolfi","Marc Huertas-Company","Maja Jablonska","Sandor Kruk","Huiling Liu","Gabriel Marchidan","Rohit Mistry","J. P. Naiman","J. E. G. Peek","Mugdha Polimera","Sergio J. Rodriguez","Kevin Schawinski","Sanjib Sharma","Michael J. Smith","Yuan-Sen Ting","Mike Walmsley"],"pdf_url":"https://arxiv.org/pdf/2408.01556v1.pdf","comment":"25 pages, 9 figures, submitted to AAS jorunals. Comments are welcome,\n  and the tools mentioned are available online at https://pfdr.app"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2402.16827v3","updated":"2024-08-02T17:59:31Z","published":"2024-02-26T18:54:35Z","title":"A Survey on Data Selection for Language Models","summary":"  A major factor in the recent success of large language models is the use of\nenormous and ever-growing text datasets for unsupervised pre-training. However,\nnaively training a model on all available data may not be optimal (or\nfeasible), as the quality of available text data can vary. Filtering out data\ncan also decrease the carbon footprint and financial costs of training models\nby reducing the amount of training required. Data selection methods aim to\ndetermine which candidate data points to include in the training dataset and\nhow to appropriately sample from the selected data points. The promise of\nimproved data selection methods has caused the volume of research in the area\nto rapidly expand. However, because deep learning is mostly driven by empirical\nevidence and experimentation on large-scale data is expensive, few\norganizations have the resources for extensive data selection research.\nConsequently, knowledge of effective data selection practices has become\nconcentrated within a few organizations, many of which do not openly share\ntheir findings and methodologies. To narrow this gap in knowledge, we present a\ncomprehensive review of existing literature on data selection methods and\nrelated research areas, providing a taxonomy of existing approaches. By\ndescribing the current landscape of research, this work aims to accelerate\nprogress in data selection by establishing an entry point for new and\nestablished researchers. Additionally, throughout this review we draw attention\nto noticeable holes in the literature and conclude the paper by proposing\npromising avenues for future research.\n","authors":["Alon Albalak","Yanai Elazar","Sang Michael Xie","Shayne Longpre","Nathan Lambert","Xinyi Wang","Niklas Muennighoff","Bairu Hou","Liangming Pan","Haewon Jeong","Colin Raffel","Shiyu Chang","Tatsunori Hashimoto","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16827v3.pdf","comment":"Paper list available at\n  https://github.com/alon-albalak/data-selection-survey"},{"id":"http://arxiv.org/abs/2408.01420v1","updated":"2024-08-02T17:55:50Z","published":"2024-08-02T17:55:50Z","title":"Mission Impossible: A Statistical Perspective on Jailbreaking LLMs","summary":"  Large language models (LLMs) are trained on a deluge of text data with\nlimited quality control. As a result, LLMs can exhibit unintended or even\nharmful behaviours, such as leaking information, fake news or hate speech.\nCountermeasures, commonly referred to as preference alignment, include\nfine-tuning the pretrained LLMs with carefully crafted text examples of desired\nbehaviour. Even then, empirical evidence shows preference aligned LLMs can be\nenticed to harmful behaviour. This so called jailbreaking of LLMs is typically\nachieved by adversarially modifying the input prompt to the LLM. Our paper\nprovides theoretical insights into the phenomenon of preference alignment and\njailbreaking from a statistical perspective. Under our framework, we first show\nthat pretrained LLMs will mimic harmful behaviour if present in the training\ncorpus. Under that same framework, we then introduce a statistical notion of\nalignment, and lower-bound the jailbreaking probability, showing that it is\nunpreventable under reasonable assumptions. Based on our insights, we propose\nan alteration to the currently prevalent alignment strategy RLHF. Specifically,\nwe introduce a simple modification to the RLHF objective, we call E-RLHF, that\naims to increase the likelihood of safe responses. E-RLHF brings no additional\ntraining cost, and is compatible with other methods. Empirically, we\ndemonstrate that E-RLHF outperforms RLHF on all alignment problems put forward\nby the AdvBench and HarmBench project without sacrificing model performance as\nmeasured by the MT-Bench project.\n","authors":["Jingtong Su","Julia Kempe","Karen Ullrich"],"pdf_url":"https://arxiv.org/pdf/2408.01420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01417v1","updated":"2024-08-02T17:51:57Z","published":"2024-08-02T17:51:57Z","title":"Talk Less, Interact Better: Evaluating In-context Conversational\n  Adaptation in Multimodal LLMs","summary":"  Humans spontaneously use increasingly efficient language as interactions\nprogress, by adapting and forming ad-hoc conventions. This phenomenon has been\nstudied extensively using reference games, showing properties of human language\nthat go beyond relaying intents. It remains unexplored whether multimodal large\nlanguage models (MLLMs) similarly increase communication efficiency during\ninteractions, and what mechanisms they may adopt for this purpose. We introduce\nICCA, an automated framework to evaluate such conversational adaptation as an\nin-context behavior in MLLMs. We evaluate several state-of-the-art MLLMs, and\nobserve that while they may understand the increasingly efficient language of\ntheir interlocutor, they do not spontaneously make their own language more\nefficient over time. This latter ability can only be elicited in some models\n(e.g., GPT-4) with heavy-handed prompting. This shows that this property of\nlinguistic interaction does not arise from current training regimes, even\nthough it is a common hallmark of human language. ICCA is available at\nhttps://github.com/lil-lab/ICCA.\n","authors":["Yilun Hua","Yoav Artzi"],"pdf_url":"https://arxiv.org/pdf/2408.01417v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.01416v1","updated":"2024-08-02T17:51:42Z","published":"2024-08-02T17:51:42Z","title":"The Quest for the Right Mediator: A History, Survey, and Theoretical\n  Grounding of Causal Interpretability","summary":"  Interpretability provides a toolset for understanding how and why neural\nnetworks behave in certain ways. However, there is little unity in the field:\nmost studies employ ad-hoc evaluations and do not share theoretical\nfoundations, making it difficult to measure progress and compare the pros and\ncons of different techniques. Furthermore, while mechanistic understanding is\nfrequently discussed, the basic causal units underlying these mechanisms are\noften not explicitly defined. In this paper, we propose a perspective on\ninterpretability research grounded in causal mediation analysis. Specifically,\nwe describe the history and current state of interpretability taxonomized\naccording to the types of causal units (mediators) employed, as well as methods\nused to search over mediators. We discuss the pros and cons of each mediator,\nproviding insights as to when particular kinds of mediators and search methods\nare most appropriate depending on the goals of a given study. We argue that\nthis framing yields a more cohesive narrative of the field, as well as\nactionable insights for future work. Specifically, we recommend a focus on\ndiscovering new mediators with better trade-offs between human-interpretability\nand compute-efficiency, and which can uncover more sophisticated abstractions\nfrom neural networks than the primarily linear mediators employed in current\nwork. We also argue for more standardized evaluations that enable principled\ncomparisons across mediator types, such that we can better understand when\nparticular causal units are better suited to particular use cases.\n","authors":["Aaron Mueller","Jannik Brinkmann","Millicent Li","Samuel Marks","Koyena Pal","Nikhil Prakash","Can Rager","Aruna Sankaranarayanan","Arnab Sen Sharma","Jiuding Sun","Eric Todd","David Bau","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2408.01416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01415v1","updated":"2024-08-02T17:43:34Z","published":"2024-08-02T17:43:34Z","title":"Conditional LoRA Parameter Generation","summary":"  Generative models have achieved remarkable success in image, video, and text\ndomains. Inspired by this, researchers have explored utilizing generative\nmodels to generate neural network parameters. However, these efforts have been\nlimited by the parameter size and the practicality of generating\nhigh-performance parameters. In this paper, we propose COND P-DIFF, a novel\napproach that demonstrates the feasibility of controllable high-performance\nparameter generation, particularly for LoRA (Low-Rank Adaptation) weights,\nduring the fine-tuning process. Specifically, we employ an autoencoder to\nextract efficient latent representations for parameters. We then train a\nconditional latent diffusion model to synthesize high-performing model\nparameters from random noise based on specific task conditions. Experimental\nresults in both computer vision and natural language processing domains\nconsistently demonstrate that COND P-DIFF can generate high-performance\nparameters conditioned on the given task. Moreover, we observe that the\nparameter distribution generated by COND P-DIFF exhibits differences compared\nto the distribution obtained through normal optimization methods, indicating a\ncertain level of generalization capability. Our work paves the way for further\nexploration of condition-driven parameter generation, offering a promising\ndirection for task-specific adaptation of neural networks.\n","authors":["Xiaolong Jin","Kai Wang","Dongwen Tang","Wangbo Zhao","Yukun Zhou","Junshu Tang","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.01415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01408v1","updated":"2024-08-02T17:33:52Z","published":"2024-08-02T17:33:52Z","title":"Derivation of Back-propagation for Graph Convolutional Networks using\n  Matrix Calculus and its Application to Explainable Artificial Intelligence","summary":"  This paper provides a comprehensive and detailed derivation of the\nbackpropagation algorithm for graph convolutional neural networks using matrix\ncalculus. The derivation is extended to include arbitrary element-wise\nactivation functions and an arbitrary number of layers. The study addresses two\nfundamental problems, namely node classification and link prediction. To\nvalidate our method, we compare it with reverse-mode automatic differentiation.\nThe experimental results demonstrate that the median sum of squared errors of\nthe updated weight matrices, when comparing our method to the approach using\nreverse-mode automatic differentiation, falls within the range of $10^{-18}$ to\n$10^{-14}$. These outcomes are obtained from conducting experiments on a\nfive-layer graph convolutional network, applied to a node classification\nproblem on Zachary's karate club social network and a link prediction problem\non a drug-drug interaction network. Finally, we show how the derived\nclosed-form solution can facilitate the development of explainable AI and\nsensitivity analysis.\n","authors":["Yen-Che Hsiao","Rongting Yue","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2408.01408v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2110.15501v4","updated":"2024-08-02T17:31:24Z","published":"2021-10-29T02:38:54Z","title":"Doubly Robust Interval Estimation for Optimal Policy Evaluation in\n  Online Learning","summary":"  Evaluating the performance of an ongoing policy plays a vital role in many\nareas such as medicine and economics, to provide crucial instructions on the\nearly-stop of the online experiment and timely feedback from the environment.\nPolicy evaluation in online learning thus attracts increasing attention by\ninferring the mean outcome of the optimal policy (i.e., the value) in\nreal-time. Yet, such a problem is particularly challenging due to the dependent\ndata generated in the online environment, the unknown optimal policy, and the\ncomplex exploration and exploitation trade-off in the adaptive experiment. In\nthis paper, we aim to overcome these difficulties in policy evaluation for\nonline learning. We explicitly derive the probability of exploration that\nquantifies the probability of exploring non-optimal actions under commonly used\nbandit algorithms. We use this probability to conduct valid inference on the\nonline conditional mean estimator under each action and develop the doubly\nrobust interval estimation (DREAM) method to infer the value under the\nestimated optimal policy in online learning. The proposed value estimator\nprovides double protection for consistency and is asymptotically normal with a\nWald-type confidence interval provided. Extensive simulation studies and real\ndata applications are conducted to demonstrate the empirical validity of the\nproposed DREAM method.\n","authors":["Ye Shen","Hengrui Cai","Rui Song"],"pdf_url":"https://arxiv.org/pdf/2110.15501v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01402v1","updated":"2024-08-02T17:25:34Z","published":"2024-08-02T17:25:34Z","title":"Pre-trained Language Models Improve the Few-shot Prompt Ability of\n  Decision Transformer","summary":"  Decision Transformer (DT) has emerged as a promising class of algorithms in\noffline reinforcement learning (RL) tasks, leveraging pre-collected datasets\nand Transformer's capability to model long sequences. Recent works have\ndemonstrated that using parts of trajectories from training tasks as prompts in\nDT enhances its performance on unseen tasks, giving rise to Prompt-DT methods.\nHowever, collecting data from specific environments can be both costly and\nunsafe in many scenarios, leading to suboptimal performance and limited\nfew-shot prompt abilities due to the data-hungry nature of Transformer-based\nmodels. Additionally, the limited datasets used in pre-training make it\nchallenging for Prompt-DT type of methods to distinguish between various RL\ntasks through prompts alone. To address these challenges, we introduce the\nLanguage model-initialized Prompt Decision Transformer (LPDT), which leverages\npre-trained language models for meta-RL tasks and fine-tunes the model using\nLow-rank Adaptation (LoRA). We further incorporate prompt regularization to\neffectively differentiate between tasks based on prompt feature\nrepresentations. Our approach integrates pre-trained language model and RL\ntasks seamlessly. Extensive empirical studies demonstrate that initializing\nwith a pre-trained language model significantly enhances the performance of\nPrompt-DT on unseen tasks compared to baseline methods.\n","authors":["Yu Yang","Pan Xu"],"pdf_url":"https://arxiv.org/pdf/2408.01402v1.pdf","comment":"2 figures, 8 tables. Accepted by the Training Agents with Foundation\n  Models Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2407.19631v2","updated":"2024-08-02T17:10:43Z","published":"2024-07-29T01:22:04Z","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System\n  Decision-making Competencies through Factorized Machine Self-confidence","summary":"  How can intelligent machines assess their competencies in completing tasks?\nThis question has come into focus for autonomous systems that algorithmically\nreason and make decisions under uncertainty. It is argued here that machine\nself-confidence - a form of meta-reasoning based on self-assessments of an\nagent's knowledge about the state of the world and itself, as well as its\nability to reason about and execute tasks - leads to many eminently computable\nand useful competency indicators for such agents. This paper presents a\nculmination of work on this concept in the form of a computational framework\ncalled Factorized Machine Self-confidence (FaMSeC), which provides a holistic\nengineering-focused description of factors driving an algorithmic\ndecision-making process, including: outcome assessment, solver quality, model\nquality, alignment quality, and past experience. In FaMSeC, self confidence\nindicators are derived from hierarchical `problem-solving statistics' embedded\nwithin broad classes of probabilistic decision-making algorithms such as Markov\ndecision processes. The problem-solving statistics are obtained by evaluating\nand grading probabilistic exceedance margins with respect to given competency\nstandards, which are specified for each of the various decision-making\ncompetency factors by the informee (e.g. a non-expert user or an expert system\ndesigner). This approach allows `algorithmic goodness of fit' evaluations to be\neasily incorporated into the design of many kinds of autonomous agents in the\nform of human-interpretable competency self-assessment reports. Detailed\ndescriptions and application examples for a Markov decision process agent show\nhow two of the FaMSeC factors (outcome assessment and solver quality) can be\ncomputed and reported for a range of possible tasking contexts through novel\nuse of meta-utility functions, behavior simulations, and surrogate prediction\nmodels.\n","authors":["Brett Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"pdf_url":"https://arxiv.org/pdf/2407.19631v2.pdf","comment":"59 pages, 22 figures, draft to be submitted for journal review"},{"id":"http://arxiv.org/abs/2408.01391v1","updated":"2024-08-02T17:01:36Z","published":"2024-08-02T17:01:36Z","title":"FT K-Means: A High-Performance K-Means on GPU with Fault Tolerance","summary":"  K-Means is a widely used algorithm in clustering, however, its efficiency is\nprimarily constrained by the computational cost of distance computing. Existing\nimplementations suffer from suboptimal utilization of computational units and\nlack resilience against soft errors. To address these challenges, we introduce\nFT K-Means, a high-performance GPU-accelerated implementation of K-Means with\nonline fault tolerance. We first present a stepwise optimization strategy that\nachieves competitive performance compared to NVIDIA's cuML library. We further\nimprove FT K-Means with a template-based code generation framework that\nsupports different data types and adapts to different input shapes. A novel\nwarp-level tensor-core error correction scheme is proposed to address the\nfailure of existing fault tolerance methods due to memory asynchronization\nduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100\nGPU demonstrate that FT K-Means without fault tolerance outperforms cuML's\nK-Means implementation, showing a performance increase of 10\\%-300\\% in\nscenarios involving irregular data shapes. Moreover, the fault tolerance\nfeature of FT K-Means introduces only an overhead of 11\\%, maintaining robust\nperformance even with tens of errors injected per second.\n","authors":["Shixun Wu","Yitong Ding","Yujia Zhai","Jinyang Liu","Jiajun Huang","Zizhe Jian","Huangliang Dai","Sheng Di","Bryan M. Wong","Zizhong Chen","Franck Cappello"],"pdf_url":"https://arxiv.org/pdf/2408.01391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01387v1","updated":"2024-08-02T16:55:08Z","published":"2024-08-02T16:55:08Z","title":"NeuralBeta: Estimating Beta Using Deep Learning","summary":"  Traditional approaches to estimating beta in finance often involve rigid\nassumptions and fail to adequately capture beta dynamics, limiting their\neffectiveness in use cases like hedging. To address these limitations, we have\ndeveloped a novel method using neural networks called NeuralBeta, which is\ncapable of handling both univariate and multivariate scenarios and tracking the\ndynamic behavior of beta. To address the issue of interpretability, we\nintroduce a new output layer inspired by regularized weighted linear\nregression, which provides transparency into the model's decision-making\nprocess. We conducted extensive experiments on both synthetic and market data,\ndemonstrating NeuralBeta's superior performance compared to benchmark methods\nacross various scenarios, especially instances where beta is highly\ntime-varying, e.g., during regime shifts in the market. This model not only\nrepresents an advancement in the field of beta estimation, but also shows\npotential for applications in other financial contexts that assume linear\nrelationships.\n","authors":["Yuxin Liu","Jimin Lin","Achintya Gopal"],"pdf_url":"https://arxiv.org/pdf/2408.01387v1.pdf","comment":"8 pages, 9 figures"},{"id":"http://arxiv.org/abs/2403.20328v2","updated":"2024-08-02T16:51:52Z","published":"2024-03-29T17:59:05Z","title":"Learning Visual Quadrupedal Loco-Manipulation from Demonstrations","summary":"  Quadruped robots are progressively being integrated into human environments.\nDespite the growing locomotion capabilities of quadrupedal robots, their\ninteraction with objects in realistic scenes is still limited. While additional\nrobotic arms on quadrupedal robots enable manipulating objects, they are\nsometimes redundant given that a quadruped robot is essentially a mobile unit\nequipped with four limbs, each possessing 3 degrees of freedom (DoFs). Hence,\nwe aim to empower a quadruped robot to execute real-world manipulation tasks\nusing only its legs. We decompose the loco-manipulation process into a\nlow-level reinforcement learning (RL)-based controller and a high-level\nBehavior Cloning (BC)-based planner. By parameterizing the manipulation\ntrajectory, we synchronize the efforts of the upper and lower layers, thereby\nleveraging the advantages of both RL and BC. Our approach is validated through\nsimulations and real-world experiments, demonstrating the robot's ability to\nperform tasks that demand mobility and high precision, such as lifting a basket\nfrom the ground while moving, closing a dishwasher, pressing a button, and\npushing a door. Project website: https://zhengmaohe.github.io/leg-manip\n","authors":["Zhengmao He","Kun Lei","Yanjie Ze","Koushil Sreenath","Zhongyu Li","Huazhe Xu"],"pdf_url":"https://arxiv.org/pdf/2403.20328v2.pdf","comment":"Published at IROS 2024. Project website:\n  https://zhengmaohe.github.io/leg-manip"},{"id":"http://arxiv.org/abs/2408.01382v1","updated":"2024-08-02T16:40:58Z","published":"2024-08-02T16:40:58Z","title":"Explaining a probabilistic prediction on the simplex with Shapley\n  compositions","summary":"  Originating in game theory, Shapley values are widely used for explaining a\nmachine learning model's prediction by quantifying the contribution of each\nfeature's value to the prediction. This requires a scalar prediction as in\nbinary classification, whereas a multiclass probabilistic prediction is a\ndiscrete probability distribution, living on a multidimensional simplex. In\nsuch a multiclass setting the Shapley values are typically computed separately\non each class in a one-vs-rest manner, ignoring the compositional nature of the\noutput distribution. In this paper, we introduce Shapley compositions as a\nwell-founded way to properly explain a multiclass probabilistic prediction,\nusing the Aitchison geometry from compositional data analysis. We prove that\nthe Shapley composition is the unique quantity satisfying linearity, symmetry\nand efficiency on the Aitchison simplex, extending the corresponding axiomatic\nproperties of the standard Shapley value. We demonstrate this proper multiclass\ntreatment in a range of scenarios.\n","authors":["Paul-Gauthier No","Miquel Perell-Nieto","Jean-Franois Bonastre","Peter Flach"],"pdf_url":"https://arxiv.org/pdf/2408.01382v1.pdf","comment":"To be published in ECAI2024's proceedings"},{"id":"http://arxiv.org/abs/2408.01379v1","updated":"2024-08-02T16:37:33Z","published":"2024-08-02T16:37:33Z","title":"Resampling and averaging coordinates on data","summary":"  We introduce algorithms for robustly computing intrinsic coordinates on point\nclouds. Our approach relies on generating many candidate coordinates by\nsubsampling the data and varying hyperparameters of the embedding algorithm\n(e.g., manifold learning). We then identify a subset of representative\nembeddings by clustering the collection of candidate coordinates and using\nshape descriptors from topological data analysis. The final output is the\nembedding obtained as an average of the representative embeddings using\ngeneralized Procrustes analysis. We validate our algorithm on both synthetic\ndata and experimental measurements from genomics, demonstrating robustness to\nnoise and outliers.\n","authors":["Andrew J. Blumberg","Mathieu Carriere","Jun Hou Fung","Michael A. Mandell"],"pdf_url":"https://arxiv.org/pdf/2408.01379v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01375v1","updated":"2024-08-02T16:32:30Z","published":"2024-08-02T16:32:30Z","title":"Adaptive Recruitment Resource Allocation to Improve Cohort\n  Representativeness in Participatory Biomedical Datasets","summary":"  Large participatory biomedical studies, studies that recruit individuals to\njoin a dataset, are gaining popularity and investment, especially for analysis\nby modern AI methods. Because they purposively recruit participants, these\nstudies are uniquely able to address a lack of historical representation, an\nissue that has affected many biomedical datasets. In this work, we define\nrepresentativeness as the similarity to a target population distribution of a\nset of attributes and our goal is to mirror the U.S. population across\ndistributions of age, gender, race, and ethnicity. Many participatory studies\nrecruit at several institutions, so we introduce a computational approach to\nadaptively allocate recruitment resources among sites to improve\nrepresentativeness. In simulated recruitment of 10,000-participant cohorts from\nmedical centers in the STAR Clinical Research Network, we show that our\napproach yields a more representative cohort than existing baselines. Thus, we\nhighlight the value of computational modeling in guiding recruitment efforts.\n","authors":["Victor Borza","Andrew Estornell","Ellen Wright Clayton","Chien-Ju Ho","Russell Rothman","Yevgeniy Vorobeychik","Bradley Malin"],"pdf_url":"https://arxiv.org/pdf/2408.01375v1.pdf","comment":"Accepted for publication at the American Medical Informatics\n  Association Annual Symposium 2024, 10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.11652v3","updated":"2024-08-02T16:30:55Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v3.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.01374v1","updated":"2024-08-02T16:29:54Z","published":"2024-08-02T16:29:54Z","title":"Hybrid Coordinate Descent for Efficient Neural Network Learning Using\n  Line Search and Gradient Descent","summary":"  This paper presents a novel coordinate descent algorithm leveraging a\ncombination of one-directional line search and gradient information for\nparameter updates for a squared error loss function. Each parameter undergoes\nupdates determined by either the line search or gradient method, contingent\nupon whether the modulus of the gradient of the loss with respect to that\nparameter surpasses a predefined threshold. Notably, a larger threshold value\nenhances algorithmic efficiency. Despite the potentially slower nature of the\nline search method relative to gradient descent, its parallelizability\nfacilitates computational time reduction. Experimental validation conducted on\na 2-layer Rectified Linear Unit network with synthetic data elucidates the\nimpact of hyperparameters on convergence rates and computational efficiency.\n","authors":["Yen-Che Hsiao","Abhishek Dutta"],"pdf_url":"https://arxiv.org/pdf/2408.01374v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01365v1","updated":"2024-08-02T16:17:59Z","published":"2024-08-02T16:17:59Z","title":"Data Debugging is NP-hard for Classifiers Trained with SGD","summary":"  Data debugging is to find a subset of the training data such that the model\nobtained by retraining on the subset has a better accuracy. A bunch of\nheuristic approaches are proposed, however, none of them are guaranteed to\nsolve this problem effectively. This leaves an open issue whether there exists\nan efficient algorithm to find the subset such that the model obtained by\nretraining on it has a better accuracy. To answer this open question and\nprovide theoretical basis for further study on developing better algorithms for\ndata debugging, we investigate the computational complexity of the problem\nnamed Debuggable. Given a machine learning model $\\mathcal{M}$ obtained by\ntraining on dataset $D$ and a test instance\n$(\\mathbf{x}_\\text{test},y_\\text{test})$ where\n$\\mathcal{M}(\\mathbf{x}_\\text{test})\\neq y_\\text{test}$, Debuggable is to\ndetermine whether there exists a subset $D^\\prime$ of $D$ such that the model\n$\\mathcal{M}^\\prime$ obtained by retraining on $D^\\prime$ satisfies\n$\\mathcal{M}^\\prime(\\mathbf{x}_\\text{test})=y_\\text{test}$. To cover a wide\nrange of commonly used models, we take SGD-trained linear classifier as the\nmodel and derive the following main results. (1) If the loss function and the\ndimension of the model are not fixed, Debuggable is NP-complete regardless of\nthe training order in which all the training samples are processed during SGD.\n(2) For hinge-like loss functions, a comprehensive analysis on the\ncomputational complexity of Debuggable is provided; (3) If the loss function is\na linear function, Debuggable can be solved in linear time, that is, data\ndebugging can be solved easily in this case. These results not only highlight\nthe limitations of current approaches but also offer new insights into data\ndebugging.\n","authors":["Zizheng Guo","Pengyu Chen","Yanzhang Fu","Dongjing Miao"],"pdf_url":"https://arxiv.org/pdf/2408.01365v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01362v1","updated":"2024-08-02T16:13:51Z","published":"2024-08-02T16:13:51Z","title":"Autoencoders in Function Space","summary":"  Autoencoders have found widespread application, in both their original\ndeterministic form and in their variational formulation (VAEs). In scientific\napplications it is often of interest to consider data that are comprised of\nfunctions; the same perspective is useful in image processing. In practice,\ndiscretisation (of differential equations arising in the sciences) or\npixellation (of images) renders problems finite dimensional, but conceiving\nfirst of algorithms that operate on functions, and only then discretising or\npixellating, leads to better algorithms that smoothly operate between different\nlevels of discretisation or pixellation. In this paper function-space versions\nof the autoencoder (FAE) and variational autoencoder (FVAE) are introduced,\nanalysed, and deployed. Well-definedness of the objective function governing\nVAEs is a subtle issue, even in finite dimension, and more so on function\nspace. The FVAE objective is well defined whenever the data distribution is\ncompatible with the chosen generative model; this happens, for example, when\nthe data arise from a stochastic differential equation. The FAE objective is\nvalid much more broadly, and can be straightforwardly applied to data governed\nby differential equations. Pairing these objectives with neural operator\narchitectures, which can thus be evaluated on any mesh, enables new\napplications of autoencoders to inpainting, superresolution, and generative\nmodelling of scientific data.\n","authors":["Justin Bunker","Mark Girolami","Hefin Lambley","Andrew M. Stuart","T. J. Sullivan"],"pdf_url":"https://arxiv.org/pdf/2408.01362v1.pdf","comment":"56 pages, 25 figures"},{"id":"http://arxiv.org/abs/2406.04551v2","updated":"2024-08-02T16:09:49Z","published":"2024-06-06T23:35:51Z","title":"Improving Geo-diversity of Generated Images with Contextualized Vendi\n  Score Guidance","summary":"  With the growing popularity of text-to-image generative models, there has\nbeen increasing focus on understanding their risks and biases. Recent work has\nfound that state-of-the-art models struggle to depict everyday objects with the\ntrue diversity of the real world and have notable gaps between geographic\nregions. In this work, we aim to increase the diversity of generated images of\ncommon objects such that per-region variations are representative of the real\nworld. We introduce an inference time intervention, contextualized Vendi Score\nGuidance (c-VSG), that guides the backwards steps of latent diffusion models to\nincrease the diversity of a sample as compared to a \"memory bank\" of previously\ngenerated images while constraining the amount of variation within that of an\nexemplar set of real-world contextualizing images. We evaluate c-VSG with two\ngeographically representative datasets and find that it substantially increases\nthe diversity of generated images, both for the worst performing regions and on\naverage, while simultaneously maintaining or improving image quality and\nconsistency. Additionally, qualitative analyses reveal that diversity of\ngenerated images is significantly improved, including along the lines of\nreductive region portrayals present in the original model. We hope that this\nwork is a step towards text-to-image generative models that reflect the true\ngeographic diversity of the world.\n","authors":["Reyhane Askari Hemmat","Melissa Hall","Alicia Sun","Candace Ross","Michal Drozdzal","Adriana Romero-Soriano"],"pdf_url":"https://arxiv.org/pdf/2406.04551v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.05737v3","updated":"2024-08-02T16:09:14Z","published":"2023-02-11T16:26:57Z","title":"A Reparameterized Discrete Diffusion Model for Text Generation","summary":"  This work studies discrete diffusion probabilistic models with applications\nto natural language generation. We derive an alternative yet equivalent\nformulation of the sampling from discrete diffusion processes and leverage this\ninsight to develop a family of reparameterized discrete diffusion models. The\nderived generic framework is highly flexible, offers a fresh perspective of the\ngeneration process in discrete diffusion models, and features more effective\ntraining and decoding techniques. We conduct extensive experiments to evaluate\nthe text generation capability of our model, demonstrating significant\nimprovements over existing diffusion models.\n","authors":["Lin Zheng","Jianbo Yuan","Lei Yu","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2302.05737v3.pdf","comment":"COLM 2024; Code available at\n  https://github.com/hkunlp/reparam-discrete-diffusion"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2403.13940v2","updated":"2024-08-02T15:54:21Z","published":"2024-03-20T19:25:11Z","title":"A multi-criteria approach for selecting an explanation from the set of\n  counterfactuals produced by an ensemble of explainers","summary":"  Counterfactuals are widely used to explain ML model predictions by providing\nalternative scenarios for obtaining the more desired predictions. They can be\ngenerated by a variety of methods that optimize different, sometimes\nconflicting, quality measures and produce quite different solutions. However,\nchoosing the most appropriate explanation method and one of the generated\ncounterfactuals is not an easy task. Instead of forcing the user to test many\ndifferent explanation methods and analysing conflicting solutions, in this\npaper, we propose to use a multi-stage ensemble approach that will select\nsingle counterfactual based on the multiple-criteria analysis. It offers a\ncompromise solution that scores well on several popular quality measures. This\napproach exploits the dominance relation and the ideal point decision aid\nmethod, which selects one counterfactual from the Pareto front. The conducted\nexperiments demonstrated that the proposed approach generates fully actionable\ncounterfactuals with attractive compromise values of the considered quality\nmeasures.\n","authors":["Ignacy Stpka","Mateusz Lango","Jerzy Stefanowski"],"pdf_url":"https://arxiv.org/pdf/2403.13940v2.pdf","comment":"17 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.01343v1","updated":"2024-08-02T15:41:16Z","published":"2024-08-02T15:41:16Z","title":"StitchFusion: Weaving Any Visual Modalities to Enhance Multimodal\n  Semantic Segmentation","summary":"  Multimodal semantic segmentation shows significant potential for enhancing\nsegmentation accuracy in complex scenes. However, current methods often\nincorporate specialized feature fusion modules tailored to specific modalities,\nthereby restricting input flexibility and increasing the number of training\nparameters. To address these challenges, we propose StitchFusion, a\nstraightforward yet effective modal fusion framework that integrates\nlarge-scale pre-trained models directly as encoders and feature fusers. This\napproach facilitates comprehensive multi-modal and multi-scale feature fusion,\naccommodating any visual modal inputs. Specifically, Our framework achieves\nmodal integration during encoding by sharing multi-modal visual information. To\nenhance information exchange across modalities, we introduce a\nmulti-directional adapter module (MultiAdapter) to enable cross-modal\ninformation transfer during encoding. By leveraging MultiAdapter to propagate\nmulti-scale information across pre-trained encoders during the encoding\nprocess, StitchFusion achieves multi-modal visual information integration\nduring encoding. Extensive comparative experiments demonstrate that our model\nachieves state-of-the-art performance on four multi-modal segmentation datasets\nwith minimal additional parameters. Furthermore, the experimental integration\nof MultiAdapter with existing Feature Fusion Modules (FFMs) highlights their\ncomplementary nature. Our code is available at StitchFusion_repo.\n","authors":["Bingyu Li","Da Zhang","Zhiyuan Zhao","Junyu Gao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11891v3","updated":"2024-08-02T15:38:38Z","published":"2023-10-18T11:20:59Z","title":"A Hyperparameter Study for Quantum Kernel Methods","summary":"  Quantum kernel methods are a promising method in quantum machine learning\nthanks to the guarantees connected to them. Their accessibility for analytic\nconsiderations also opens up the possibility of prescreening datasets based on\ntheir potential for a quantum advantage. To do so, earlier works developed the\ngeometric difference, which can be understood as a closeness measure between\ntwo kernel-based machine learning approaches, most importantly between a\nquantum kernel and a classical kernel. This metric links the quantum and\nclassical model complexities, and it was developed to bound generalization\nerror. Therefore, it raises the question of how this metric behaves in an\nempirical setting. In this work, we investigate the effects of hyperparameter\nchoice on the model performance and the generalization gap between classical\nand quantum kernels. The importance of hyperparameters is well known also for\nclassical machine learning. Of special interest are hyperparameters associated\nwith the quantum Hamiltonian evolution feature map, as well as the number of\nqubits to trace out before computing a projected quantum kernel. We conduct a\nthorough investigation of the hyperparameters across 11 datasets and we\nidentify certain aspects that can be exploited. Analyzing the effects of\ncertain hyperparameter settings on the empirical performance, as measured by\ncross validation accuracy, and generalization ability, as measured by geometric\ndifference described above, brings us one step closer to understanding the\npotential of quantum kernel methods on classical datasets.\n","authors":["Sebastian Egginger","Alona Sakhnenko","Jeanette Miriam Lorenz"],"pdf_url":"https://arxiv.org/pdf/2310.11891v3.pdf","comment":"Expanded implications of the paper"},{"id":"http://arxiv.org/abs/2408.01337v1","updated":"2024-08-02T15:34:05Z","published":"2024-08-02T15:34:05Z","title":"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language\n  Models","summary":"  Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.\n","authors":["Benno Weck","Ilaria Manco","Emmanouil Benetos","Elio Quinton","George Fazekas","Dmitry Bogdanov"],"pdf_url":"https://arxiv.org/pdf/2408.01337v1.pdf","comment":"Accepted at ISMIR 2024. Data: https://doi.org/10.5281/zenodo.12709974\n  Code: https://github.com/mulab-mir/muchomusic Supplementary material:\n  https://mulab-mir.github.io/muchomusic"},{"id":"http://arxiv.org/abs/2408.01336v1","updated":"2024-08-02T15:33:04Z","published":"2024-08-02T15:33:04Z","title":"Sparse Linear Regression when Noises and Covariates are Heavy-Tailed and\n  Contaminated by Outliers","summary":"  We investigate a problem estimating coefficients of linear regression under\nsparsity assumption when covariates and noises are sampled from heavy tailed\ndistributions. Additionally, we consider the situation where not only\ncovariates and noises are sampled from heavy tailed distributions but also\ncontaminated by outliers. Our estimators can be computed efficiently, and\nexhibit sharp error bounds.\n","authors":["Takeyuki Sasai","Hironori Fujisawa"],"pdf_url":"https://arxiv.org/pdf/2408.01336v1.pdf","comment":"This research builds on and improves the results of arxiv:2206.07594.\n  There will be no further update for the earlier manuscript"},{"id":"http://arxiv.org/abs/2404.02476v3","updated":"2024-08-02T15:30:14Z","published":"2024-04-03T05:32:10Z","title":"Deep Reinforcement Learning for Traveling Purchaser Problems","summary":"  The traveling purchaser problem (TPP) is an important combinatorial\noptimization problem with broad applications. Due to the coupling between\nrouting and purchasing, existing works on TPPs commonly address route\nconstruction and purchase planning simultaneously, which, however, leads to\nexact methods with high computational cost and heuristics with sophisticated\ndesign but limited performance. In sharp contrast, we propose a novel approach\nbased on deep reinforcement learning (DRL), which addresses route construction\nand purchase planning separately, while evaluating and optimizing the solution\nfrom a global perspective. The key components of our approach include a\nbipartite graph representation for TPPs to capture the market-product\nrelations, and a policy network that extracts information from the bipartite\ngraph and uses it to sequentially construct the route. One significant benefit\nof our framework is that we can efficiently construct the route using the\npolicy network, and once the route is determined, the associated purchasing\nplan can be easily derived through linear programming, while, leveraging DRL,\nwe can train the policy network to optimize the global solution objective.\nFurthermore, by introducing a meta-learning strategy, the policy network can be\ntrained stably on large-sized TPP instances, and generalize well across\ninstances of varying sizes and distributions, even to much larger instances\nthat are never seen during training. Experiments on various synthetic TPP\ninstances and the TPPLIB benchmark demonstrate that our DRL-based approach can\nsignificantly outperform well-established TPP heuristics, reducing the\noptimality gap by 40%-90%, and also showing an advantage in runtime, especially\non large-sized instances.\n","authors":["Haofeng Yuan","Rongping Zhu","Wanlu Yang","Shiji Song","Keyou You","Yuli Zhang","C. L. Philip Chen"],"pdf_url":"https://arxiv.org/pdf/2404.02476v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01332v1","updated":"2024-08-02T15:29:59Z","published":"2024-08-02T15:29:59Z","title":"HMDN: Hierarchical Multi-Distribution Network for Click-Through Rate\n  Prediction","summary":"  As the recommendation service needs to address increasingly diverse\ndistributions, such as multi-population, multi-scenario, multitarget, and\nmulti-interest, more and more recent works have focused on multi-distribution\nmodeling and achieved great progress. However, most of them only consider\nmodeling in a single multi-distribution manner, ignoring that mixed\nmulti-distributions often coexist and form hierarchical relationships. To\naddress these challenges, we propose a flexible modeling paradigm, named\nHierarchical Multi-Distribution Network (HMDN), which efficiently models these\nhierarchical relationships and can seamlessly integrate with existing\nmulti-distribution methods, such as Mixture of-Experts (MoE) and Dynamic-Weight\n(DW) models. Specifically, we first design a hierarchical multi-distribution\nrepresentation refinement module, employing a multi-level residual quantization\nto obtain fine-grained hierarchical representation. Then, the refined\nhierarchical representation is integrated into the existing single\nmulti-distribution models, seamlessly expanding them into mixed\nmulti-distribution models. Experimental results on both public and industrial\ndatasets validate the effectiveness and flexibility of HMDN.\n","authors":["Xingyu Lou","Yu Yang","Kuiyao Dong","Heyuan Huang","Wenyi Yu","Ping Wang","Xiu Li","Jun Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01331v1","updated":"2024-08-02T15:29:39Z","published":"2024-08-02T15:29:39Z","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","summary":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","authors":["Sifat Ut Taki","Spyridon Mastorakis","Arthi Padmanabhan"],"pdf_url":"https://arxiv.org/pdf/2408.01331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.00736v4","updated":"2024-08-02T15:26:37Z","published":"2023-01-02T16:11:05Z","title":"Mixed moving average field guided learning for spatio-temporal data","summary":"  Influenced mixed moving average fields are a versatile modeling class for\nspatio-temporal data. However, their predictive distribution is not generally\nknown. Under this modeling assumption, we define a novel spatio-temporal\nembedding and a theory-guided machine learning approach that employs a\ngeneralized Bayesian algorithm to make ensemble forecasts. We use Lipschitz\npredictors and determine fixed-time and any-time PAC Bayesian bounds in the\nbatch learning setting. Performing causal forecast is a highlight of our\nmethodology as its potential application to data with spatial and temporal\nshort and long-range dependence. We then test the performance of our learning\nmethodology by using linear predictors and data sets simulated from a\nspatio-temporal Ornstein-Uhlenbeck process.\n","authors":["Imma Valentina Curato","Orkun Furat","Lorenzo Proietti","Bennet Stroeh"],"pdf_url":"https://arxiv.org/pdf/2301.00736v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02659v2","updated":"2024-08-02T15:13:26Z","published":"2024-07-02T20:49:21Z","title":"LLMs Plagiarize: Ensuring Responsible Sourcing of Large Language Model\n  Training Data Through Knowledge Graph Comparison","summary":"  In light of recent legal allegations brought by publishers, newspapers, and\nother creators of copyrighted corpora against large language model developers\nwho use their copyrighted materials for training or fine-tuning purposes, we\npropose a novel system, a variant of a plagiarism detection system, that\nassesses whether a knowledge source has been used in the training or\nfine-tuning of a large language model. Unlike current methods, we utilize an\napproach that uses Resource Description Framework (RDF) triples to create\nknowledge graphs from both a source document and an LLM continuation of that\ndocument. These graphs are then analyzed with respect to content using cosine\nsimilarity and with respect to structure using a normalized version of graph\nedit distance that shows the degree of isomorphism. Unlike traditional\nplagiarism systems that focus on content matching and keyword identification\nbetween a source and a target corpus, our approach enables a broader and more\naccurate evaluation of similarity between a source document and LLM\ncontinuation by focusing on relationships between ideas and their organization\nwith regards to others. Additionally, our approach does not require access to\nLLM metrics like perplexity that may be unavailable in closed large language\nmodel \"black-box\" systems, as well as the training corpus. We thus assess\nwhether an LLM has \"plagiarized\" a corpus in its continuation through\nsimilarity measures. A prototype of our system will be found on a hyperlinked\nGitHub repository.\n","authors":["Devam Mondal","Carlo Lipizzi"],"pdf_url":"https://arxiv.org/pdf/2407.02659v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01318v1","updated":"2024-08-02T15:12:52Z","published":"2024-08-02T15:12:52Z","title":"Point Prediction for Streaming Data","summary":"  We present two new approaches for point prediction with streaming data. One\nis based on the Count-Min sketch (CMS) and the other is based on Gaussian\nprocess priors with a random bias. These methods are intended for the most\ngeneral predictive problems where no true model can be usefully formulated for\nthe data stream. In statistical contexts, this is often called the\n$\\mathcal{M}$-open problem class. Under the assumption that the data consists\nof i.i.d samples from a fixed distribution function $F$, we show that the\nCMS-based estimates of the distribution function are consistent.\n  We compare our new methods with two established predictors in terms of\ncumulative $L^1$ error. One is based on the Shtarkov solution (often called the\nnormalized maximum likelihood) in the normal experts setting and the other is\nbased on Dirichlet process priors. These comparisons are for two cases. The\nfirst is one-pass meaning that the updating of the predictors is done using the\nfact that the CMS is a sketch. For predictors that are not one-pass, we use\nstreaming $K$-means to give a representative subset of fixed size that can be\nupdated as data accumulate.\n  Preliminary computational work suggests that the one-pass median version of\nthe CMS method is rarely outperformed by the other methods for sufficiently\ncomplex data. We also find that predictors based on Gaussian process priors\nwith random biases perform well. The Shtarkov predictors we use here did not\nperform as well probably because we were only using the simplest example. The\nother predictors seemed to perform well mainly when the data did not look like\nthey came from an M-open data generator.\n","authors":["Aleena Chanda","N. V. Vinodchandran","Bertrand Clarke"],"pdf_url":"https://arxiv.org/pdf/2408.01318v1.pdf","comment":"42 pages, two figures"},{"id":"http://arxiv.org/abs/2408.01307v1","updated":"2024-08-02T15:00:04Z","published":"2024-08-02T15:00:04Z","title":"Decentralized Smoothing ADMM for Quantile Regression with Non-Convex\n  Sparse Penalties","summary":"  In the rapidly evolving internet-of-things (IoT) ecosystem, effective data\nanalysis techniques are crucial for handling distributed data generated by\nsensors. Addressing the limitations of existing methods, such as the\nsub-gradient approach, which fails to distinguish between active and non-active\ncoefficients effectively, this paper introduces the decentralized smoothing\nalternating direction method of multipliers (DSAD) for penalized quantile\nregression. Our method leverages non-convex sparse penalties like the minimax\nconcave penalty (MCP) and smoothly clipped absolute deviation (SCAD), improving\nthe identification and retention of significant predictors. DSAD incorporates a\ntotal variation norm within a smoothing ADMM framework, achieving consensus\namong distributed nodes and ensuring uniform model performance across disparate\ndata sources. This approach overcomes traditional convergence challenges\nassociated with non-convex penalties in decentralized settings. We present\ntheoretical proofs and extensive simulation results to validate the\neffectiveness of the DSAD, demonstrating its superiority in achieving reliable\nconvergence and enhancing estimation accuracy compared with prior methods.\n","authors":["Reza Mirzaeifard","Diyako Ghaderyan","Stefan Werner"],"pdf_url":"https://arxiv.org/pdf/2408.01307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13586v2","updated":"2024-08-02T14:59:48Z","published":"2024-05-22T12:30:25Z","title":"Bond Graphs for multi-physics informed Neural Networks for multi-variate\n  time series","summary":"  In the trend of hybrid Artificial Intelligence techniques, Physical-Informed\nMachine Learning has seen a growing interest. It operates mainly by imposing\ndata, learning, or architecture bias with simulation data, Partial Differential\nEquations, or equivariance and invariance properties. While it has shown great\nsuccess on tasks involving one physical domain, such as fluid dynamics,\nexisting methods are not adapted to tasks with complex multi-physical and\nmulti-domain phenomena. In addition, it is mainly formulated as an end-to-end\nlearning scheme. To address these challenges, we propose to leverage Bond\nGraphs, a multi-physics modeling approach, together with Message Passing Graph\nNeural Networks. We propose a Neural Bond graph Encoder (NBgE) producing\nmulti-physics-informed representations that can be fed into any task-specific\nmodel. It provides a unified way to integrate both data and architecture biases\nin deep learning. Our experiments on two challenging multi-domain physical\nsystems - a Direct Current Motor and the Respiratory System - demonstrate the\neffectiveness of our approach on a multivariate time-series forecasting task.\n","authors":["Alexis-Raja Brachet","Pierre-Yves Richard","Cline Hudelot"],"pdf_url":"https://arxiv.org/pdf/2405.13586v2.pdf","comment":"9 pages, 3 figures, paper under review"},{"id":"http://arxiv.org/abs/2407.21043v2","updated":"2024-08-02T14:58:54Z","published":"2024-07-22T04:07:12Z","title":"CP-Prompt: Composition-Based Cross-modal Prompting for\n  Domain-Incremental Continual Learning","summary":"  The key challenge of cross-modal domain-incremental learning (DIL) is to\nenable the learning model to continuously learn from novel data with different\nfeature distributions under the same task without forgetting old ones. However,\nexisting top-performing methods still cause high forgetting rates, by lacking\nintra-domain knowledge extraction and inter-domain common prompting strategy.\nIn this paper, we propose a simple yet effective framework, CP-Prompt, by\ntraining limited parameters to instruct a pre-trained model to learn new\ndomains and avoid forgetting existing feature distributions. CP-Prompt captures\nintra-domain knowledge by compositionally inserting personalized prompts on\nmulti-head self-attention layers and then learns the inter-domain knowledge\nwith a common prompting strategy. CP-Prompt shows superiority compared with\nstate-of-the-art baselines among three widely evaluated DIL tasks. The source\ncode is available at https://github.com/dannis97500/CP_Prompt.\n","authors":["Yu Feng","Zhen Tian","Yifan Zhu","Zongfu Han","Haoran Luo","Guangwei Zhang","Meina Song"],"pdf_url":"https://arxiv.org/pdf/2407.21043v2.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2401.13979v2","updated":"2024-08-02T14:50:05Z","published":"2024-01-25T06:45:32Z","title":"Routoo: Learning to Route to Large Language Models Effectively","summary":"  Developing foundational large language models (LLMs) is becoming increasingly\ncostly and inefficient. Also, closed-source and larger open-source models\ngenerally offer better response quality but come with higher inference costs\nthan smaller models. In this paper, we introduce Routoo, an architecture\ndesigned to optimize the selection of LLMs for specific prompts based on\nperformance, cost, and efficiency. Routoo consists of two key components: a\nperformance predictor and a cost-aware decoding. The performance predictor is a\nlightweight LLM that estimates the performance of various underlying LLMs\nwithout needing to execute and evaluate them. The cost-aware decoding then\nselects the most suitable model based on these predictions and other\nconstraints like cost and latency. We evaluated Routoo using the MMLU benchmark\nacross 57 domains employing open-source models. Our results show that Routoo\nmatches the performance of the Mixtral 8x7b model while reducing inference\ncosts by one-third. Additionally, by allowing increased costs, Routoo surpasses\nMixtral's accuracy by over 5% at equivalent costs, achieving an accuracy of\n75.9%. When integrating GPT4 into our model pool, Routoo nearly matches GPT4's\nperformance at half the cost and exceeds it with a 25% cost reduction. These\noutcomes highlight Routoo's potential to create new SOTA in a cost-effective\nmanner by leveraging the collective knowledge of multiple LLMs.\n","authors":["Alireza Mohammadshahi","Arshad Rafiq Shaikh","Majid Yazdani"],"pdf_url":"https://arxiv.org/pdf/2401.13979v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01301v1","updated":"2024-08-02T14:43:45Z","published":"2024-08-02T14:43:45Z","title":"A Decision-driven Methodology for Designing Uncertainty-aware AI\n  Self-Assessment","summary":"  Artificial intelligence (AI) has revolutionized decision-making processes and\nsystems throughout society and, in particular, has emerged as a significant\ntechnology in high-impact scenarios of national interest. Yet, despite AI's\nimpressive predictive capabilities in controlled settings, it still suffers\nfrom a range of practical setbacks preventing its widespread use in various\ncritical scenarios. In particular, it is generally unclear if a given AI\nsystem's predictions can be trusted by decision-makers in downstream\napplications. To address the need for more transparent, robust, and trustworthy\nAI systems, a suite of tools has been developed to quantify the uncertainty of\nAI predictions and, more generally, enable AI to \"self-assess\" the reliability\nof its predictions. In this manuscript, we categorize methods for AI\nself-assessment along several key dimensions and provide guidelines for\nselecting and designing the appropriate method for a practitioner's needs. In\nparticular, we focus on uncertainty estimation techniques that consider the\nimpact of self-assessment on the choices made by downstream decision-makers and\non the resulting costs and benefits of decision outcomes. To demonstrate the\nutility of our methodology for self-assessment design, we illustrate its use\nfor two realistic national-interest scenarios. This manuscript is a practical\nguide for machine learning engineers and AI system users to select the ideal\nself-assessment techniques for each problem.\n","authors":["Gregory Canal","Vladimir Leung","Philip Sage","Eric Heim","I-Jeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01300v1","updated":"2024-08-02T14:41:36Z","published":"2024-08-02T14:41:36Z","title":"Assessing Robustness of Machine Learning Models using Covariate\n  Perturbations","summary":"  As machine learning models become increasingly prevalent in critical\ndecision-making models and systems in fields like finance, healthcare, etc.,\nensuring their robustness against adversarial attacks and changes in the input\ndata is paramount, especially in cases where models potentially overfit. This\npaper proposes a comprehensive framework for assessing the robustness of\nmachine learning models through covariate perturbation techniques. We explore\nvarious perturbation strategies to assess robustness and examine their impact\non model predictions, including separate strategies for numeric and non-numeric\nvariables, summaries of perturbations to assess and compare model robustness\nacross different scenarios, and local robustness diagnosis to identify any\nregions in the data where a model is particularly unstable. Through empirical\nstudies on real world dataset, we demonstrate the effectiveness of our approach\nin comparing robustness across models, identifying the instabilities in the\nmodel, and enhancing model robustness.\n","authors":["Arun Prakash R","Anwesha Bhattacharyya","Joel Vaughan","Vijayan N. Nair"],"pdf_url":"https://arxiv.org/pdf/2408.01300v1.pdf","comment":"31 pages, 11 figures, 14 tables"},{"id":"http://arxiv.org/abs/2408.00713v2","updated":"2024-08-02T14:40:19Z","published":"2024-08-01T16:58:54Z","title":"Reinforcement Learning applied to Insurance Portfolio Pursuit","summary":"  When faced with a new customer, many factors contribute to an insurance\nfirm's decision of what offer to make to that customer. In addition to the\nexpected cost of providing the insurance, the firm must consider the other\noffers likely to be made to the customer, and how sensitive the customer is to\ndifferences in price. Moreover, firms often target a specific portfolio of\ncustomers that could depend on, e.g., age, location, and occupation. Given such\na target portfolio, firms may choose to modulate an individual customer's offer\nbased on whether the firm desires the customer within their portfolio. We term\nthe problem of modulating offers to achieve a desired target portfolio the\nportfolio pursuit problem. Having formulated the portfolio pursuit problem as a\nsequential decision making problem, we devise a novel reinforcement learning\nalgorithm for its solution. We test our method on a complex synthetic market\nenvironment, and demonstrate that it outperforms a baseline method which mimics\ncurrent industry approaches to portfolio pursuit.\n","authors":["Edward James Young","Alistair Rogers","Elliott Tong","James Jordon"],"pdf_url":"https://arxiv.org/pdf/2408.00713v2.pdf","comment":"16 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.01297v1","updated":"2024-08-02T14:37:28Z","published":"2024-08-02T14:37:28Z","title":"Optimal Mixed Integer Linear Optimization Trained Multivariate\n  Classification Trees","summary":"  Multivariate decision trees are powerful machine learning tools for\nclassification and regression that attract many researchers and industry\nprofessionals. An optimal binary tree has two types of vertices, (i) branching\nvertices which have exactly two children and where datapoints are assessed on a\nset of discrete features and (ii) leaf vertices at which datapoints are given a\nprediction, and can be obtained by solving a biobjective optimization problem\nthat seeks to (i) maximize the number of correctly classified datapoints and\n(ii) minimize the number of branching vertices. Branching vertices are linear\ncombinations of training features and therefore can be thought of as\nhyperplanes. In this paper, we propose two cut-based mixed integer linear\noptimization (MILO) formulations for designing optimal binary classification\ntrees (leaf vertices assign discrete classes). Our models leverage on-the-fly\nidentification of minimal infeasible subsystems (MISs) from which we derive\ncutting planes that hold the form of packing constraints. We show theoretical\nimprovements on the strongest flow-based MILO formulation currently in the\nliterature and conduct experiments on publicly available datasets to show our\nmodels' ability to scale, strength against traditional branch and bound\napproaches, and robustness in out-of-sample test performance. Our code and data\nare available on GitHub.\n","authors":["Brandon Alston","Illya V. Hicks"],"pdf_url":"https://arxiv.org/pdf/2408.01297v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2206.04857"},{"id":"http://arxiv.org/abs/2405.03389v2","updated":"2024-08-02T14:33:32Z","published":"2024-05-06T11:51:09Z","title":"Don't Waste Your Time: Early Stopping Cross-Validation","summary":"  State-of-the-art automated machine learning systems for tabular data often\nemploy cross-validation; ensuring that measured performances generalize to\nunseen data, or that subsequent ensembling does not overfit. However, using\nk-fold cross-validation instead of holdout validation drastically increases the\ncomputational cost of validating a single configuration. While ensuring better\ngeneralization and, by extension, better performance, the additional cost is\noften prohibitive for effective model selection within a time budget. We aim to\nmake model selection with cross-validation more effective. Therefore, we study\nearly stopping the process of cross-validation during model selection. We\ninvestigate the impact of early stopping on random search for two algorithms,\nMLP and random forest, across 36 classification datasets. We further analyze\nthe impact of the number of folds by considering 3-, 5-, and 10-folds. In\naddition, we investigate the impact of early stopping with Bayesian\noptimization instead of random search and also repeated cross-validation. Our\nexploratory study shows that even a simple-to-understand and easy-to-implement\nmethod consistently allows model selection to converge faster; in ~94% of all\ndatasets, on average by ~214%. Moreover, stopping cross-validation enables\nmodel selection to explore the search space more exhaustively by considering\n+167% configurations on average within one hour, while also obtaining better\noverall performance.\n","authors":["Edward Bergman","Lennart Purucker","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2405.03389v2.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024); for code, see\n  https://github.com/automl/DontWasteYourTime-early-stopping"},{"id":"http://arxiv.org/abs/2408.01294v1","updated":"2024-08-02T14:31:37Z","published":"2024-08-02T14:31:37Z","title":"Feature Clock: High-Dimensional Effects in Two-Dimensional Plots","summary":"  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n","authors":["Olga Ovcharenko","Rita Sevastjanova","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2408.01294v1.pdf","comment":"To be published in IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2407.14962v3","updated":"2024-08-02T14:26:55Z","published":"2024-07-20T18:48:35Z","title":"Recent Advances in Generative AI and Large Language Models: Current\n  Status, Challenges, and Perspectives","summary":"  The emergence of Generative Artificial Intelligence (AI) and Large Language\nModels (LLMs) has marked a new era of Natural Language Processing (NLP),\nintroducing unprecedented capabilities that are revolutionizing various\ndomains. This paper explores the current state of these cutting-edge\ntechnologies, demonstrating their remarkable advancements and wide-ranging\napplications. Our paper contributes to providing a holistic perspective on the\ntechnical foundations, practical applications, and emerging challenges within\nthe evolving landscape of Generative AI and LLMs. We believe that understanding\nthe generative capabilities of AI systems and the specific context of LLMs is\ncrucial for researchers, practitioners, and policymakers to collaboratively\nshape the responsible and ethical integration of these technologies into\nvarious domains. Furthermore, we identify and address main research gaps,\nproviding valuable insights to guide future research endeavors within the AI\nresearch community.\n","authors":["Desta Haileselassie Hagos","Rick Battle","Danda B. Rawat"],"pdf_url":"https://arxiv.org/pdf/2407.14962v3.pdf","comment":"This version is accepted for publication in the journal of IEEE\n  Transactions on Artificial Intelligence (TAI)"},{"id":"http://arxiv.org/abs/2308.11635v2","updated":"2024-08-02T14:25:40Z","published":"2023-08-13T23:54:40Z","title":"Semi-Supervised Dual-Stream Self-Attentive Adversarial Graph Contrastive\n  Learning for Cross-Subject EEG-based Emotion Recognition","summary":"  Electroencephalography (EEG) is an objective tool for emotion recognition\nwith promising applications. However, the scarcity of labeled data remains a\nmajor challenge in this field, limiting the widespread use of EEG-based emotion\nrecognition. In this paper, a semi-supervised Dual-stream Self-Attentive\nAdversarial Graph Contrastive learning framework (termed as DS-AGC) is proposed\nto tackle the challenge of limited labeled data in cross-subject EEG-based\nemotion recognition. The DS-AGC framework includes two parallel streams for\nextracting non-structural and structural EEG features. The non-structural\nstream incorporates a semi-supervised multi-domain adaptation method to\nalleviate distribution discrepancy among labeled source domain, unlabeled\nsource domain, and unknown target domain. The structural stream develops a\ngraph contrastive learning method to extract effective graph-based feature\nrepresentation from multiple EEG channels in a semi-supervised manner. Further,\na self-attentive fusion module is developed for feature fusion, sample\nselection, and emotion recognition, which highlights EEG features more relevant\nto emotions and data samples in the labeled source domain that are closer to\nthe target domain. Extensive experiments conducted on two benchmark databases\n(SEED and SEED-IV) using a semi-supervised cross-subject leave-one-subject-out\ncross-validation evaluation scheme show that the proposed model outperforms\nexisting methods under different incomplete label conditions (with an average\nimprovement of 5.83% on SEED and 6.99% on SEED-IV), demonstrating its\neffectiveness in addressing the label scarcity problem in cross-subject\nEEG-based emotion recognition.\n","authors":["Weishan Ye","Zhiguo Zhang","Fei Teng","Min Zhang","Jianhong Wang","Dong Ni","Fali Li","Peng Xu","Zhen Liang"],"pdf_url":"https://arxiv.org/pdf/2308.11635v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.06496"},{"id":"http://arxiv.org/abs/2408.01283v1","updated":"2024-08-02T14:09:39Z","published":"2024-08-02T14:09:39Z","title":"A Tiny Supervised ODL Core with Auto Data Pruning for Human Activity\n  Recognition","summary":"  In this paper, we introduce a low-cost and low-power tiny supervised\non-device learning (ODL) core that can address the distributional shift of\ninput data for human activity recognition. Although ODL for resource-limited\nedge devices has been studied recently, how exactly to provide the training\nlabels to these devices at runtime remains an open-issue. To address this\nproblem, we propose to combine an automatic data pruning with supervised ODL to\nreduce the number queries needed to acquire predicted labels from a nearby\nteacher device and thus save power consumption during model retraining. The\ndata pruning threshold is automatically tuned, eliminating a manual threshold\ntuning. As a tinyML solution at a few mW for the human activity recognition, we\ndesign a supervised ODL core that supports our automatic data pruning using a\n45nm CMOS process technology. We show that the required memory size for the\ncore is smaller than the same-shaped multilayer perceptron (MLP) and the power\nconsumption is only 3.39mW. Experiments using a human activity recognition\ndataset show that the proposed automatic data pruning reduces the communication\nvolume by 55.7% and power consumption accordingly with only 0.9% accuracy loss.\n","authors":["Hiroki Matsutani","Radu Marculescu"],"pdf_url":"https://arxiv.org/pdf/2408.01283v1.pdf","comment":"IEEE BSN 2024 (accepted)"},{"id":"http://arxiv.org/abs/2310.04561v2","updated":"2024-08-02T14:08:59Z","published":"2023-10-06T19:55:40Z","title":"DragD3D: Realistic Mesh Editing with Rigidity Control Driven by 2D\n  Diffusion Priors","summary":"  Direct mesh editing and deformation are key components in the geometric\nmodeling and animation pipeline. Mesh editing methods are typically framed as\noptimization problems combining user-specified vertex constraints with a\nregularizer that determines the position of the rest of the vertices. The\nchoice of the regularizer is key to the realism and authenticity of the final\nresult. Physics and geometry-based regularizers are not aware of the global\ncontext and semantics of the object, and the more recent deep learning priors\nare limited to a specific class of 3D object deformations. Our main\ncontribution is a vertex-based mesh editing method called DragD3D based on (1)\na novel optimization formulation that decouples the rotation and stretch\ncomponents of the deformation and combines a 3D geometric regularizer with (2)\nthe recently introduced DDS loss which scores the faithfulness of the rendered\n2D image to one from a diffusion model. Thus, our deformation method achieves\nglobally realistic shape deformation which is not restricted to any class of\nobjects. Our new formulation optimizes directly the transformation of the\nneural Jacobian field explicitly separating the rotational and stretching\ncomponents. The objective function of the optimization combines the approximate\ngradients of DDS and the gradients from the geometric loss to satisfy the\nvertex constraints. Additional user control over desired global shape\ndeformation is made possible by allowing explicit per-triangle deformation\ncontrol as well as explicit separation of rotational and stretching components\nof the deformation. We show that our deformations can be controlled to yield\nrealistic shape deformations that are aware of the global context of the\nobjects, and provide better results than just using geometric regularizers.\n","authors":["Tianhao Xie","Eugene Belilovsky","Sudhir Mudur","Tiberiu Popa"],"pdf_url":"https://arxiv.org/pdf/2310.04561v2.pdf","comment":"11 pages, 8 figures, project page:\n  https://tianhaoxie.github.io/project/DragD3D/"},{"id":"http://arxiv.org/abs/2404.09916v2","updated":"2024-08-02T13:59:12Z","published":"2024-04-15T16:43:13Z","title":"Comprehensive Library of Variational LSE Solvers","summary":"  Linear systems of equations can be found in various mathematical domains, as\nwell as in the field of machine learning. By employing noisy intermediate-scale\nquantum devices, variational solvers promise to accelerate finding solutions\nfor large systems. Although there is a wealth of theoretical research on these\nalgorithms, only fragmentary implementations exist. To fill this gap, we have\ndeveloped the variational-lse-solver framework, which realizes existing\napproaches in literature, and introduces several enhancements. The\nuser-friendly interface is designed for researchers that work at the\nabstraction level of identifying and developing end-to-end applications.\n","authors":["Nico Meyer","Martin Rhn","Jakob Murauer","Axel Plinge","Christopher Mutschler","Daniel D. Scherer"],"pdf_url":"https://arxiv.org/pdf/2404.09916v2.pdf","comment":"Accepted to the 2nd International Workshop on Quantum Machine\n  Learning: From Research to Practice (QML@QCE 2024), Montr\\'eal, Qu\\'ebec,\n  Canada. 4 pages, 2 figures, 1 table"},{"id":"http://arxiv.org/abs/2408.01273v1","updated":"2024-08-02T13:55:26Z","published":"2024-08-02T13:55:26Z","title":"Certified Robust Invariant Polytope Training in Neural Controlled ODEs","summary":"  We consider a nonlinear control system modeled as an ordinary differential\nequation subject to disturbance, with a state feedback controller parameterized\nas a feedforward neural network. We propose a framework for training\ncontrollers with certified robust forward invariant polytopes, where any\ntrajectory initialized inside the polytope remains within the polytope,\nregardless of the disturbance. First, we parameterize a family of lifted\ncontrol systems in a higher dimensional space, where the original neural\ncontrolled system evolves on an invariant subspace of each lifted system. We\nuse interval analysis and neural network verifiers to further construct a\nfamily of lifted embedding systems, carefully capturing the knowledge of this\ninvariant subspace. If the vector field of any lifted embedding system\nsatisfies a sign constraint at a single point, then a certain convex polytope\nof the original system is robustly forward invariant. Treating the neural\nnetwork controller and the lifted system parameters as variables, we propose an\nalgorithm to train controllers with certified forward invariant polytopes in\nthe closed-loop control system. Through two examples, we demonstrate how the\nsimplicity of the sign constraint allows our approach to scale with system\ndimension to over $50$ states, and outperform state-of-the-art Lyapunov-based\nsampling approaches in runtime.\n","authors":["Akash Harapanahalli","Samuel Coogan"],"pdf_url":"https://arxiv.org/pdf/2408.01273v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02604v2","updated":"2024-08-02T13:45:53Z","published":"2024-07-02T18:43:10Z","title":"D-Rax: Domain-specific Radiologic assistant leveraging multi-modal data\n  and eXpert model predictions","summary":"  Large vision language models (VLMs) have progressed incredibly from research\nto applicability for general-purpose use cases. LLaVA-Med, a pioneering large\nlanguage and vision assistant for biomedicine, can perform multi-modal\nbiomedical image and data analysis to provide a natural language interface for\nradiologists. While it is highly generalizable and works with multi-modal data,\nit is currently limited by well-known challenges that exist in the large\nlanguage model space. Hallucinations and imprecision in responses can lead to\nmisdiagnosis which currently hinder the clinical adaptability of VLMs. To\ncreate precise, user-friendly models in healthcare, we propose D-Rax -- a\ndomain-specific, conversational, radiologic assistance tool that can be used to\ngain insights about a particular radiologic image. In this study, we enhance\nthe conversational analysis of chest X-ray (CXR) images to support radiological\nreporting, offering comprehensive insights from medical imaging and aiding in\nthe formulation of accurate diagnosis. D-Rax is achieved by fine-tuning the\nLLaVA-Med architecture on our curated enhanced instruction-following data,\ncomprising of images, instructions, as well as disease diagnosis and\ndemographic predictions derived from MIMIC-CXR imaging data, CXR-related visual\nquestion answer (VQA) pairs, and predictive outcomes from multiple expert AI\nmodels. We observe statistically significant improvement in responses when\nevaluated for both open and close-ended conversations. Leveraging the power of\nstate-of-the-art diagnostic models combined with VLMs, D-Rax empowers\nclinicians to interact with medical images using natural language, which could\npotentially streamline their decision-making process, enhance diagnostic\naccuracy, and conserve their time.\n","authors":["Hareem Nisar","Syed Muhammad Anwar","Zhifan Jiang","Abhijeet Parida","Ramon Sanchez-Jacob","Vishwesh Nath","Holger R. Roth","Marius George Linguraru"],"pdf_url":"https://arxiv.org/pdf/2407.02604v2.pdf","comment":"accepted to the MICCAI 2024 Second International Workshop on\n  Foundation Models for General Medical AI"},{"id":"http://arxiv.org/abs/2408.01257v1","updated":"2024-08-02T13:27:56Z","published":"2024-08-02T13:27:56Z","title":"Detection and Characterization of Coordinated Online Behavior: A Survey","summary":"  Coordination is a fundamental aspect of life. The advent of social media has\nmade it integral also to online human interactions, such as those that\ncharacterize thriving online communities and social movements. At the same\ntime, coordination is also core to effective disinformation, manipulation, and\nhate campaigns. This survey collects, categorizes, and critically discusses the\nbody of work produced as a result of the growing interest on coordinated online\nbehavior. We reconcile industry and academic definitions, propose a\ncomprehensive framework to study coordinated online behavior, and review and\ncritically discuss the existing detection and characterization methods. Our\nanalysis identifies open challenges and promising directions of research,\nserving as a guide for scholars, practitioners, and policymakers in\nunderstanding and addressing the complexities inherent to online coordination.\n","authors":["Lorenzo Mannocci","Michele Mazza","Anna Monreale","Maurizio Tesconi","Stefano Cresci"],"pdf_url":"https://arxiv.org/pdf/2408.01257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15327v2","updated":"2024-08-02T13:25:16Z","published":"2024-06-21T17:40:46Z","title":"Fine-grained Attention in Hierarchical Transformers for Tabular\n  Time-series","summary":"  Tabular data is ubiquitous in many real-life systems. In particular,\ntime-dependent tabular data, where rows are chronologically related, is\ntypically used for recording historical events, e.g., financial transactions,\nhealthcare records, or stock history. Recently, hierarchical variants of the\nattention mechanism of transformer architectures have been used to model\ntabular time-series data. At first, rows (or columns) are encoded separately by\ncomputing attention between their fields. Subsequently, encoded rows (or\ncolumns) are attended to one another to model the entire tabular time-series.\nWhile efficient, this approach constrains the attention granularity and limits\nits ability to learn patterns at the field-level across separate rows, or\ncolumns. We take a first step to address this gap by proposing Fieldy, a\nfine-grained hierarchical model that contextualizes fields at both the row and\ncolumn levels. We compare our proposal against state of the art models on\nregression and classification tasks using public tabular time-series datasets.\nOur results show that combining row-wise and column-wise attention improves\nperformance without increasing model size. Code and data are available at\nhttps://github.com/raphaaal/fieldy.\n","authors":["Raphael Azorin","Zied Ben Houidi","Massimo Gallo","Alessandro Finamore","Pietro Michiardi"],"pdf_url":"https://arxiv.org/pdf/2406.15327v2.pdf","comment":"9 pages; Camera Ready version"},{"id":"http://arxiv.org/abs/2305.18493v3","updated":"2024-08-02T13:16:48Z","published":"2023-05-29T13:47:51Z","title":"Insights from the Design Space Exploration of Flow-Guided Nanoscale\n  Localization","summary":"  Nanodevices with Terahertz (THz)-based wireless communication capabilities\nare providing a primer for flow-guided localization within the human\nbloodstreams. Such localization is allowing for assigning the locations of\nsensed events with the events themselves, providing benefits along the lines of\nearly and precise diagnostics, and reduced costs and invasiveness. Flow-guided\nlocalization is still in a rudimentary phase, with only a handful of works\ntargeting the problem. Nonetheless, the performance assessments of the proposed\nsolutions are already carried out in a non-standardized way, usually along a\nsingle performance metric, and ignoring various aspects that are relevant at\nsuch a scale (e.g., nanodevices' limited energy) and for such a challenging\nenvironment (e.g., extreme attenuation of in-body THz propagation). As such,\nthese assessments feature low levels of realism and cannot be compared in an\nobjective way. Toward addressing this issue, we account for the environmental\nand scale-related peculiarities of the scenario and assess the performance of\ntwo state-of-the-art flow-guided localization approaches along a set of\nheterogeneous performance metrics such as the accuracy and reliability of\nlocalization.\n","authors":["Filip Lemic","Gerard Calvo Bartra","Arnau Brosa Lpez","Jorge Torres Gmez","Jakob Struye","Falko Dressler","Sergi Abadal","Xavier Costa Perez"],"pdf_url":"https://arxiv.org/pdf/2305.18493v3.pdf","comment":"6 pages, 4 figures, 2 tables, 14 references, accepted at ACM\n  NanoCom'24"},{"id":"http://arxiv.org/abs/2408.01248v1","updated":"2024-08-02T13:10:33Z","published":"2024-08-02T13:10:33Z","title":"Deep progressive reinforcement learning-based flexible resource\n  scheduling framework for IRS and UAV-assisted MEC system","summary":"  The intelligent reflection surface (IRS) and unmanned aerial vehicle\n(UAV)-assisted mobile edge computing (MEC) system is widely used in temporary\nand emergency scenarios. Our goal is to minimize the energy consumption of the\nMEC system by jointly optimizing UAV locations, IRS phase shift, task\noffloading, and resource allocation with a variable number of UAVs. To this\nend, we propose a Flexible REsource Scheduling (FRES) framework by employing a\nnovel deep progressive reinforcement learning which includes the following\ninnovations: Firstly, a novel multi-task agent is presented to deal with the\nmixed integer nonlinear programming (MINLP) problem. The multi-task agent has\ntwo output heads designed for different tasks, in which a classified head is\nemployed to make offloading decisions with integer variables while a fitting\nhead is applied to solve resource allocation with continuous variables.\nSecondly, a progressive scheduler is introduced to adapt the agent to the\nvarying number of UAVs by progressively adjusting a part of neurons in the\nagent. This structure can naturally accumulate experiences and be immune to\ncatastrophic forgetting. Finally, a light taboo search (LTS) is introduced to\nenhance the global search of the FRES. The numerical results demonstrate the\nsuperiority of the FRES framework which can make real-time and optimal resource\nscheduling even in dynamic MEC systems.\n","authors":["Li Dong","Feibo Jiang","Minjie Wang","Yubo Peng","Xiaolong Li"],"pdf_url":"https://arxiv.org/pdf/2408.01248v1.pdf","comment":"13 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.01244v1","updated":"2024-08-02T13:05:33Z","published":"2024-08-02T13:05:33Z","title":"Automated Classification of Dry Bean Varieties Using XGBoost and SVM\n  Models","summary":"  This paper presents a comparative study on the automated classification of\nseven different varieties of dry beans using machine learning models.\nLeveraging a dataset of 12,909 dry bean samples, reduced from an initial 13,611\nthrough outlier removal and feature extraction, we applied Principal Component\nAnalysis (PCA) for dimensionality reduction and trained two multiclass\nclassifiers: XGBoost and Support Vector Machine (SVM). The models were\nevaluated using nested cross-validation to ensure robust performance assessment\nand hyperparameter tuning. The XGBoost and SVM models achieved overall correct\nclassification rates of 94.00% and 94.39%, respectively. The results underscore\nthe efficacy of these machine learning approaches in agricultural applications,\nparticularly in enhancing the uniformity and efficiency of seed classification.\nThis study contributes to the growing body of work on precision agriculture,\ndemonstrating that automated systems can significantly support seed quality\ncontrol and crop yield optimization. Future work will explore incorporating\nmore diverse datasets and advanced algorithms to further improve classification\naccuracy.\n","authors":["Ramtin Ardeshirifar"],"pdf_url":"https://arxiv.org/pdf/2408.01244v1.pdf","comment":"8 pages, 4 figurs"},{"id":"http://arxiv.org/abs/2407.21310v2","updated":"2024-08-02T13:03:00Z","published":"2024-07-31T03:26:14Z","title":"MSMA: Multi-agent Trajectory Prediction in Connected and Autonomous\n  Vehicle Environment with Multi-source Data Integration","summary":"  The prediction of surrounding vehicle trajectories is crucial for\ncollision-free path planning. In this study, we focus on a scenario where a\nconnected and autonomous vehicle (CAV) serves as the central agent, utilizing\nboth sensors and communication technologies to perceive its surrounding\ntraffics consisting of autonomous vehicles (AVs), connected vehicles (CVs), and\nhuman-driven vehicles (HDVs). Our trajectory prediction task is aimed at all\nthe detected surrounding vehicles. To effectively integrate the multi-source\ndata from both sensor and communication technologies, we propose a deep\nlearning framework called MSMA utilizing a cross-attention module for\nmulti-source data fusion. Vector map data is utilized to provide contextual\ninformation. The trajectory dataset is collected in CARLA simulator with\nsynthesized data errors introduced. Numerical experiments demonstrate that in a\nmixed traffic flow scenario, the integration of data from different sources\nenhances our understanding of the environment. This notably improves trajectory\nprediction accuracy, particularly in situations with a high CV market\npenetration rate. The code is available at: https://github.com/xichennn/MSMA.\n","authors":["Xi Chen","Rahul Bhadani","Zhanbo Sun","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2407.21310v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09602v2","updated":"2024-08-02T13:00:54Z","published":"2024-07-12T18:00:02Z","title":"Real-time gravitational-wave inference for binary neutron stars using\n  machine learning","summary":"  Mergers of binary neutron stars (BNSs) emit signals in both the\ngravitational-wave (GW) and electromagnetic (EM) spectra. Famously, the 2017\nmulti-messenger observation of GW170817 led to scientific discoveries across\ncosmology, nuclear physics, and gravity. Central to these results were the sky\nlocalization and distance obtained from GW data, which, in the case of\nGW170817, helped to identify the associated EM transient, AT 2017gfo, 11 hours\nafter the GW signal. Fast analysis of GW data is critical for directing\ntime-sensitive EM observations; however, due to challenges arising from the\nlength and complexity of signals, it is often necessary to make approximations\nthat sacrifice accuracy. Here, we present a machine learning framework that\nperforms complete BNS inference in just one second without making any such\napproximations. Our approach enhances multi-messenger observations by providing\n(i) accurate localization even before the merger; (ii) improved localization\nprecision by $\\sim30\\%$ compared to approximate low-latency methods; and (iii)\ndetailed information on luminosity distance, inclination, and masses, which can\nbe used to prioritize expensive telescope time. Additionally, the flexibility\nand reduced cost of our method open new opportunities for equation-of-state\nstudies. Finally, we demonstrate that our method scales to extremely long\nsignals, up to an hour in length, thus serving as a blueprint for data analysis\nfor next-generation ground- and space-based detectors.\n","authors":["Maximilian Dax","Stephen R. Green","Jonathan Gair","Nihar Gupte","Michael Prrer","Vivien Raymond","Jonas Wildberger","Jakob H. Macke","Alessandra Buonanno","Bernhard Schlkopf"],"pdf_url":"https://arxiv.org/pdf/2407.09602v2.pdf","comment":"8+8 pages, 3+7 figures"},{"id":"http://arxiv.org/abs/2408.00374v2","updated":"2024-08-02T13:00:46Z","published":"2024-08-01T08:32:03Z","title":"Conformal Trajectory Prediction with Multi-View Data Integration in\n  Cooperative Driving","summary":"  Current research on trajectory prediction primarily relies on data collected\nby onboard sensors of an ego vehicle. With the rapid advancement in connected\ntechnologies, such as vehicle-to-vehicle (V2V) and vehicle-to-infrastructure\n(V2I) communication, valuable information from alternate views becomes\naccessible via wireless networks. The integration of information from\nalternative views has the potential to overcome the inherent limitations\nassociated with a single viewpoint, such as occlusions and limited field of\nview. In this work, we introduce V2INet, a novel trajectory prediction\nframework designed to model multi-view data by extending existing single-view\nmodels. Unlike previous approaches where the multi-view data is manually fused\nor formulated as a separate training stage, our model supports end-to-end\ntraining, enhancing both flexibility and performance. Moreover, the predicted\nmultimodal trajectories are calibrated by a post-hoc conformal prediction\nmodule to get valid and efficient confidence regions. We evaluated the entire\nframework using the real-world V2I dataset V2X-Seq. Our results demonstrate\nsuperior performance in terms of Final Displacement Error (FDE) and Miss Rate\n(MR) using a single GPU. The code is publicly available at:\n\\url{https://github.com/xichennn/V2I_trajectory_prediction}.\n","authors":["Xi Chen","Rahul Bhadani","Larry Head"],"pdf_url":"https://arxiv.org/pdf/2408.00374v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01239v1","updated":"2024-08-02T12:58:08Z","published":"2024-08-02T12:58:08Z","title":"Tailoring Graph Neural Network-based Flow-guided Localization to\n  Individual Bloodstreams and Activities","summary":"  Flow-guided localization using in-body nanodevices in the bloodstream is\nexpected to be beneficial for early disease detection, continuous monitoring of\nbiological conditions, and targeted treatment. The nanodevices face size and\npower constraints that produce erroneous raw data for localization purposes.\nOn-body anchors receive this data, and use it to derive the locations of\ndiagnostic events of interest. Different Machine Learning (ML) approaches have\nbeen recently proposed for this task, yet they are currently restricted to a\nreference bloodstream of a resting patient. As such, they are unable to deal\nwith the physical diversity of patients' bloodstreams and cannot provide\ncontinuous monitoring due to changes in individual patient's activities. Toward\naddressing these issues for the current State-of-the-Art (SotA) flow-guided\nlocalization approach based on Graph Neural Networks (GNNs), we propose a\npipeline for GNN adaptation based on individual physiological indicators\nincluding height, weight, and heart rate. Our results indicate that the\nproposed adaptions are beneficial in reconciling the individual differences\nbetween bloodstreams and activities.\n","authors":["Pablo Galvn","Filip Lemic","Gerard Calvo Bartra","Sergi Abadal","Xavier Costa Prez"],"pdf_url":"https://arxiv.org/pdf/2408.01239v1.pdf","comment":"7 pages, 9 figures, 2 tables, 16 references, accepted at ACM\n  NanoCom'25"},{"id":"http://arxiv.org/abs/2408.01230v1","updated":"2024-08-02T12:40:01Z","published":"2024-08-02T12:40:01Z","title":"HeteroMorpheus: Universal Control Based on Morphological Heterogeneity\n  Modeling","summary":"  In the field of robotic control, designing individual controllers for each\nrobot leads to high computational costs. Universal control policies, applicable\nacross diverse robot morphologies, promise to mitigate this challenge.\nPredominantly, models based on Graph Neural Networks (GNN) and Transformers are\nemployed, owing to their effectiveness in capturing relational dynamics across\na robot's limbs. However, these models typically employ homogeneous graph\nstructures that overlook the functional diversity of different limbs. To bridge\nthis gap, we introduce HeteroMorpheus, a novel method based on heterogeneous\ngraph Transformer. This method uniquely addresses limb heterogeneity, fostering\nbetter representation of robot dynamics of various morphologies. Through\nextensive experiments we demonstrate the superiority of HeteroMorpheus against\nstate-of-the-art methods in the capability of policy generalization, including\nzero-shot generalization and sample-efficient transfer to unfamiliar robot\nmorphologies.\n","authors":["YiFan Hao","Yang Yang","Junru Song","Wei Peng","Weien Zhou","Tingsong Jiang","Wen Yao"],"pdf_url":"https://arxiv.org/pdf/2408.01230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01215v1","updated":"2024-08-02T12:04:19Z","published":"2024-08-02T12:04:19Z","title":"ZNorm: Z-Score Gradient Normalization for Accelerating Neural Network\n  Training","summary":"  The rapid advancements in deep learning necessitate efficient training\nmethods for deep neural networks (DNNs). As models grow in complexity,\nvanishing and exploding gradients impede convergence and performance. We\npropose Z-Score Normalization for Gradient Descent (ZNorm), an innovative\ntechnique that adjusts only the gradients to enhance training efficiency and\nimprove model performance. ZNorm normalizes the overall gradients, providing\nconsistent gradient scaling across layers, thereby reducing the risks of\nvanishing and exploding gradients. Our extensive experiments on CIFAR-10 and\nmedical datasets demonstrate that ZNorm not only accelerates convergence but\nalso enhances performance metrics. ZNorm consistently outperforms existing\nmethods, achieving superior results using the same computational settings. In\nmedical imaging applications, ZNorm improves tumor prediction and segmentation\nperformances, underscoring its practical utility. These findings highlight\nZNorm's potential as a robust and versatile tool for improving the efficiency\nand effectiveness of deep neural network training across a wide range of\narchitectures and applications.\n","authors":["Juyoung Yun","Hoyoung Kim","Suin Cho","Hangil Kang"],"pdf_url":"https://arxiv.org/pdf/2408.01215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00950v2","updated":"2024-08-02T11:54:57Z","published":"2023-12-13T12:57:55Z","title":"Unsupervised Graph-based Learning Method for Sub-band Allocation in 6G\n  Subnetworks","summary":"  In this paper, we present an unsupervised approach for frequency sub-band\nallocation in wireless networks using graph-based learning. We consider a dense\ndeployment of subnetworks in the factory environment with a limited number of\nsub-bands which must be optimally allocated to coordinate inter-subnetwork\ninterference. We model the subnetwork deployment as a conflict graph and\npropose an unsupervised learning approach inspired by the graph colouring\nheuristic and the Potts model to optimize the sub-band allocation using graph\nneural networks. The numerical evaluation shows that the proposed method\nachieves close performance to the centralized greedy colouring sub-band\nallocation heuristic with lower computational time complexity. In addition, it\nincurs reduced signalling overhead compared to iterative optimization\nheuristics that require all the mutual interfering channel information. We\nfurther demonstrate that the method is robust to different network settings.\n","authors":["Daniel Abode","Ramoni Adeogun","Lou Salan","Renato Abreu","Thomas Jacobsen","Gilberto Berardinelli"],"pdf_url":"https://arxiv.org/pdf/2401.00950v2.pdf","comment":"Accepted in VTC Fall 2024"},{"id":"http://arxiv.org/abs/2408.01200v1","updated":"2024-08-02T11:29:21Z","published":"2024-08-02T11:29:21Z","title":"Certifiably Robust Encoding Schemes","summary":"  Quantum machine learning uses principles from quantum mechanics to process\ndata, offering potential advances in speed and performance. However, previous\nwork has shown that these models are susceptible to attacks that manipulate\ninput data or exploit noise in quantum circuits. Following this, various\nstudies have explored the robustness of these models. These works focus on the\nrobustness certification of manipulations of the quantum states. We extend this\nline of research by investigating the robustness against perturbations in the\nclassical data for a general class of data encoding schemes. We show that for\nsuch schemes, the addition of suitable noise channels is equivalent to\nevaluating the mean value of the noiseless classifier at the smoothed data,\nakin to Randomized Smoothing from classical machine learning. Using our general\nframework, we show that suitable additions of phase-damping noise channels\nimprove empirical and provable robustness for the considered class of encoding\nschemes.\n","authors":["Aman Saxena","Tom Wollschlger","Nicola Franco","Jeanette Miriam Lorenz","Stephan Gnnemann"],"pdf_url":"https://arxiv.org/pdf/2408.01200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01187v1","updated":"2024-08-02T11:14:41Z","published":"2024-08-02T11:14:41Z","title":"Optimizing Variational Quantum Circuits Using Metaheuristic Strategies\n  in Reinforcement Learning","summary":"  Quantum Reinforcement Learning (QRL) offers potential advantages over\nclassical Reinforcement Learning, such as compact state space representation\nand faster convergence in certain scenarios. However, practical benefits\nrequire further validation. QRL faces challenges like flat solution landscapes,\nwhere traditional gradient-based methods are inefficient, necessitating the use\nof gradient-free algorithms. This work explores the integration of\nmetaheuristic algorithms -- Particle Swarm Optimization, Ant Colony\nOptimization, Tabu Search, Genetic Algorithm, Simulated Annealing, and Harmony\nSearch -- into QRL. These algorithms provide flexibility and efficiency in\nparameter optimization. Evaluations in $5\\times5$ MiniGrid Reinforcement\nLearning environments show that, all algorithms yield near-optimal results,\nwith Simulated Annealing and Particle Swarm Optimization performing best. In\nthe Cart Pole environment, Simulated Annealing, Genetic Algorithms, and\nParticle Swarm Optimization achieve optimal results, while the others perform\nslightly better than random action selection. These findings demonstrate the\npotential of Particle Swarm Optimization and Simulated Annealing for efficient\nQRL learning, emphasizing the need for careful algorithm selection and\nadaptation.\n","authors":["Michael Klle","Daniel Seidl","Maximilian Zorn","Philipp Altmann","Jonas Stein","Thomas Gabor"],"pdf_url":"https://arxiv.org/pdf/2408.01187v1.pdf","comment":"Accepted at QCE24 - QCRL24 Workshop"},{"id":"http://arxiv.org/abs/2408.01180v1","updated":"2024-08-02T11:02:38Z","published":"2024-08-02T11:02:38Z","title":"Nested Music Transformer: Sequentially Decoding Compound Tokens in\n  Symbolic Music and Audio Generation","summary":"  Representing symbolic music with compound tokens, where each token consists\nof several different sub-tokens representing a distinct musical feature or\nattribute, offers the advantage of reducing sequence length. While previous\nresearch has validated the efficacy of compound tokens in music sequence\nmodeling, predicting all sub-tokens simultaneously can lead to suboptimal\nresults as it may not fully capture the interdependencies between them. We\nintroduce the Nested Music Transformer (NMT), an architecture tailored for\ndecoding compound tokens autoregressively, similar to processing flattened\ntokens, but with low memory usage. The NMT consists of two transformers: the\nmain decoder that models a sequence of compound tokens and the sub-decoder for\nmodeling sub-tokens of each compound token. The experiment results showed that\napplying the NMT to compound tokens can enhance the performance in terms of\nbetter perplexity in processing various symbolic music datasets and discrete\naudio tokens from the MAESTRO dataset.\n","authors":["Jiwoo Ryu","Hao-Wen Dong","Jongmin Jung","Dasaem Jeong"],"pdf_url":"https://arxiv.org/pdf/2408.01180v1.pdf","comment":"Accepted at 25th International Society for Music Information\n  Retrieval Conference (ISMIR 2024)"},{"id":"http://arxiv.org/abs/2408.01173v1","updated":"2024-08-02T10:47:10Z","published":"2024-08-02T10:47:10Z","title":"Sustainable Diffusion-based Incentive Mechanism for Generative AI-driven\n  Digital Twins in Industrial Cyber-Physical Systems","summary":"  Industrial Cyber-Physical Systems (ICPSs) are an integral component of modern\nmanufacturing and industries. By digitizing data throughout the product life\ncycle, Digital Twins (DTs) in ICPSs enable a shift from current industrial\ninfrastructures to intelligent and adaptive infrastructures. Thanks to data\nprocess capability, Generative Artificial Intelligence (GAI) can drive the\nconstruction and update of DTs to improve predictive accuracy and prepare for\ndiverse smart manufacturing. However, mechanisms that leverage sensing\nIndustrial Internet of Things (IIoT) devices to share data for the construction\nof DTs are susceptible to adverse selection problems. In this paper, we first\ndevelop a GAI-driven DT architecture for ICPSs. To address the adverse\nselection problem caused by information asymmetry, we propose a contract theory\nmodel and develop the sustainable diffusion-based soft actor-critic algorithm\nto identify the optimal feasible contract. Specifically, we leverage the\ndynamic structured pruning technique to reduce parameter numbers of actor\nnetworks, allowing sustainability and efficient implementation of the proposed\nalgorithm. Finally, numerical results demonstrate the effectiveness of the\nproposed scheme.\n","authors":["Jinbo Wen","Jiawen Kang","Dusit Niyato","Yang Zhang","Shiwen Mao"],"pdf_url":"https://arxiv.org/pdf/2408.01173v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01163v1","updated":"2024-08-02T10:25:19Z","published":"2024-08-02T10:25:19Z","title":"Domain Adaptation-Enhanced Searchlight: Enabling brain decoding from\n  visual perception to mental imagery","summary":"  In cognitive neuroscience and brain-computer interface research, accurately\npredicting imagined stimuli is crucial. This study investigates the\neffectiveness of Domain Adaptation (DA) in enhancing imagery prediction using\nprimarily visual data from fMRI scans of 18 subjects. Initially, we train a\nbaseline model on visual stimuli to predict imagined stimuli, utilizing data\nfrom 14 brain regions. We then develop several models to improve imagery\nprediction, comparing different DA methods. Our results demonstrate that DA\nsignificantly enhances imagery prediction, especially with the Regular Transfer\napproach. We then conduct a DA-enhanced searchlight analysis using Regular\nTransfer, followed by permutation-based statistical tests to identify brain\nregions where imagery decoding is consistently above chance across subjects.\nOur DA-enhanced searchlight predicts imagery contents in a highly distributed\nset of brain regions, including the visual cortex and the frontoparietal\ncortex, thereby outperforming standard cross-domain classification methods. The\ncomplete code and data for this paper have been made openly available for the\nuse of the scientific community.\n","authors":["Alexander Olza","David Soto","Roberto Santana"],"pdf_url":"https://arxiv.org/pdf/2408.01163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01156v1","updated":"2024-08-02T10:16:28Z","published":"2024-08-02T10:16:28Z","title":"TCR-GPT: Integrating Autoregressive Model and Reinforcement Learning for\n  T-Cell Receptor Repertoires Generation","summary":"  T-cell receptors (TCRs) play a crucial role in the immune system by\nrecognizing and binding to specific antigens presented by infected or cancerous\ncells. Understanding the sequence patterns of TCRs is essential for developing\ntargeted immune therapies and designing effective vaccines. Language models,\nsuch as auto-regressive transformers, offer a powerful solution to this problem\nby learning the probability distributions of TCR repertoires, enabling the\ngeneration of new TCR sequences that inherit the underlying patterns of the\nrepertoire. We introduce TCR-GPT, a probabilistic model built on a decoder-only\ntransformer architecture, designed to uncover and replicate sequence patterns\nin TCR repertoires. TCR-GPT demonstrates an accuracy of 0.953 in inferring\nsequence probability distributions measured by Pearson correlation coefficient.\nFurthermore, by leveraging Reinforcement Learning(RL), we adapted the\ndistribution of TCR sequences to generate TCRs capable of recognizing specific\npeptides, offering significant potential for advancing targeted immune\ntherapies and vaccine development. With the efficacy of RL, fine-tuned\npretrained TCR-GPT models demonstrated the ability to produce TCR repertoires\nlikely to bind specific peptides, illustrating RL's efficiency in enhancing the\nmodel's adaptability to the probability distributions of biologically relevant\nTCR sequences.\n","authors":["Yicheng Lin","Dandan Zhang","Yun Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01156v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01144v1","updated":"2024-08-02T09:44:18Z","published":"2024-08-02T09:44:18Z","title":"Enhanced Prediction of Ventilator-Associated Pneumonia in Patients with\n  Traumatic Brain Injury Using Advanced Machine Learning Techniques","summary":"  Background: Ventilator-associated pneumonia (VAP) in traumatic brain injury\n(TBI) patients poses a significant mortality risk and imposes a considerable\nfinancial burden on patients and healthcare systems. Timely detection and\nprognostication of VAP in TBI patients are crucial to improve patient outcomes\nand alleviate the strain on healthcare resources.\n  Methods: We implemented six machine learning models using the MIMIC-III\ndatabase. Our methodology included preprocessing steps, such as feature\nselection with CatBoost and expert opinion, addressing class imbalance with the\nSynthetic Minority Oversampling Technique (SMOTE), and rigorous model tuning\nthrough 5-fold cross-validation to optimize hyperparameters. Key models\nevaluated included SVM, Logistic Regression, Random Forest, XGBoost, ANN, and\nAdaBoost. Additionally, we conducted SHAP analysis to determine feature\nimportance and performed an ablation study to assess feature impacts on model\nperformance.\n  Results: XGBoost outperformed the baseline models and the best existing\nliterature. We used metrics, including AUC, Accuracy, Specificity, Sensitivity,\nF1 Score, PPV, and NPV. XGBoost demonstrated the highest performance with an\nAUC of 0.940 and an Accuracy of 0.875, which are 23.4% and 23.5% higher than\nthe best results in the existing literature, with an AUC of 0.706 and an\nAccuracy of 0.640, respectively. This enhanced performance underscores the\nmodels' effectiveness in clinical settings.\n  Conclusions: This study enhances the predictive modeling of VAP in TBI\npatients, improving early detection and intervention potential. Refined feature\nselection and advanced ensemble techniques significantly boosted model accuracy\nand reliability, offering promising directions for future clinical applications\nand medical diagnostics research.\n","authors":["Negin Ashrafi","Armin Abdollahi","Maryam Pishgar"],"pdf_url":"https://arxiv.org/pdf/2408.01144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01141v1","updated":"2024-08-02T09:37:55Z","published":"2024-08-02T09:37:55Z","title":"Machine learning topological energy braiding of non-Bloch bands","summary":"  Machine learning has been used to identify phase transitions in a variety of\nphysical systems. However, there is still a lack of relevant research on\nnon-Bloch energy braiding in non-Hermitian systems. In this work, we study\nnon-Bloch energy braiding in one-dimensional non-Hermitian systems using\nunsupervised and supervised methods. In unsupervised learning, we use diffusion\nmaps to successfully identify non-Bloch energy braiding without any prior\nknowledge and combine it with k-means to cluster different topological elements\ninto clusters, such as Unlink and Hopf link. In supervised learning, we train a\nConvolutional Neural Network (CNN) based on Bloch energy data to predict not\nonly Bloch energy braiding but also non-Bloch energy braiding with an accuracy\napproaching 100%. By analysing the CNN, we can ascertain that the network has\nsuccessfully acquired the ability to recognise the braiding topology of the\nenergy bands. The present study demonstrates the considerable potential of\nmachine learning in the identification of non-Hermitian topological phases and\nenergy braiding.\n","authors":["Shuwei Shi","Shibing Chu","Yuee Xie","Yuanping Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01141v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01129v1","updated":"2024-08-02T09:18:41Z","published":"2024-08-02T09:18:41Z","title":"A Survey of Mamba","summary":"  Deep learning, as a vital technique, has sparked a notable revolution in\nartificial intelligence. As the most representative architecture, Transformers\nhave empowered numerous advanced models, especially the large language models\nthat comprise billions of parameters, becoming a cornerstone in deep learning.\nDespite the impressive achievements, Transformers still face inherent\nlimitations, particularly the time-consuming inference resulting from the\nquadratic computation complexity of attention calculation. Recently, a novel\narchitecture named Mamba, drawing inspiration from classical state space\nmodels, has emerged as a promising alternative for building foundation models,\ndelivering comparable modeling abilities to Transformers while preserving\nnear-linear scalability concerning sequence length. This has sparked an\nincreasing number of studies actively exploring Mamba's potential to achieve\nimpressive performance across diverse domains. Given such rapid evolution,\nthere is a critical need for a systematic review that consolidates existing\nMamba-empowered models, offering a comprehensive understanding of this emerging\nmodel architecture. In this survey, we therefore conduct an in-depth\ninvestigation of recent Mamba-associated studies, covering from three main\naspects: the advancements of Mamba-based models, the techniques of adapting\nMamba to diverse data, and the applications where Mamba can excel.\nSpecifically, we first recall the foundational knowledge of various\nrepresentative deep learning models and the details of Mamba as preliminaries.\nThen, to showcase the significance of Mamba, we comprehensively review the\nrelated studies focusing on Mamba models' architecture design, data\nadaptability, and applications. Finally, we present an discussion of current\nlimitations and explore various promising research directions to provide deeper\ninsights for future investigations.\n","authors":["Haohao Qu","Liangbo Ning","Rui An","Wenqi Fan","Tyler Derr","Xin Xu","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.01129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v3","updated":"2024-08-02T08:55:52Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.765 and an AP value of 0.415 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.15132v2","updated":"2024-08-02T08:49:14Z","published":"2024-02-23T06:33:51Z","title":"Improving Sentence Embeddings with Automatic Generation of Training Data\n  Using Few-shot Examples","summary":"  Decoder-based large language models (LLMs) have shown high performance on\nmany tasks in natural language processing. This is also true for sentence\nembedding learning, where a decoder-based model, PromptEOL, has achieved the\nbest performance on semantic textual similarity (STS) tasks. However, PromptEOL\nrequires a manually annotated natural language inference (NLI) dataset for\nfine-tuning. We aim to improve sentence embeddings without using large manually\nannotated datasets by automatically generating an NLI dataset with an LLM and\nusing it for fine-tuning of PromptEOL. To achieve this, we explore methods of\ndata generation suitable for sentence embedding learning in this study.\nSpecifically, we will focus on automatic dataset generation through few-shot\nlearning and explore the appropriate methods to leverage few-shot examples.\nExperimental results on the STS tasks demonstrate that our approach outperforms\nexisting models in settings without large manually annotated datasets.\n","authors":["Soma Sato","Hayato Tsukagoshi","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2402.15132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18819v2","updated":"2024-08-02T08:22:57Z","published":"2024-02-29T03:06:10Z","title":"Dual Operating Modes of In-Context Learning","summary":"  In-context learning (ICL) exhibits dual operating modes: task learning, i.e.,\nacquiring a new skill from in-context samples, and task retrieval, i.e.,\nlocating and activating a relevant pretrained skill. Recent theoretical work\ninvestigates various mathematical models to analyze ICL, but existing models\nexplain only one operating mode at a time. We introduce a probabilistic model,\nwith which one can explain the dual operating modes of ICL simultaneously.\nFocusing on in-context learning of linear functions, we extend existing models\nfor pretraining data by introducing multiple task groups and task-dependent\ninput distributions. We then analyze the behavior of the optimally pretrained\nmodel under the squared loss, i.e., the MMSE estimator of the label given\nin-context examples. Regarding pretraining task distribution as prior and\nin-context examples as the observation, we derive the closed-form expression of\nthe task posterior distribution. With the closed-form expression, we obtain a\nquantitative understanding of the two operating modes of ICL. Furthermore, we\nshed light on an unexplained phenomenon observed in practice: under certain\nsettings, the ICL risk initially increases and then decreases with more\nin-context examples. Our model offers a plausible explanation for this \"early\nascent\" phenomenon: a limited number of in-context samples may lead to the\nretrieval of an incorrect skill, thereby increasing the risk, which will\neventually diminish as task learning takes effect with more in-context samples.\nWe also theoretically analyze ICL with biased labels, e.g., zero-shot ICL,\nwhere in-context examples are assigned random labels. Lastly, we validate our\nfindings and predictions via experiments involving Transformers and large\nlanguage models.\n","authors":["Ziqian Lin","Kangwook Lee"],"pdf_url":"https://arxiv.org/pdf/2402.18819v2.pdf","comment":"54 pages, 23 figures"},{"id":"http://arxiv.org/abs/2408.01094v1","updated":"2024-08-02T08:13:18Z","published":"2024-08-02T08:13:18Z","title":"An Encoding--Searching Separation Perspective on Bi-Encoder Neural\n  Search","summary":"  This paper reviews, analyzes, and proposes a new perspective on the\nbi-encoder architecture for neural search. While the bi-encoder architecture is\nwidely used due to its simplicity and scalability at test time, it has some\nnotable issues such as low performance on seen datasets and weak zero-shot\nperformance on new datasets. In this paper, we analyze these issues and\nsummarize two main critiques: the encoding information bottleneck problem and\nlimitations of the basic assumption of embedding search. We then construct a\nthought experiment to logically analyze the encoding and searching operations\nand challenge the basic assumption of embedding search. Building on these\nobservations, we propose a new perspective on the bi-encoder architecture\ncalled the \\textit{encoding--searching separation} perspective, which\nconceptually and practically separates the encoding and searching operations.\nThis new perspective is applied to explain the root cause of the identified\nissues and discuss ways to mitigate the problems. Finally, we discuss the\nimplications of the ideas underlying the new perspective, the design surface\nthat it exposes and the potential research directions arising from it.\n","authors":["Hung-Nghiep Tran","Akiko Aizawa","Atsuhiro Takasu"],"pdf_url":"https://arxiv.org/pdf/2408.01094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11660v2","updated":"2024-08-02T07:42:37Z","published":"2024-01-22T02:33:38Z","title":"Differentiable Tree Search Network","summary":"  In decision-making problems with limited training data, policy functions\napproximated using deep neural networks often exhibit suboptimal performance.\nAn alternative approach involves learning a world model from the limited data\nand determining actions through online search. However, the performance is\nadversely affected by compounding errors arising from inaccuracies in the\nlearned world model. While methods like TreeQN have attempted to address these\ninaccuracies by incorporating algorithmic inductive biases into the neural\nnetwork architectures, the biases they introduce are often weak and\ninsufficient for complex decision-making tasks. In this work, we introduce\nDifferentiable Tree Search Network (D-TSN), a novel neural network architecture\nthat significantly strengthens the inductive bias by embedding the algorithmic\nstructure of a best-first online search algorithm. D-TSN employs a learned\nworld model to conduct a fully differentiable online search. The world model is\njointly optimized with the search algorithm, enabling the learning of a robust\nworld model and mitigating the effect of prediction inaccuracies. Further, we\nnote that a naive incorporation of best-first search could lead to a\ndiscontinuous loss function in the parameter space. We address this issue by\nadopting a stochastic tree expansion policy, formulating search tree expansion\nas another decision-making task, and introducing an effective variance\nreduction technique for the gradient computation. We evaluate D-TSN in an\noffline-RL setting with a limited training data scenario on Procgen games and\ngrid navigation task, and demonstrate that D-TSN outperforms popular model-free\nand model-based baselines.\n","authors":["Dixant Mittal","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2401.11660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01062v1","updated":"2024-08-02T07:29:49Z","published":"2024-08-02T07:29:49Z","title":"Universality of kernel random matrices and kernel regression in the\n  quadratic regime","summary":"  Kernel ridge regression (KRR) is a popular class of machine learning models\nthat has become an important tool for understanding deep learning. Much of the\nfocus has been on studying the proportional asymptotic regime, $n \\asymp d$,\nwhere $n$ is the number of training samples and $d$ is the dimension of the\ndataset. In this regime, under certain conditions on the data distribution, the\nkernel random matrix involved in KRR exhibits behavior akin to that of a linear\nkernel. In this work, we extend the study of kernel regression to the quadratic\nasymptotic regime, where $n \\asymp d^2$. In this regime, we demonstrate that a\nbroad class of inner-product kernels exhibit behavior similar to a quadratic\nkernel. Specifically, we establish an operator norm approximation bound for the\ndifference between the original kernel random matrix and a quadratic kernel\nrandom matrix with additional correction terms compared to the Taylor expansion\nof the kernel functions. The approximation works for general data distributions\nunder a Gaussian-moment-matching assumption with a covariance structure. This\nnew approximation is utilized to obtain a limiting spectral distribution of the\noriginal kernel matrix and characterize the precise asymptotic training and\ngeneralization errors for KRR in the quadratic regime when $n/d^2$ converges to\na non-zero constant. The generalization errors are obtained for both\ndeterministic and random teacher models. Our proof techniques combine moment\nmethods, Wick's formula, orthogonal polynomials, and resolvent analysis of\nrandom matrices with correlated entries.\n","authors":["Parthe Pandit","Zhichao Wang","Yizhe Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01062v1.pdf","comment":"75 pages"},{"id":"http://arxiv.org/abs/2408.01050v1","updated":"2024-08-02T06:56:59Z","published":"2024-08-02T06:56:59Z","title":"The Impact of Hyperparameters on Large Language Model Inference\n  Performance: An Evaluation of vLLM and HuggingFace Pipelines","summary":"  The recent surge of open-source large language models (LLMs) enables\ndevelopers to create AI-based solutions while maintaining control over aspects\nsuch as privacy and compliance, thereby providing governance and ownership of\nthe model deployment process. To utilize these LLMs, inference engines are\nneeded. These engines load the model's weights onto available resources, such\nas GPUs, and process queries to generate responses. The speed of inference, or\nperformance, of the LLM, is critical for real-time applications, as it computes\nmillions or billions of floating point operations per inference. Recently,\nadvanced inference engines such as vLLM have emerged, incorporating novel\nmechanisms such as efficient memory management to achieve state-of-the-art\nperformance. In this paper, we analyze the performance, particularly the\nthroughput (tokens generated per unit of time), of 20 LLMs using two inference\nlibraries: vLLM and HuggingFace's pipelines. We investigate how various\nhyperparameters, which developers must configure, influence inference\nperformance. Our results reveal that throughput landscapes are irregular, with\ndistinct peaks, highlighting the importance of hyperparameter optimization to\nachieve maximum performance. We also show that applying hyperparameter\noptimization when upgrading or downgrading the GPU model used for inference can\nimprove throughput from HuggingFace pipelines by an average of 9.16% and 13.7%,\nrespectively.\n","authors":["Matias Martinez"],"pdf_url":"https://arxiv.org/pdf/2408.01050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01040v1","updated":"2024-08-02T06:24:39Z","published":"2024-08-02T06:24:39Z","title":"Privacy-Preserving Split Learning with Vision Transformers using\n  Patch-Wise Random and Noisy CutMix","summary":"  In computer vision, the vision transformer (ViT) has increasingly superseded\nthe convolutional neural network (CNN) for improved accuracy and robustness.\nHowever, ViT's large model sizes and high sample complexity make it difficult\nto train on resource-constrained edge devices. Split learning (SL) emerges as a\nviable solution, leveraging server-side resources to train ViTs while utilizing\nprivate data from distributed devices. However, SL requires additional\ninformation exchange for weight updates between the device and the server,\nwhich can be exposed to various attacks on private training data. To mitigate\nthe risk of data breaches in classification tasks, inspired from the CutMix\nregularization, we propose a novel privacy-preserving SL framework that injects\nGaussian noise into smashed data and mixes randomly chosen patches of smashed\ndata across clients, coined DP-CutMixSL. Our analysis demonstrates that\nDP-CutMixSL is a differentially private (DP) mechanism that strengthens privacy\nprotection against membership inference attacks during forward propagation.\nThrough simulations, we show that DP-CutMixSL improves privacy protection\nagainst membership inference attacks, reconstruction attacks, and label\ninference attacks, while also improving accuracy compared to DP-SL and\nDP-MixSL.\n","authors":["Seungeun Oh","Sihun Baek","Jihong Park","Hyelin Nam","Praneeth Vepakomma","Ramesh Raskar","Mehdi Bennis","Seong-Lyun Kim"],"pdf_url":"https://arxiv.org/pdf/2408.01040v1.pdf","comment":"23 pages, 11 figures, 8 tables, to be published in Transactions on\n  Machine Learning Research (TMLR)"},{"id":"http://arxiv.org/abs/2408.00023v2","updated":"2024-08-02T06:05:19Z","published":"2024-07-31T05:31:28Z","title":"On the Perturbed States for Transformed Input-robust Reinforcement\n  Learning","summary":"  Reinforcement Learning (RL) agents demonstrating proficiency in a training\nenvironment exhibit vulnerability to adversarial perturbations in input\nobservations during deployment. This underscores the importance of building a\nrobust agent before its real-world deployment. To alleviate the challenging\npoint, prior works focus on developing robust training-based procedures,\nencompassing efforts to fortify the deep neural network component's robustness\nor subject the agent to adversarial training against potent attacks. In this\nwork, we propose a novel method referred to as Transformed Input-robust RL\n(TIRL), which explores another avenue to mitigate the impact of adversaries by\nemploying input transformation-based defenses. Specifically, we introduce two\nprinciples for applying transformation-based defenses in learning robust RL\nagents: (1) autoencoder-styled denoising to reconstruct the original state and\n(2) bounded transformations (bit-depth reduction and vector quantization (VQ))\nto achieve close transformed inputs. The transformations are applied to the\nstate before feeding it into the policy network. Extensive experiments on\nmultiple MuJoCo environments demonstrate that input transformation-based\ndefenses, i.e., VQ, defend against several adversaries in the state\nobservations. The official code is available at\nhttps://github.com/tunglm2203/tirl\n","authors":["Tung M. Luu","Haeyong Kang","Tri Ton","Thanh Nguyen","Chang D. Yoo"],"pdf_url":"https://arxiv.org/pdf/2408.00023v2.pdf","comment":"12 pages (Code: https://github.com/tunglm2203/tirl)"},{"id":"http://arxiv.org/abs/2407.16944v3","updated":"2024-08-02T06:05:10Z","published":"2024-07-24T02:23:18Z","title":"An Adaptive Gradient Regularization Method","summary":"  Optimizer plays an important role in neural network training with high\nefficiency and performance. Weight update based on its gradient is the central\npart of the optimizer. It has been shown that normalization and standardization\noperation on weight and gradient can accelerate the training process and\nimprove performance such as Weight Standardization (WS), weight normalization\n(WN) and gradient normalization (GN); there is also gradient centralization\n(GC). In this work, we introduce a new optimization technique based on the\ngradient magnitude in a gradient vector named adaptive gradient regularization\n(AGR), which normalizes the gradient vector in all dimensions as a coefficient\nvector and subtracts the product of the gradient and its coefficient vector by\nthe vanilla gradient. It can be viewed as an adaptive gradient clipping method.\nWe show that the AGR can improve the loss function Lipschitzness with a more\nstable training process and better generalization performance. AGR is very\nsimple to be embedded into vanilla optimizers such as Adan and AdamW with only\nthree lines of code. Our experiments are conducted in image generation, image\nclassification and language representation, which shows that our AGR improves\nthe training result.\n","authors":["Huixiu Jiang","Ling Yang","Yu Bao","Rutong Si"],"pdf_url":"https://arxiv.org/pdf/2407.16944v3.pdf","comment":"11 pages, 11 figures"},{"id":"http://arxiv.org/abs/2408.01023v1","updated":"2024-08-02T05:48:15Z","published":"2024-08-02T05:48:15Z","title":"Distilling interpretable causal trees from causal forests","summary":"  Machine learning methods for estimating treatment effect heterogeneity\npromise greater flexibility than existing methods that test a few pre-specified\nhypotheses. However, one problem these methods can have is that it can be\nchallenging to extract insights from complicated machine learning models. A\nhigh-dimensional distribution of conditional average treatment effects may give\naccurate, individual-level estimates, but it can be hard to understand the\nunderlying patterns; hard to know what the implications of the analysis are.\nThis paper proposes the Distilled Causal Tree, a method for distilling a\nsingle, interpretable causal tree from a causal forest. This compares well to\nexisting methods of extracting a single tree, particularly in noisy data or\nhigh-dimensional data where there are many correlated features. Here it even\noutperforms the base causal forest in most simulations. Its estimates are\ndoubly robust and asymptotically normal just as those of the causal forest are.\n","authors":["Patrick Rehill"],"pdf_url":"https://arxiv.org/pdf/2408.01023v1.pdf","comment":"17 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.01022v1","updated":"2024-08-02T05:46:17Z","published":"2024-08-02T05:46:17Z","title":"A Family of Distributions of Random Subsets for Controlling Positive and\n  Negative Dependence","summary":"  Positive and negative dependence are fundamental concepts that characterize\nthe attractive and repulsive behavior of random subsets. Although some\nprobabilistic models are known to exhibit positive or negative dependence, it\nis challenging to seamlessly bridge them with a practicable probabilistic\nmodel. In this study, we introduce a new family of distributions, named the\ndiscrete kernel point process (DKPP), which includes determinantal point\nprocesses and parts of Boltzmann machines. We also develop some computational\nmethods for probabilistic operations and inference with DKPPs, such as\ncalculating marginal and conditional probabilities and learning the parameters.\nOur numerical experiments demonstrate the controllability of positive and\nnegative dependence and the effectiveness of the computational methods for\nDKPPs.\n","authors":["Takahiro Kawashima","Hideitsu Hino"],"pdf_url":"https://arxiv.org/pdf/2408.01022v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01018v1","updated":"2024-08-02T05:36:14Z","published":"2024-08-02T05:36:14Z","title":"GNN-MolKAN: Harnessing the Power of KAN to Advance Molecular\n  Representation Learning with GNNs","summary":"  Effective molecular representation learning is crucial for molecular property\nprediction and drug design. However, existing approaches struggle with\nlimitations in insufficient annotations and suboptimal architecture design. For\ninstance, Graph Neural Networks (GNNs) suffer from over-squashing, causing the\nloss of important structural details in molecules, thus impairing molecular\nrepresentations. In this work, we propose a new class of GNNs, GNN-MolKAN and\nits augmented variant, GNN-MolKAN+, that integrate the Kolmogorov-Arnold\nNetworks (KAN) architecture from AI + Science into GNNs to address these\nchallenges. Additionally, we introduce Adaptive FastKAN (AdFastKAN), an\nadvanced KAN that offers increased stability and speed, further enhancing the\nperformance of standard GNNs. Notably, our approach holds three key benefits:\n1) Superior Performance: GNN-MolKAN and GNN-MolKAN+ demonstrate superior\nprediction ability, robust generalization to unseen scaffolds, and versatile\ntransferability across different GNN architectures. 2) Efficiency: These models\nrequire less computational time and fewer parameters while matching or\nsurpassing the state-of-the-art (SOTA) self-supervised methods. 3) Few-shot\nLearning Ability: GNN-MolKAN demonstrates great potential in few-shot learning\nscenarios, achieving an average improvement of 6.97% across few-shot\nbenchmarks. Overall, we validate our architecture on 6 classification datasets,\n6 regression datasets, and 4 few-shot learning datasets, consistently achieving\nhighly competitive results across all of them.\n","authors":["Ruifeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.01018v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01016v1","updated":"2024-08-02T05:23:19Z","published":"2024-08-02T05:23:19Z","title":"IBB Traffic Graph Data: Benchmarking and Road Traffic Prediction Model","summary":"  Road traffic congestion prediction is a crucial component of intelligent\ntransportation systems, since it enables proactive traffic management, enhances\nsuburban experience, reduces environmental impact, and improves overall safety\nand efficiency. Although there are several public datasets, especially for\nmetropolitan areas, these datasets may not be applicable to practical scenarios\ndue to insufficiency in the scale of data (i.e. number of sensors and road\nlinks) and several external factors like different characteristics of the\ntarget area such as urban, highways and the data collection location. To\naddress this, this paper introduces a novel IBB Traffic graph dataset as an\nalternative benchmark dataset to mitigate these limitations and enrich the\nliterature with new geographical characteristics. IBB Traffic graph dataset\ncovers the sensor data collected at 2451 distinct locations. Moreover, we\npropose a novel Road Traffic Prediction Model that strengthens temporal links\nthrough feature engineering, node embedding with GLEE to represent\ninter-related relationships within the traffic network, and traffic prediction\nwith ExtraTrees. The results indicate that the proposed model consistently\noutperforms the baseline models, demonstrating an average accuracy improvement\nof 4%.\n","authors":["Eren Olug","Kiymet Kaya","Resul Tugay","Sule Gunduz Oguducu"],"pdf_url":"https://arxiv.org/pdf/2408.01016v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2104.13894v2","updated":"2024-08-02T05:05:48Z","published":"2021-04-28T17:26:29Z","title":"Weighed l1 on the simplex: Compressive sensing meets locality","summary":"  Sparse manifold learning algorithms combine techniques in manifold learning\nand sparse optimization to learn features that could be utilized for downstream\ntasks. The standard setting of compressive sensing can not be immediately\napplied to this setup. Due to the intrinsic geometric structure of data,\ndictionary atoms might be redundant and do not satisfy the restricted isometry\nproperty or coherence condition. In addition, manifold learning emphasizes\nlearning local geometry which is not reflected in a standard $\\ell_1$\nminimization problem. We propose weighted $\\ell_0$ and weighted $\\ell_1$\nmetrics that encourage representation via neighborhood atoms suited for\ndictionary based manifold learning. Assuming that the data is generated from\nDelaunay triangulation, we show the equivalence of weighted $\\ell_0$ and\nweighted $\\ell_1$. We discuss an optimization program that learns the\ndictionaries and sparse coefficients and demonstrate the utility of our\nregularization on synthetic and real datasets.\n","authors":["Abiy Tasissa","Pranay Tankala","Demba Ba"],"pdf_url":"https://arxiv.org/pdf/2104.13894v2.pdf","comment":"7 pages, 2 figures. The proof of theorem 1 in v1 does not hold true\n  in general without additional assumptions. This version fixes this problem.\n  For more details, we refer the interested reader to arXiv:2012.02134 which is\n  the journal version of the workshop paper v1"},{"id":"http://arxiv.org/abs/2408.01008v1","updated":"2024-08-02T04:45:58Z","published":"2024-08-02T04:45:58Z","title":"Tensor Train Low-rank Approximation (TT-LoRA): Democratizing AI with\n  Accelerated LLMs","summary":"  In recent years, Large Language Models (LLMs) have demonstrated remarkable\ncapabilities across a wide range of natural language processing (NLP) tasks,\nsuch as question-answering, sentiment analysis, text summarization, and machine\ntranslation. However, the ever-growing complexity of LLMs demands immense\ncomputational resources, hindering the broader research and application of\nthese models. To address this, various parameter-efficient fine-tuning\nstrategies, such as Low-Rank Approximation (LoRA) and Adapters, have been\ndeveloped. Despite their potential, these methods often face limitations in\ncompressibility. Specifically, LoRA struggles to scale effectively with the\nincreasing number of trainable parameters in modern large scale LLMs.\nAdditionally, Low-Rank Economic Tensor-Train Adaptation (LoRETTA), which\nutilizes tensor train decomposition, has not yet achieved the level of\ncompression necessary for fine-tuning very large scale models with limited\nresources. This paper introduces Tensor Train Low-Rank Approximation (TT-LoRA),\na novel parameter-efficient fine-tuning (PEFT) approach that extends LoRETTA\nwith optimized tensor train (TT) decomposition integration. By eliminating\nAdapters and traditional LoRA-based structures, TT-LoRA achieves greater model\ncompression without compromising downstream task performance, along with\nreduced inference latency and computational overhead. We conduct an exhaustive\nparameter search to establish benchmarks that highlight the trade-off between\nmodel compression and performance. Our results demonstrate significant\ncompression of LLMs while maintaining comparable performance to larger models,\nfacilitating their deployment on resource-constraint platforms.\n","authors":["Afia Anjum","Maksim E. Eren","Ismael Boureima","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.01008v1.pdf","comment":"LA-UR-24-28177"},{"id":"http://arxiv.org/abs/2408.01005v1","updated":"2024-08-02T04:40:15Z","published":"2024-08-02T04:40:15Z","title":"Enhancing Financial Market Predictions: Causality-Driven Feature\n  Selection","summary":"  This paper introduces the FinSen dataset that revolutionizes financial market\nanalysis by integrating economic and financial news articles from 197 countries\nwith stock market data. The dataset's extensive coverage spans 15 years from\n2007 to 2023 with temporal information, offering a rich, global perspective\nwith 160,000 records on financial market news. Our study leverages causally\nvalidated sentiment scores and LSTM models to enhance market forecast accuracy\nand reliability. Utilizing the FinSen dataset, we introduce an innovative Focal\nCalibration Loss, reducing Expected Calibration Error (ECE) to 3.34 percent\nwith the DAN 3 model. This not only improves prediction accuracy but also\naligns probabilistic forecasts closely with real outcomes, crucial for the\nfinancial sector where predicted probability is paramount. Our approach\ndemonstrates the effectiveness of combining sentiment analysis with precise\ncalibration techniques for trustworthy financial forecasting where the cost of\nmisinterpretation can be high. Finsen Data can be found at [this github\nURL](https://github.com/EagleAdelaide/FinSen_Dataset.git).\n","authors":["Wenhao Liang","Zhengyang Li","Weitong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01005v1.pdf","comment":"Accepted by The 20th International Conference Advanced Data Mining\n  and Applications 2024 (ADMA 2024)"},{"id":"http://arxiv.org/abs/2408.01000v1","updated":"2024-08-02T04:19:25Z","published":"2024-08-02T04:19:25Z","title":"Adaptive Two-Stage Cloud Resource Scaling via Hierarchical\n  Multi-Indicator Forecasting and Bayesian Decision-Making","summary":"  The surging demand for cloud computing resources, driven by the rapid growth\nof sophisticated large-scale models and data centers, underscores the critical\nimportance of efficient and adaptive resource allocation. As major tech\nenterprises deploy massive infrastructures with thousands of GPUs, existing\ncloud platforms still struggle with low resource utilization due to key\nchallenges: capturing hierarchical indicator structures, modeling non-Gaussian\ndistributions, and decision-making under uncertainty. To address these\nchallenges, we propose HRAMONY, an adaptive Hierarchical Attention-based\nResource Modeling and Decision-Making System. HARMONY combines hierarchical\nmulti-indicator distribution forecasting and uncertainty-aware Bayesian\ndecision-making. It introduces a novel hierarchical attention mechanism that\ncomprehensively models complex inter-indicator dependencies, enabling accurate\npredictions that can adapt to evolving environment states. By transforming\nGaussian projections into adaptive non-Gaussian distributions via Normalizing\nFlows. Crucially, HARMONY leverages the full predictive distributions in an\nadaptive Bayesian process, proactively incorporating uncertainties to optimize\nresource allocation while robustly meeting SLA constraints under varying\nconditions. Extensive evaluations across four large-scale cloud datasets\ndemonstrate HARMONY's state-of-the-art performance, significantly outperforming\nnine established methods. A month-long real-world deployment validated\nHARMONY's substantial practical impact, realizing over 35,000 GPU hours in\nsavings and translating to $100K+ in cost reduction, showcasing its remarkable\neconomic value through adaptive, uncertainty-aware scaling. Our code is\navailable at https://github.com/Floating-LY/HARMONY1.\n","authors":["Yang Luo","Shiyu Wang","Zhemeng Yu","Wei Lu","Xiaofeng Gao","Lintao Ma","Guihai Chen"],"pdf_url":"https://arxiv.org/pdf/2408.01000v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00996v1","updated":"2024-08-02T04:09:15Z","published":"2024-08-02T04:09:15Z","title":"IncidentNet: Traffic Incident Detection, Localization and Severity\n  Estimation with Sparse Sensing","summary":"  Prior art in traffic incident detection relies on high sensor coverage and is\nprimarily based on decision-tree and random forest models that have limited\nrepresentation capacity and, as a result, cannot detect incidents with high\naccuracy. This paper presents IncidentNet - a novel approach for classifying,\nlocalizing, and estimating the severity of traffic incidents using deep\nlearning models trained on data captured from sparsely placed sensors in urban\nenvironments. Our model works on microscopic traffic data that can be collected\nusing cameras installed at traffic intersections. Due to the unavailability of\ndatasets that provide microscopic traffic details and traffic incident details\nsimultaneously, we also present a methodology to generate a synthetic\nmicroscopic traffic dataset that matches given macroscopic traffic data.\nIncidentNet achieves a traffic incident detection rate of 98%, with false alarm\nrates of less than 7% in 197 seconds on average in urban environments with\ncameras on less than 20% of the traffic intersections.\n","authors":["Sai Shashank Peddiraju","Kaustubh Harapanahalli","Edward Andert","Aviral Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.00996v1.pdf","comment":"6 pages, 6 figures, 2024 IEEE 27th International Conference on\n  Intelligent Transportation Systems (ITSC)"},{"id":"http://arxiv.org/abs/2303.12653v3","updated":"2024-08-02T04:02:26Z","published":"2023-03-09T05:30:53Z","title":"Robust Millimeter Beamforming via Self-Supervised Hybrid Deep Learning","summary":"  Beamforming with large-scale antenna arrays has been widely used in recent\nyears, which is acknowledged as an important part in 5G and incoming 6G. Thus,\nvarious techniques are leveraged to improve its performance, e.g., deep\nlearning, advanced optimization algorithms, etc. Although its performance in\nmany previous research scenarios with deep learning is quite attractive,\nusually it drops rapidly when the environment or dataset is changed. Therefore,\ndesigning effective beamforming network with strong robustness is an open issue\nfor the intelligent wireless communications. In this paper, we propose a robust\nbeamforming self-supervised network, and verify it in two kinds of different\ndatasets with various scenarios. Simulation results show that the proposed\nself-supervised network with hybrid learning performs well in both classic\nDeepMIMO and new WAIR-D dataset with the strong robustness under the various\nenvironments. Also, we present the principle to explain the rationality of this\nkind of hybrid learning, which is instructive to apply with more kinds of\ndatasets.\n","authors":["Fenghao Zhu","Bohao Wang","Zhaohui Yang","Chongwen Huang","Zhaoyang Zhang","George C. Alexandropoulos","Chau Yuen","Merouane Debbah"],"pdf_url":"https://arxiv.org/pdf/2303.12653v3.pdf","comment":"Accept by EUSIPCO 2023"},{"id":"http://arxiv.org/abs/2408.00465v2","updated":"2024-08-02T03:56:14Z","published":"2024-08-01T11:09:01Z","title":"Infrequent Resolving Algorithm for Online Linear Programming","summary":"  Online linear programming (OLP) has gained significant attention from both\nresearchers and practitioners due to its extensive applications, such as online\nauction, network revenue management and advertising. Existing OLP algorithms\nfall into two categories: LP-based algorithms and LP-free algorithms. The\nformer one typically guarantees better performance, even offering a constant\nregret, but requires solving a large number of LPs, which could be\ncomputationally expensive. In contrast, LP-free algorithm only requires\nfirst-order computations but induces a worse performance, lacking a constant\nregret bound. In this work, we bridge the gap between these two extremes by\nproposing an algorithm that achieves a constant regret while solving LPs only\n$O(\\log\\log T)$ times over the time horizon $T$. Moreover, when we are allowed\nto solve LPs only $M$ times, we propose an algorithm that can guarantee an\n$O\\left(T^{(1/2+\\epsilon)^{M-1}}\\right)$ regret. Furthermore, when the arrival\nprobabilities are known at the beginning, our algorithm can guarantee a\nconstant regret by solving LPs $O(\\log\\log T)$ times, and an\n$O\\left(T^{(1/2+\\epsilon)^{M}}\\right)$ regret by solving LPs only $M$ times.\nNumerical experiments are conducted to demonstrate the efficiency of the\nproposed algorithms.\n","authors":["Guokai Li","Zizhuo Wang","Jingwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00465v2.pdf","comment":"35 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00992v1","updated":"2024-08-02T03:44:14Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.01258v2","updated":"2024-08-02T03:39:18Z","published":"2024-01-02T15:59:00Z","title":"Towards Model-Free LQR Control over Rate-Limited Channels","summary":"  Given the success of model-free methods for control design in many problem\nsettings, it is natural to ask how things will change if realistic\ncommunication channels are utilized for the transmission of gradients or\npolicies. While the resulting problem has analogies with the formulations\nstudied under the rubric of networked control systems, the rich literature in\nthat area has typically assumed that the model of the system is known. As a\nstep towards bridging the fields of model-free control design and networked\ncontrol systems, we ask: \\textit{Is it possible to solve basic control problems\n- such as the linear quadratic regulator (LQR) problem - in a model-free manner\nover a rate-limited channel?} Toward answering this question, we study a\nsetting where a worker agent transmits quantized policy gradients (of the LQR\ncost) to a server over a noiseless channel with a finite bit-rate. We propose a\nnew algorithm titled Adaptively Quantized Gradient Descent (\\texttt{AQGD}), and\nprove that above a certain finite threshold bit-rate, \\texttt{AQGD} guarantees\nexponentially fast convergence to the globally optimal policy, with \\textit{no\ndeterioration of the exponent relative to the unquantized setting}. More\ngenerally, our approach reveals the benefits of adaptive quantization in\npreserving fast linear convergence rates, and, as such, may be of independent\ninterest to the literature on compressed optimization.\n","authors":["Aritra Mitra","Lintao Ye","Vijay Gupta"],"pdf_url":"https://arxiv.org/pdf/2401.01258v2.pdf","comment":"Accepted for an Oral Presentation at the 6th Annual Learning for\n  Dynamics & Control Conference"},{"id":"http://arxiv.org/abs/2407.11046v2","updated":"2024-08-02T03:22:22Z","published":"2024-07-08T12:32:10Z","title":"A Survey on LoRA of Large Language Models","summary":"  Low-Rank Adaptation~(LoRA), which updates the dense neural network layers\nwith pluggable low-rank matrices, is one of the best performed parameter\nefficient fine-tuning paradigms. Furthermore, it has significant advantages in\ncross-task generalization and privacy-preserving. Hence, LoRA has gained much\nattention recently, and the number of related literature demonstrates\nexponential growth. It is necessary to conduct a comprehensive overview of the\ncurrent progress on LoRA. This survey categorizes and reviews the progress from\nthe perspectives of (1) downstream adaptation improving variants that improve\nLoRA's performance on downstream tasks; (2) cross-task generalization methods\nthat mix multiple LoRA plugins to achieve cross-task generalization; (3)\nefficiency-improving methods that boost the computation-efficiency of LoRA; (4)\ndata privacy-preserving methods that use LoRA in federated learning; (5)\napplication. Besides, this survey also discusses the future directions in this\nfield. At last, we provide a Github page\n(https://github.com/ZJU-LLMs/Awesome-LoRAs.git) for readers to check the\nupdates and initiate discussions on this survey paper.\n","authors":["Yuren Mao","Yuhang Ge","Yijiang Fan","Wenyi Xu","Yu Mi","Zhonghao Hu","Yunjun Gao"],"pdf_url":"https://arxiv.org/pdf/2407.11046v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.15870v5","updated":"2024-08-02T03:16:07Z","published":"2023-07-29T02:35:37Z","title":"SemiSFL: Split Federated Learning on Unlabeled and Non-IID Data","summary":"  Federated Learning (FL) has emerged to allow multiple clients to\ncollaboratively train machine learning models on their private data at the\nnetwork edge. However, training and deploying large-scale models on\nresource-constrained devices is challenging. Fortunately, Split Federated\nLearning (SFL) offers a feasible solution by alleviating the computation and/or\ncommunication burden on clients. However, existing SFL works often assume\nsufficient labeled data on clients, which is usually impractical. Besides, data\nnon-IIDness poses another challenge to ensure efficient model training. To our\nbest knowledge, the above two issues have not been simultaneously addressed in\nSFL. Herein, we propose a novel Semi-supervised SFL system, termed SemiSFL,\nwhich incorporates clustering regularization to perform SFL with unlabeled and\nnon-IID client data. Moreover, our theoretical and experimental investigations\ninto model convergence reveal that the inconsistent training processes on\nlabeled and unlabeled data have an influence on the effectiveness of clustering\nregularization. To mitigate the training inconsistency, we develop an algorithm\nfor dynamically adjusting the global updating frequency, so as to improve\ntraining performance. Extensive experiments on benchmark models and datasets\nshow that our system provides a 3.8x speed-up in training time, reduces the\ncommunication cost by about 70.3% while reaching the target accuracy, and\nachieves up to 5.8% improvement in accuracy under non-IID scenarios compared to\nthe state-of-the-art baselines.\n","authors":["Yang Xu","Yunming Liao","Hongli Xu","Zhipeng Sun","Liusheng Huang","Chunming Qiao"],"pdf_url":"https://arxiv.org/pdf/2307.15870v5.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.00985v1","updated":"2024-08-02T03:02:39Z","published":"2024-08-02T03:02:39Z","title":"Reconstructing Richtmyer-Meshkov instabilities from noisy radiographs\n  using low dimensional features and attention-based neural networks","summary":"  A trained attention-based transformer network can robustly recover the\ncomplex topologies given by the Richtmyer-Meshkoff instability from a sequence\nof hydrodynamic features derived from radiographic images corrupted with blur,\nscatter, and noise. This approach is demonstrated on ICF-like double shell\nhydrodynamic simulations. The key component of this network is a transformer\nencoder that acts on a sequence of features extracted from noisy radiographs.\nThis encoder includes numerous self-attention layers that act to learn temporal\ndependencies in the input sequences and increase the expressiveness of the\nmodel. This approach is demonstrated to exhibit an excellent ability to\naccurately recover the Richtmyer-Meshkov instability growth rates, even despite\nthe gas-metal interface being greatly obscured by radiographic noise.\n","authors":["Daniel A. Serino","Marc L. Klasky","Balasubramanya T. Nadiga","Xiaojian Xu","Trevor Wilcox"],"pdf_url":"https://arxiv.org/pdf/2408.00985v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00973v1","updated":"2024-08-02T01:49:29Z","published":"2024-08-02T01:49:29Z","title":"META-ANOVA: Screening interactions for interpretable machine learning","summary":"  There are two things to be considered when we evaluate predictive models. One\nis prediction accuracy,and the other is interpretability. Over the recent\ndecades, many prediction models of high performance, such as ensemble-based\nmodels and deep neural networks, have been developed. However, these models are\noften too complex, making it difficult to intuitively interpret their\npredictions. This complexity in interpretation limits their use in many\nreal-world fields that require accountability, such as medicine, finance, and\ncollege admissions. In this study, we develop a novel method called Meta-ANOVA\nto provide an interpretable model for any given prediction model. The basic\nidea of Meta-ANOVA is to transform a given black-box prediction model to the\nfunctional ANOVA model. A novel technical contribution of Meta-ANOVA is a\nprocedure of screening out unnecessary interaction before transforming a given\nblack-box model to the functional ANOVA model. This screening procedure allows\nthe inclusion of higher order interactions in the transformed functional ANOVA\nmodel without computational difficulties. We prove that the screening procedure\nis asymptotically consistent. Through various experiments with synthetic and\nreal-world datasets, we empirically demonstrate the superiority of Meta-ANOVA\n","authors":["Yongchan Choi","Seokhun Park","Chanmoo Park","Dongha Kim","Yongdai Kim"],"pdf_url":"https://arxiv.org/pdf/2408.00973v1.pdf","comment":"26 pages"},{"id":"http://arxiv.org/abs/2308.14250v3","updated":"2024-08-02T01:38:16Z","published":"2023-08-28T01:57:38Z","title":"Rule-Based Error Detection and Correction to Operationalize Movement\n  Trajectory Classification","summary":"  Classification of movement trajectories has many applications in\ntransportation and is a key component for large-scale movement trajectory\ngeneration and anomaly detection which has key safety applications in the\naftermath of a disaster or other external shock. However, the current\nstate-of-the-art (SOTA) are based on supervised deep learning - which leads to\nchallenges when the distribution of trajectories changes due to such a shock.\nWe provide a neuro-symbolic rule-based framework to conduct error correction\nand detection of these models to integrate into our movement trajectory\nplatform. We provide a suite of experiments on several recent SOTA models where\nwe show highly accurate error detection, the ability to improve accuracy with a\nchanging test distribution, and accuracy improvement for the base use case in\naddition to a suite of theoretical properties that informed algorithm\ndevelopment. Specifically, we show an F1 scores for predicting errors of up to\n0.984, significant performance increase for out-of distribution accuracy (8.51%\nimprovement over SOTA for zero-shot accuracy), and accuracy improvement over\nthe SOTA model.\n","authors":["Bowen Xi","Kevin Scaria","Divyagna Bavikadi","Paulo Shakarian"],"pdf_url":"https://arxiv.org/pdf/2308.14250v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.03452v2","updated":"2024-08-02T01:33:25Z","published":"2023-09-07T02:26:55Z","title":"Multimodal Guidance Network for Missing-Modality Inference in Content\n  Moderation","summary":"  Multimodal deep learning, especially vision-language models, have gained\nsignificant traction in recent years, greatly improving performance on many\ndownstream tasks, including content moderation and violence detection. However,\nstandard multimodal approaches often assume consistent modalities between\ntraining and inference, limiting applications in many real-world use cases, as\nsome modalities may not be available during inference. While existing research\nmitigates this problem through reconstructing the missing modalities, they\nunavoidably increase unnecessary computational cost, which could be just as\ncritical, especially for large, deployed infrastructures in industry. To this\nend, we propose a novel guidance network that promotes knowledge sharing during\ntraining, taking advantage of the multimodal representations to train better\nsingle-modality models to be used for inference. Real-world experiments in\nviolence detection shows that our proposed framework trains single-modality\nmodels that significantly outperform traditionally trained counterparts, while\navoiding increases in computational cost for inference.\n","authors":["Zhuokai Zhao","Harish Palani","Tianyi Liu","Lena Evans","Ruth Toner"],"pdf_url":"https://arxiv.org/pdf/2309.03452v2.pdf","comment":"ICME 2024 Camera Ready. Code is available at\n  https://github.com/zhuokaizhao/multimodal-guidance-network"},{"id":"http://arxiv.org/abs/2408.00963v1","updated":"2024-08-02T00:35:18Z","published":"2024-08-02T00:35:18Z","title":"MIS-ME: A Multi-modal Framework for Soil Moisture Estimation","summary":"  Soil moisture estimation is an important task to enable precision agriculture\nin creating optimal plans for irrigation, fertilization, and harvest. It is\ncommon to utilize statistical and machine learning models to estimate soil\nmoisture from traditional data sources such as weather forecasts, soil\nproperties, and crop properties. However, there is a growing interest in\nutilizing aerial and geospatial imagery to estimate soil moisture. Although\nthese images capture high-resolution crop details, they are expensive to curate\nand challenging to interpret. Imagine, an AI-enhanced software tool that\npredicts soil moisture using visual cues captured by smartphones and\nstatistical data given by weather forecasts. This work is a first step towards\nthat goal of developing a multi-modal approach for soil moisture estimation. In\nparticular, we curate a dataset consisting of real-world images taken from\nground stations and their corresponding weather data. We also propose MIS-ME -\nMeteorological & Image based Soil Moisture Estimator, a multi-modal framework\nfor soil moisture estimation. Our extensive analysis shows that MIS-ME achieves\na MAPE of 10.79%, outperforming traditional unimodal approaches with a\nreduction of 2.6% in MAPE for meteorological data and 1.5% in MAPE for image\ndata, highlighting the effectiveness of tailored multi-modal approaches.\n","authors":["Mohammed Rakib","Adil Aman Mohammed","Cole Diggins","Sumit Sharma","Jeff Michael Sadler","Tyson Ochsner","Arun Bagavathi"],"pdf_url":"https://arxiv.org/pdf/2408.00963v1.pdf","comment":"Accepted by DSAA2024"},{"id":"http://arxiv.org/abs/2405.16522v4","updated":"2024-08-02T00:21:41Z","published":"2024-05-26T11:17:49Z","title":"Multi-State TD Target for Model-Free Reinforcement Learning","summary":"  Temporal difference (TD) learning is a fundamental technique in reinforcement\nlearning that updates value estimates for states or state-action pairs using a\nTD target. This target represents an improved estimate of the true value by\nincorporating both immediate rewards and the estimated value of subsequent\nstates. Traditionally, TD learning relies on the value of a single subsequent\nstate. We propose an enhanced multi-state TD (MSTD) target that utilizes the\nestimated values of multiple subsequent states. Building on this new MSTD\nconcept, we develop complete actor-critic algorithms that include management of\nreplay buffers in two modes, and integrate with deep deterministic policy\noptimization (DDPG) and soft actor-critic (SAC). Experimental results\ndemonstrate that algorithms employing the MSTD target significantly improve\nlearning performance compared to traditional methods.The code is provided on\nGitHub.\n","authors":["Wuhao Wang","Zhiyong Chen","Lepeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.16522v4.pdf","comment":"8 pages, 16 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.01363v1","updated":"2024-08-02T16:15:25Z","published":"2024-08-02T16:15:25Z","title":"Toward Automatic Relevance Judgment using Vision--Language Models for\n  Image--Text Retrieval Evaluation","summary":"  Vision--Language Models (VLMs) have demonstrated success across diverse\napplications, yet their potential to assist in relevance judgments remains\nuncertain. This paper assesses the relevance estimation capabilities of VLMs,\nincluding CLIP, LLaVA, and GPT-4V, within a large-scale \\textit{ad hoc}\nretrieval task tailored for multimedia content creation in a zero-shot fashion.\nPreliminary experiments reveal the following: (1) Both LLaVA and GPT-4V,\nencompassing open-source and closed-source visual-instruction-tuned Large\nLanguage Models (LLMs), achieve notable Kendall's $\\tau \\sim 0.4$ when compared\nto human relevance judgments, surpassing the CLIPScore metric. (2) While\nCLIPScore is strongly preferred, LLMs are less biased towards CLIP-based\nretrieval systems. (3) GPT-4V's score distribution aligns more closely with\nhuman judgments than other models, achieving a Cohen's $\\kappa$ value of around\n0.08, which outperforms CLIPScore at approximately -0.096. These findings\nunderscore the potential of LLM-powered VLMs in enhancing relevance judgments.\n","authors":["Jheng-Hong Yang","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01363v1.pdf","comment":"Accepted by ACM SIGIR 2024 LLM4Eval Workshop:\n  https://llm4eval.github.io/papers"},{"id":"http://arxiv.org/abs/2408.01355v1","updated":"2024-08-02T16:07:15Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v1.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2408.01349v1","updated":"2024-08-02T15:54:49Z","published":"2024-08-02T15:54:49Z","title":"PC$^2$: Pseudo-Classification Based Pseudo-Captioning for Noisy\n  Correspondence Learning in Cross-Modal Retrieval","summary":"  In the realm of cross-modal retrieval, seamlessly integrating diverse\nmodalities within multimedia remains a formidable challenge, especially given\nthe complexities introduced by noisy correspondence learning (NCL). Such noise\noften stems from mismatched data pairs, which is a significant obstacle\ndistinct from traditional noisy labels. This paper introduces\nPseudo-Classification based Pseudo-Captioning (PC$^2$) framework to address\nthis challenge. PC$^2$ offers a threefold strategy: firstly, it establishes an\nauxiliary \"pseudo-classification\" task that interprets captions as categorical\nlabels, steering the model to learn image-text semantic similarity through a\nnon-contrastive mechanism. Secondly, unlike prevailing margin-based techniques,\ncapitalizing on PC$^2$'s pseudo-classification capability, we generate\npseudo-captions to provide more informative and tangible supervision for each\nmismatched pair. Thirdly, the oscillation of pseudo-classification is borrowed\nto assistant the correction of correspondence. In addition to technical\ncontributions, we develop a realistic NCL dataset called Noise of Web (NoW),\nwhich could be a new powerful NCL benchmark where noise exists naturally.\nEmpirical evaluations of PC$^2$ showcase marked improvements over existing\nstate-of-the-art robust cross-modal retrieval techniques on both simulated and\nrealistic datasets with various NCL settings. The contributed dataset and\nsource code are released at https://github.com/alipay/PC2-NoiseofWeb.\n","authors":["Yue Duan","Zhangxuan Gu","Zhenzhe Ying","Lei Qi","Changhua Meng","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2408.01349v1.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.01337v1","updated":"2024-08-02T15:34:05Z","published":"2024-08-02T15:34:05Z","title":"MuChoMusic: Evaluating Music Understanding in Multimodal Audio-Language\n  Models","summary":"  Multimodal models that jointly process audio and language hold great promise\nin audio understanding and are increasingly being adopted in the music domain.\nBy allowing users to query via text and obtain information about a given audio\ninput, these models have the potential to enable a variety of music\nunderstanding tasks via language-based interfaces. However, their evaluation\nposes considerable challenges, and it remains unclear how to effectively assess\ntheir ability to correctly interpret music-related inputs with current methods.\nMotivated by this, we introduce MuChoMusic, a benchmark for evaluating music\nunderstanding in multimodal language models focused on audio. MuChoMusic\ncomprises 1,187 multiple-choice questions, all validated by human annotators,\non 644 music tracks sourced from two publicly available music datasets, and\ncovering a wide variety of genres. Questions in the benchmark are crafted to\nassess knowledge and reasoning abilities across several dimensions that cover\nfundamental musical concepts and their relation to cultural and functional\ncontexts. Through the holistic analysis afforded by the benchmark, we evaluate\nfive open-source models and identify several pitfalls, including an\nover-reliance on the language modality, pointing to a need for better\nmultimodal integration. Data and code are open-sourced.\n","authors":["Benno Weck","Ilaria Manco","Emmanouil Benetos","Elio Quinton","George Fazekas","Dmitry Bogdanov"],"pdf_url":"https://arxiv.org/pdf/2408.01337v1.pdf","comment":"Accepted at ISMIR 2024. Data: https://doi.org/10.5281/zenodo.12709974\n  Code: https://github.com/mulab-mir/muchomusic Supplementary material:\n  https://mulab-mir.github.io/muchomusic"},{"id":"http://arxiv.org/abs/2408.01284v1","updated":"2024-08-02T14:10:20Z","published":"2024-08-02T14:10:20Z","title":"Out-Of-Distribution Detection for Audio-visual Generalized Zero-Shot\n  Learning: A General Framework","summary":"  Generalized Zero-Shot Learning (GZSL) is a challenging task requiring\naccurate classification of both seen and unseen classes. Within this domain,\nAudio-visual GZSL emerges as an extremely exciting yet difficult task, given\nthe inclusion of both visual and acoustic features as multi-modal inputs.\nExisting efforts in this field mostly utilize either embedding-based or\ngenerative-based methods. However, generative training is difficult and\nunstable, while embedding-based methods often encounter domain shift problem.\nThus, we find it promising to integrate both methods into a unified framework\nto leverage their advantages while mitigating their respective disadvantages.\nOur study introduces a general framework employing out-of-distribution (OOD)\ndetection, aiming to harness the strengths of both approaches. We first employ\ngenerative adversarial networks to synthesize unseen features, enabling the\ntraining of an OOD detector alongside classifiers for seen and unseen classes.\nThis detector determines whether a test feature belongs to seen or unseen\nclasses, followed by classification utilizing separate classifiers for each\nfeature type. We test our framework on three popular audio-visual datasets and\nobserve a significant improvement comparing to existing state-of-the-art works.\nCodes can be found in https://github.com/liuyuan-wen/AV-OOD-GZSL.\n","authors":["Liuyuan Wen"],"pdf_url":"https://arxiv.org/pdf/2408.01284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00970v1","updated":"2024-08-02T01:30:18Z","published":"2024-08-02T01:30:18Z","title":"Multimodal Fusion via Hypergraph Autoencoder and Contrastive Learning\n  for Emotion Recognition in Conversation","summary":"  Multimodal emotion recognition in conversation (MERC) seeks to identify the\nspeakers' emotions expressed in each utterance, offering significant potential\nacross diverse fields. The challenge of MERC lies in balancing speaker modeling\nand context modeling, encompassing both long-distance and short-distance\ncontexts, as well as addressing the complexity of multimodal information\nfusion. Recent research adopts graph-based methods to model intricate\nconversational relationships effectively. Nevertheless, the majority of these\nmethods utilize a fixed fully connected structure to link all utterances,\nrelying on convolution to interpret complex context. This approach can\ninherently heighten the redundancy in contextual messages and excessive graph\nnetwork smoothing, particularly in the context of long-distance conversations.\nTo address this issue, we propose a framework that dynamically adjusts\nhypergraph connections by variational hypergraph autoencoder (VHGAE), and\nemploys contrastive learning to mitigate uncertainty factors during the\nreconstruction process. Experimental results demonstrate the effectiveness of\nour proposal against the state-of-the-art methods on IEMOCAP and MELD datasets.\nWe release the code to support the reproducibility of this work at\nhttps://github.com/yzjred/-HAUCL.\n","authors":["Zijian Yi","Ziming Zhao","Zhishu Shen","Tiehua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00970v1.pdf","comment":"Accepted by ACM MULTIMEDIA 2024"},{"id":"http://arxiv.org/abs/2311.10709v2","updated":"2024-08-02T18:55:25Z","published":"2023-11-17T18:59:04Z","title":"Emu Video: Factorizing Text-to-Video Generation by Explicit Image\n  Conditioning","summary":"  We present Emu Video, a text-to-video generation model that factorizes the\ngeneration into two steps: first generating an image conditioned on the text,\nand then generating a video conditioned on the text and the generated image. We\nidentify critical design decisions--adjusted noise schedules for diffusion, and\nmulti-stage training that enable us to directly generate high quality and high\nresolution videos, without requiring a deep cascade of models as in prior work.\nIn human evaluations, our generated videos are strongly preferred in quality\ncompared to all prior work--81% vs. Google's Imagen Video, 90% vs. Nvidia's\nPYOCO, and 96% vs. Meta's Make-A-Video. Our model outperforms commercial\nsolutions such as RunwayML's Gen2 and Pika Labs. Finally, our factorizing\napproach naturally lends itself to animating images based on a user's text\nprompt, where our generations are preferred 96% over prior work.\n","authors":["Rohit Girdhar","Mannat Singh","Andrew Brown","Quentin Duval","Samaneh Azadi","Sai Saketh Rambhatla","Akbar Shah","Xi Yin","Devi Parikh","Ishan Misra"],"pdf_url":"https://arxiv.org/pdf/2311.10709v2.pdf","comment":"ECCV 2024. Project page: https://emu-video.metademolab.com"},{"id":"http://arxiv.org/abs/2408.01532v1","updated":"2024-08-02T18:45:01Z","published":"2024-08-02T18:45:01Z","title":"Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and\n  Localization","summary":"  In the digital age, the emergence of deepfakes and synthetic media presents a\nsignificant threat to societal and political integrity. Deepfakes based on\nmulti-modal manipulation, such as audio-visual, are more realistic and pose a\ngreater threat. Current multi-modal deepfake detectors are often based on the\nattention-based fusion of heterogeneous data streams from multiple modalities.\nHowever, the heterogeneous nature of the data (such as audio and visual\nsignals) creates a distributional modality gap and poses a significant\nchallenge in effective fusion and hence multi-modal deepfake detection. In this\npaper, we propose a novel multi-modal attention framework based on recurrent\nneural networks (RNNs) that leverages contextual information for audio-visual\ndeepfake detection. The proposed approach applies attention to multi-modal\nmulti-sequence representations and learns the contributing features among them\nfor deepfake detection and localization. Thorough experimental validations on\naudio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and\nLAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison\nwith the published studies demonstrates superior performance of our approach\nwith an improved accuracy and precision by 3.47% and 2.05% in deepfake\ndetection and localization, respectively. Thus, obtaining state-of-the-art\nperformance. To facilitate reproducibility, the code and the datasets\ninformation is available at https://github.com/vcbsl/audiovisual-deepfake/.\n","authors":["Vinaya Sree Katamneni","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2408.01532v1.pdf","comment":null}]},"2024-08-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.02666v1","updated":"2024-08-05T17:57:02Z","published":"2024-08-05T17:57:02Z","title":"Self-Taught Evaluators","summary":"  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n","authors":["Tianlu Wang","Ilia Kulikov","Olga Golovneva","Ping Yu","Weizhe Yuan","Jane Dwivedi-Yu","Richard Yuanzhe Pang","Maryam Fazel-Zarandi","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.02666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11173v3","updated":"2024-08-05T17:39:17Z","published":"2024-06-17T03:26:02Z","title":"BSRBF-KAN: A combination of B-splines and Radial Basic Functions in\n  Kolmogorov-Arnold Networks","summary":"  In this paper, we introduce BSRBF-KAN, a Kolmogorov Arnold Network (KAN) that\ncombines Bsplines and radial basis functions (RBFs) to fit input vectors in\ndata training. We perform experiments with BSRBF-KAN, MLP, and other popular\nKANs, including EfficientKAN, FastKAN, FasterKAN, and GottliebKAN over the\nMNIST and Fashion-MNIST datasets. BSRBF-KAN shows stability in 5 training\nsessions with a competitive average accuracy of 97.55% on MNIST and 89.33% on\nFashionMNIST and obtains convergence better than other networks. We expect\nBSRBF-KAN to open many combinations of mathematical functions to design KANs.\nOur repo is publicly available at: https://github.com/hoangthangta/BSRBF-KAN.\n","authors":["Hoang-Thang Ta"],"pdf_url":"https://arxiv.org/pdf/2406.11173v3.pdf","comment":"8 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2408.02651v1","updated":"2024-08-05T17:27:29Z","published":"2024-08-05T17:27:29Z","title":"Can Reinforcement Learning Unlock the Hidden Dangers in Aligned Large\n  Language Models?","summary":"  Large Language Models (LLMs) have demonstrated impressive capabilities in\nnatural language tasks, but their safety and morality remain contentious due to\ntheir training on internet text corpora. To address these concerns, alignment\ntechniques have been developed to improve the public usability and safety of\nLLMs. Yet, the potential for generating harmful content through these models\nseems to persist. This paper explores the concept of jailbreaking\nLLMs-reversing their alignment through adversarial triggers. Previous methods,\nsuch as soft embedding prompts, manually crafted prompts, and gradient-based\nautomatic prompts, have had limited success on black-box models due to their\nrequirements for model access and for producing a low variety of manually\ncrafted prompts, making them susceptible to being blocked. This paper\nintroduces a novel approach using reinforcement learning to optimize\nadversarial triggers, requiring only inference API access to the target model\nand a small surrogate model. Our method, which leverages a BERTScore-based\nreward function, enhances the transferability and effectiveness of adversarial\ntriggers on new black-box models. We demonstrate that this approach improves\nthe performance of adversarial triggers on a previously untested language\nmodel.\n","authors":["Mohammad Bahrami Karkevandi","Nishant Vishwamitra","Peyman Najafirad"],"pdf_url":"https://arxiv.org/pdf/2408.02651v1.pdf","comment":"Accepted to AI4CYBER - KDD 2024"},{"id":"http://arxiv.org/abs/2408.02632v1","updated":"2024-08-05T16:55:06Z","published":"2024-08-05T16:55:06Z","title":"SEAS: Self-Evolving Adversarial Safety Optimization for Large Language\n  Models","summary":"  As large language models (LLMs) continue to advance in capability and\ninfluence, ensuring their security and preventing harmful outputs has become\ncrucial. A promising approach to address these concerns involves training\nmodels to automatically generate adversarial prompts for red teaming. However,\nthe evolving subtlety of vulnerabilities in LLMs challenges the effectiveness\nof current adversarial methods, which struggle to specifically target and\nexplore the weaknesses of these models. To tackle these challenges, we\nintroduce the $\\mathbf{S}\\text{elf-}\\mathbf{E}\\text{volving\n}\\mathbf{A}\\text{dversarial }\\mathbf{S}\\text{afety }\\mathbf{(SEAS)}$\noptimization framework, which enhances security by leveraging data generated by\nthe model itself. SEAS operates through three iterative stages: Initialization,\nAttack, and Adversarial Optimization, refining both the Red Team and Target\nmodels to improve robustness and safety. This framework reduces reliance on\nmanual testing and significantly enhances the security capabilities of LLMs.\nOur contributions include a novel adversarial framework, a comprehensive safety\ndataset, and after three iterations, the Target model achieves a security level\ncomparable to GPT-4, while the Red Team model shows a marked increase in attack\nsuccess rate (ASR) against advanced models.\n","authors":["Muxi Diao","Rumei Li","Shiyang Liu","Guogang Liao","Jingang Wang","Xunliang Cai","Weiran Xu"],"pdf_url":"https://arxiv.org/pdf/2408.02632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02622v1","updated":"2024-08-05T16:47:22Z","published":"2024-08-05T16:47:22Z","title":"Language Model Can Listen While Speaking","summary":"  Dialogue serves as the most natural manner of human-computer interaction\n(HCI). Recent advancements in speech language models (SLM) have significantly\nenhanced speech-based conversational AI. However, these models are limited to\nturn-based conversation, lacking the ability to interact with humans in\nreal-time spoken scenarios, for example, being interrupted when the generated\ncontent is not satisfactory. To address these limitations, we explore full\nduplex modeling (FDM) in interactive speech language models (iSLM), focusing on\nenhancing real-time interaction and, more explicitly, exploring the\nquintessential ability of interruption. We introduce a novel model design,\nnamely listening-while-speaking language model (LSLM), an end-to-end system\nequipped with both listening and speaking channels. Our LSLM employs a\ntoken-based decoder-only TTS for speech generation and a streaming\nself-supervised learning (SSL) encoder for real-time audio input. LSLM fuses\nboth channels for autoregressive generation and detects turn-taking in real\ntime. Three fusion strategies -- early fusion, middle fusion, and late fusion\n-- are explored, with middle fusion achieving an optimal balance between speech\ngeneration and real-time interaction. Two experimental settings, command-based\nFDM and voice-based FDM, demonstrate LSLM's robustness to noise and sensitivity\nto diverse instructions. Our results highlight LSLM's capability to achieve\nduplex communication with minimal impact on existing systems. This study aims\nto advance the development of interactive speech dialogue systems, enhancing\ntheir applicability in real-world contexts.\n","authors":["Ziyang Ma","Yakun Song","Chenpeng Du","Jian Cong","Zhuo Chen","Yuping Wang","Yuxuan Wang","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02622v1.pdf","comment":"Demo can be found at https://ddlbojack.github.io/LSLM"},{"id":"http://arxiv.org/abs/2408.02600v1","updated":"2024-08-05T16:21:36Z","published":"2024-08-05T16:21:36Z","title":"BioMamba: A Pre-trained Biomedical Language Representation Model\n  Leveraging Mamba","summary":"  The advancement of natural language processing (NLP) in biology hinges on\nmodels' ability to interpret intricate biomedical literature. Traditional\nmodels often struggle with the complex and domain-specific language in this\nfield. In this paper, we present BioMamba, a pre-trained model specifically\ndesigned for biomedical text mining. BioMamba builds upon the Mamba\narchitecture and is pre-trained on an extensive corpus of biomedical\nliterature. Our empirical studies demonstrate that BioMamba significantly\noutperforms models like BioBERT and general-domain Mamba across various\nbiomedical tasks. For instance, BioMamba achieves a 100 times reduction in\nperplexity and a 4 times reduction in cross-entropy loss on the BioASQ test\nset. We provide an overview of the model architecture, pre-training process,\nand fine-tuning techniques. Additionally, we release the code and trained model\nto facilitate further research.\n","authors":["Ling Yue","Sixue Xing","Yingzhou Lu","Tianfan Fu"],"pdf_url":"https://arxiv.org/pdf/2408.02600v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02599v1","updated":"2024-08-05T16:21:17Z","published":"2024-08-05T16:21:17Z","title":"Progressively Selective Label Enhancement for Language Model Alignment","summary":"  Large Language Models have demonstrated impressive capabilities in various\nlanguage tasks but may produce content that misaligns with human expectations,\nraising ethical and legal concerns. Therefore, it is important to explore the\nlimitations and implement restrictions on the models to ensure safety and\ncompliance, with Reinforcement Learning from Human Feedback (RLHF) being the\nprimary method. Due to challenges in stability and scalability with the RLHF\nstages, researchers are exploring alternative methods to achieve effects\ncomparable to those of RLHF. However, these methods often depend on large\nhigh-quality datasets and inefficiently utilize generated data. To deal with\nthis problem, we propose PSLE, i.e., Progressively Selective Label Enhancement\nfor Language Model Alignment, a framework that fully utilizes all generated\ndata by guiding the model with principles to align outputs with human\nexpectations. Using a dynamically updated threshold, our approach ensures\nefficient data utilization by incorporating all generated responses and\nweighting them based on their corresponding reward scores. Experimental results\non multiple datasets demonstrate the effectiveness of PSLE compared to existing\nlanguage model alignment methods.\n","authors":["Biao Liu","Ning Xu","Xin Geng"],"pdf_url":"https://arxiv.org/pdf/2408.02599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03598v2","updated":"2024-08-05T16:01:13Z","published":"2024-04-04T17:09:52Z","title":"Intent Detection and Entity Extraction from BioMedical Literature","summary":"  Biomedical queries have become increasingly prevalent in web searches,\nreflecting the growing interest in accessing biomedical literature. Despite\nrecent research on large-language models (LLMs) motivated by endeavours to\nattain generalized intelligence, their efficacy in replacing task and\ndomain-specific natural language understanding approaches remains questionable.\nIn this paper, we address this question by conducting a comprehensive empirical\nevaluation of intent detection and named entity recognition (NER) tasks from\nbiomedical text. We show that Supervised Fine Tuned approaches are still\nrelevant and more effective than general-purpose LLMs. Biomedical transformer\nmodels such as PubMedBERT can surpass ChatGPT on NER task with only 5\nsupervised examples.\n","authors":["Ankan Mullick","Mukur Gupta","Pawan Goyal"],"pdf_url":"https://arxiv.org/pdf/2404.03598v2.pdf","comment":"Accepted to CL4Health LREC-COLING 2024"},{"id":"http://arxiv.org/abs/2408.02584v1","updated":"2024-08-05T16:00:21Z","published":"2024-08-05T16:00:21Z","title":"Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality\n  Aspect-Based Summarization","summary":"  The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.\n","authors":["Ankan Mullick","Sombit Bose","Rounak Saha","Ayan Kumar Bhowmick","Aditya Vempaty","Pawan Goyal","Niloy Ganguly","Prasenjit Dey","Ravi Kokku"],"pdf_url":"https://arxiv.org/pdf/2408.02584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06093v2","updated":"2024-08-05T15:51:50Z","published":"2024-05-09T20:45:58Z","title":"Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human\n  Annotation: A Case Study Using Schedule-of-Event Table Detection","summary":"  Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.\n","authors":["Bhawesh Kumar","Jonathan Amar","Eric Yang","Nan Li","Yugang Jia"],"pdf_url":"https://arxiv.org/pdf/2405.06093v2.pdf","comment":"23 pages. Published in MLHC 2024"},{"id":"http://arxiv.org/abs/2408.02575v1","updated":"2024-08-05T15:48:51Z","published":"2024-08-05T15:48:51Z","title":"Artificial Intelligence for Public Health Surveillance in Africa:\n  Applications and Opportunities","summary":"  Artificial Intelligence (AI) is revolutionizing various fields, including\npublic health surveillance. In Africa, where health systems frequently\nencounter challenges such as limited resources, inadequate infrastructure,\nfailed health information systems and a shortage of skilled health\nprofessionals, AI offers a transformative opportunity. This paper investigates\nthe applications of AI in public health surveillance across the continent,\npresenting successful case studies and examining the benefits, opportunities,\nand challenges of implementing AI technologies in African healthcare settings.\nOur paper highlights AI's potential to enhance disease monitoring and health\noutcomes, and support effective public health interventions. The findings\npresented in the paper demonstrate that AI can significantly improve the\naccuracy and timeliness of disease detection and prediction, optimize resource\nallocation, and facilitate targeted public health strategies. Additionally, our\npaper identified key barriers to the widespread adoption of AI in African\npublic health systems and proposed actionable recommendations to overcome these\nchallenges.\n","authors":["Jean Marie Tshimula","Mitterrand Kalengayi","Dieumerci Makenga","Dorcas Lilonge","Marius Asumani","Dborah Madiya","lie Nkuba Kalonji","Hugues Kanda","Ren Manass Galekwa","Josias Kumbu","Hardy Mikese","Grace Tshimula","Jean Tshibangu Muabila","Christian N. Mayemba","D'Jeff K. Nkashama","Kalonji Kalala","Steve Ataky","Tighana Wenge Basele","Mbuyi Mukendi Didier","Selain K. Kasereka","Maximilien V. Dialufuma","Godwill Ilunga Wa Kumwita","Lionel Muyuku","Jean-Paul Kimpesa","Dominique Muteba","Aaron Aruna Abedi","Lambert Mukendi Ntobo","Gloria M. Bundutidi","Dsir Kulimba Mashinda","Emmanuel Kabengele Mpinga","Nathanal M. Kasoro"],"pdf_url":"https://arxiv.org/pdf/2408.02575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02559v1","updated":"2024-08-05T15:36:46Z","published":"2024-08-05T15:36:46Z","title":"Evaluating and Enhancing LLMs Agent based on Theory of Mind in Guandan:\n  A Multi-Player Cooperative Game under Imperfect Information","summary":"  Large language models (LLMs) have shown success in handling simple games with\nimperfect information and enabling multi-agent coordination, but their ability\nto facilitate practical collaboration against other agents in complex,\nimperfect information environments, especially in a non-English environment,\nstill needs to be explored. This study investigates the applicability of\nknowledge acquired by open-source and API-based LLMs to sophisticated\ntext-based games requiring agent collaboration under imperfect information,\ncomparing their performance to established baselines using other types of\nagents. We propose a Theory of Mind (ToM) planning technique that allows LLM\nagents to adapt their strategy against various adversaries using only game\nrules, current state, and historical context as input. An external tool was\nincorporated to mitigate the challenge of dynamic and extensive action spaces\nin this card game. Our results show that although a performance gap exists\nbetween current LLMs and state-of-the-art reinforcement learning (RL) models,\nLLMs demonstrate ToM capabilities in this game setting. It consistently\nimproves their performance against opposing agents, suggesting their ability to\nunderstand the actions of allies and adversaries and establish collaboration\nwith allies. To encourage further research and understanding, we have made our\ncodebase openly accessible.\n","authors":["Yauwai Yim","Chunkit Chan","Tianyu Shi","Zheye Deng","Wei Fan","Tianshi Zheng","Yangqiu Song"],"pdf_url":"https://arxiv.org/pdf/2408.02559v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02545v1","updated":"2024-08-05T15:16:24Z","published":"2024-08-05T15:16:24Z","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation","summary":"  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n","authors":["Daniel Fleischer","Moshe Berchansky","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2408.02545v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.02544v1","updated":"2024-08-05T15:16:22Z","published":"2024-08-05T15:16:22Z","title":"Caution for the Environment: Multimodal Agents are Susceptible to\n  Environmental Distractions","summary":"  This paper investigates the faithfulness of multimodal large language model\n(MLLM) agents in the graphical user interface (GUI) environment, aiming to\naddress the research question of whether multimodal GUI agents can be\ndistracted by environmental context. A general setting is proposed where both\nthe user and the agent are benign, and the environment, while not malicious,\ncontains unrelated content. A wide range of MLLMs are evaluated as GUI agents\nusing our simulated dataset, following three working patterns with different\nlevels of perception. Experimental results reveal that even the most powerful\nmodels, whether generalist agents or specialist GUI agents, are susceptible to\ndistractions. While recent studies predominantly focus on the helpfulness\n(i.e., action accuracy) of multimodal agents, our findings indicate that these\nagents are prone to environmental distractions, resulting in unfaithful\nbehaviors. Furthermore, we switch to the adversarial perspective and implement\nenvironment injection, demonstrating that such unfaithfulness can be exploited,\nleading to unexpected risks.\n","authors":["Xinbei Ma","Yiting Wang","Yao Yao","Tongxin Yuan","Aston Zhang","Zhuosheng Zhang","Hai Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.02544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09298v2","updated":"2024-08-05T15:10:25Z","published":"2024-07-12T14:31:05Z","title":"Transformer Layers as Painters","summary":"  Despite their nearly universal adoption for large language models, the\ninternal workings of transformers are not well understood. We aim to better\nunderstand the impact of removing or reorganizing information throughout the\nlayers of a pretrained transformer. Such an understanding could both yield\nbetter usage of existing models as well as to make architectural improvements\nto produce new variants. We present a series of empirical studies on frozen\nmodels that show that the lower and final layers of pretrained transformers\ndiffer from middle layers, but that middle layers have a surprising amount of\nuniformity. We further show that some classes of problems have robustness to\nskipping layers, running the layers in an order different from how they were\ntrained, or running the layers in parallel. Our observations suggest that even\nfrozen pretrained models may gracefully trade accuracy for latency by skipping\nlayers or running layers in parallel.\n","authors":["Qi Sun","Marc Pickett","Aakash Kumar Nain","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2407.09298v2.pdf","comment":"12 pages total, including references and appendices"},{"id":"http://arxiv.org/abs/2406.04216v3","updated":"2024-08-05T15:08:02Z","published":"2024-06-06T16:15:34Z","title":"What Do Language Models Learn in Context? The Structured Task Hypothesis","summary":"  Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.\n","authors":["Jiaoda Li","Yifan Hou","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04216v3.pdf","comment":"This work is published in ACL 2024"},{"id":"http://arxiv.org/abs/2408.02520v1","updated":"2024-08-05T14:40:40Z","published":"2024-08-05T14:40:40Z","title":"OneLove beyond the field -- A few-shot pipeline for topic and sentiment\n  analysis during the FIFA World Cup in Qatar","summary":"  The FIFA World Cup in Qatar was discussed extensively in the news and on\nsocial media. Due to news reports with allegations of human rights violations,\nthere were calls to boycott it. Wearing a OneLove armband was part of a planned\nprotest activity. Controversy around the armband arose when FIFA threatened to\nsanction captains who wear it. To understand what topics Twitter users Tweeted\nabout and what the opinion of German Twitter users was towards the OneLove\narmband, we performed an analysis of German Tweets published during the World\nCup using in-context learning with LLMs. We validated the labels on human\nannotations. We found that Twitter users initially discussed the armband's\nimpact, LGBT rights, and politics; after the ban, the conversation shifted\ntowards politics in sports in general, accompanied by a subtle shift in\nsentiment towards neutrality. Our evaluation serves as a framework for future\nresearch to explore the impact of sports activism and evolving public\nsentiment. This is especially useful in settings where labeling datasets for\nspecific opinions is unfeasible, such as when events are unfolding.\n","authors":["Christoph Rauchegger","Sonja Mei Wang","Pieter Delobelle"],"pdf_url":"https://arxiv.org/pdf/2408.02520v1.pdf","comment":"Accepted at KONVENS 2024"},{"id":"http://arxiv.org/abs/2408.02503v1","updated":"2024-08-05T14:27:39Z","published":"2024-08-05T14:27:39Z","title":"UnifiedMLLM: Enabling Unified Representation for Multi-modal Multi-tasks\n  With Large Language Model","summary":"  Significant advancements has recently been achieved in the field of\nmulti-modal large language models (MLLMs), demonstrating their remarkable\ncapabilities in understanding and reasoning across diverse tasks. However,\nthese models are often trained for specific tasks and rely on task-specific\ninput-output formats, limiting their applicability to a broader range of tasks.\nThis raises a fundamental question: Can we develop a unified approach to\nrepresent and handle different multi-modal tasks to maximize the\ngeneralizability of MLLMs? In this paper, we propose UnifiedMLLM, a\ncomprehensive model designed to represent various tasks using a unified\nrepresentation. Our model exhibits strong capabilities in comprehending the\nimplicit intent of user instructions and preforming reasoning. In addition to\ngenerating textual responses, our model also outputs task tokens and grounding\ntokens, serving as indicators of task types and task granularity. These outputs\nare subsequently routed through the task router and directed to specific expert\nmodels for task completion. To train our model, we construct a task-specific\ndataset and an 100k multi-task dataset encompassing complex scenarios.\nEmploying a three-stage training strategy, we equip our model with robust\nreasoning and task processing capabilities while preserving its generalization\ncapacity and knowledge reservoir. Extensive experiments showcase the impressive\nperformance of our unified representation approach across various tasks,\nsurpassing existing methodologies. Furthermore, our approach exhibits\nexceptional scalability and generality. Our code, model, and dataset will be\navailable at \\url{https://github.com/lzw-lzw/UnifiedMLLM}.\n","authors":["Zhaowei Li","Wei Wang","YiQing Cai","Xu Qi","Pengyu Wang","Dong Zhang","Hang Song","Botian Jiang","Zhida Huang","Tao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02875v2","updated":"2024-08-05T14:01:26Z","published":"2024-03-05T11:38:48Z","title":"Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples","summary":"  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n","authors":["Philipp J. Rsch","Norbert Oswald","Michaela Geierhos","Jindich Libovick"],"pdf_url":"https://arxiv.org/pdf/2403.02875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02479v1","updated":"2024-08-05T14:01:15Z","published":"2024-08-05T14:01:15Z","title":"From LLMs to LLM-based Agents for Software Engineering: A Survey of\n  Current, Challenges and Future","summary":"  With the rise of large language models (LLMs), researchers are increasingly\nexploring their applications in var ious vertical domains, such as software\nengineering. LLMs have achieved remarkable success in areas including code\ngeneration and vulnerability detection. However, they also exhibit numerous\nlimitations and shortcomings. LLM-based agents, a novel tech nology with the\npotential for Artificial General Intelligence (AGI), combine LLMs as the core\nfor decision-making and action-taking, addressing some of the inherent\nlimitations of LLMs such as lack of autonomy and self-improvement. Despite\nnumerous studies and surveys exploring the possibility of using LLMs in\nsoftware engineering, it lacks a clear distinction between LLMs and LLM based\nagents. It is still in its early stage for a unified standard and benchmarking\nto qualify an LLM solution as an LLM-based agent in its domain. In this survey,\nwe broadly investigate the current practice and solutions for LLMs and\nLLM-based agents for software engineering. In particular we summarise six key\ntopics: requirement engineering, code generation, autonomous decision-making,\nsoftware design, test generation, and software maintenance. We review and\ndifferentiate the work of LLMs and LLM-based agents from these six topics,\nexamining their differences and similarities in tasks, benchmarks, and\nevaluation metrics. Finally, we discuss the models and benchmarks used,\nproviding a comprehensive analysis of their applications and effectiveness in\nsoftware engineering. We anticipate this work will shed some lights on pushing\nthe boundaries of LLM-based agents in software engineering for future research.\n","authors":["Haolin Jin","Linghan Huang","Haipeng Cai","Jun Yan","Bo Li","Huaming Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13679v4","updated":"2024-08-05T13:32:40Z","published":"2024-03-20T15:38:36Z","title":"SocialBench: Sociality Evaluation of Role-Playing Conversational Agents","summary":"  Large language models (LLMs) have advanced the development of various AI\nconversational agents, including role-playing conversational agents that mimic\ndiverse characters and human behaviors. While prior research has predominantly\nfocused on enhancing the conversational capability, role-specific knowledge,\nand stylistic attributes of these agents, there has been a noticeable gap in\nassessing their social intelligence. In this paper, we introduce SocialBench,\nthe first benchmark designed to systematically evaluate the sociality of\nrole-playing conversational agents at both individual and group levels of\nsocial interactions. The benchmark is constructed from a variety of sources and\ncovers a wide range of 500 characters and over 6,000 question prompts and\n30,800 multi-turn role-playing utterances. We conduct comprehensive evaluations\non this benchmark using mainstream open-source and closed-source LLMs. We find\nthat agents excelling in individual level does not imply their proficiency in\ngroup level. Moreover, the behavior of individuals may drift as a result of the\ninfluence exerted by other agents within the group. Experimental results on\nSocialBench confirm its significance as a testbed for assessing the social\ninteraction of role-playing conversational agents. The benchmark is publicly\naccessible at https://github.com/X-PLUG/SocialBench.\n","authors":["Hongzhan Chen","Hehong Chen","Ming Yan","Wenshen Xu","Xing Gao","Weizhou Shen","Xiaojun Quan","Chenliang Li","Ji Zhang","Fei Huang","Jingren Zhou"],"pdf_url":"https://arxiv.org/pdf/2403.13679v4.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2404.07768v4","updated":"2024-08-05T13:12:32Z","published":"2024-04-11T14:06:39Z","title":"Using Letter Positional Probabilities to Assess Word Complexity","summary":"  Word complexity is defined in a number of different ways. Psycholinguistic,\nmorphological and lexical proxies are often used. Human ratings are also used.\nThe problem here is that these proxies do not measure complexity directly, and\nhuman ratings are susceptible to subjective bias. In this study we contend that\nsome form of 'latent complexity' can be approximated by using samples of simple\nand complex words. We use a sample of 'simple' words from primary school\npicture books and a sample of 'complex' words from high school and academic\nsettings. In order to analyse the differences between these classes, we look at\nthe letter positional probabilities (LPPs). We find strong statistical\nassociations between several LPPs and complexity. For example, simple words are\nsignificantly (p<.001) more likely to start with w, b, s, h, g, k, j, t, y or\nf, while complex words are significantly (p<.001) more likely to start with i,\na, e, r, v, u or d. We find similar strong associations for subsequent letter\npositions, with 84 letter-position variables in the first 6 positions being\nsignificant at the p<.001 level. We then use LPPs as variables in creating a\nclassifier which can classify the two classes with an 83% accuracy. We test\nthese findings using a second data set, with 66 LPPs significant (p<.001) in\nthe first 6 positions common to both datasets. We use these 66 variables to\ncreate a classifier that is able to classify a third dataset with an accuracy\nof 70%. Finally, we create a fourth sample by combining the extreme high and\nlow scoring words generated by three classifiers built on the first three\nseparate datasets and use this sample to build a classifier which has an\naccuracy of 97%. We use this to score the four levels of English word groups\nfrom an ESL program.\n","authors":["Michael Dalvean"],"pdf_url":"https://arxiv.org/pdf/2404.07768v4.pdf","comment":"30 Pages, 15 Tables"},{"id":"http://arxiv.org/abs/2407.08103v3","updated":"2024-08-05T13:08:31Z","published":"2024-07-11T00:25:01Z","title":"Automata-based constraints for language model decoding","summary":"  Language models (LMs) are often expected to generate strings in some formal\nlanguage; for example, structured data, API calls, or code snippets. Although\nLMs can be tuned to improve their adherence to formal syntax, this does not\nguarantee conformance, especially with smaller LMs suitable for large-scale\ndeployment. In addition, tuning requires significant resources, making it\nimpractical for uncommon or task-specific formats. To prevent downstream\nparsing errors we would ideally constrain the LM to only produce valid output,\nbut this is severely complicated by tokenization, which is typically both\nambiguous and misaligned with the formal grammar. We solve these issues through\nthe application of automata theory, deriving an efficient closed-form solution\nfor the regular languages, a broad class of formal languages with many\npractical applications, including API calls or schema-guided JSON and YAML. We\nalso discuss pragmatic extensions for coping with the issue of high branching\nfactor, and extend our techniques to deterministic context-free languages,\nwhich similarly admit an efficient closed-form solution. Previous work on this\ntopic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading\nto problems with speed, correctness, and extensibility. Instead, we reformulate\nthe entire task in terms of automata so we can leverage well-studied and\nwell-optimized algorithms. Our system compiles constraints ~7,000x faster, is\nprovably correct, and can be extended in a modular fashion.\n","authors":["Terry Koo","Frederick Liu","Luheng He"],"pdf_url":"https://arxiv.org/pdf/2407.08103v3.pdf","comment":"COLM 2024 Camera-ready version, responding to feedback from reviewers"},{"id":"http://arxiv.org/abs/2408.02442v1","updated":"2024-08-05T13:08:24Z","published":"2024-08-05T13:08:24Z","title":"Let Me Speak Freely? A Study on the Impact of Format Restrictions on\n  Performance of Large Language Models","summary":"  Structured generation, the process of producing content in standardized\nformats like JSON and XML, is widely utilized in real-world applications to\nextract key output information from large language models (LLMs). This study\ninvestigates whether such constraints on generation space impact LLMs'\nabilities, including reasoning and domain knowledge comprehension.\nSpecifically, we evaluate LLMs' performance when restricted to adhere to\nstructured formats versus generating free-form responses across various common\ntasks. Surprisingly, we observe a significant decline in LLMs' reasoning\nabilities under format restrictions. Furthermore, we find that stricter format\nconstraints generally lead to greater performance degradation in reasoning\ntasks.\n","authors":["Zhi Rui Tam","Cheng-Kuang Wu","Yi-Lin Tsai","Chieh-Yen Lin","Hung-yi Lee","Yun-Nung Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02442v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2408.02439v1","updated":"2024-08-05T12:59:35Z","published":"2024-08-05T12:59:35Z","title":"Long Input Benchmark for Russian Analysis","summary":"  Recent advancements in Natural Language Processing (NLP) have fostered the\ndevelopment of Large Language Models (LLMs) that can solve an immense variety\nof tasks. One of the key aspects of their application is their ability to work\nwith long text documents and to process long sequences of tokens. This has\ncreated a demand for proper evaluation of long-context understanding. To\naddress this need for the Russian language, we propose LIBRA (Long Input\nBenchmark for Russian Analysis), which comprises 21 adapted datasets to study\nthe LLM's abilities to understand long texts thoroughly. The tests are divided\ninto four complexity groups and allow the evaluation of models across various\ncontext lengths ranging from 4k up to 128k tokens. We provide the open-source\ndatasets, codebase, and public leaderboard for LIBRA to guide forthcoming\nresearch.\n","authors":["Igor Churin","Murat Apishev","Maria Tikhonova","Denis Shevelev","Aydar Bulatov","Yuri Kuratov","Sergej Averkiev","Alena Fenogenova"],"pdf_url":"https://arxiv.org/pdf/2408.02439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02417v1","updated":"2024-08-05T12:21:04Z","published":"2024-08-05T12:21:04Z","title":"Infusing Emotions into Task-oriented Dialogue Systems: Understanding,\n  Management, and Generation","summary":"  Emotions are indispensable in human communication, but are often overlooked\nin task-oriented dialogue (ToD) modelling, where the task success is the\nprimary focus. While existing works have explored user emotions or similar\nconcepts in some ToD tasks, none has so far included emotion modelling into a\nfully-fledged ToD system nor conducted interaction with human or simulated\nusers. In this work, we incorporate emotion into the complete ToD processing\nloop, involving understanding, management, and generation. To this end, we\nextend the EmoWOZ dataset (Feng et al., 2022) with system affective behaviour\nlabels. Through interactive experimentation involving both simulated and human\nusers, we demonstrate that our proposed framework significantly enhances the\nuser's emotional experience as well as the task success.\n","authors":["Shutong Feng","Hsien-chin Lin","Christian Geishauser","Nurul Lubis","Carel van Niekerk","Michael Heck","Benjamin Ruppik","Renato Vukovic","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.02417v1.pdf","comment":"Accepted by SIGDIAL 2024"},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02416v1","updated":"2024-08-05T12:20:39Z","published":"2024-08-05T12:20:39Z","title":"Why Are My Prompts Leaked? Unraveling Prompt Extraction Threats in\n  Customized Large Language Models","summary":"  The drastic increase of large language models' (LLMs) parameters has led to a\nnew research direction of fine-tuning-free downstream customization by prompts,\ni.e., task descriptions. While these prompt-based services (e.g. OpenAI's GPTs)\nplay an important role in many businesses, there has emerged growing concerns\nabout the prompt leakage, which undermines the intellectual properties of these\nservices and causes downstream attacks. In this paper, we analyze the\nunderlying mechanism of prompt leakage, which we refer to as prompt\nmemorization, and develop corresponding defending strategies. By exploring the\nscaling laws in prompt extraction, we analyze key attributes that influence\nprompt extraction, including model sizes, prompt lengths, as well as the types\nof prompts. Then we propose two hypotheses that explain how LLMs expose their\nprompts. The first is attributed to the perplexity, i.e. the familiarity of\nLLMs to texts, whereas the second is based on the straightforward token\ntranslation path in attention matrices. To defend against such threats, we\ninvestigate whether alignments can undermine the extraction of prompts. We find\nthat current LLMs, even those with safety alignments like GPT-4, are highly\nvulnerable to prompt extraction attacks, even under the most straightforward\nuser attacks. Therefore, we put forward several defense strategies with the\ninspiration of our findings, which achieve 83.8\\% and 71.0\\% drop in the prompt\nextraction rate for Llama2-7B and GPT-3.5, respectively. Source code is\navaliable at \\url{https://github.com/liangzid/PromptExtractionEval}.\n","authors":["Zi Liang","Haibo Hu","Qingqing Ye","Yaxin Xiao","Haoyang Li"],"pdf_url":"https://arxiv.org/pdf/2408.02416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.12881v2","updated":"2024-08-05T12:13:39Z","published":"2023-09-22T14:11:23Z","title":"Affect Recognition in Conversations Using Large Language Models","summary":"  Affect recognition, encompassing emotions, moods, and feelings, plays a\npivotal role in human communication. In the realm of conversational artificial\nintelligence, the ability to discern and respond to human affective cues is a\ncritical factor for creating engaging and empathetic interactions. This study\ninvestigates the capacity of large language models (LLMs) to recognise human\naffect in conversations, with a focus on both open-domain chit-chat dialogues\nand task-oriented dialogues. Leveraging three diverse datasets, namely IEMOCAP\n(Busso et al., 2008), EmoWOZ (Feng et al., 2022), and DAIC-WOZ (Gratch et al.,\n2014), covering a spectrum of dialogues from casual conversations to clinical\ninterviews, we evaluate and compare LLMs' performance in affect recognition.\nOur investigation explores the zero-shot and few-shot capabilities of LLMs\nthrough in-context learning as well as their model capacities through\ntask-specific fine-tuning. Additionally, this study takes into account the\npotential impact of automatic speech recognition errors on LLM predictions.\nWith this work, we aim to shed light on the extent to which LLMs can replicate\nhuman-like affect recognition capabilities in conversations.\n","authors":["Shutong Feng","Guangzhi Sun","Nurul Lubis","Wen Wu","Chao Zhang","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2309.12881v2.pdf","comment":"Accepted by SIGDIAL 2024"},{"id":"http://arxiv.org/abs/2309.01446v4","updated":"2024-08-05T11:34:10Z","published":"2023-09-04T08:54:20Z","title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models","summary":"  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n","authors":["Raz Lapid","Ron Langberg","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2309.01446v4.pdf","comment":"Accepted at SeT-LLM @ ICLR 2024"},{"id":"http://arxiv.org/abs/2408.02377v1","updated":"2024-08-05T11:06:36Z","published":"2024-08-05T11:06:36Z","title":"A Few-Shot Approach for Relation Extraction Domain Adaptation using\n  Large Language Models","summary":"  Knowledge graphs (KGs) have been successfully applied to the analysis of\ncomplex scientific and technological domains, with automatic KG generation\nmethods typically building upon relation extraction models capturing\nfine-grained relations between domain entities in text. While these relations\nare fully applicable across scientific areas, existing models are trained on\nfew domain-specific datasets such as SciERC and do not perform well on new\ntarget domains. In this paper, we experiment with leveraging in-context\nlearning capabilities of Large Language Models to perform schema-constrained\ndata annotation, collecting in-domain training instances for a\nTransformer-based relation extraction model deployed on titles and abstracts of\nresearch papers in the Architecture, Construction, Engineering and Operations\n(AECO) domain. By assessing the performance gain with respect to a baseline\nDeep Learning architecture trained on off-domain data, we show that by using a\nfew-shot learning strategy with structured prompts and only minimal expert\nannotation the presented approach can potentially support domain adaptation of\na science KG generation model.\n","authors":["Vanni Zavarella","Juan Carlos Gamero-Salinas","Sergio Consoli"],"pdf_url":"https://arxiv.org/pdf/2408.02377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16969v5","updated":"2024-08-05T10:54:39Z","published":"2024-05-27T09:06:24Z","title":"The Multi-Range Theory of Translation Quality Measurement: MQM scoring\n  models and Statistical Quality Control","summary":"  The year 2024 marks the 10th anniversary of the Multidimensional Quality\nMetrics (MQM) framework for analytic translation quality evaluation. The MQM\nerror typology has been widely used by practitioners in the translation and\nlocalization industry and has served as the basis for many derivative projects.\nThe annual Conference on Machine Translation (WMT) shared tasks on both human\nand automatic translation quality evaluations used the MQM error typology.\n  The metric stands on two pillars: error typology and the scoring model. The\nscoring model calculates the quality score from annotation data, detailing how\nto convert error type and severity counts into numeric scores to determine if\nthe content meets specifications. Previously, only the raw scoring model had\nbeen published. This April, the MQM Council published the Linear Calibrated\nScoring Model, officially presented herein, along with the Non-Linear Scoring\nModel, which had not been published before.\n  This paper details the latest MQM developments and presents a universal\napproach to translation quality measurement across three sample size ranges. It\nalso explains why Statistical Quality Control should be used for very small\nsample sizes, starting from a single sentence.\n","authors":["Arle Lommel","Serge Gladkoff","Alan Melby","Sue Ellen Wright","Ingemar Strandvik","Katerina Gasova","Angelika Vaasa","Andy Benzo","Romina Marazzato Sparano","Monica Foresi","Johani Innis","Lifeng Han","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2405.16969v5.pdf","comment":"Accepted by AMTA2024"},{"id":"http://arxiv.org/abs/2408.02361v1","updated":"2024-08-05T10:10:01Z","published":"2024-08-05T10:10:01Z","title":"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought\n  Decoding","summary":"  State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models.\n","authors":["Renato Vukovic","David Arps","Carel van Niekerk","Benjamin Matthias Ruppik","Hsien-Chin Lin","Michael Heck","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.02361v1.pdf","comment":"Accepted to appear at SIGDIAL 2024. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.10805v2","updated":"2024-08-05T09:51:15Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval","summary":"  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02341v1","updated":"2024-08-05T09:38:07Z","published":"2024-08-05T09:38:07Z","title":"An approach to optimize inference of the DIART speaker diarization\n  pipeline","summary":"  Speaker diarization answers the question \"who spoke when\" for an audio file.\nIn some diarization scenarios, low latency is required for transcription.\nSpeaker diarization with low latency is referred to as online speaker\ndiarization. The DIART pipeline is an online speaker diarization system. It\nconsists of a segmentation and an embedding model. The embedding model has the\nlargest share of the overall latency. The aim of this paper is to optimize the\ninference latency of the DIART pipeline. Different inference optimization\nmethods such as knowledge distilation, pruning, quantization and layer fusion\nare applied to the embedding model of the pipeline. It turns out that knowledge\ndistillation optimizes the latency, but has a negative effect on the accuracy.\nQuantization and layer fusion also have a positive influence on the latency\nwithout worsening the accuracy. Pruning, on the other hand, does not improve\nlatency.\n","authors":["Roman Aperdannier","Sigurd Schacht","Alexander Piazza"],"pdf_url":"https://arxiv.org/pdf/2408.02341v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.13244v3","updated":"2024-08-05T09:31:18Z","published":"2024-03-20T02:15:55Z","title":"Instruction Multi-Constraint Molecular Generation Using a\n  Teacher-Student Large Language Model","summary":"  While various models and computational tools have been proposed for structure\nand property analysis of molecules, generating molecules that conform to all\ndesired structures and properties remains a challenge. Here, we introduce a\nmulti-constraint molecular generation large language model, TSMMG, which, akin\nto a student, incorporates knowledge from various small models and tools,\nnamely, the 'teachers'. To train TSMMG, we construct a large set of\ntext-molecule pairs by extracting molecular knowledge from these 'teachers',\nenabling it to generate novel molecules that conform to the descriptions\nthrough various text prompts. We experimentally show that TSMMG remarkably\nperforms in generating molecules meeting complex, natural language-described\nproperty requirements across two-, three-, and four-constraint tasks, with an\naverage molecular validity of over 99% and success ratio of 82.58%, 68.03%, and\n67.48%, respectively. The model also exhibits adaptability through zero-shot\ntesting, creating molecules that satisfy combinations of properties that have\nnot been encountered. It can comprehend text inputs with various language\nstyles, extending beyond the confines of outlined prompts, as confirmed through\nempirical validation. Additionally, the knowledge distillation feature of TSMMG\ncontributes to the continuous enhancement of small models, while the innovative\napproach to dataset construction effectively addresses the issues of data\nscarcity and quality, which positions TSMMG as a promising tool in the domains\nof drug discovery and materials science.\n","authors":["Peng Zhou","Jianmin Wang","Chunyan Li","Zixu Wang","Yiping Liu","Chubo Liu","Siqi Sun","Jianxin Lin","Leyi Wei","Xibao Cai","Houtim Lai","Wei Liu","Longyue Wang","Xiangxiang Zeng","Kenli Li"],"pdf_url":"https://arxiv.org/pdf/2403.13244v3.pdf","comment":"37 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.02337v1","updated":"2024-08-05T09:23:49Z","published":"2024-08-05T09:23:49Z","title":"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR\n  Dataset Construction","summary":"  Advancements in AI and natural language processing have revolutionized\nmachine-human language interactions, with question answering (QA) systems\nplaying a pivotal role. The knowledge base question answering (KBQA) task,\nutilizing structured knowledge graphs (KG), allows for handling extensive\nknowledge-intensive questions. However, a significant gap exists in KBQA\ndatasets, especially for low-resource languages. Many existing construction\npipelines for these datasets are outdated and inefficient in human labor, and\nmodern assisting tools like Large Language Models (LLM) are not utilized to\nreduce the workload. To address this, we have designed and implemented a\nmodern, semi-automated approach for creating datasets, encompassing tasks such\nas KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),\ntailored explicitly for low-resource environments. We executed this pipeline\nand introduced the PUGG dataset, the first Polish KBQA dataset, and novel\ndatasets for MRC and IR. Additionally, we provide a comprehensive\nimplementation, insightful findings, detailed statistics, and evaluation of\nbaseline models.\n","authors":["Albert Sawczyn","Katsiaryna Viarenich","Konrad Wojtasik","Aleksandra Domogaa","Marcin Oleksy","Maciej Piasecki","Tomasz Kajdanowicz"],"pdf_url":"https://arxiv.org/pdf/2408.02337v1.pdf","comment":"Accepted for ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2306.16143v5","updated":"2024-08-05T08:45:44Z","published":"2023-06-28T12:17:45Z","title":"Generative User-Experience Research for Developing Domain-specific\n  Natural Language Processing Applications","summary":"  User experience (UX) is a part of human-computer interaction (HCI) research\nand focuses on increasing intuitiveness, transparency, simplicity, and trust\nfor the system users. Most UX research for machine learning (ML) or natural\nlanguage processing (NLP) focuses on a data-driven methodology. It engages\ndomain users mainly for usability evaluation. Moreover, more typical UX methods\ntailor the systems towards user usability, unlike learning about the user needs\nfirst. This paper proposes a new methodology for integrating generative UX\nresearch into developing domain NLP applications. Generative UX research\nemploys domain users at the initial stages of prototype development, i.e.,\nideation and concept evaluation, and the last stage for evaluating system\nusefulness and user utility. The methodology emerged from and is evaluated on a\ncase study about the full-cycle prototype development of a domain-specific\nsemantic search for daily operations in the process industry. A key finding of\nour case study is that involving domain experts increases their interest and\ntrust in the final NLP application. The combined UX+NLP research of the\nproposed method efficiently considers data- and user-driven opportunities and\nconstraints, which can be crucial for developing NLP applications.\n","authors":["Anastasia Zhukova","Lukas von Sperl","Christian E. Matt","Bela Gipp"],"pdf_url":"https://arxiv.org/pdf/2306.16143v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00230v2","updated":"2024-08-05T08:36:20Z","published":"2024-08-01T01:54:17Z","title":"Lost in Translation: Latent Concept Misalignment in Text-to-Image\n  Diffusion Models","summary":"  Advancements in text-to-image diffusion models have broadened extensive\ndownstream practical applications, but such models often encounter misalignment\nissues between text and image. Taking the generation of a combination of two\ndisentangled concepts as an example, say given the prompt \"a tea cup of iced\ncoke\", existing models usually generate a glass cup of iced coke because the\niced coke usually co-occurs with the glass cup instead of the tea one during\nmodel training. The root of such misalignment is attributed to the confusion in\nthe latent semantic space of text-to-image diffusion models, and hence we refer\nto the \"a tea cup of iced coke\" phenomenon as Latent Concept Misalignment\n(LC-Mis). We leverage large language models (LLMs) to thoroughly investigate\nthe scope of LC-Mis, and develop an automated pipeline for aligning the latent\nsemantics of diffusion models to text prompts. Empirical assessments confirm\nthe effectiveness of our approach, substantially reducing LC-Mis errors and\nenhancing the robustness and versatility of text-to-image diffusion models. The\ncode and dataset are here: https://github.com/RossoneriZhao/iced_coke.\n","authors":["Juntu Zhao","Junyu Deng","Yixin Ye","Chongxuan Li","Zhijie Deng","Dequan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.00230v2.pdf","comment":"Accepted by the 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02302v1","updated":"2024-08-05T08:24:24Z","published":"2024-08-05T08:24:24Z","title":"SNFinLLM: Systematic and Nuanced Financial Domain Adaptation of Chinese\n  Large Language Models","summary":"  Large language models (LLMs) have become powerful tools for advancing natural\nlanguage processing applications in the financial industry. However, existing\nfinancial LLMs often face challenges such as hallucinations or superficial\nparameter training, resulting in suboptimal performance, particularly in\nfinancial computing and machine reading comprehension (MRC). To address these\nissues, we propose a novel large language model specifically designed for the\nChinese financial domain, named SNFinLLM. SNFinLLM excels in domain-specific\ntasks such as answering questions, summarizing financial research reports,\nanalyzing sentiment, and executing financial calculations. We then perform the\nsupervised fine-tuning (SFT) to enhance the model's proficiency across various\nfinancial domains. Specifically, we gather extensive financial data and create\na high-quality instruction dataset composed of news articles, professional\npapers, and research reports of finance domain. Utilizing both domain-specific\nand general datasets, we proceed with continuous pre-training on an established\nopen-source base model, resulting in SNFinLLM-base. Following this, we engage\nin supervised fine-tuning (SFT) to bolster the model's capability across\nmultiple financial tasks. Crucially, we employ a straightforward Direct\nPreference Optimization (DPO) method to better align the model with human\npreferences. Extensive experiments conducted on finance benchmarks and our\nevaluation dataset demonstrate that SNFinLLM markedly outperforms other\nstate-of-the-art financial language models. For more details, check out our\ndemo video here: https://www.youtube.com/watch?v=GYT-65HZwus.\n","authors":["Shujuan Zhao","Lingfeng Qiao","Kangyang Luo","Qian-Wen Zhang","Junru Lu","Di Yin"],"pdf_url":"https://arxiv.org/pdf/2408.02302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.08331v2","updated":"2024-08-05T08:13:37Z","published":"2024-07-11T09:28:27Z","title":"Towards Explainable Evolution Strategies with Large Language Models","summary":"  This paper introduces an approach that integrates self-adaptive Evolution\nStrategies (ES) with Large Language Models (LLMs) to enhance the explainability\nof complex optimization processes. By employing a self-adaptive ES equipped\nwith a restart mechanism, we effectively navigate the challenging landscapes of\nbenchmark functions, capturing detailed logs of the optimization journey. The\nlogs include fitness evolution, step-size adjustments and restart events due to\nstagnation. An LLM is then utilized to process these logs, generating concise,\nuser-friendly summaries that highlight key aspects such as convergence\nbehavior, optimal fitness achievements, and encounters with local optima. Our\ncase study on the Rastrigin function demonstrates how our approach makes the\ncomplexities of ES optimization transparent. Our findings highlight the\npotential of using LLMs to bridge the gap between advanced optimization\nalgorithms and their interpretability.\n","authors":["Jill Baumann","Oliver Kramer"],"pdf_url":"https://arxiv.org/pdf/2407.08331v2.pdf","comment":"Accepted at ESANN 2024"},{"id":"http://arxiv.org/abs/2401.16458v2","updated":"2024-08-05T07:59:19Z","published":"2024-01-29T10:11:05Z","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending","summary":"  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n","authors":["Mario Sanz-Guerrero","Javier Arroyo"],"pdf_url":"https://arxiv.org/pdf/2401.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02290v1","updated":"2024-08-05T07:58:58Z","published":"2024-08-05T07:58:58Z","title":"Decoupled Vocabulary Learning Enables Zero-Shot Translation from Unseen\n  Languages","summary":"  Multilingual neural machine translation systems learn to map sentences of\ndifferent languages into a common representation space. Intuitively, with a\ngrowing number of seen languages the encoder sentence representation grows more\nflexible and easily adaptable to new languages. In this work, we test this\nhypothesis by zero-shot translating from unseen languages. To deal with unknown\nvocabularies from unknown languages we propose a setup where we decouple\nlearning of vocabulary and syntax, i.e. for each language we learn word\nrepresentations in a separate step (using cross-lingual word embeddings), and\nthen train to translate while keeping those word representations frozen. We\ndemonstrate that this setup enables zero-shot translation from entirely unseen\nlanguages. Zero-shot translating with a model trained on Germanic and Romance\nlanguages we achieve scores of 42.6 BLEU for Portuguese-English and 20.7 BLEU\nfor Russian-English on TED domain. We explore how this zero-shot translation\ncapability develops with varying number of languages seen by the encoder.\nLastly, we explore the effectiveness of our decoupled learning strategy for\nunsupervised machine translation. By exploiting our model's zero-shot\ntranslation capability for iterative back-translation we attain near parity\nwith a supervised setting.\n","authors":["Carlos Mullov","Ngoc-Quan Pham","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2408.02290v1.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2206.02963v3","updated":"2024-08-05T07:55:34Z","published":"2022-06-07T01:49:22Z","title":"Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding","summary":"  Knowledge Graph Embedding (KGE), which projects entities and relations into\ncontinuous vector spaces, has garnered significant attention. Although\nhigh-dimensional KGE methods offer better performance, they come at the expense\nof significant computation and memory overheads. Decreasing embedding\ndimensions significantly deteriorates model performance. While several recent\nefforts utilize knowledge distillation or non-Euclidean representation learning\nto augment the effectiveness of low-dimensional KGE, they either necessitate a\npre-trained high-dimensional teacher model or involve complex non-Euclidean\noperations, thereby incurring considerable additional computational costs. To\naddress this, this work proposes Confidence-aware Self-Knowledge Distillation\n(CSD) that learns from the model itself to enhance KGE in a low-dimensional\nspace. Specifically, CSD extracts knowledge from embeddings in previous\niterations, which would be utilized to supervise the learning of the model in\nthe next iterations. Moreover, a specific semantic module is developed to\nfilter reliable knowledge by estimating the confidence of previously learned\nembeddings. This straightforward strategy bypasses the need for time-consuming\npre-training of teacher models and can be integrated into various KGE methods\nto improve their performance. Our comprehensive experiments on six KGE\nbackbones and four datasets underscore the effectiveness of the proposed CSD.\n","authors":["Yichen Liu","Jiawei Chen","Defang Chen","Zhehui Zhou","Yan Feng","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2206.02963v3.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2408.02288v1","updated":"2024-08-05T07:54:01Z","published":"2024-08-05T07:54:01Z","title":"Spin glass model of in-context learning","summary":"  Large language models show a surprising in-context learning ability -- being\nable to use a prompt to form a prediction for a query, yet without additional\ntraining, in stark contrast to old-fashioned supervised learning. Providing a\nmechanistic interpretation and linking the empirical phenomenon to physics are\nthus challenging and remain unsolved. We study a simple yet expressive\ntransformer with linear attention, and map this structure to a spin glass model\nwith real-valued spins, where the couplings and fields explain the intrinsic\ndisorder in data. The spin glass model explains how the weight parameters\ninteract with each other during pre-training, and most importantly why an\nunseen function can be predicted by providing only a prompt yet without\ntraining. Our theory reveals that for single instance learning, increasing the\ntask diversity leads to the emergence of the in-context learning, by allowing\nthe Boltzmann distribution to converge to a unique correct solution of weight\nparameters. Therefore the pre-trained transformer displays a prediction power\nin a novel prompt setting. The proposed spin glass model thus establishes a\nfoundation to understand the empirical success of large language models.\n","authors":["Yuhao Li","Ruoran Bai","Haiping Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02288v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.05502v2","updated":"2024-08-05T07:22:58Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are\nplaying a pivotal role in information search and are being adopted globally.\nAlthough the multilingual capability of LLMs offers new opportunities to bridge\nthe language barrier, do these capabilities translate into real-life scenarios\nwhere linguistic divide and knowledge conflicts between multilingual sources\nare known occurrences? In this paper, we studied LLM's linguistic preference in\na RAG-based information search setting. We found that LLMs displayed systemic\nbias towards information in the same language as the query language in both\ninformation retrieval and answer generation. Furthermore, in scenarios where\nthere is little information in the language of the query, LLMs prefer documents\nin high-resource languages, reinforcing the dominant views. Such bias exists\nfor both factual and opinion-based queries. Our results highlight the\nlinguistic divide within multilingual LLMs in information search systems. The\nseemingly beneficial multilingual capability of LLMs may backfire on\ninformation parity by reinforcing language-specific information cocoons or\nfilter bubbles further marginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v4","updated":"2024-08-05T07:06:14Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02272v1","updated":"2024-08-05T07:00:10Z","published":"2024-08-05T07:00:10Z","title":"COM Kitchens: An Unedited Overhead-view Video Dataset as a\n  Vision-Language Benchmark","summary":"  Procedural video understanding is gaining attention in the vision and\nlanguage community. Deep learning-based video analysis requires extensive data.\nConsequently, existing works often use web videos as training resources, making\nit challenging to query instructional contents from raw video observations. To\naddress this issue, we propose a new dataset, COM Kitchens. The dataset\nconsists of unedited overhead-view videos captured by smartphones, in which\nparticipants performed food preparation based on given recipes. Fixed-viewpoint\nvideo datasets often lack environmental diversity due to high camera setup\ncosts. We used modern wide-angle smartphone lenses to cover cooking counters\nfrom sink to cooktop in an overhead view, capturing activity without in-person\nassistance. With this setup, we collected a diverse dataset by distributing\nsmartphones to participants. With this dataset, we propose the novel\nvideo-to-text retrieval task Online Recipe Retrieval (OnRR) and new video\ncaptioning domain Dense Video Captioning on unedited Overhead-View videos\n(DVC-OV). Our experiments verified the capabilities and limitations of current\nweb-video-based SOTA methods in handling these tasks.\n","authors":["Koki Maeda","Tosho Hirasawa","Atsushi Hashimoto","Jun Harashima","Leszek Rybicki","Yusuke Fukasawa","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2408.02272v1.pdf","comment":"ECCV2024 accepted"},{"id":"http://arxiv.org/abs/2408.02271v1","updated":"2024-08-05T06:59:56Z","published":"2024-08-05T06:59:56Z","title":"StyEmp: Stylizing Empathetic Response Generation via Multi-Grained\n  Prefix Encoder and Personality Reinforcement","summary":"  Recent approaches for empathetic response generation mainly focus on\nemotional resonance and user understanding, without considering the system's\npersonality. Consistent personality is evident in real human expression and is\nimportant for creating trustworthy systems. To address this problem, we propose\nStyEmp, which aims to stylize the empathetic response generation with a\nconsistent personality. Specifically, it incorporates a multi-grained prefix\nmechanism designed to capture the intricate relationship between a system's\npersonality and its empathetic expressions. Furthermore, we introduce a\npersonality reinforcement module that leverages contrastive learning to\ncalibrate the generation model, ensuring that responses are both empathetic and\nreflective of a distinct personality. Automatic and human evaluations on the\nEMPATHETICDIALOGUES benchmark show that StyEmp outperforms competitive\nbaselines in terms of both empathy and personality expressions.\n","authors":["Yahui Fu","Chenhui Chu","Tatsuya Kawahara"],"pdf_url":"https://arxiv.org/pdf/2408.02271v1.pdf","comment":"Accepted by the 25th Meeting of the Special Interest Group on\n  Discourse and Dialogue (SIGDIAL 2024)"},{"id":"http://arxiv.org/abs/2408.02257v1","updated":"2024-08-05T06:16:31Z","published":"2024-08-05T06:16:31Z","title":"To Aggregate or Not to Aggregate. That is the Question: A Case Study on\n  Annotation Subjectivity in Span Prediction","summary":"  This paper explores the task of automatic prediction of text spans in a legal\nproblem description that support a legal area label. We use a corpus of problem\ndescriptions written by laypeople in English that is annotated by practising\nlawyers. Inherent subjectivity exists in our task because legal area\ncategorisation is a complex task, and lawyers often have different views on a\nproblem, especially in the face of legally-imprecise descriptions of issues.\nExperiments show that training on majority-voted spans outperforms training on\ndisaggregated ones.\n","authors":["Kemal Kurniawan","Meladel Mistica","Timothy Baldwin","Jey Han Lau"],"pdf_url":"https://arxiv.org/pdf/2408.02257v1.pdf","comment":"Accepted at WASSA 2024"},{"id":"http://arxiv.org/abs/2408.02253v1","updated":"2024-08-05T05:56:37Z","published":"2024-08-05T05:56:37Z","title":"Advancing Post-OCR Correction: A Comparative Study of Synthetic Data","summary":"  This paper explores the application of synthetic data in the post-OCR domain\non multiple fronts by conducting experiments to assess the impact of data\nvolume, augmentation, and synthetic data generation methods on model\nperformance. Furthermore, we introduce a novel algorithm that leverages\ncomputer vision feature detection algorithms to calculate glyph similarity for\nconstructing post-OCR synthetic data. Through experiments conducted across a\nvariety of languages, including several low-resource ones, we demonstrate that\nmodels like ByT5 can significantly reduce Character Error Rates (CER) without\nthe need for manually annotated data, and our proposed synthetic data\ngeneration method shows advantages over traditional methods, particularly in\nlow-resource languages.\n","authors":["Shuhao Guan","Derek Greene"],"pdf_url":"https://arxiv.org/pdf/2408.02253v1.pdf","comment":"ACL 2024 findings"},{"id":"http://arxiv.org/abs/2408.02248v1","updated":"2024-08-05T05:43:23Z","published":"2024-08-05T05:43:23Z","title":"ReDel: A Toolkit for LLM-Powered Recursive Multi-Agent Systems","summary":"  Recently, there has been increasing interest in using Large Language Models\n(LLMs) to construct complex multi-agent systems to perform tasks such as\ncompiling literature reviews, drafting consumer reports, and planning\nvacations. Many tools and libraries exist for helping create such systems,\nhowever none support recursive multi-agent systems -- where the models\nthemselves flexibly decide when to delegate tasks and how to organize their\ndelegation structure. In this work, we introduce ReDel: a toolkit for recursive\nmulti-agent systems that supports custom tool-use, delegation schemes,\nevent-based logging, and interactive replay in an easy-to-use web interface. We\nshow that, using ReDel, we are able to achieve significant performance gains on\nagentic benchmarks and easily identify potential areas of improvements through\nthe visualization and debugging tools. Our code, documentation, and PyPI\npackage are open-source and free to use under the MIT license.\n","authors":["Andrew Zhu","Liam Dugan","Chris Callison-Burch"],"pdf_url":"https://arxiv.org/pdf/2408.02248v1.pdf","comment":"In submission to EMNLP 2024 (Demo Track)"},{"id":"http://arxiv.org/abs/2408.02239v1","updated":"2024-08-05T05:15:17Z","published":"2024-08-05T05:15:17Z","title":"BOTS-LM: Training Large Language Models for Setswana","summary":"  In this work we present BOTS-LM, a series of bilingual language models\nproficient in both Setswana and English. Leveraging recent advancements in data\navailability and efficient fine-tuning, BOTS-LM achieves performance similar to\nmodels significantly larger than itself while maintaining computational\nefficiency. Our initial release features an 8 billion parameter generative\nlarge language model, with upcoming 0.5 billion and 1 billion parameter large\nlanguage models and a 278 million parameter encoder-only model soon to be\nreleased. We find the 8 billion parameter model significantly outperforms\nLlama-3-70B and Aya 23 on English-Setswana translation tasks, approaching the\nperformance of dedicated machine translation models, while approaching 70B\nparameter performance on Setswana reasoning as measured by a machine translated\nsubset of the MMLU benchmark. To accompany the BOTS-LM series of language\nmodels, we release the largest Setswana web dataset, SetsText, totalling over\n267 million tokens. In addition, we release the largest machine translated\nSetswana dataset, the first and largest synthetic Setswana dataset, training\nand evaluation code, training logs, and MMLU-tsn, a machine translated subset\nof MMLU.\n","authors":["Nathan Brown","Vukosi Marivate"],"pdf_url":"https://arxiv.org/pdf/2408.02239v1.pdf","comment":"7 pages, 3 tables"},{"id":"http://arxiv.org/abs/2408.02237v1","updated":"2024-08-05T05:09:23Z","published":"2024-08-05T05:09:23Z","title":"Do Large Language Models Speak All Languages Equally? A Comparative\n  Study in Low-Resource Settings","summary":"  Large language models (LLMs) have garnered significant interest in natural\nlanguage processing (NLP), particularly their remarkable performance in various\ndownstream tasks in resource-rich languages. Recent studies have highlighted\nthe limitations of LLMs in low-resource languages, primarily focusing on binary\nclassification tasks and giving minimal attention to South Asian languages.\nThese limitations are primarily attributed to constraints such as dataset\nscarcity, computational costs, and research gaps specific to low-resource\nlanguages. To address this gap, we present datasets for sentiment and hate\nspeech tasks by translating from English to Bangla, Hindi, and Urdu,\nfacilitating research in low-resource language processing. Further, we\ncomprehensively examine zero-shot learning using multiple LLMs in English and\nwidely spoken South Asian languages. Our findings indicate that GPT-4\nconsistently outperforms Llama 2 and Gemini, with English consistently\ndemonstrating superior performance across diverse tasks compared to\nlow-resource languages. Furthermore, our analysis reveals that natural language\ninference (NLI) exhibits the highest performance among the evaluated tasks,\nwith GPT-4 demonstrating superior capabilities.\n","authors":["Md. Arid Hasan","Prerona Tarannum","Krishno Dey","Imran Razzak","Usman Naseem"],"pdf_url":"https://arxiv.org/pdf/2408.02237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02233v1","updated":"2024-08-05T04:53:17Z","published":"2024-08-05T04:53:17Z","title":"A Multi-Source Heterogeneous Knowledge Injected Prompt Learning Method\n  for Legal Charge Prediction","summary":"  Legal charge prediction, an essential task in legal AI, seeks to assign\naccurate charge labels to case descriptions, attracting significant recent\ninterest. Existing methods primarily employ diverse neural network structures\nfor modeling case descriptions directly, failing to effectively leverage\nmulti-source external knowledge. We propose a prompt learning framework-based\nmethod that simultaneously leverages multi-source heterogeneous external\nknowledge from a legal knowledge base, a conversational LLM, and related legal\narticles. Specifically, we match knowledge snippets in case descriptions via\nthe legal knowledge base and encapsulate them into the input through a hard\nprompt template. Additionally, we retrieve legal articles related to a given\ncase description through contrastive learning, and then obtain factual elements\nwithin the case description through a conversational LLM. We fuse the embedding\nvectors of soft prompt tokens with the encoding vector of factual elements to\nachieve knowledge-enhanced model forward inference. Experimental results show\nthat our method achieved state-of-the-art results on CAIL-2018, the largest\nlegal charge prediction dataset, and our method has lower data dependency. Case\nstudies also demonstrate our method's strong interpretability.\n","authors":["Jingyun Sun","Chi Wei","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2408.02233v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2305.10276v7","updated":"2024-08-05T04:52:16Z","published":"2023-05-17T15:07:50Z","title":"Chain-of-Symbol Prompting Elicits Planning in Large Langauge Models","summary":"  In this paper, we take the initiative to investigate the performance of LLMs\non complex planning tasks that require LLMs to understand a virtual spatial\nenvironment simulated via natural language and act correspondingly in text. We\npropose a benchmark named Natural Language Planning and Action (Natala)\ncomposed of a set of novel tasks: Brick World, NLVR-based Manipulations, and\nNatural Language Navigation. We found that current popular LLMs such as ChatGPT\nstill lack abilities in complex planning. This arises a question -- do the LLMs\nhave a good understanding of the environments described in natural language, or\nmaybe other alternatives such as symbolic representations are neater and hence\nbetter to be understood by LLMs? To this end, we propose a novel method called\nCoS (Chain-of-Symbol Prompting) that represents the complex environments with\ncondensed symbolic spatial representations during the chained intermediate\nthinking steps. CoS is easy to use and does not need additional training on\nLLMs. Extensive experiments indicate that CoS clearly surpasses the performance\nof the Chain-of-Thought (CoT) Prompting in all three planning tasks with even\nfewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.\nThe performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)\non Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt\nobviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate\nsteps from demonstrations on Brick World. Code and data available at:\nhttps://github.com/hanxuhu/chain-of-symbol-planning\n","authors":["Hanxu Hu","Hongyuan Lu","Huajian Zhang","Yun-Ze Song","Wai Lam","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2305.10276v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13184v3","updated":"2024-08-05T04:45:55Z","published":"2024-02-20T17:49:46Z","title":"What if LLMs Have Different World Views: Simulating Alien Civilizations\n  with LLM-based Agents","summary":"  In this study, we introduce \"CosmoAgent,\" an innovative artificial\nintelligence framework utilizing Large Language Models (LLMs) to simulate\ncomplex interactions between human and extraterrestrial civilizations, with a\nspecial emphasis on Stephen Hawking's cautionary advice about not sending radio\nsignals haphazardly into the universe. The goal is to assess the feasibility of\npeaceful coexistence while considering potential risks that could threaten\nwell-intentioned civilizations. Employing mathematical models and state\ntransition matrices, our approach quantitatively evaluates the development\ntrajectories of civilizations, offering insights into future decision-making at\ncritical points of growth and saturation. Furthermore, the paper acknowledges\nthe vast diversity in potential living conditions across the universe, which\ncould foster unique cosmologies, ethical codes, and worldviews among various\ncivilizations. Recognizing the Earth-centric bias inherent in current LLM\ndesigns, we propose the novel concept of using LLMs with diverse ethical\nparadigms and simulating interactions between entities with distinct moral\nprinciples. This innovative research provides a new way to understand complex\ninter-civilizational dynamics, expanding our perspective while pioneering novel\nstrategies for conflict resolution, which are crucial for preventing\ninterstellar conflicts. We have also released the code and datasets to enable\nfurther academic investigation into this interesting area of research. The code\nis available at\nhttps://github.com/MingyuJ666/Simulating-Alien-Civilizations-with-LLM-based-Agents.\n","authors":["Mingyu Jin","Beichen Wang","Zhaoqian Xue","Suiyuan Zhu","Wenyue Hua","Hua Tang","Kai Mei","Mengnan Du","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.13184v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.13028v2","updated":"2024-08-05T03:47:59Z","published":"2023-10-19T07:39:07Z","title":"Reliable Academic Conference Question Answering: A Study Based on Large\n  Language Model","summary":"  As the development of academic conferences fosters global scholarly\ncommunication, researchers consistently need to obtain accurate and up-to-date\ninformation about academic conferences. Since the information is scattered,\nusing an intelligent question-answering system to efficiently handle\nresearchers' queries and ensure awareness of the latest advancements is\nnecessary. Recently, Large Language Models (LLMs) have demonstrated impressive\ncapabilities in question answering, and have been enhanced by retrieving\nexternal knowledge to deal with outdated knowledge. However, these methods fail\nto work due to the lack of the latest conference knowledge. To address this\nchallenge, we develop the ConferenceQA dataset, consisting of seven diverse\nacademic conferences. Specifically, for each conference, we first organize\nacademic conference data in a tree-structured format through a semi-automated\nmethod. Then we annotate question-answer pairs and classify the pairs into four\ndifferent types to better distinguish their difficulty. With the constructed\ndataset, we further propose a novel method STAR (STructure-Aware Retrieval) to\nimprove the question-answering abilities of LLMs, leveraging inherent\nstructural information during the retrieval process. Experimental results on\nthe ConferenceQA dataset show the effectiveness of our retrieval method. The\ndataset and code are available at https://github.com/zjukg/ConferenceQA.\n","authors":["Zhiwei Huang","Juan Li","Long Jin","Junjie Wang","Mingchen Tu","Yin Hua","Zhiqiang Liu","Jiawei Meng","Wen Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.13028v2.pdf","comment":"12 pages, 3 figures"},{"id":"http://arxiv.org/abs/2401.05631v4","updated":"2024-08-05T03:46:34Z","published":"2024-01-11T03:02:17Z","title":"DrawTalking: Building Interactive Worlds by Sketching and Speaking","summary":"  We introduce DrawTalking, an approach to building and controlling interactive\nworlds by sketching and speaking while telling stories. It emphasizes user\ncontrol and flexibility, and gives programming-like capability without\nrequiring code. An early open-ended study with our prototype shows that the\nmechanics resonate and are applicable to many creative-exploratory use cases,\nwith the potential to inspire and inform research in future natural interfaces\nfor creative exploration and authoring.\n","authors":["Karl Toby Rosenberg","Rubaiat Habib Kazi","Li-Yi Wei","Haijun Xia","Ken Perlin"],"pdf_url":"https://arxiv.org/pdf/2401.05631v4.pdf","comment":"25 pages, 27 figures; Matching version accepted at UIST 2024"},{"id":"http://arxiv.org/abs/2401.03472v2","updated":"2024-08-05T03:24:18Z","published":"2024-01-07T12:48:07Z","title":"PEneo: Unifying Line Extraction, Line Grouping, and Entity Linking for\n  End-to-end Document Pair Extraction","summary":"  Document pair extraction aims to identify key and value entities as well as\ntheir relationships from visually-rich documents. Most existing methods divide\nit into two separate tasks: semantic entity recognition (SER) and relation\nextraction (RE). However, simply concatenating SER and RE serially can lead to\nsevere error propagation, and it fails to handle cases like multi-line entities\nin real scenarios. To address these issues, this paper introduces a novel\nframework, PEneo (Pair Extraction new decoder option), which performs document\npair extraction in a unified pipeline, incorporating three concurrent\nsub-tasks: line extraction, line grouping, and entity linking. This approach\nalleviates the error accumulation problem and can handle the case of multi-line\nentities. Furthermore, to better evaluate the model's performance and to\nfacilitate future research on pair extraction, we introduce RFUND, a\nre-annotated version of the commonly used FUNSD and XFUND datasets, to make\nthem more accurate and cover realistic situations. Experiments on various\nbenchmarks demonstrate PEneo's superiority over previous pipelines, boosting\nthe performance by a large margin (e.g., 19.89%-22.91% F1 score on RFUND-EN)\nwhen combined with various backbones like LiLT and LayoutLMv3, showing its\neffectiveness and generality. Codes and the new annotations are available at\n\\href{https://github.com/ZeningLin/PEneo}{https://github.com/ZeningLin/PEneo}.\n","authors":["Zening Lin","Jiapeng Wang","Teng Li","Wenhui Liao","Dayi Huang","Longfei Xiong","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2401.03472v2.pdf","comment":"ACMMM 2024 Poster"},{"id":"http://arxiv.org/abs/2408.02201v1","updated":"2024-08-05T03:05:02Z","published":"2024-08-05T03:05:02Z","title":"Evaluating the Performance of Large Language Models for SDG Mapping\n  (Technical Report)","summary":"  The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.\n","authors":["Hui Yin","Amir Aryani","Nakul Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.02201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19220v4","updated":"2024-08-05T03:02:06Z","published":"2024-05-29T16:00:46Z","title":"WRDScore: New Metric for Evaluation of Natural Language Generation\n  Models","summary":"  Evaluating natural language generation models, particularly for method name\nprediction, poses significant challenges. A robust metric must account for the\nversatility of method naming, considering both semantic and syntactic\nvariations. Traditional overlap-based metrics fail to capture these nuances.\nExisting embedding-based metrics often suffer from imbalanced precision and\nrecall, lack normalized scores, or make unrealistic assumptions about\nsequences. To address these limitations, we propose WRDScore, a novel metric\nthat strikes a balance between simplicity and effectiveness. Our metric is\nlightweight, normalized, and precision-recall-oriented, avoiding unrealistic\nassumptions while aligning well with human judgments.\n","authors":["Ravil Mussabayev"],"pdf_url":"https://arxiv.org/pdf/2405.19220v4.pdf","comment":"Accepted to IEEE Xplore"},{"id":"http://arxiv.org/abs/2408.02193v1","updated":"2024-08-05T02:38:48Z","published":"2024-08-05T02:38:48Z","title":"CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs","summary":"  Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n","authors":["Weijie Lv","Xuan Xia","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00992v2","updated":"2024-08-05T02:09:58Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.05821v2","updated":"2024-08-05T00:51:59Z","published":"2023-05-10T00:33:08Z","title":"Context-dependent communication under environmental constraints","summary":"  There is significant evidence that real-world communication cannot be reduced\nto sending signals with context-independent meaning. In this work, based on a\nvariant of the classical Lewis (1969) signaling model, we explore the\nconditions for the emergence of context-dependent communication in a situated\nscenario. In particular, we demonstrate that pressure to minimise the\nvocabulary size is sufficient for such emergence. At the same time, we study\nthe environmental conditions and cognitive capabilities that enable contextual\ndisambiguation of symbol meanings. We show that environmental constraints on\nthe receiver's referent choice can be unilaterally exploited by the sender,\nwithout disambiguation capabilities on the receiver's end. Consistent with\ncommon assumptions, the sender's awareness of the context appears to be\nrequired for contextual communication. We suggest that context-dependent\ncommunication is a situated multilayered phenomenon, crucially influenced by\nenvironment properties such as distribution of contexts. The model developed in\nthis work is a demonstration of how signals may be ambiguous out of context,\nbut still allow for near-perfect communication accuracy.\n","authors":["Krzysztof Gwka","Julian Zubek","Joanna Rczaszek-Leonardi"],"pdf_url":"https://arxiv.org/pdf/2305.05821v2.pdf","comment":"14 pages, submitted to Cognitive Systems Research"},{"id":"http://arxiv.org/abs/2408.02865v1","updated":"2024-08-05T23:31:07Z","published":"2024-08-05T23:31:07Z","title":"VisionUnite: A Vision-Language Foundation Model for Ophthalmology\n  Enhanced with Clinical Knowledge","summary":"  The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.\n","authors":["Zihan Li","Diping Song","Zefeng Yang","Deming Wang","Fei Li","Xiulan Zhang","Paul E. Kinahan","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.02865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02861v1","updated":"2024-08-05T23:20:32Z","published":"2024-08-05T23:20:32Z","title":"A Framework for Fine-Tuning LLMs using Heterogeneous Feedback","summary":"  Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.\n","authors":["Ryan Aponte","Ryan A. Rossi","Shunan Guo","Franck Dernoncourt","Tong Yu","Xiang Chen","Subrata Mitra","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2408.02861v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2406.12038v2","updated":"2024-08-05T21:48:22Z","published":"2024-06-17T19:11:40Z","title":"Soft Prompting for Unlearning in Large Language Models","summary":"  The widespread popularity of Large Language Models (LLMs), partly due to\ntheir unique ability to perform in-context learning, has also brought to light\nthe importance of ethical and safety considerations when deploying these\npre-trained models. In this work, we focus on investigating machine unlearning\nfor LLMs motivated by data protection regulations. In contrast to the growing\nliterature on fine-tuning methods to achieve unlearning, we focus on a\ncomparatively lightweight alternative called soft prompting to realize the\nunlearning of a subset of training data. With losses designed to enforce\nforgetting as well as utility preservation, our framework \\textbf{S}oft\n\\textbf{P}rompting for \\textbf{U}n\\textbf{l}earning (SPUL) learns prompt tokens\nthat can be appended to an arbitrary query to induce unlearning of specific\nexamples at inference time without updating LLM parameters. We conduct a\nrigorous evaluation of the proposed method and our results indicate that SPUL\ncan significantly improve the trade-off between utility and forgetting in the\ncontext of text classification and question answering with LLMs. We further\nvalidate our method using multiple LLMs to highlight the scalability of our\nframework and provide detailed insights into the choice of hyperparameters and\nthe influence of the size of unlearning data. Our implementation is available\nat \\url{https://github.com/karuna-bhaila/llm_unlearning}.\n","authors":["Karuna Bhaila","Minh-Hao Van","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02838v1","updated":"2024-08-05T21:22:36Z","published":"2024-08-05T21:22:36Z","title":"Interpretation of the Intent Detection Problem as Dynamics in a\n  Low-dimensional Space","summary":"  Intent detection is a text classification task whose aim is to recognize and\nlabel the semantics behind a users query. It plays a critical role in various\nbusiness applications. The output of the intent detection module strongly\nconditions the behavior of the whole system. This sequence analysis task is\nmainly tackled using deep learning techniques. Despite the widespread use of\nthese techniques, the internal mechanisms used by networks to solve the problem\nare poorly understood. Recent lines of work have analyzed the computational\nmechanisms learned by RNNs from a dynamical systems perspective. In this work,\nwe investigate how different RNN architectures solve the SNIPS intent detection\nproblem. Sentences injected into trained networks can be interpreted as\ntrajectories traversing a hidden state space. This space is constrained to a\nlow-dimensional manifold whose dimensionality is related to the embedding and\nhidden layer sizes. To generate predictions, RNN steers the trajectories\ntowards concrete regions, spatially aligned with the output layer matrix rows\ndirections. Underlying the system dynamics, an unexpected fixed point topology\nhas been identified with a limited number of attractors. Our results provide\nnew insights into the inner workings of networks that solve the intent\ndetection task.\n","authors":["Eduardo Sanchez-Karhunen","Jose F. Quesada-Moreno","Miguel A. Gutirrez-Naranjo"],"pdf_url":"https://arxiv.org/pdf/2408.02838v1.pdf","comment":"Camera-Ready version. Accepted paper at 27th European Conference on\n  Artificial Intelligence (ECAI-2024)"},{"id":"http://arxiv.org/abs/2404.03745v2","updated":"2024-08-05T21:05:08Z","published":"2024-04-04T18:34:32Z","title":"Fakes of Varying Shades: How Warning Affects Human Perception and\n  Engagement Regarding LLM Hallucinations","summary":"  The widespread adoption and transformative effects of large language models\n(LLMs) have sparked concerns regarding their capacity to produce inaccurate and\nfictitious content, referred to as `hallucinations'. Given the potential risks\nassociated with hallucinations, humans should be able to identify them. This\nresearch aims to understand the human perception of LLM hallucinations by\nsystematically varying the degree of hallucination (genuine, minor\nhallucination, major hallucination) and examining its interaction with warning\n(i.e., a warning of potential inaccuracies: absent vs. present). Participants\n(N=419) from Prolific rated the perceived accuracy and engaged with content\n(e.g., like, dislike, share) in a Q/A format. Participants ranked content as\ntruthful in the order of genuine, minor hallucination, and major hallucination,\nand user engagement behaviors mirrored this pattern. More importantly, we\nobserved that warning improved the detection of hallucination without\nsignificantly affecting the perceived truthfulness of genuine content. We\nconclude by offering insights for future tools to aid human detection of\nhallucinations. All survey materials, demographic questions, and post-session\nquestions are available at:\nhttps://github.com/MahjabinNahar/fakes-of-varying-shades-survey-materials\n","authors":["Mahjabin Nahar","Haeseung Seo","Eun-Ju Lee","Aiping Xiong","Dongwon Lee"],"pdf_url":"https://arxiv.org/pdf/2404.03745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2211.04428v2","updated":"2024-08-05T20:24:09Z","published":"2022-11-08T18:14:09Z","title":"Review of coreference resolution in English and Persian","summary":"  Coreference resolution (CR), identifying expressions referring to the same\nreal-world entity, is a fundamental challenge in natural language processing\n(NLP). This paper explores the latest advancements in CR, spanning coreference\nand anaphora resolution. We critically analyze the diverse corpora that have\nfueled CR research, highlighting their strengths, limitations, and suitability\nfor various tasks. We examine the spectrum of evaluation metrics used to assess\nCR systems, emphasizing their advantages, disadvantages, and the need for more\nnuanced, task-specific metrics. Tracing the evolution of CR algorithms, we\nprovide a detailed overview of methodologies, from rule-based approaches to\ncutting-edge deep learning architectures. We delve into mention-pair,\nentity-based, cluster-ranking, sequence-to-sequence, and graph neural network\nmodels, elucidating their theoretical foundations and performance on benchmark\ndatasets. Recognizing the unique challenges of Persian CR, we dedicate a\nfocused analysis to this under-resourced language. We examine existing Persian\nCR systems and highlight the emergence of end-to-end neural models leveraging\npre-trained language models like ParsBERT. This review is an essential resource\nfor researchers and practitioners, offering a comprehensive overview of the\ncurrent state-of-the-art in CR, identifying key challenges, and charting a\ncourse for future research in this rapidly evolving field.\n","authors":["Hassan Haji Mohammadi","Alireza Talebpour","Ahmad Mahmoudi Aznaveh","Samaneh Yazdani"],"pdf_url":"https://arxiv.org/pdf/2211.04428v2.pdf","comment":"44 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02798v1","updated":"2024-08-05T19:28:58Z","published":"2024-08-05T19:28:58Z","title":"Examining Gender and Power on Wikipedia Through Face and Politeness","summary":"  We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power.\n","authors":["Adil Soubki","Shyne Choi","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2408.02798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02795v1","updated":"2024-08-05T19:23:20Z","published":"2024-08-05T19:23:20Z","title":"Entity Retrieval for Answering Entity-Centric Questions","summary":"  The similarity between the question and indexed documents is a crucial factor\nin document retrieval for retrieval-augmented question answering. Although this\nis typically the only method for obtaining the relevant documents, it is not\nthe sole approach when dealing with entity-centric questions. In this study, we\npropose Entity Retrieval, a novel retrieval method which rather than relying on\nquestion-document similarity, depends on the salient entities within the\nquestion to identify the retrieval documents. We conduct an in-depth analysis\nof the performance of both dense and sparse retrieval methods in comparison to\nEntity Retrieval. Our findings reveal that our method not only leads to more\naccurate answers to entity-centric questions but also operates more\nefficiently.\n","authors":["Hassan S. Shavarani","Anoop Sarkar"],"pdf_url":"https://arxiv.org/pdf/2408.02795v1.pdf","comment":"17 pages total, 10 Tables, 4 Figures"},{"id":"http://arxiv.org/abs/2408.02784v1","updated":"2024-08-05T19:00:43Z","published":"2024-08-05T19:00:43Z","title":"LLM economicus? Mapping the Behavioral Biases of LLMs via Utility Theory","summary":"  Humans are not homo economicus (i.e., rational economic beings). As humans,\nwe exhibit systematic behavioral biases such as loss aversion, anchoring,\nframing, etc., which lead us to make suboptimal economic decisions. Insofar as\nsuch biases may be embedded in text data on which large language models (LLMs)\nare trained, to what extent are LLMs prone to the same behavioral biases?\nUnderstanding these biases in LLMs is crucial for deploying LLMs to support\nhuman decision-making. We propose utility theory-a paradigm at the core of\nmodern economic theory-as an approach to evaluate the economic biases of LLMs.\nUtility theory enables the quantification and comparison of economic behavior\nagainst benchmarks such as perfect rationality or human behavior. To\ndemonstrate our approach, we quantify and compare the economic behavior of a\nvariety of open- and closed-source LLMs. We find that the economic behavior of\ncurrent LLMs is neither entirely human-like nor entirely economicus-like. We\nalso find that most current LLMs struggle to maintain consistent economic\nbehavior across settings. Finally, we illustrate how our approach can measure\nthe effect of interventions such as prompting on economic biases.\n","authors":["Jillian Ross","Yoon Kim","Andrew W. Lo"],"pdf_url":"https://arxiv.org/pdf/2408.02784v1.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.03350v1","updated":"2024-08-05T20:19:18Z","published":"2024-08-05T20:19:18Z","title":"miniCTX: Neural Theorem Proving with (Long-)Contexts","summary":"  We introduce miniCTX, which tests a model's ability to prove formal\nmathematical theorems that depend on new definitions, lemmas, or other\ncontextual information that was not observed during training. miniCTX contains\ntheorems sourced from real Lean projects and textbooks, each associated with a\ncontext that can span tens of thousands of tokens. Models are tasked with\nproving a theorem given access to code from the theorem's repository, which\ncontains context that is helpful or needed for the proof. As a baseline for\nminiCTX, we introduce file-tuning, a simple recipe that trains a model to\ngenerate a proof step conditioned on the preceding file contents. File-tuning\nsubstantially outperforms the traditional neural theorem proving approach that\nfine-tunes on states alone. Additionally, our file-tuned model improves\nperformance on the standard miniF2F benchmark, achieving a pass rate of 33.61%,\nwhich is a new state-of-the-art for 1.3B parameter models. Alongside miniCTX,\nwe offer ntp-toolkit for automatically extracting and annotating theorem\nproving data, making it easy to add new projects into miniCTX to ensure that\ncontexts are not seen during training. miniCTX offers a challenging and\nrealistic perspective on evaluating neural theorem provers.\n","authors":["Jiewen Hu","Thomas Zhu","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2408.03350v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.02672v1","updated":"2024-08-05T17:59:51Z","published":"2024-08-05T17:59:51Z","title":"Latent-INR: A Flexible Framework for Implicit Representations of Videos\n  with Discriminative Semantics","summary":"  Implicit Neural Networks (INRs) have emerged as powerful representations to\nencode all forms of data, including images, videos, audios, and scenes. With\nvideo, many INRs for video have been proposed for the compression task, and\nrecent methods feature significant improvements with respect to encoding time,\nstorage, and reconstruction quality. However, these encoded representations\nlack semantic meaning, so they cannot be used for any downstream tasks that\nrequire such properties, such as retrieval. This can act as a barrier for\nadoption of video INRs over traditional codecs as they do not offer any\nsignificant edge apart from compression. To alleviate this, we propose a\nflexible framework that decouples the spatial and temporal aspects of the video\nINR. We accomplish this with a dictionary of per-frame latents that are learned\njointly with a set of video specific hypernetworks, such that given a latent,\nthese hypernetworks can predict the INR weights to reconstruct the given frame.\nThis framework not only retains the compression efficiency, but the learned\nlatents can be aligned with features from large vision models, which grants\nthem discriminative properties. We align these latents with CLIP and show good\nperformance for both compression and video retrieval tasks. By aligning with\nVideoLlama, we are able to perform open-ended chat with our learned latents as\nthe visual inputs. Additionally, the learned latents serve as a proxy for the\nunderlying weights, allowing us perform tasks like video interpolation. These\nsemantic properties and applications, existing simultaneously with ability to\nperform compression, interpolation, and superresolution properties, are a first\nin this field of work.\n","authors":["Shishira R Maiya","Anubhav Gupta","Matthew Gwilliam","Max Ehrlich","Abhinav Shrivastava"],"pdf_url":"https://arxiv.org/pdf/2408.02672v1.pdf","comment":"equal contribution for first two authors; accepted to ECCV2024; 14\n  pages, 4 tables, 10 figures in main paper, supplementary after bibliography"},{"id":"http://arxiv.org/abs/2405.04634v3","updated":"2024-08-05T17:53:28Z","published":"2024-05-07T19:37:22Z","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes","summary":"  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n","authors":["Charles Gaydon","Michel Daab","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2405.04634v3.pdf","comment":"15 pages | 9 figures | 8 tables | Dataset is available at\n  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at\n  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning\n  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data\n  engineering code repository is on Github at https://github.com/IGNF/pacasam"},{"id":"http://arxiv.org/abs/2407.11913v2","updated":"2024-08-05T17:50:03Z","published":"2024-07-16T17:05:20Z","title":"Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data","summary":"  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n","authors":["Tim Elsner","Paula Usinger","Victor Czech","Gregor Kobsik","Yanjiang He","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2407.11913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02657v1","updated":"2024-08-05T17:46:53Z","published":"2024-08-05T17:46:53Z","title":"Lumina-mGPT: Illuminate Flexible Photorealistic Text-to-Image Generation\n  with Multimodal Generative Pretraining","summary":"  We present Lumina-mGPT, a family of multimodal autoregressive models capable\nof various vision and language tasks, particularly excelling in generating\nflexible photorealistic images from text descriptions. Unlike existing\nautoregressive image generation approaches, Lumina-mGPT employs a pretrained\ndecoder-only transformer as a unified framework for modeling multimodal token\nsequences. Our key insight is that a simple decoder-only transformer with\nmultimodal Generative PreTraining (mGPT), utilizing the next-token prediction\nobjective on massive interleaved text-image sequences, can learn broad and\ngeneral multimodal capabilities, thereby illuminating photorealistic\ntext-to-image generation. Building on these pretrained models, we propose\nFlexible Progressive Supervised Finetuning (FP-SFT) on high-quality image-text\npairs to fully unlock their potential for high-aesthetic image synthesis at any\nresolution while maintaining their general multimodal capabilities.\nFurthermore, we introduce Ominiponent Supervised Finetuning (Omni-SFT),\ntransforming Lumina-mGPT into a foundation model that seamlessly achieves\nomnipotent task unification. The resulting model demonstrates versatile\nmultimodal capabilities, including visual generation tasks like flexible\ntext-to-image generation and controllable generation, visual recognition tasks\nlike segmentation and depth estimation, and vision-language tasks like\nmultiturn visual question answering. Additionally, we analyze the differences\nand similarities between diffusion-based and autoregressive methods in a direct\ncomparison.\n","authors":["Dongyang Liu","Shitian Zhao","Le Zhuo","Weifeng Lin","Yu Qiao","Hongsheng Li","Peng Gao"],"pdf_url":"https://arxiv.org/pdf/2408.02657v1.pdf","comment":"Code available at: https://github.com/Alpha-VLLM/Lumina-mGPT"},{"id":"http://arxiv.org/abs/2408.02654v1","updated":"2024-08-05T17:33:09Z","published":"2024-08-05T17:33:09Z","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight\n  Initialization","summary":"  The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.\n","authors":["Andriy Miranskyy","Adam Sorrenti","Viral Thakar"],"pdf_url":"https://arxiv.org/pdf/2408.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02635v1","updated":"2024-08-05T16:58:56Z","published":"2024-08-05T16:58:56Z","title":"Interactive 3D Medical Image Segmentation with SAM 2","summary":"  Interactive medical image segmentation (IMIS) has shown significant potential\nin enhancing segmentation accuracy by integrating iterative feedback from\nmedical professionals. However, the limited availability of enough 3D medical\ndata restricts the generalization and robustness of most IMIS methods. The\nSegment Anything Model (SAM), though effective for 2D images, requires\nexpensive semi-auto slice-by-slice annotations for 3D medical images. In this\npaper, we explore the zero-shot capabilities of SAM 2, the next-generation Meta\nSAM model trained on videos, for 3D medical image segmentation. By treating\nsequential 2D slices of 3D images as video frames, SAM 2 can fully\nautomatically propagate annotations from a single frame to the entire 3D\nvolume. We propose a practical pipeline for using SAM 2 in 3D medical image\nsegmentation and present key findings highlighting its efficiency and potential\nfor further optimization. Concretely, numerical experiments on the BraTS2020\nand the medical segmentation decathlon datasets demonstrate that SAM 2 still\nhas a gap with supervised methods but can narrow the gap in specific settings\nand organ types, significantly reducing the annotation burden on medical\nprofessionals. Our code will be open-sourced and available at\nhttps://github.com/Chuyun-Shen/SAM_2_Medical_3D.\n","authors":["Chuyun Shen","Wenhao Li","Yuhang Shi","Xiangfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02635v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02629v1","updated":"2024-08-05T16:53:23Z","published":"2024-08-05T16:53:23Z","title":"VidGen-1M: A Large-Scale Dataset for Text-to-video Generation","summary":"  The quality of video-text pairs fundamentally determines the upper bound of\ntext-to-video models. Currently, the datasets used for training these models\nsuffer from significant shortcomings, including low temporal consistency,\npoor-quality captions, substandard video quality, and imbalanced data\ndistribution. The prevailing video curation process, which depends on image\nmodels for tagging and manual rule-based curation, leads to a high\ncomputational load and leaves behind unclean data. As a result, there is a lack\nof appropriate training datasets for text-to-video models. To address this\nproblem, we present VidGen-1M, a superior training dataset for text-to-video\nmodels. Produced through a coarse-to-fine curation strategy, this dataset\nguarantees high-quality videos and detailed captions with excellent temporal\nconsistency. When used to train the video generation model, this dataset has\nled to experimental results that surpass those obtained with other models.\n","authors":["Zhiyu Tan","Xiaomeng Yang","Luozheng Qin","Hao Li"],"pdf_url":"https://arxiv.org/pdf/2408.02629v1.pdf","comment":"project page: https://sais-fuxi.github.io/projects/vidgen-1m"},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2408.02623v1","updated":"2024-08-05T16:48:03Z","published":"2024-08-05T16:48:03Z","title":"YOWOv3: An Efficient and Generalized Framework for Human Action\n  Detection and Recognition","summary":"  In this paper, we propose a new framework called YOWOv3, which is an improved\nversion of YOWOv2, designed specifically for the task of Human Action Detection\nand Recognition. This framework is designed to facilitate extensive\nexperimentation with different configurations and supports easy customization\nof various components within the model, reducing efforts required for\nunderstanding and modifying the code. YOWOv3 demonstrates its superior\nperformance compared to YOWOv2 on two widely used datasets for Human Action\nDetection and Recognition: UCF101-24 and AVAv2.2. Specifically, the predecessor\nmodel YOWOv2 achieves an mAP of 85.2% and 20.3% on UCF101-24 and AVAv2.2,\nrespectively, with 109.7M parameters and 53.6 GFLOPs. In contrast, our model -\nYOWOv3, with only 59.8M parameters and 39.8 GFLOPs, achieves an mAP of 88.33%\nand 20.31% on UCF101-24 and AVAv2.2, respectively. The results demonstrate that\nYOWOv3 significantly reduces the number of parameters and GFLOPs while still\nachieving comparable performance.\n","authors":["Duc Manh Nguyen Dang","Viet Hang Duong","Jia Ching Wang","Nhan Bui Duc"],"pdf_url":"https://arxiv.org/pdf/2408.02623v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02615v1","updated":"2024-08-05T16:39:39Z","published":"2024-08-05T16:39:39Z","title":"LaMamba-Diff: Linear-Time High-Fidelity Diffusion Models Based on Local\n  Attention and Mamba","summary":"  Recent Transformer-based diffusion models have shown remarkable performance,\nlargely attributed to the ability of the self-attention mechanism to accurately\ncapture both global and local contexts by computing all-pair interactions among\ninput tokens. However, their quadratic complexity poses significant\ncomputational challenges for long-sequence inputs. Conversely, a recent state\nspace model called Mamba offers linear complexity by compressing a filtered\nglobal context into a hidden state. Despite its efficiency, compression\ninevitably leads to information loss of fine-grained local dependencies among\ntokens, which are crucial for effective visual generative modeling. Motivated\nby these observations, we introduce Local Attentional Mamba (LaMamba) blocks\nthat combine the strengths of self-attention and Mamba, capturing both global\ncontexts and local details with linear complexity. Leveraging the efficient\nU-Net architecture, our model exhibits exceptional scalability and surpasses\nthe performance of DiT across various model scales on ImageNet at 256x256\nresolution, all while utilizing substantially fewer GFLOPs and a comparable\nnumber of parameters. Compared to state-of-the-art diffusion models on ImageNet\n256x256 and 512x512, our largest model presents notable advantages, such as a\nreduction of up to 62\\% GFLOPs compared to DiT-XL/2, while achieving superior\nperformance with comparable or fewer parameters.\n","authors":["Yunxiang Fu","Chaoqi Chen","Yizhou Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02615v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11515v2","updated":"2024-08-05T16:39:15Z","published":"2024-03-18T07:01:21Z","title":"SSAP: A Shape-Sensitive Adversarial Patch for Comprehensive Disruption\n  of Monocular Depth Estimation in Autonomous Navigation Applications","summary":"  Monocular depth estimation (MDE) has advanced significantly, primarily\nthrough the integration of convolutional neural networks (CNNs) and more\nrecently, Transformers. However, concerns about their susceptibility to\nadversarial attacks have emerged, especially in safety-critical domains like\nautonomous driving and robotic navigation. Existing approaches for assessing\nCNN-based depth prediction methods have fallen short in inducing comprehensive\ndisruptions to the vision system, often limited to specific local areas. In\nthis paper, we introduce SSAP (Shape-Sensitive Adversarial Patch), a novel\napproach designed to comprehensively disrupt monocular depth estimation (MDE)\nin autonomous navigation applications. Our patch is crafted to selectively\nundermine MDE in two distinct ways: by distorting estimated distances or by\ncreating the illusion of an object disappearing from the system's perspective.\nNotably, our patch is shape-sensitive, meaning it considers the specific shape\nand scale of the target object, thereby extending its influence beyond\nimmediate proximity. Furthermore, our patch is trained to effectively address\ndifferent scales and distances from the camera. Experimental results\ndemonstrate that our approach induces a mean depth estimation error surpassing\n0.5, impacting up to 99% of the targeted region for CNN-based MDE models.\nAdditionally, we investigate the vulnerability of Transformer-based MDE models\nto patch-based attacks, revealing that SSAP yields a significant error of 0.59\nand exerts substantial influence over 99% of the target region on these models.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Bassem Ouni","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2403.11515v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01351"},{"id":"http://arxiv.org/abs/2303.01351v3","updated":"2024-08-05T16:37:37Z","published":"2023-03-02T15:31:53Z","title":"APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth\n  Estimation for Autonomous Navigation","summary":"  In recent times, monocular depth estimation (MDE) has experienced significant\nadvancements in performance, largely attributed to the integration of\ninnovative architectures, i.e., convolutional neural networks (CNNs) and\nTransformers. Nevertheless, the susceptibility of these models to adversarial\nattacks has emerged as a noteworthy concern, especially in domains where safety\nand security are paramount. This concern holds particular weight for MDE due to\nits critical role in applications like autonomous driving and robotic\nnavigation, where accurate scene understanding is pivotal. To assess the\nvulnerability of CNN-based depth prediction methods, recent work tries to\ndesign adversarial patches against MDE. However, the existing approaches fall\nshort of inducing a comprehensive and substantially disruptive impact on the\nvision system. Instead, their influence is partial and confined to specific\nlocal areas. These methods lead to erroneous depth predictions only within the\noverlapping region with the input image, without considering the\ncharacteristics of the target object, such as its size, shape, and position. In\nthis paper, we introduce a novel adversarial patch named APARATE. This patch\npossesses the ability to selectively undermine MDE in two distinct ways: by\ndistorting the estimated distances or by creating the illusion of an object\ndisappearing from the perspective of the autonomous system. Notably, APARATE is\ndesigned to be sensitive to the shape and scale of the target object, and its\ninfluence extends beyond immediate proximity. APARATE, results in a mean depth\nestimation error surpassing $0.5$, significantly impacting as much as $99\\%$ of\nthe targeted region when applied to CNN-based MDE models. Furthermore, it\nyields a significant error of $0.34$ and exerts substantial influence over\n$94\\%$ of the target region in the context of Transformer-based MDE.\n","authors":["Amira Guesmi","Muhammad Abdullah Hanif","Ihsen Alouani","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2303.01351v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.07338v2","updated":"2024-08-05T16:34:43Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n","authors":["Da-Wei Zhou","Zi-Wen Cai","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v2.pdf","comment":"Accepted to IJCV. Code is available at:\n  https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2408.02595v1","updated":"2024-08-05T16:07:31Z","published":"2024-08-05T16:07:31Z","title":"Modelling Visual Semantics via Image Captioning to extract Enhanced\n  Multi-Level Cross-Modal Semantic Incongruity Representation with Attention\n  for Multimodal Sarcasm Detection","summary":"  Sarcasm is a type of irony, characterized by an inherent mismatch between the\nliteral interpretation and the intended connotation. Though sarcasm detection\nin text has been extensively studied, there are situations in which textual\ninput alone might be insufficient to perceive sarcasm. The inclusion of\nadditional contextual cues, such as images, is essential to recognize sarcasm\nin social media data effectively. This study presents a novel framework for\nmultimodal sarcasm detection that can process input triplets. Two components of\nthese triplets comprise the input text and its associated image, as provided in\nthe datasets. Additionally, a supplementary modality is introduced in the form\nof descriptive image captions. The motivation behind incorporating this visual\nsemantic representation is to more accurately capture the discrepancies between\nthe textual and visual content, which are fundamental to the sarcasm detection\ntask. The primary contributions of this study are: (1) a robust textual feature\nextraction branch that utilizes a cross-lingual language model; (2) a visual\nfeature extraction branch that incorporates a self-regulated residual ConvNet\nintegrated with a lightweight spatially aware attention module; (3) an\nadditional modality in the form of image captions generated using an\nencoder-decoder architecture capable of reading text embedded in images; (4)\ndistinct attention modules to effectively identify the incongruities between\nthe text and two levels of image representations; (5) multi-level cross-domain\nsemantic incongruity representation achieved through feature fusion. Compared\nwith cutting-edge baselines, the proposed model achieves the best accuracy of\n92.89% and 64.48%, respectively, on the Twitter multimodal sarcasm and\nMultiBully datasets.\n","authors":["Sajal Aggarwal","Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02571v1","updated":"2024-08-05T15:45:59Z","published":"2024-08-05T15:45:59Z","title":"Contrastive Learning-based Multi Modal Architecture for Emoticon\n  Prediction by Employing Image-Text Pairs","summary":"  The emoticons are symbolic representations that generally accompany the\ntextual content to visually enhance or summarize the true intention of a\nwritten message. Although widely utilized in the realm of social media, the\ncore semantics of these emoticons have not been extensively explored based on\nmultiple modalities. Incorporating textual and visual information within a\nsingle message develops an advanced way of conveying information. Hence, this\nresearch aims to analyze the relationship among sentences, visuals, and\nemoticons. For an orderly exposition, this paper initially provides a detailed\nexamination of the various techniques for extracting multimodal features,\nemphasizing the pros and cons of each method. Through conducting a\ncomprehensive examination of several multimodal algorithms, with specific\nemphasis on the fusion approaches, we have proposed a novel contrastive\nlearning based multimodal architecture. The proposed model employs the joint\ntraining of dual-branch encoder along with the contrastive learning to\naccurately map text and images into a common latent space. Our key finding is\nthat by integrating the principle of contrastive learning with that of the\nother two branches yields superior results. The experimental results\ndemonstrate that our suggested methodology surpasses existing multimodal\napproaches in terms of accuracy and robustness. The proposed model attained an\naccuracy of 91% and an MCC-score of 90% while assessing emoticons using the\nMultimodal-Twitter Emoticon dataset acquired from Twitter. We provide evidence\nthat deep features acquired by contrastive learning are more efficient,\nsuggesting that the proposed fusion technique also possesses strong\ngeneralisation capabilities for recognising emoticons across several modes.\n","authors":["Ananya Pandey","Dinesh Kumar Vishwakarma"],"pdf_url":"https://arxiv.org/pdf/2408.02571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02568v1","updated":"2024-08-05T15:43:56Z","published":"2024-08-05T15:43:56Z","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","summary":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","authors":["Pawe Zyblewski","Leandro L. Minku"],"pdf_url":"https://arxiv.org/pdf/2408.02568v1.pdf","comment":"10 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02561v1","updated":"2024-08-05T15:37:18Z","published":"2024-08-05T15:37:18Z","title":"HQOD: Harmonious Quantization for Object Detection","summary":"  Task inharmony problem commonly occurs in modern object detectors, leading to\ninconsistent qualities between classification and regression tasks. The\npredicted boxes with high classification scores but poor localization positions\nor low classification scores but accurate localization positions will worsen\nthe performance of detectors after Non-Maximum Suppression. Furthermore, when\nobject detectors collaborate with Quantization-Aware Training (QAT), we observe\nthat the task inharmony problem will be further exacerbated, which is\nconsidered one of the main causes of the performance degradation of quantized\ndetectors. To tackle this issue, we propose the Harmonious Quantization for\nObject Detection (HQOD) framework, which consists of two components. Firstly,\nwe propose a task-correlated loss to encourage detectors to focus on improving\nsamples with lower task harmony quality during QAT. Secondly, a harmonious\nIntersection over Union (IoU) loss is incorporated to balance the optimization\nof the regression branch across different IoU levels. The proposed HQOD can be\neasily integrated into different QAT algorithms and detectors. Remarkably, on\nthe MS COCO dataset, our 4-bit ATSS with ResNet-50 backbone achieves a\nstate-of-the-art mAP of 39.6%, even surpassing the full-precision one.\n","authors":["Long Huang","Zhiwei Dong","Song-Lu Chen","Ruiyao Zhang","Shutong Ti","Feng Chen","Xu-Cheng Yin"],"pdf_url":"https://arxiv.org/pdf/2408.02561v1.pdf","comment":"2024 IEEE International Conference on Multimedia and Expo (ICME),\n  July 15 - July 19, 2024, Niagra Falls, Ontario, Canada"},{"id":"http://arxiv.org/abs/2408.02555v1","updated":"2024-08-05T15:33:45Z","published":"2024-08-05T15:33:45Z","title":"MeshAnything V2: Artist-Created Mesh Generation With Adjacent Mesh\n  Tokenization","summary":"  We introduce MeshAnything V2, an autoregressive transformer that generates\nArtist-Created Meshes (AM) aligned to given shapes. It can be integrated with\nvarious 3D asset production pipelines to achieve high-quality, highly\ncontrollable AM generation. MeshAnything V2 surpasses previous methods in both\nefficiency and performance using models of the same size. These improvements\nare due to our newly proposed mesh tokenization method: Adjacent Mesh\nTokenization (AMT). Different from previous methods that represent each face\nwith three vertices, AMT uses a single vertex whenever possible. Compared to\nprevious methods, AMT requires about half the token sequence length to\nrepresent the same mesh in average. Furthermore, the token sequences from AMT\nare more compact and well-structured, fundamentally benefiting AM generation.\nOur extensive experiments show that AMT significantly improves the efficiency\nand performance of AM generation. Project Page:\nhttps://buaacyw.github.io/meshanything-v2/\n","authors":["Yiwen Chen","Yikai Wang","Yihao Luo","Zhengyi Wang","Zilong Chen","Jun Zhu","Chi Zhang","Guosheng Lin"],"pdf_url":"https://arxiv.org/pdf/2408.02555v1.pdf","comment":"Project Page: https://buaacyw.github.io/meshanything-v2/ Github:\n  https://github.com/buaacyw/MeshAnythingV2"},{"id":"http://arxiv.org/abs/2408.02507v1","updated":"2024-08-05T14:31:09Z","published":"2024-08-05T14:31:09Z","title":"Estimating Pore Location of PBF-LB/M Processes with Segmentation Models","summary":"  Reliably manufacturing defect free products is still an open challenge for\nLaser Powder Bed Fusion processes. Particularly, pores that occur frequently\nhave a negative impact on mechanical properties like fatigue performance.\nTherefore, an accurate localisation of pores is mandatory for quality\nassurance, but requires time-consuming post-processing steps like computer\ntomography scans. Although existing solutions using in-situ monitoring data can\ndetect pore occurrence within a layer, they are limited in their localisation\nprecision. Therefore, we propose a pore localisation approach that estimates\ntheir position within a single layer using a Gaussian kernel density\nestimation. This allows segmentation models to learn the correlation between\nin-situ monitoring data and the derived probability distribution of pore\noccurrence. Within our experiments, we compare the prediction performance of\ndifferent segmentation models depending on machine parameter configuration and\ngeometry features. From our results, we conclude that our approach allows a\nprecise localisation of pores that requires minimal data preprocessing. Our\nresearch extends the literature by providing a foundation for more precise pore\ndetection systems.\n","authors":["Hans Aoyang Zhou","Jan Theunissen","Marco Kemmerling","Anas Abdelrazeq","Johannes Henrich Schleifenbaum","Robert H. Schmitt"],"pdf_url":"https://arxiv.org/pdf/2408.02507v1.pdf","comment":"20 pages, 7 figures, This work has been submitted to the Journal\n  Progress in Additive Manufacturing"},{"id":"http://arxiv.org/abs/2408.02496v1","updated":"2024-08-05T14:19:03Z","published":"2024-08-05T14:19:03Z","title":"Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts","summary":"  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n","authors":["Lisa Hemforth","Baptiste Couvy-Duchesne","Kevin De Matos","Camille Brianceau","Matthieu Joulot","Tobias Banaschewski","Arun L. W. Bokde","Sylvane Desrivires","Herta Flor","Antoine Grigis","Hugh Garavan","Penny Gowland","Andreas Heinz","Rdiger Brhl","Jean-Luc Martinot","Marie-Laure Paillre Martinot","Eric Artiges","Dimitri Papadopoulos","Herve Lemaitre","Tomas Paus","Luise Poustka","Sarah Hohmann","Nathalie Holz","Juliane H. Frhner","Michael N. Smolka","Nilakshi Vaidya","Henrik Walter","Robert Whelan","Gunter Schumann","Christian Bchel","JB Poline","Bernd Itterman","Vincent Frouin","Alexandre Martin","IMAGEN study group","Claire Cury","Olivier Colliot"],"pdf_url":"https://arxiv.org/pdf/2408.02496v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016"},{"id":"http://arxiv.org/abs/2408.02494v1","updated":"2024-08-05T14:18:29Z","published":"2024-08-05T14:18:29Z","title":"HyperSpaceX: Radial and Angular Exploration of HyperSpherical Dimensions","summary":"  Traditional deep learning models rely on methods such as softmax\ncross-entropy and ArcFace loss for tasks like classification and face\nrecognition. These methods mainly explore angular features in a hyperspherical\nspace, often resulting in entangled inter-class features due to dense angular\ndata across many classes. In this paper, a new field of feature exploration is\nproposed known as HyperSpaceX which enhances class discrimination by exploring\nboth angular and radial dimensions in multi-hyperspherical spaces, facilitated\nby a novel DistArc loss. The proposed DistArc loss encompasses three feature\narrangement components: two angular and one radial, enforcing intra-class\nbinding and inter-class separation in multi-radial arrangement, improving\nfeature discriminability. Evaluation of HyperSpaceX framework for the novel\nrepresentation utilizes a proposed predictive measure that accounts for both\nangular and radial elements, providing a more comprehensive assessment of model\naccuracy beyond standard metrics. Experiments across seven object\nclassification and six face recognition datasets demonstrate state-of-the-art\n(SoTA) results obtained from HyperSpaceX, achieving up to a 20% performance\nimprovement on large-scale object datasets in lower dimensions and up to 6%\ngain in higher dimensions.\n","authors":["Chiranjeev Chiranjeev","Muskan Dosi","Kartik Thakral","Mayank Vatsa","Richa Singh"],"pdf_url":"https://arxiv.org/pdf/2408.02494v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.10107v2","updated":"2024-08-05T14:06:35Z","published":"2024-06-14T15:08:04Z","title":"Annotation Cost-Efficient Active Learning for Deep Metric Learning\n  Driven Remote Sensing Image Retrieval","summary":"  Deep metric learning (DML) has shown to be effective for content-based image\nretrieval (CBIR) in remote sensing (RS). Most of DML methods for CBIR rely on a\nhigh number of annotated images to accurately learn model parameters of deep\nneural networks (DNNs). However, gathering such data is time-consuming and\ncostly. To address this, we propose an annotation cost-efficient active\nlearning (ANNEAL) method tailored to DML-driven CBIR in RS. ANNEAL aims to\ncreate a small but informative training set made up of similar and dissimilar\nimage pairs to be utilized for accurately learning a metric space. The\ninformativeness of image pairs is evaluated by combining uncertainty and\ndiversity criteria. To assess the uncertainty of image pairs, we introduce two\nalgorithms: 1) metric-guided uncertainty estimation (MGUE); and 2) binary\nclassifier guided uncertainty estimation (BCGUE). MGUE algorithm automatically\nestimates a threshold value that acts as a boundary between similar and\ndissimilar image pairs based on the distances in the metric space. The closer\nthe similarity between image pairs is to the estimated threshold value the\nhigher their uncertainty. BCGUE algorithm estimates the uncertainty of the\nimage pairs based on the confidence of the classifier in assigning correct\nsimilarity labels. The diversity criterion is assessed through a\nclustering-based strategy. ANNEAL combines either MGUE or BCGUE algorithm with\nthe clustering-based strategy to select the most informative image pairs, which\nare then labelled by expert annotators as similar or dissimilar. This way of\nannotating images significantly reduces the annotation cost compared to\nannotating images with land-use land-cover class labels. Experimental results\non two RS benchmark datasets demonstrate the effectiveness of our method. The\ncode of this work is publicly available at\n\\url{https://git.tu-berlin.de/rsim/anneal_tgrs}.\n","authors":["Genc Hoxha","Gencer Sumbul","Julia Henkel","Lars Mllenbrok","Begm Demir"],"pdf_url":"https://arxiv.org/pdf/2406.10107v2.pdf","comment":"Accepted for publication in the IEEE Transactions on Geoscience and\n  Remote Sensing (TGRS)"},{"id":"http://arxiv.org/abs/2408.02484v1","updated":"2024-08-05T14:05:25Z","published":"2024-08-05T14:05:25Z","title":"Exploring Conditional Multi-Modal Prompts for Zero-shot HOI Detection","summary":"  Zero-shot Human-Object Interaction (HOI) detection has emerged as a frontier\ntopic due to its capability to detect HOIs beyond a predefined set of\ncategories. This task entails not only identifying the interactiveness of\nhuman-object pairs and localizing them but also recognizing both seen and\nunseen interaction categories. In this paper, we introduce a novel framework\nfor zero-shot HOI detection using Conditional Multi-Modal Prompts, namely CMMP.\nThis approach enhances the generalization of large foundation models, such as\nCLIP, when fine-tuned for HOI detection. Unlike traditional prompt-learning\nmethods, we propose learning decoupled vision and language prompts for\ninteractiveness-aware visual feature extraction and generalizable interaction\nclassification, respectively. Specifically, we integrate prior knowledge of\ndifferent granularity into conditional vision prompts, including an\ninput-conditioned instance prior and a global spatial pattern prior. The former\nencourages the image encoder to treat instances belonging to seen or\npotentially unseen HOI concepts equally while the latter provides\nrepresentative plausible spatial configuration of the human and object under\ninteraction. Besides, we employ language-aware prompt learning with a\nconsistency constraint to preserve the knowledge of the large foundation model\nto enable better generalization in the text branch. Extensive experiments\ndemonstrate the efficacy of our detector with conditional multi-modal prompts,\noutperforming previous state-of-the-art on unseen classes of various zero-shot\nsettings. The code and models are available at\n\\url{https://github.com/ltttpku/CMMP}.\n","authors":["Ting Lei","Shaofeng Yin","Yuxin Peng","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02484v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.02875v2","updated":"2024-08-05T14:01:26Z","published":"2024-03-05T11:38:48Z","title":"Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples","summary":"  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n","authors":["Philipp J. Rsch","Norbert Oswald","Michaela Geierhos","Jindich Libovick"],"pdf_url":"https://arxiv.org/pdf/2403.02875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02464v1","updated":"2024-08-05T13:44:22Z","published":"2024-08-05T13:44:22Z","title":"Fairness and Bias Mitigation in Computer Vision: A Survey","summary":"  Computer vision systems have witnessed rapid progress over the past two\ndecades due to multiple advances in the field. As these systems are\nincreasingly being deployed in high-stakes real-world applications, there is a\ndire need to ensure that they do not propagate or amplify any discriminatory\ntendencies in historical or human-curated data or inadvertently learn biases\nfrom spurious correlations. This paper presents a comprehensive survey on\nfairness that summarizes and sheds light on ongoing trends and successes in the\ncontext of computer vision. The topics we discuss include 1) The origin and\ntechnical definitions of fairness drawn from the wider fair machine learning\nliterature and adjacent disciplines. 2) Work that sought to discover and\nanalyze biases in computer vision systems. 3) A summary of methods proposed to\nmitigate bias in computer vision systems in recent years. 4) A comprehensive\nsummary of resources and datasets produced by researchers to measure, analyze,\nand mitigate bias and enhance fairness. 5) Discussion of the field's success,\ncontinuing trends in the context of multimodal foundation and generative\nmodels, and gaps that still need to be addressed. The presented\ncharacterization should help researchers understand the importance of\nidentifying and mitigating bias in computer vision and the state of the field\nand identify potential directions for future research.\n","authors":["Sepehr Dehdashtian","Ruozhen He","Yi Li","Guha Balakrishnan","Nuno Vasconcelos","Vicente Ordonez","Vishnu Naresh Boddeti"],"pdf_url":"https://arxiv.org/pdf/2408.02464v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02462v1","updated":"2024-08-05T13:40:33Z","published":"2024-08-05T13:40:33Z","title":"An investigation into the causes of race bias in AI-based cine CMR\n  segmentation","summary":"  Artificial intelligence (AI) methods are being used increasingly for the\nautomated segmentation of cine cardiac magnetic resonance (CMR) imaging.\nHowever, these methods have been shown to be subject to race bias, i.e. they\nexhibit different levels of performance for different races depending on the\n(im)balance of the data used to train the AI model. In this paper we\ninvestigate the source of this bias, seeking to understand its root cause(s) so\nthat it can be effectively mitigated. We perform a series of classification and\nsegmentation experiments on short-axis cine CMR images acquired from Black and\nWhite subjects from the UK Biobank and apply AI interpretability methods to\nunderstand the results. In the classification experiments, we found that race\ncan be predicted with high accuracy from the images alone, but less accurately\nfrom ground truth segmentations, suggesting that the distributional shift\nbetween races, which is often the cause of AI bias, is mostly image-based\nrather than segmentation-based. The interpretability methods showed that most\nattention in the classification models was focused on non-heart regions, such\nas subcutaneous fat. Cropping the images tightly around the heart reduced\nclassification accuracy to around chance level. Similarly, race can be\npredicted from the latent representations of a biased segmentation model,\nsuggesting that race information is encoded in the model. Cropping images\ntightly around the heart reduced but did not eliminate segmentation bias. We\nalso investigate the influence of possible confounders on the bias observed.\n","authors":["Tiarna Lee","Esther Puyol-Anton","Bram Ruijsink","Sebastien Roujol","Theodore Barfoot","Shaheim Ogbomo-Harmitt","Miaojing Shi","Andrew P. King"],"pdf_url":"https://arxiv.org/pdf/2408.02462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15269v2","updated":"2024-08-05T13:23:17Z","published":"2024-06-21T16:04:14Z","title":"You Only Acquire Sparse-channel (YOAS): A Unified Framework for\n  Dense-channel EEG Generation","summary":"  High-precision acquisition of dense-channel electroencephalogram (EEG)\nsignals is often impeded by the costliness and lack of portability of\nequipment. In contrast, generating dense-channel EEG signals effectively from\nsparse channels shows promise and economic viability. However, sparse-channel\nEEG poses challenges such as reduced spatial resolution, information loss,\nsignal mixing, and heightened susceptibility to noise and interference. To\naddress these challenges, we first theoretically formulate the dense-channel\nEEG generation problem as by optimizing a set of cross-channel EEG signal\ngeneration problems. Then, we propose the YOAS framework for generating\ndense-channel data from sparse-channel EEG signals. The YOAS totally consists\nof four sequential stages: Data Preparation, Data Preprocessing, Biased-EEG\nGeneration, and Synthetic EEG Generation. Data Preparation and Preprocessing\ncarefully consider the distribution of EEG electrodes and low signal-to-noise\nratio problem of EEG signals. Biased-EEG Generation includes sub-modules of\nBiasEEGGanFormer and BiasEEGDiffFormer, which facilitate long-term feature\nextraction with attention and generate signals by combining electrode position\nalignment with diffusion model, respectively. Synthetic EEG Generation\nsynthesizes the final signals, employing a deduction paradigm for multi-channel\nEEG generation. Extensive experiments confirmed YOAS's feasibility, efficiency,\nand theoretical validity, even remarkably enhancing data discernibility. This\nbreakthrough in dense-channel EEG signal generation from sparse-channel data\nopens new avenues for exploration in EEG signal processing and application.\n","authors":["Hongyu Chen","Weiming Zeng","Luhui Cai","Lei Wang","Jia Lu","Yueyang Li","Hongjie Yan","Wai Ting Siok","Nizhuan Wang"],"pdf_url":"https://arxiv.org/pdf/2406.15269v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12890v3","updated":"2024-08-05T13:10:02Z","published":"2023-11-21T06:24:09Z","title":"De-fine: Decomposing and Refining Visual Programs with Auto-Feedback","summary":"  Visual programming, a modular and generalizable paradigm, integrates\ndifferent modules and Python operators to solve various vision-language tasks.\nUnlike end-to-end models that need task-specific data, it advances in\nperforming visual processing and reasoning in an unsupervised manner. Current\nvisual programming methods generate programs in a single pass for each task\nwhere the ability to evaluate and optimize based on feedback, unfortunately, is\nlacking, which consequentially limits their effectiveness for complex,\nmulti-step problems. Drawing inspiration from benders decomposition, we\nintroduce De-fine, a training-free framework that automatically decomposes\ncomplex tasks into simpler subtasks and refines programs through auto-feedback.\nThis model-agnostic approach can improve logical reasoning performance by\nintegrating the strengths of multiple models. Our experiments across various\nvisual tasks show that De-fine creates more robust programs. Moreover, viewing\neach feedback module as an independent agent will yield fresh prospects for the\nfield of agent research.\n","authors":["Minghe Gao","Juncheng Li","Hao Fei","Liang Pang","Wei Ji","Guoming Wang","Zheqi Lv","Wenqiao Zhang","Siliang Tang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2311.12890v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07990v2","updated":"2024-08-05T12:55:47Z","published":"2024-04-11T17:59:56Z","title":"OpenBias: Open-set Bias Detection in Text-to-Image Generative Models","summary":"  Text-to-image generative models are becoming increasingly popular and\naccessible to the general public. As these models see large-scale deployments,\nit is necessary to deeply investigate their safety and fairness to not\ndisseminate and perpetuate any kind of biases. However, existing works focus on\ndetecting closed sets of biases defined a priori, limiting the studies to\nwell-known concepts. In this paper, we tackle the challenge of open-set bias\ndetection in text-to-image generative models presenting OpenBias, a new\npipeline that identifies and quantifies the severity of biases agnostically,\nwithout access to any precompiled set. OpenBias has three stages. In the first\nphase, we leverage a Large Language Model (LLM) to propose biases given a set\nof captions. Secondly, the target generative model produces images using the\nsame set of captions. Lastly, a Vision Question Answering model recognizes the\npresence and extent of the previously proposed biases. We study the behavior of\nStable Diffusion 1.5, 2, and XL emphasizing new biases, never investigated\nbefore. Via quantitative experiments, we demonstrate that OpenBias agrees with\ncurrent closed-set bias detection methods and human judgement.\n","authors":["Moreno D'Inc","Elia Peruzzo","Massimiliano Mancini","Dejia Xu","Vidit Goel","Xingqian Xu","Zhangyang Wang","Humphrey Shi","Nicu Sebe"],"pdf_url":"https://arxiv.org/pdf/2404.07990v2.pdf","comment":"CVPR 2024 Highlight - Code:\n  https://github.com/Picsart-AI-Research/OpenBias"},{"id":"http://arxiv.org/abs/2303.10571v2","updated":"2024-08-05T12:44:04Z","published":"2023-03-19T05:20:52Z","title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","summary":"  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n","authors":["Haobin Jiang","Junpeng Yue","Hao Luo","Ziluo Ding","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10571v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2404.11129v2","updated":"2024-08-05T12:39:06Z","published":"2024-04-17T07:20:56Z","title":"Fact :Teaching MLLMs with Faithful, Concise and Transferable Rationales","summary":"  The remarkable performance of Multimodal Large Language Models (MLLMs) has\nunequivocally demonstrated their proficient understanding capabilities in\nhandling a wide array of visual tasks. Nevertheless, the opaque nature of their\nblack-box reasoning processes persists as an enigma, rendering them\nuninterpretable and struggling with hallucination. Their ability to execute\nintricate compositional reasoning tasks is also constrained, culminating in a\nstagnation of learning progression for these models. In this work, we introduce\nFact, a novel paradigm designed to generate multimodal rationales that are\nfaithful, concise, and transferable for teaching MLLMs. This paradigm utilizes\nverifiable visual programming to generate executable code guaranteeing\nfaithfulness and precision. Subsequently, through a series of operations\nincluding pruning, merging, and bridging, the rationale enhances its\nconciseness. Furthermore, we filter rationales that can be transferred to\nend-to-end paradigms from programming paradigms to guarantee transferability.\nEmpirical evidence from experiments demonstrates the superiority of our method\nacross models of varying parameter sizes, significantly enhancing their\ncompositional reasoning and generalization ability. Our approach also reduces\nhallucinations owing to its high correlation between images and text.\n","authors":["Minghe Gao","Shuang Chen","Liang Pang","Yuan Yao","Jisheng Dang","Wenqiao Zhang","Juncheng Li","Siliang Tang","Yueting Zhuang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2404.11129v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02427v1","updated":"2024-08-05T12:34:49Z","published":"2024-08-05T12:34:49Z","title":"Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders","summary":"  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n","authors":["Andreas Bjerregaard","David Schumacher","Jon Sporring"],"pdf_url":"https://arxiv.org/pdf/2408.02427v1.pdf","comment":"Implementation on https://github.com/yhsure/porosity"},{"id":"http://arxiv.org/abs/2408.02426v1","updated":"2024-08-05T12:33:07Z","published":"2024-08-05T12:33:07Z","title":"FPT+: A Parameter and Memory Efficient Transfer Learning Method for\n  High-resolution Medical Image Classification","summary":"  The success of large-scale pre-trained models has established fine-tuning as\na standard method for achieving significant improvements in downstream tasks.\nHowever, fine-tuning the entire parameter set of a pre-trained model is costly.\nParameter-efficient transfer learning (PETL) has recently emerged as a\ncost-effective alternative for adapting pre-trained models to downstream tasks.\nDespite its advantages, the increasing model size and input resolution present\nchallenges for PETL, as the training memory consumption is not reduced as\neffectively as the parameter usage. In this paper, we introduce Fine-grained\nPrompt Tuning plus (FPT+), a PETL method designed for high-resolution medical\nimage classification, which significantly reduces memory consumption compared\nto other PETL methods. FPT+ performs transfer learning by training a\nlightweight side network and accessing pre-trained knowledge from a large\npre-trained model (LPM) through fine-grained prompts and fusion modules.\nSpecifically, we freeze the LPM and construct a learnable lightweight side\nnetwork. The frozen LPM processes high-resolution images to extract\nfine-grained features, while the side network employs the corresponding\ndown-sampled low-resolution images to minimize the memory usage. To enable the\nside network to leverage pre-trained knowledge, we propose fine-grained prompts\nand fusion modules, which collaborate to summarize information through the\nLPM's intermediate activations. We evaluate FPT+ on eight medical image\ndatasets of varying sizes, modalities, and complexities. Experimental results\ndemonstrate that FPT+ outperforms other PETL methods, using only 1.03% of the\nlearnable parameters and 3.18% of the memory required for fine-tuning an entire\nViT-B model. Our code is available at https://github.com/YijinHuang/FPT.\n","authors":["Yijin Huang","Pujin Cheng","Roger Tam","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2408.02426v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19719v2","updated":"2024-08-05T12:29:47Z","published":"2024-07-29T06:03:13Z","title":"Revolutionizing Urban Safety Perception Assessments: Integrating\n  Multimodal Large Language Models with Street View Images","summary":"  Measuring urban safety perception is an important and complex task that\ntraditionally relies heavily on human resources. This process often involves\nextensive field surveys, manual data collection, and subjective assessments,\nwhich can be time-consuming, costly, and sometimes inconsistent. Street View\nImages (SVIs), along with deep learning methods, provide a way to realize\nlarge-scale urban safety detection. However, achieving this goal often requires\nextensive human annotation to train safety ranking models, and the\narchitectural differences between cities hinder the transferability of these\nmodels. Thus, a fully automated method for conducting safety evaluations is\nessential. Recent advances in multimodal large language models (MLLMs) have\ndemonstrated powerful reasoning and analytical capabilities. Cutting-edge\nmodels, e.g., GPT-4 have shown surprising performance in many tasks. We\nemployed these models for urban safety ranking on a human-annotated anchor set\nand validated that the results from MLLMs align closely with human perceptions.\nAdditionally, we proposed a method based on the pre-trained Contrastive\nLanguage-Image Pre-training (CLIP) feature and K-Nearest Neighbors (K-NN)\nretrieval to quickly assess the safety index of the entire city. Experimental\nresults show that our method outperforms existing training needed deep learning\napproaches, achieving efficient and accurate urban safety evaluations. The\nproposed automation for urban safety perception assessment is a valuable tool\nfor city planners, policymakers, and researchers aiming to improve urban\nenvironments.\n","authors":["Jiaxin Zhang","Yunqin Li","Tomohiro Fukuda","Bowen Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19719v2.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02421v1","updated":"2024-08-05T12:27:28Z","published":"2024-08-05T12:27:28Z","title":"FE-Adapter: Adapting Image-based Emotion Classifiers to Videos","summary":"  Utilizing large pre-trained models for specific tasks has yielded impressive\nresults. However, fully fine-tuning these increasingly large models is becoming\nprohibitively resource-intensive. This has led to a focus on more\nparameter-efficient transfer learning, primarily within the same modality. But\nthis approach has limitations, particularly in video understanding where\nsuitable pre-trained models are less common. Addressing this, our study\nintroduces a novel cross-modality transfer learning approach from images to\nvideos, which we call parameter-efficient image-to-video transfer learning. We\npresent the Facial-Emotion Adapter (FE-Adapter), designed for efficient\nfine-tuning in video tasks. This adapter allows pre-trained image models, which\ntraditionally lack temporal processing capabilities, to analyze dynamic video\ncontent efficiently. Notably, it uses about 15 times fewer parameters than\nprevious methods, while improving accuracy. Our experiments in video emotion\nrecognition demonstrate that the FE-Adapter can match or even surpass existing\nfine-tuning and video emotion models in both performance and efficiency. This\nbreakthrough highlights the potential for cross-modality approaches in\nenhancing the capabilities of AI models, particularly in fields like video\nemotion analysis where the demand for efficiency and accuracy is constantly\nrising.\n","authors":["Shreyank N Gowda","Boyan Gao","David A. Clifton"],"pdf_url":"https://arxiv.org/pdf/2408.02421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19905v2","updated":"2024-08-05T12:12:48Z","published":"2024-06-28T13:20:17Z","title":"Solving Token Gradient Conflict in Mixture-of-Experts for Large\n  Vision-Language Model","summary":"  The Mixture-of-Experts (MoE) has gained increasing attention in studying\nLarge Vision-Language Models (LVLMs). It uses a sparse model to replace the\ndense model, achieving comparable performance while activating fewer parameters\nduring inference, thus significantly reducing the inference cost. Existing MoE\nmethods in LVLMs encourage different experts to handle different tokens, and\nthey usually employ a router to predict the routing of each token. However, the\npredictions are based solely on sample features and do not truly reveal the\noptimization directions of tokens. This may lead to severe optimization\ninterference between different tokens assigned to an expert. To address this\nproblem, this paper proposes a novel method based on token-level gradient\nanalysis, i.e., Solving Token Gradient Conflict (STGC). Specifically, we first\nuse token-level gradients to identify conflicting tokens in experts. After\nthat, we add a specialized loss tailored to eliminate conflicts among tokens\nwithin each expert. Our method can serve as a plug-in for diverse Large\nVision-Language Models, and extensive experimental results demonstrate its\neffectiveness. The code will be publicly available at\nhttps://github.com/longrongyang/STGC.\n","authors":["Longrong Yang","Dong Shen","Chaoxiang Cai","Fan Yang","Size Li","Di Zhang","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2406.19905v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02408v1","updated":"2024-08-05T12:09:38Z","published":"2024-08-05T12:09:38Z","title":"Multi-weather Cross-view Geo-localization Using Denoising Diffusion\n  Models","summary":"  Cross-view geo-localization in GNSS-denied environments aims to determine an\nunknown location by matching drone-view images with the correct geo-tagged\nsatellite-view images from a large gallery. Recent research shows that learning\ndiscriminative image representations under specific weather conditions can\nsignificantly enhance performance. However, the frequent occurrence of unseen\nextreme weather conditions hinders progress. This paper introduces MCGF, a\nMulti-weather Cross-view Geo-localization Framework designed to dynamically\nadapt to unseen weather conditions. MCGF establishes a joint optimization\nbetween image restoration and geo-localization using denoising diffusion\nmodels. For image restoration, MCGF incorporates a shared encoder and a\nlightweight restoration module to help the backbone eliminate weather-specific\ninformation. For geo-localization, MCGF uses EVA-02 as a backbone for feature\nextraction, with cross-entropy loss for training and cosine distance for\ntesting. Extensive experiments on University160k-WX demonstrate that MCGF\nachieves competitive results for geo-localization in varying weather\nconditions.\n","authors":["Tongtong Feng","Qing Li","Xin Wang","Mingzi Wang","Guangyao Li","Wenwu Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.02408v1.pdf","comment":"Accepted by ACM MM24 workshop"},{"id":"http://arxiv.org/abs/2408.02398v1","updated":"2024-08-05T11:42:41Z","published":"2024-08-05T11:42:41Z","title":"Tensorial template matching for fast cross-correlation with rotations\n  and its application for tomography","summary":"  Object detection is a main task in computer vision. Template matching is the\nreference method for detecting objects with arbitrary templates. However,\ntemplate matching computational complexity depends on the rotation accuracy,\nbeing a limiting factor for large 3D images (tomograms). Here, we implement a\nnew algorithm called tensorial template matching, based on a mathematical\nframework that represents all rotations of a template with a tensor field.\nContrary to standard template matching, the computational complexity of the\npresented algorithm is independent of the rotation accuracy. Using both,\nsynthetic and real data from tomography, we demonstrate that tensorial template\nmatching is much faster than template matching and has the potential to improve\nits accuracy\n","authors":["Antonio Martinez-Sanchez","Ulrike Homberg","Jos Mara Almira","Harold Phelippeau"],"pdf_url":"https://arxiv.org/pdf/2408.02398v1.pdf","comment":"Accepted in The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02394v1","updated":"2024-08-05T11:40:59Z","published":"2024-08-05T11:40:59Z","title":"CMR-Agent: Learning a Cross-Modal Agent for Iterative Image-to-Point\n  Cloud Registration","summary":"  Image-to-point cloud registration aims to determine the relative camera pose\nof an RGB image with respect to a point cloud. It plays an important role in\ncamera localization within pre-built LiDAR maps. Despite the modality gaps,\nmost learning-based methods establish 2D-3D point correspondences in feature\nspace without any feedback mechanism for iterative optimization, resulting in\npoor accuracy and interpretability. In this paper, we propose to reformulate\nthe registration procedure as an iterative Markov decision process, allowing\nfor incremental adjustments to the camera pose based on each intermediate\nstate. To achieve this, we employ reinforcement learning to develop a\ncross-modal registration agent (CMR-Agent), and use imitation learning to\ninitialize its registration policy for stability and quick-start of the\ntraining. According to the cross-modal observations, we propose a 2D-3D hybrid\nstate representation that fully exploits the fine-grained features of RGB\nimages while reducing the useless neutral states caused by the spatial\ntruncation of camera frustum. Additionally, the overall framework is\nwell-designed to efficiently reuse one-shot cross-modal embeddings, avoiding\nrepetitive and time-consuming feature extraction. Extensive experiments on the\nKITTI-Odometry and NuScenes datasets demonstrate that CMR-Agent achieves\ncompetitive accuracy and efficiency in registration. Once the one-shot\nembeddings are completed, each iteration only takes a few milliseconds.\n","authors":["Gongxin Yao","Yixin Xuan","Xinyang Li","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02394v1.pdf","comment":"Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2024"},{"id":"http://arxiv.org/abs/2408.02392v1","updated":"2024-08-05T11:39:22Z","published":"2024-08-05T11:39:22Z","title":"MaFreeI2P: A Matching-Free Image-to-Point Cloud Registration Paradigm\n  with Active Camera Pose Retrieval","summary":"  Image-to-point cloud registration seeks to estimate their relative camera\npose, which remains an open question due to the data modality gaps. The recent\nmatching-based methods tend to tackle this by building 2D-3D correspondences.\nIn this paper, we reveal the information loss inherent in these methods and\npropose a matching-free paradigm, named MaFreeI2P. Our key insight is to\nactively retrieve the camera pose in SE(3) space by contrasting the geometric\nfeatures between the point cloud and the query image. To achieve this, we first\nsample a set of candidate camera poses and construct their cost volume using\nthe cross-modal features. Superior to matching, cost volume can preserve more\ninformation and its feature similarity implicitly reflects the confidence level\nof the sampled poses. Afterwards, we employ a convolutional network to\nadaptively formulate a similarity assessment function, where the input cost\nvolume is further improved by filtering and pose-based weighting. Finally, we\nupdate the camera pose based on the similarity scores, and adopt a heuristic\nstrategy to iteratively shrink the pose sampling space for convergence. Our\nMaFreeI2P achieves a very competitive registration accuracy and recall on the\nKITTI-Odometry and Apollo-DaoxiangLake datasets.\n","authors":["Gongxin Yao","Xinyang Li","Yixin Xuan","Yu Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02392v1.pdf","comment":"Accepted to IEEE Conference on Multimedia Expo 2024"},{"id":"http://arxiv.org/abs/2309.01446v4","updated":"2024-08-05T11:34:10Z","published":"2023-09-04T08:54:20Z","title":"Open Sesame! Universal Black Box Jailbreaking of Large Language Models","summary":"  Large language models (LLMs), designed to provide helpful and safe responses,\noften rely on alignment techniques to align with user intent and social\nguidelines. Unfortunately, this alignment can be exploited by malicious actors\nseeking to manipulate an LLM's outputs for unintended purposes. In this paper\nwe introduce a novel approach that employs a genetic algorithm (GA) to\nmanipulate LLMs when model architecture and parameters are inaccessible. The GA\nattack works by optimizing a universal adversarial prompt that -- when combined\nwith a user's query -- disrupts the attacked model's alignment, resulting in\nunintended and potentially harmful outputs. Our novel approach systematically\nreveals a model's limitations and vulnerabilities by uncovering instances where\nits responses deviate from expected behavior. Through extensive experiments we\ndemonstrate the efficacy of our technique, thus contributing to the ongoing\ndiscussion on responsible AI development by providing a diagnostic tool for\nevaluating and enhancing alignment of LLMs with human intent. To our knowledge\nthis is the first automated universal black box jailbreak attack.\n","authors":["Raz Lapid","Ron Langberg","Moshe Sipper"],"pdf_url":"https://arxiv.org/pdf/2309.01446v4.pdf","comment":"Accepted at SeT-LLM @ ICLR 2024"},{"id":"http://arxiv.org/abs/2408.02382v1","updated":"2024-08-05T11:14:23Z","published":"2024-08-05T11:14:23Z","title":"Cross Psuedo Supervision Framework for Sparsely Labelled Geo-spatial\n  Images","summary":"  Land Use Land Cover (LULC) mapping is essential for urban and resource\nplanning and is one of the key elements in developing smart and sustainable\ncities. This study introduces a semi-supervised segmentation model for LULC\nprediction using high-resolution satellite images with a huge diversity in data\ndistributions in different areas from the country of India. Our approach\nensures a robust generalization across different types of buildings, roads,\ntrees, and water bodies within these distinct areas. We propose a modified\nCross Pseudo Supervision framework to train image segmentation models on\nsparsely labelled data. The proposed framework addresses the limitations of the\npopular \"Cross Pseudo Supervision\" technique for semi-supervised learning.\nSpecifically, it tackles the challenges of training segmentation models on\nnoisy satellite image data with sparse and inaccurate labels. This\ncomprehensive approach enhances the accuracy and utility of LULC mapping for\nvarious urban planning applications.\n","authors":["Yash Dixit","Naman Srivastava","Joel D Joy","Rohan Olikara","Swarup E","Rakshit Ramesh"],"pdf_url":"https://arxiv.org/pdf/2408.02382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02369v1","updated":"2024-08-05T10:38:50Z","published":"2024-08-05T10:38:50Z","title":"The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC\n  2024","summary":"  This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including the\nfixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In\nterms of data processing, we leverage the lip motion extractor from the\nbaseline1 to produce multiscale video data. Besides, various augmentation\ntechniques are applied during training, encompassing speed perturbation, random\nrotation, horizontal flipping, and color transformation. The VSR model adopts\nan end-to-end architecture with joint CTC/attention loss, introducing Enhanced\nResNet3D visual frontend, E-Branchformer encoder, and Bi-directional\nTransformer decoder. Our approach yields a 30.47% CER for the Single-Speaker\nTask and 34.30% CER for the Multi-Speaker Task, securing second place in the\nopen track of the Single-Speaker Task and first place in the other three\ntracks.\n","authors":["He Wang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2408.02369v1.pdf","comment":"2 pages, 2 figures, CNVSRC 2024 System Report"},{"id":"http://arxiv.org/abs/2408.02367v1","updated":"2024-08-05T10:32:06Z","published":"2024-08-05T10:32:06Z","title":"StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n","authors":["Perla Mayo","Matteo Cencini","Carolin M. Pirkl","Marion I. Menzel","Michela Tosetti","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2408.02367v1.pdf","comment":"10 pages, 2 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2405.10802v2","updated":"2024-08-05T10:20:11Z","published":"2024-05-17T14:16:40Z","title":"Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression","summary":"  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n","authors":["Mateusz Gabor","Rafa Zdunek"],"pdf_url":"https://arxiv.org/pdf/2405.10802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02348v1","updated":"2024-08-05T09:50:16Z","published":"2024-08-05T09:50:16Z","title":"Earth System Data Cubes: Avenues for advancing Earth system research","summary":"  Recent advancements in Earth system science have been marked by the\nexponential increase in the availability of diverse, multivariate datasets\ncharacterised by moderate to high spatio-temporal resolutions. Earth System\nData Cubes (ESDCs) have emerged as one suitable solution for transforming this\nflood of data into a simple yet robust data structure. ESDCs achieve this by\norganising data into an analysis-ready format aligned with a spatio-temporal\ngrid, facilitating user-friendly analysis and diminishing the need for\nextensive technical data processing knowledge. Despite these significant\nbenefits, the completion of the entire ESDC life cycle remains a challenging\ntask. Obstacles are not only of a technical nature but also relate to\ndomain-specific problems in Earth system research. There exist barriers to\nrealising the full potential of data collections in light of novel cloud-based\ntechnologies, particularly in curating data tailored for specific application\ndomains. These include transforming data to conform to a spatio-temporal grid\nwith minimum distortions and managing complexities such as spatio-temporal\nautocorrelation issues. Addressing these challenges is pivotal for the\neffective application of Artificial Intelligence (AI) approaches. Furthermore,\nadhering to open science principles for data dissemination, reproducibility,\nvisualisation, and reuse is crucial for fostering sustainable research.\nOvercoming these challenges offers a substantial opportunity to advance\ndata-driven Earth system research, unlocking the full potential of an\nintegrated, multidimensional view of Earth system processes. This is\nparticularly true when such research is coupled with innovative research\nparadigms and technological progress.\n","authors":["David Montero","Guido Kraemer","Anca Anghelea","Csar Aybar","Gunnar Brandt","Gustau Camps-Valls","Felix Cremer","Ida Flik","Fabian Gans","Sarah Habershon","Chaonan Ji","Teja Kattenborn","Laura Martnez-Ferrer","Francesco Martinuzzi","Martin Reinhardt","Maximilian Schting","Khalil Teber","Miguel D. Mahecha"],"pdf_url":"https://arxiv.org/pdf/2408.02348v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00938v2","updated":"2024-08-05T09:32:30Z","published":"2024-08-01T22:01:42Z","title":"CIResDiff: A Clinically-Informed Residual Diffusion Model for Predicting\n  Idiopathic Pulmonary Fibrosis Progression","summary":"  The progression of Idiopathic Pulmonary Fibrosis (IPF) significantly\ncorrelates with higher patient mortality rates. Early detection of IPF\nprogression is critical for initiating timely treatment, which can effectively\nslow down the advancement of the disease. However, the current clinical\ncriteria define disease progression requiring two CT scans with a one-year\ninterval, presenting a dilemma: a disease progression is identified only after\nthe disease has already progressed. To this end, in this paper, we develop a\nnovel diffusion model to accurately predict the progression of IPF by\ngenerating patient's follow-up CT scan from the initial CT scan. Specifically,\nfrom the clinical prior knowledge, we tailor improvements to the traditional\ndiffusion model and propose a Clinically-Informed Residual Diffusion model,\ncalled CIResDiff. The key innovations of CIResDiff include 1) performing the\ntarget region pre-registration to align the lung regions of two CT scans at\ndifferent time points for reducing the generation difficulty, 2) adopting the\nresidual diffusion instead of traditional diffusion to enable the model focus\nmore on differences (i.e., lesions) between the two CT scans rather than the\nlargely identical anatomical content, and 3) designing the clinically-informed\nprocess based on CLIP technology to integrate lung function information which\nis highly relevant to diagnosis into the reverse process for assisting\ngeneration. Extensive experiments on clinical data demonstrate that our\napproach can outperform state-of-the-art methods and effectively predict the\nprogression of IPF.\n","authors":["Caiwen Jiang","Xiaodan Xing","Zaixin Ou","Mianxin Liu","Walsh Simon","Guang Yang","Dinggang Shen"],"pdf_url":"https://arxiv.org/pdf/2408.00938v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02336v1","updated":"2024-08-05T09:19:52Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.16248v3","updated":"2024-08-05T09:05:59Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v3.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2311.01090v2","updated":"2024-08-05T08:47:19Z","published":"2023-11-02T08:55:11Z","title":"Infusion: internal diffusion for inpainting of dynamic textures and\n  complex motion","summary":"  Video inpainting is the task of filling a region in a video in a visually\nconvincing manner. It is very challenging due to the high dimensionality of the\ndata and the temporal consistency required for obtaining convincing results.\nRecently, diffusion models have shown impressive results in modeling complex\ndata distributions, including images and videos. Such models remain nonetheless\nvery expensive to train and to perform inference with, which strongly reduce\ntheir applicability to videos, and yields unreasonable computational loads. We\nshow that in the case of video inpainting, thanks to the highly auto-similar\nnature of videos, the training data of a diffusion model can be restricted to\nthe input video and still produce very satisfying results. This leads us to\nadopt an internal learning approach, which also allows us to greatly reduce the\nneural network size by about three orders of magnitude less than current\ndiffusion models used for image inpainting. We also introduce a new method for\nefficient training and inference of diffusion models in the context of internal\nlearning, by splitting the diffusion process into different learning intervals\ncorresponding to different noise levels of the diffusion process. To the best\nof our knowledge, this is the first video inpainting method based purely on\ndiffusion. Other methods require additional components such as optical flow\nestimation, which limits their performance in the case of dynamic textures and\ncomplex motions. We show qualitative and quantitative results, demonstrating\nthat our method reaches state of the art performance in the case of dynamic\ntextures and complex dynamic backgrounds.\n","authors":["Nicolas Cherel","Andrs Almansa","Yann Gousseau","Alasdair Newson"],"pdf_url":"https://arxiv.org/pdf/2311.01090v2.pdf","comment":"11 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.02307v1","updated":"2024-08-05T08:36:13Z","published":"2024-08-05T08:36:13Z","title":"Low-Cost Self-Ensembles Based on Multi-Branch Transformation and Grouped\n  Convolution","summary":"  Recent advancements in low-cost ensemble learning have demonstrated improved\nefficiency for image classification. However, the existing low-cost ensemble\nmethods show relatively lower accuracy compared to conventional ensemble\nlearning. In this paper, we propose a new low-cost ensemble learning, which can\nsimultaneously achieve high efficiency and classification performance. A CNN is\ntransformed into a multi-branch structure without introduction of additional\ncomponents, which maintains the computational complexity as that of the\noriginal single model and also enhances diversity among the branches' outputs\nvia sufficient separation between different pathways of the branches. In\naddition, we propose a new strategy that applies grouped convolution in the\nbranches with different numbers of groups in different branches, which boosts\nthe diversity of the branches' outputs. For training, we employ knowledge\ndistillation using the ensemble of the outputs as the teacher signal. The high\ndiversity among the outputs enables to form a powerful teacher, enhancing the\nindividual branch's classification performance and consequently the overall\nensemble performance. Experimental results show that our method achieves\nstate-of-the-art classification accuracy and higher uncertainty estimation\nperformance compared to previous low-cost ensemble methods. The code is\navailable at https://github.com/hjdw2/SEMBG.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02306v1","updated":"2024-08-05T08:35:59Z","published":"2024-08-05T08:35:59Z","title":"Mixture-of-Noises Enhanced Forgery-Aware Predictor for Multi-Face\n  Manipulation Detection and Localization","summary":"  With the advancement of face manipulation technology, forgery images in\nmulti-face scenarios are gradually becoming a more complex and realistic\nchallenge. Despite this, detection and localization methods for such multi-face\nmanipulations remain underdeveloped. Traditional manipulation localization\nmethods either indirectly derive detection results from localization masks,\nresulting in limited detection performance, or employ a naive two-branch\nstructure to simultaneously obtain detection and localization results, which\ncannot effectively benefit the localization capability due to limited\ninteraction between two tasks. This paper proposes a new framework, namely\nMoNFAP, specifically tailored for multi-face manipulation detection and\nlocalization. The MoNFAP primarily introduces two novel modules: the\nForgery-aware Unified Predictor (FUP) Module and the Mixture-of-Noises Module\n(MNM). The FUP integrates detection and localization tasks using a token\nlearning strategy and multiple forgery-aware transformers, which facilitates\nthe use of classification information to enhance localization capability.\nBesides, motivated by the crucial role of noise information in forgery\ndetection, the MNM leverages multiple noise extractors based on the concept of\nthe mixture of experts to enhance the general RGB features, further boosting\nthe performance of our framework. Finally, we establish a comprehensive\nbenchmark for multi-face detection and localization and the proposed\n\\textit{MoNFAP} achieves significant performance. The codes will be made\navailable.\n","authors":["Changtao Miao","Qi Chu","Tao Gong","Zhentao Tan","Zhenchao Jin","Wanyi Zhuang","Man Luo","Honggang Hu","Nenghai Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02306v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15362v2","updated":"2024-08-05T08:26:24Z","published":"2024-07-22T04:09:27Z","title":"A Multimodal Knowledge-enhanced Whole-slide Pathology Foundation Model","summary":"  Remarkable strides in computational pathology have been made in the\ntask-agnostic foundation model that advances the performance of a wide array of\ndownstream clinical tasks. Despite the promising performance, there are still\nseveral challenges. First, prior works have resorted to either vision-only or\nvision-captions data, disregarding invaluable pathology reports and gene\nexpression profiles which respectively offer distinct knowledge for versatile\nclinical applications. Second, the current progress in pathology FMs\npredominantly concentrates on the patch level, where the restricted context of\npatch-level pretraining fails to capture whole-slide patterns. Here we curated\nthe largest multimodal dataset consisting of H\\&E diagnostic whole slide images\nand their associated pathology reports and RNA-Seq data, resulting in 26,169\nslide-level modality pairs from 10,275 patients across 32 cancer types. To\nleverage these data for CPath, we propose a novel whole-slide pretraining\nparadigm which injects multimodal knowledge at the whole-slide context into the\npathology FM, called Multimodal Self-TAught PRetraining (mSTAR). The proposed\nparadigm revolutionizes the workflow of pretraining for CPath, which enables\nthe pathology FM to acquire the whole-slide context. To our knowledge, this is\nthe first attempt to incorporate multimodal knowledge at the slide level for\nenhancing pathology FMs, expanding the modelling context from unimodal to\nmultimodal knowledge and from patch-level to slide-level. To systematically\nevaluate the capabilities of mSTAR, extensive experiments including slide-level\nunimodal and multimodal applications, are conducted across 7 diverse types of\ntasks on 43 subtasks, resulting in the largest spectrum of downstream tasks.\nThe average performance in various slide-level applications consistently\ndemonstrates significant performance enhancements for mSTAR compared to SOTA\nFMs.\n","authors":["Yingxue Xu","Yihui Wang","Fengtao Zhou","Jiabo Ma","Shu Yang","Huangjing Lin","Xin Wang","Jiguang Wang","Li Liang","Anjia Han","Ronald Cheong Kin Chan","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2407.15362v2.pdf","comment":"45 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02301v1","updated":"2024-08-05T08:23:59Z","published":"2024-08-05T08:23:59Z","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","summary":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02297v1","updated":"2024-08-05T08:14:28Z","published":"2024-08-05T08:14:28Z","title":"Perception Matters: Enhancing Embodied AI with Uncertainty-Aware\n  Semantic Segmentation","summary":"  Embodied AI has made significant progress acting in unexplored environments.\nHowever, tasks such as object search have largely focused on efficient policy\nlearning. In this work, we identify several gaps in current search methods:\nThey largely focus on dated perception models, neglect temporal aggregation,\nand transfer from ground truth directly to noisy perception at test time,\nwithout accounting for the resulting overconfidence in the perceived state. We\naddress the identified problems through calibrated perception probabilities and\nuncertainty across aggregation and found decisions, thereby adapting the models\nfor sequential tasks. The resulting methods can be directly integrated with\npretrained models across a wide family of existing search approaches at no\nadditional training cost. We perform extensive evaluations of aggregation\nmethods across both different semantic perception models and policies,\nconfirming the importance of calibrated uncertainties in both the aggregation\nand found decisions. We make the code and trained models available at\nhttp://semantic-search.cs.uni-freiburg.de.\n","authors":["Sai Prasanna","Daniel Honerkamp","Kshitij Sirohi","Tim Welschehold","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2408.02297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02291v1","updated":"2024-08-05T08:00:30Z","published":"2024-08-05T08:00:30Z","title":"SelfGeo: Self-supervised and Geodesic-consistent Estimation of Keypoints\n  on Deformable Shapes","summary":"  Unsupervised 3D keypoints estimation from Point Cloud Data (PCD) is a complex\ntask, even more challenging when an object shape is deforming. As keypoints\nshould be semantically and geometrically consistent across all the 3D frames -\neach keypoint should be anchored to a specific part of the deforming shape\nirrespective of intrinsic and extrinsic motion. This paper presents, \"SelfGeo\",\na self-supervised method that computes persistent 3D keypoints of non-rigid\nobjects from arbitrary PCDs without the need of human annotations. The gist of\nSelfGeo is to estimate keypoints between frames that respect invariant\nproperties of deforming bodies. Our main contribution is to enforce that\nkeypoints deform along with the shape while keeping constant geodesic distances\namong them. This principle is then propagated to the design of a set of losses\nwhich minimization let emerge repeatable keypoints in specific semantic\nlocations of the non-rigid shape. We show experimentally that the use of\ngeodesic has a clear advantage in challenging dynamic scenes and with different\nclasses of deforming shapes (humans and animals). Code and data are available\nat: https://github.com/IIT-PAVIS/SelfGeo\n","authors":["Mohammad Zohaib","Luca Cosmo","Alessio Del Bue"],"pdf_url":"https://arxiv.org/pdf/2408.02291v1.pdf","comment":"This paper has been accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2407.07720v4","updated":"2024-08-05T07:56:29Z","published":"2024-07-10T14:53:37Z","title":"Exploiting Scale-Variant Attention for Segmenting Small Medical Objects","summary":"  Early detection and accurate diagnosis can predict the risk of malignant\ndisease transformation, thereby increasing the probability of effective\ntreatment. Identifying mild syndrome with small pathological regions serves as\nan ominous warning and is fundamental in the early diagnosis of diseases. While\ndeep learning algorithms, particularly convolutional neural networks (CNNs),\nhave shown promise in segmenting medical objects, analyzing small areas in\nmedical images remains challenging. This difficulty arises due to information\nlosses and compression defects from convolution and pooling operations in CNNs,\nwhich become more pronounced as the network deepens, especially for small\nmedical objects. To address these challenges, we propose a novel scale-variant\nattention-based network (SvANet) for accurately segmenting small-scale objects\nin medical images. The SvANet consists of scale-variant attention, cross-scale\nguidance, Monte Carlo attention, and vision transformer, which incorporates\ncross-scale features and alleviates compression artifacts for enhancing the\ndiscrimination of small medical objects. Quantitative experimental results\ndemonstrate the superior performance of SvANet, achieving 96.12%, 96.11%,\n89.79%, 84.15%, 80.25%, 73.05%, and 72.58% in mean Dice coefficient for\nsegmenting kidney tumors, skin lesions, hepatic tumors, polyps, surgical\nexcision cells, retinal vasculatures, and sperms, which occupy less than 1% of\nthe image areas in KiTS23, ISIC 2018, ATLAS, PolypGen, TissueNet, FIVES, and\nSpermHealth datasets, respectively.\n","authors":["Wei Dai","Rui Liu","Zixuan Wu","Tianyi Wu","Min Wang","Junxian Zhou","Yixuan Yuan","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2407.07720v4.pdf","comment":"14 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2408.02285v1","updated":"2024-08-05T07:37:55Z","published":"2024-08-05T07:37:55Z","title":"Joint-Motion Mutual Learning for Pose Estimation in Videos","summary":"  Human pose estimation in videos has long been a compelling yet challenging\ntask within the realm of computer vision. Nevertheless, this task remains\ndifficult because of the complex video scenes, such as video defocus and\nself-occlusion. Recent methods strive to integrate multi-frame visual features\ngenerated by a backbone network for pose estimation. However, they often ignore\nthe useful joint information encoded in the initial heatmap, which is a\nby-product of the backbone generation. Comparatively, methods that attempt to\nrefine the initial heatmap fail to consider any spatio-temporal motion\nfeatures. As a result, the performance of existing methods for pose estimation\nfalls short due to the lack of ability to leverage both local joint (heatmap)\ninformation and global motion (feature) dynamics.\n  To address this problem, we propose a novel joint-motion mutual learning\nframework for pose estimation, which effectively concentrates on both local\njoint dependency and global pixel-level motion dynamics. Specifically, we\nintroduce a context-aware joint learner that adaptively leverages initial\nheatmaps and motion flow to retrieve robust local joint feature. Given that\nlocal joint feature and global motion flow are complementary, we further\npropose a progressive joint-motion mutual learning that synergistically\nexchanges information and interactively learns between joint feature and motion\nflow to improve the capability of the model. More importantly, to capture more\ndiverse joint and motion cues, we theoretically analyze and propose an\ninformation orthogonality objective to avoid learning redundant information\nfrom multi-cues. Empirical experiments show our method outperforms prior arts\non three challenging benchmarks.\n","authors":["Sifan Wu","Haipeng Chen","Yifang Yin","Sihao Hu","Runyang Feng","Yingying Jiao","Ziqi Yang","Zhenguang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02285v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.18534v2","updated":"2024-08-05T07:37:06Z","published":"2024-07-26T06:29:09Z","title":"Boosting Cross-Domain Point Classification via Distilling Relational\n  Priors from 2D Transformers","summary":"  Semantic pattern of an object point cloud is determined by its topological\nconfiguration of local geometries. Learning discriminative representations can\nbe challenging due to large shape variations of point sets in local regions and\nincomplete surface in a global perspective, which can be made even more severe\nin the context of unsupervised domain adaptation (UDA). In specific,\ntraditional 3D networks mainly focus on local geometric details and ignore the\ntopological structure between local geometries, which greatly limits their\ncross-domain generalization. Recently, the transformer-based models have\nachieved impressive performance gain in a range of image-based tasks,\nbenefiting from its strong generalization capability and scalability stemming\nfrom capturing long range correlation across local patches. Inspired by such\nsuccesses of visual transformers, we propose a novel Relational Priors\nDistillation (RPD) method to extract relational priors from the well-trained\ntransformers on massive images, which can significantly empower cross-domain\nrepresentations with consistent topological priors of objects. To this end, we\nestablish a parameter-frozen pre-trained transformer module shared between 2D\nteacher and 3D student models, complemented by an online knowledge distillation\nstrategy for semantically regularizing the 3D student model. Furthermore, we\nintroduce a novel self-supervised task centered on reconstructing masked point\ncloud patches using corresponding masked multi-view image features, thereby\nempowering the model with incorporating 3D geometric information. Experiments\non the PointDA-10 and the Sim-to-Real datasets verify that the proposed method\nconsistently achieves the state-of-the-art performance of UDA for point cloud\nclassification. The source code of this work is available at\nhttps://github.com/zou-longkun/RPD.git.\n","authors":["Longkun Zou","Wanru Zhu","Ke Chen","Lihua Guo","Kailing Guo","Kui Jia","Yaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18534v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02284v1","updated":"2024-08-05T07:34:44Z","published":"2024-08-05T07:34:44Z","title":"Cascading Refinement Video Denoising with Uncertainty Adaptivity","summary":"  Accurate alignment is crucial for video denoising. However, estimating\nalignment in noisy environments is challenging. This paper introduces a\ncascading refinement video denoising method that can refine alignment and\nrestore images simultaneously. Better alignment enables restoration of more\ndetailed information in each frame. Furthermore, better image quality leads to\nbetter alignment. This method has achieved SOTA performance by a large margin\non the CRVD dataset. Simultaneously, aiming to deal with multi-level noise, an\nuncertainty map was created after each iteration. Because of this, redundant\ncomputation on the easily restored videos was avoided. By applying this method,\nthe entire computation was reduced by 25% on average.\n","authors":["Xinyuan Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04485v2","updated":"2024-08-05T07:18:25Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v2.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.02275v1","updated":"2024-08-05T07:10:40Z","published":"2024-08-05T07:10:40Z","title":"Geometric Algebra Meets Large Language Models: Instruction-Based\n  Transformations of Separate Meshes in 3D, Interactive and Controllable Scenes","summary":"  This paper introduces a novel integration of Large Language Models (LLMs)\nwith Conformal Geometric Algebra (CGA) to revolutionize controllable 3D scene\nediting, particularly for object repositioning tasks, which traditionally\nrequires intricate manual processes and specialized expertise. These\nconventional methods typically suffer from reliance on large training datasets\nor lack a formalized language for precise edits. Utilizing CGA as a robust\nformal language, our system, shenlong, precisely models spatial transformations\nnecessary for accurate object repositioning. Leveraging the zero-shot learning\ncapabilities of pre-trained LLMs, shenlong translates natural language\ninstructions into CGA operations which are then applied to the scene,\nfacilitating exact spatial transformations within 3D scenes without the need\nfor specialized pre-training. Implemented in a realistic simulation\nenvironment, shenlong ensures compatibility with existing graphics pipelines.\nTo accurately assess the impact of CGA, we benchmark against robust Euclidean\nSpace baselines, evaluating both latency and accuracy. Comparative performance\nevaluations indicate that shenlong significantly reduces LLM response times by\n16% and boosts success rates by 9.6% on average compared to the traditional\nmethods. Notably, shenlong achieves a 100% perfect success rate in common\npractical queries, a benchmark where other systems fall short. These\nadvancements underscore shenlong's potential to democratize 3D scene editing,\nenhancing accessibility and fostering innovation across sectors such as\neducation, digital entertainment, and virtual reality.\n","authors":["Dimitris Angelis","Prodromos Kolyvakis","Manos Kamarianakis","George Papagiannakis"],"pdf_url":"https://arxiv.org/pdf/2408.02275v1.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02272v1","updated":"2024-08-05T07:00:10Z","published":"2024-08-05T07:00:10Z","title":"COM Kitchens: An Unedited Overhead-view Video Dataset as a\n  Vision-Language Benchmark","summary":"  Procedural video understanding is gaining attention in the vision and\nlanguage community. Deep learning-based video analysis requires extensive data.\nConsequently, existing works often use web videos as training resources, making\nit challenging to query instructional contents from raw video observations. To\naddress this issue, we propose a new dataset, COM Kitchens. The dataset\nconsists of unedited overhead-view videos captured by smartphones, in which\nparticipants performed food preparation based on given recipes. Fixed-viewpoint\nvideo datasets often lack environmental diversity due to high camera setup\ncosts. We used modern wide-angle smartphone lenses to cover cooking counters\nfrom sink to cooktop in an overhead view, capturing activity without in-person\nassistance. With this setup, we collected a diverse dataset by distributing\nsmartphones to participants. With this dataset, we propose the novel\nvideo-to-text retrieval task Online Recipe Retrieval (OnRR) and new video\ncaptioning domain Dense Video Captioning on unedited Overhead-View videos\n(DVC-OV). Our experiments verified the capabilities and limitations of current\nweb-video-based SOTA methods in handling these tasks.\n","authors":["Koki Maeda","Tosho Hirasawa","Atsushi Hashimoto","Jun Harashima","Leszek Rybicki","Yusuke Fukasawa","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2408.02272v1.pdf","comment":"ECCV2024 accepted"},{"id":"http://arxiv.org/abs/2407.18289v2","updated":"2024-08-05T06:53:43Z","published":"2024-07-25T15:18:28Z","title":"MARINE: A Computer Vision Model for Detecting Rare Predator-Prey\n  Interactions in Animal Videos","summary":"  Encounters between predator and prey play an essential role in ecosystems,\nbut their rarity makes them difficult to detect in video recordings. Although\nadvances in action recognition (AR) and temporal action detection (AD),\nespecially transformer-based models and vision foundation models, have achieved\nhigh performance on human action datasets, animal videos remain relatively\nunder-researched. This thesis addresses this gap by proposing the model MARINE,\nwhich utilizes motion-based frame selection designed for fast animal actions\nand DINOv2 feature extraction with a trainable classification head for action\nrecognition. MARINE outperforms VideoMAE in identifying predator attacks in\nvideos of fish, both on a small and specific coral reef dataset (81.53\\%\nagainst 52.64\\% accuracy), and on a subset of the more extensive Animal Kingdom\ndataset (94.86\\% against 83.14\\% accuracy). In a multi-label setting on a\nrepresentative sample of Animal Kingdom, MARINE achieves 23.79\\% mAP,\npositioning it mid-field among existing benchmarks. Furthermore, in an AD task\non the coral reef dataset, MARINE achieves 80.78\\% AP (against VideoMAE's\n34.89\\%) although at a lowered t-IoU threshold of 25\\%. Therefore, despite room\nfor improvement, MARINE offers an effective starter framework to apply to AR\nand AD tasks on animal recordings and thus contribute to the study of natural\necosystems.\n","authors":["Zsfia Katona","Seyed Sahand Mohammadi Ziabari","Fatemeh Karimi Nejadasl"],"pdf_url":"https://arxiv.org/pdf/2407.18289v2.pdf","comment":"This is an MSc thesis by Zsofia Katona, supervised by the two other\n  authors"},{"id":"http://arxiv.org/abs/2407.18288v2","updated":"2024-08-05T06:50:44Z","published":"2024-07-25T14:21:35Z","title":"Leveraging Foundation Models via Knowledge Distillation in Multi-Object\n  Tracking: Distilling DINOv2 Features to FairMOT","summary":"  Multiple Object Tracking (MOT) is a computer vision task that has been\nemployed in a variety of sectors. Some common limitations in MOT are varying\nobject appearances, occlusions, or crowded scenes. To address these challenges,\nmachine learning methods have been extensively deployed, leveraging large\ndatasets, sophisticated models, and substantial computational resources. Due to\npractical limitations, access to the above is not always an option. However,\nwith the recent release of foundation models by prominent AI companies,\npretrained models have been trained on vast datasets and resources using\nstate-of-the-art methods. This work tries to leverage one such foundation\nmodel, called DINOv2, through using knowledge distillation. The proposed method\nuses a teacher-student architecture, where DINOv2 is the teacher and the\nFairMOT backbone HRNetv2 W18 is the student. The results imply that although\nthe proposed method shows improvements in certain scenarios, it does not\nconsistently outperform the original FairMOT model. These findings highlight\nthe potential and limitations of applying foundation models in knowledge\n","authors":["Niels G. Faber","Seyed Sahand Mohammadi Ziabari","Fatemeh Karimi Nejadasl"],"pdf_url":"https://arxiv.org/pdf/2407.18288v2.pdf","comment":"This is an MSc thesis by Niels Faber, supervised by the two other\n  authors"},{"id":"http://arxiv.org/abs/2408.02265v1","updated":"2024-08-05T06:42:00Z","published":"2024-08-05T06:42:00Z","title":"Explain via Any Concept: Concept Bottleneck Model with Open Vocabulary\n  Concepts","summary":"  The concept bottleneck model (CBM) is an interpretable-by-design framework\nthat makes decisions by first predicting a set of interpretable concepts, and\nthen predicting the class label based on the given concepts. Existing CBMs are\ntrained with a fixed set of concepts (concepts are either annotated by the\ndataset or queried from language models). However, this closed-world assumption\nis unrealistic in practice, as users may wonder about the role of any desired\nconcept in decision-making after the model is deployed. Inspired by the large\nsuccess of recent vision-language pre-trained models such as CLIP in zero-shot\nclassification, we propose \"OpenCBM\" to equip the CBM with open vocabulary\nconcepts via: (1) Aligning the feature space of a trainable image feature\nextractor with that of a CLIP's image encoder via a prototype based feature\nalignment; (2) Simultaneously training an image classifier on the downstream\ndataset; (3) Reconstructing the trained classification head via any set of\nuser-desired textual concepts encoded by CLIP's text encoder. To reveal\npotentially missing concepts from users, we further propose to iteratively find\nthe closest concept embedding to the residual parameters during the\nreconstruction until the residual is small enough. To the best of our\nknowledge, our \"OpenCBM\" is the first CBM with concepts of open vocabularies,\nproviding users the unique benefit such as removing, adding, or replacing any\ndesired concept to explain the model's prediction even after a model is\ntrained. Moreover, our model significantly outperforms the previous\nstate-of-the-art CBM by 9% in the classification accuracy on the benchmark\ndataset CUB-200-2011.\n","authors":["Andong Tan","Fengtao Zhou","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02265v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2305.15213v3","updated":"2024-08-05T06:40:52Z","published":"2023-05-24T14:51:18Z","title":"GTNet: Graph Transformer Network for 3D Point Cloud Classification and\n  Semantic Segmentation","summary":"  Recently, graph-based and Transformer-based deep learning networks have\ndemonstrated excellent performances on various point cloud tasks. Most of the\nexisting graph methods are based on static graph, which take a fixed input to\nestablish graph relations. Moreover, many graph methods apply maximization and\naveraging to aggregate neighboring features, so that only a single neighboring\npoint affects the feature of centroid or different neighboring points have the\nsame influence on the centroid's feature, which ignoring the correlation and\ndifference between points. Most Transformer-based methods extract point cloud\nfeatures based on global attention and lack the feature learning on local\nneighbors. To solve the problems of these two types of models, we propose a new\nfeature extraction block named Graph Transformer and construct a 3D point point\ncloud learning network called GTNet to learn features of point clouds on local\nand global patterns. Graph Transformer integrates the advantages of graph-based\nand Transformer-based methods, and consists of Local Transformer and Global\nTransformer modules. Local Transformer uses a dynamic graph to calculate all\nneighboring point weights by intra-domain cross-attention with dynamically\nupdated graph relations, so that every neighboring point could affect the\nfeatures of centroid with different weights; Global Transformer enlarges the\nreceptive field of Local Transformer by a global self-attention. In addition,\nto avoid the disappearance of the gradient caused by the increasing depth of\nnetwork, we conduct residual connection for centroid features in GTNet; we also\nadopt the features of centroid and neighbors to generate the local geometric\ndescriptors in Local Transformer to strengthen the local information learning\ncapability of the model. Finally, we use GTNet for shape classification, part\nsegmentation and semantic segmentation tasks in this paper.\n","authors":["Wei Zhou","Qian Wang","Weiwei Jin","Xinzhe Shi","Ying He"],"pdf_url":"https://arxiv.org/pdf/2305.15213v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02263v1","updated":"2024-08-05T06:38:43Z","published":"2024-08-05T06:38:43Z","title":"VoxelTrack: Exploring Voxel Representation for 3D Point Cloud Object\n  Tracking","summary":"  Current LiDAR point cloud-based 3D single object tracking (SOT) methods\ntypically rely on point-based representation network. Despite demonstrated\nsuccess, such networks suffer from some fundamental problems: 1) It contains\npooling operation to cope with inherently disordered point clouds, hindering\nthe capture of 3D spatial information that is useful for tracking, a regression\ntask. 2) The adopted set abstraction operation hardly handles\ndensity-inconsistent point clouds, also preventing 3D spatial information from\nbeing modeled. To solve these problems, we introduce a novel tracking\nframework, termed VoxelTrack. By voxelizing inherently disordered point clouds\ninto 3D voxels and extracting their features via sparse convolution blocks,\nVoxelTrack effectively models precise and robust 3D spatial information,\nthereby guiding accurate position prediction for tracked objects. Moreover,\nVoxelTrack incorporates a dual-stream encoder with cross-iterative feature\nfusion module to further explore fine-grained 3D spatial information for\ntracking. Benefiting from accurate 3D spatial information being modeled, our\nVoxelTrack simplifies tracking pipeline with a single regression loss.\nExtensive experiments are conducted on three widely-adopted datasets including\nKITTI, NuScenes and Waymo Open Dataset. The experimental results confirm that\nVoxelTrack achieves state-of-the-art performance (88.3%, 71.4% and 63.6% mean\nprecision on the three datasets, respectively), and outperforms the existing\ntrackers with a real-time speed of 36 Fps on a single TITAN RTX GPU. The source\ncode and model will be released.\n","authors":["Yuxuan Lu","Jiahao Nie","Zhiwei He","Hongjie Gu","Xudong Lv"],"pdf_url":"https://arxiv.org/pdf/2408.02263v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08651v4","updated":"2024-08-05T06:37:48Z","published":"2024-03-13T16:06:07Z","title":"HAIFIT: Fashion Image Translation for Human-to-AI Style Learning and\n  Generation","summary":"  In the realm of fashion design, sketches serve as the canvas for expressing\nan artist's distinctive drawing style and creative vision, capturing intricate\ndetails like stroke variations and texture nuances. The advent of\nsketch-to-image cross-modal translation technology has notably aided designers.\nHowever, existing methods often compromise these sketch details during image\ngeneration, resulting in images that deviate from the designer's intended\nconcept. This limitation hampers the ability to offer designers a precise\npreview of the final output. To overcome this challenge, we introduce HAIFIT, a\nnovel approach that transforms sketches into high-fidelity, lifelike clothing\nimages by integrating multi-scale features and capturing extensive feature map\ndependencies from diverse perspectives. Through extensive qualitative and\nquantitative evaluations conducted on our self-collected dataset, our method\ndemonstrates superior performance compared to existing methods in generating\nphotorealistic clothing images. Our method excels in preserving the distinctive\nstyle and intricate details essential for fashion design applications. In\naddition, our method also has obvious advantages in model training and\ninference speed, contributing to reducing designers' time costs and improving\ndesign efficiency.\n","authors":["Jianan Jiang","Xinglin Li","Weiren Yu","Di Wu"],"pdf_url":"https://arxiv.org/pdf/2403.08651v4.pdf","comment":"10 pages,8 figures"},{"id":"http://arxiv.org/abs/2408.02261v1","updated":"2024-08-05T06:32:20Z","published":"2024-08-05T06:32:20Z","title":"Cross-Domain Semantic Segmentation on Inconsistent Taxonomy using VLMs","summary":"  The challenge of semantic segmentation in Unsupervised Domain Adaptation\n(UDA) emerges not only from domain shifts between source and target images but\nalso from discrepancies in class taxonomies across domains. Traditional UDA\nresearch assumes consistent taxonomy between the source and target domains,\nthereby limiting their ability to recognize and adapt to the taxonomy of the\ntarget domain. This paper introduces a novel approach, Cross-Domain Semantic\nSegmentation on Inconsistent Taxonomy using Vision Language Models (CSI), which\neffectively performs domain-adaptive semantic segmentation even in situations\nof source-target class mismatches. CSI leverages the semantic generalization\npotential of Visual Language Models (VLMs) to create synergy with previous UDA\nmethods. It leverages segment reasoning obtained through traditional UDA\nmethods, combined with the rich semantic knowledge embedded in VLMs, to relabel\nnew classes in the target domain. This approach allows for effective adaptation\nto extended taxonomies without requiring any ground truth label for the target\ndomain. Our method has shown to be effective across various benchmarks in\nsituations of inconsistent taxonomy settings (coarse-to-fine taxonomy and open\ntaxonomy) and demonstrates consistent synergy effects when integrated with\nprevious state-of-the-art UDA methods. The implementation is available at\nhttp://github.com/jkee58/CSI.\n","authors":["Jeongkee Lim","Yusung Kim"],"pdf_url":"https://arxiv.org/pdf/2408.02261v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2312.16477v3","updated":"2024-08-05T05:51:21Z","published":"2023-12-27T08:52:41Z","title":"Group Multi-View Transformer for 3D Shape Analysis with Spatial Encoding","summary":"  In recent years, the results of view-based 3D shape recognition methods have\nsaturated, and models with excellent performance cannot be deployed on\nmemory-limited devices due to their huge size of parameters. To address this\nproblem, we introduce a compression method based on knowledge distillation for\nthis field, which largely reduces the number of parameters while preserving\nmodel performance as much as possible. Specifically, to enhance the\ncapabilities of smaller models, we design a high-performing large model called\nGroup Multi-view Vision Transformer (GMViT). In GMViT, the view-level ViT first\nestablishes relationships between view-level features. Additionally, to capture\ndeeper features, we employ the grouping module to enhance view-level features\ninto group-level features. Finally, the group-level ViT aggregates group-level\nfeatures into complete, well-formed 3D shape descriptors. Notably, in both\nViTs, we introduce spatial encoding of camera coordinates as innovative\nposition embeddings. Furthermore, we propose two compressed versions based on\nGMViT, namely GMViT-simple and GMViT-mini. To enhance the training\neffectiveness of the small models, we introduce a knowledge distillation method\nthroughout the GMViT process, where the key outputs of each GMViT component\nserve as distillation targets. Extensive experiments demonstrate the efficacy\nof the proposed method. The large model GMViT achieves excellent 3D\nclassification and retrieval results on the benchmark datasets ModelNet,\nShapeNetCore55, and MCB. The smaller models, GMViT-simple and GMViT-mini,\nreduce the parameter size by 8 and 17.6 times, respectively, and improve shape\nrecognition speed by 1.5 times on average, while preserving at least 90% of the\nclassification and retrieval performance. The code is available at\nhttps://github.com/bigdata-graph/GMViT.\n","authors":["Lixiang Xu","Qingzhe Cui","Richang Hong","Wei Xu","Enhong Chen","Xin Yuan","Chenglong Li","Yuanyan Tang"],"pdf_url":"https://arxiv.org/pdf/2312.16477v3.pdf","comment":"13pages, 8 figuers"},{"id":"http://arxiv.org/abs/2408.02250v1","updated":"2024-08-05T05:48:45Z","published":"2024-08-05T05:48:45Z","title":"Hierarchical Clustering using Reversible Binary Cellular Automata for\n  High-Dimensional Data","summary":"  This work proposes a hierarchical clustering algorithm for high-dimensional\ndatasets using the cyclic space of reversible finite cellular automata. In\ncellular automaton (CA) based clustering, if two objects belong to the same\ncycle, they are closely related and considered as part of the same cluster.\nHowever, if a high-dimensional dataset is clustered using the cycles of one CA,\nclosely related objects may belong to different cycles. This paper identifies\nthe relationship between objects in two different cycles based on the median of\nall elements in each cycle so that they can be grouped in the next stage.\nFurther, to minimize the number of intermediate clusters which in turn reduces\nthe computational cost, a rule selection strategy is taken to find the best\nrules based on information propagation and cycle structure. After encoding the\ndataset using frequency-based encoding such that the consecutive data elements\nmaintain a minimum hamming distance in encoded form, our proposed clustering\nalgorithm iterates over three stages to finally cluster the data elements into\nthe desired number of clusters given by user. This algorithm can be applied to\nvarious fields, including healthcare, sports, chemical research, agriculture,\netc. When verified over standard benchmark datasets with various performance\nmetrics, our algorithm is at par with the existing algorithms with quadratic\ntime complexity.\n","authors":["Baby C. J.","Kamalika Bhattacharjee"],"pdf_url":"https://arxiv.org/pdf/2408.02250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02245v1","updated":"2024-08-05T05:33:59Z","published":"2024-08-05T05:33:59Z","title":"Curriculum learning based pre-training using Multi-Modal Contrastive\n  Masked Autoencoders","summary":"  In this paper, we propose a new pre-training method for image understanding\ntasks under Curriculum Learning (CL) paradigm which leverages RGB-D. The method\nutilizes Multi-Modal Contrastive Masked Autoencoder and Denoising techniques.\nRecent approaches either use masked autoencoding (e.g., MultiMAE) or\ncontrastive learning(e.g., Pri3D, or combine them in a single contrastive\nmasked autoencoder architecture such as CMAE and CAV-MAE. However, none of the\nsingle contrastive masked autoencoder is applicable to RGB-D datasets. To\nimprove the performance and efficacy of such methods, we propose a new\npre-training strategy based on CL. Specifically, in the first stage, we\npre-train the model using contrastive learning to learn cross-modal\nrepresentations. In the second stage, we initialize the modality-specific\nencoders using the weights from the first stage and then pre-train the model\nusing masked autoencoding and denoising/noise prediction used in diffusion\nmodels. Masked autoencoding focuses on reconstructing the missing patches in\nthe input modality using local spatial correlations, while denoising learns\nhigh frequency components of the input data. Our approach is scalable, robust\nand suitable for pre-training with limited RGB-D datasets. Extensive\nexperiments on multiple datasets such as ScanNet, NYUv2 and SUN RGB-D show the\nefficacy and superior performance of our approach. Specifically, we show an\nimprovement of +1.0% mIoU against Mask3D on ScanNet semantic segmentation. We\nfurther demonstrate the effectiveness of our approach in low-data regime by\nevaluating it for semantic segmentation task against the state-of-the-art\nmethods.\n","authors":["Muhammad Abdullah Jamal","Omid Mohareri"],"pdf_url":"https://arxiv.org/pdf/2408.02245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02244v1","updated":"2024-08-05T05:30:36Z","published":"2024-08-05T05:30:36Z","title":"Evaluating Vision-Language Models for Zero-Shot Detection,\n  Classification, and Association of Motorcycles, Passengers, and Helmets","summary":"  Motorcycle accidents pose significant risks, particularly when riders and\npassengers do not wear helmets. This study evaluates the efficacy of an\nadvanced vision-language foundation model, OWLv2, in detecting and classifying\nvarious helmet-wearing statuses of motorcycle occupants using video data. We\nextend the dataset provided by the CVPR AI City Challenge and employ a cascaded\nmodel approach for detection and classification tasks, integrating OWLv2 and\nCNN models. The results highlight the potential of zero-shot learning to\naddress challenges arising from incomplete and biased training datasets,\ndemonstrating the usage of such models in detecting motorcycles, helmet usage,\nand occupant positions under varied conditions. We have achieved an average\nprecision of 0.5324 for helmet detection and provided precision-recall curves\ndetailing the detection and classification performance. Despite limitations\nsuch as low-resolution data and poor visibility, our research shows promising\nadvancements in automated vehicle safety and traffic safety enforcement\nsystems.\n","authors":["Lucas Choi","Ross Greer"],"pdf_url":"https://arxiv.org/pdf/2408.02244v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.11070v2","updated":"2024-08-05T05:13:06Z","published":"2024-04-17T04:59:36Z","title":"Sky-GVIO: an enhanced GNSS/INS/Vision navigation with FCN-based\n  sky-segmentation in urban canyon","summary":"  Accurate, continuous, and reliable positioning is a critical component of\nachieving autonomous driving. However, in complex urban canyon environments,\nthe vulnerability of a stand-alone sensor and non-line-of-sight (NLOS) caused\nby high buildings, trees, and elevated structures seriously affect positioning\nresults. To address these challenges, a sky-view images segmentation algorithm\nbased on Fully Convolutional Network (FCN) is proposed for GNSS NLOS detection.\nBuilding upon this, a novel NLOS detection and mitigation algorithm (named\nS-NDM) is extended to the tightly coupled Global Navigation Satellite Systems\n(GNSS), Inertial Measurement Units (IMU), and visual feature system which is\ncalled Sky-GVIO, with the aim of achieving continuous and accurate positioning\nin urban canyon environments. Furthermore, the system harmonizes Single Point\nPositioning (SPP) with Real-Time Kinematic (RTK) methodologies to bolster its\noperational versatility and resilience. In urban canyon environments, the\npositioning performance of S-NDM algorithm proposed in this paper is evaluated\nunder different tightly coupled SPP-related and RTK-related models. The results\nexhibit that Sky-GVIO system achieves meter-level accuracy under SPP mode and\nsub-decimeter precision with RTK, surpassing the performance of GNSS/INS/Vision\nframeworks devoid of S-NDM. Additionally, the sky-view image dataset, inclusive\nof training and evaluation subsets, has been made publicly accessible for\nscholarly exploration at https://github.com/whuwangjr/sky-view-images .\n","authors":["Jingrong Wang","Bo Xu","Ronghe Jin","Shoujian Zhang","Kefu Gao","Jingnan Liu"],"pdf_url":"https://arxiv.org/pdf/2404.11070v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02231v1","updated":"2024-08-05T04:51:46Z","published":"2024-08-05T04:51:46Z","title":"REVISION: Rendering Tools Enable Spatial Fidelity in Vision-Language\n  Models","summary":"  Text-to-Image (T2I) and multimodal large language models (MLLMs) have been\nadopted in solutions for several computer vision and multimodal learning tasks.\nHowever, it has been found that such vision-language models lack the ability to\ncorrectly reason over spatial relationships. To tackle this shortcoming, we\ndevelop the REVISION framework which improves spatial fidelity in\nvision-language models. REVISION is a 3D rendering based pipeline that\ngenerates spatially accurate synthetic images, given a textual prompt. REVISION\nis an extendable framework, which currently supports 100+ 3D assets, 11 spatial\nrelationships, all with diverse camera perspectives and backgrounds. Leveraging\nimages from REVISION as additional guidance in a training-free manner\nconsistently improves the spatial consistency of T2I models across all spatial\nrelationships, achieving competitive performance on the VISOR and T2I-CompBench\nbenchmarks. We also design RevQA, a question-answering benchmark to evaluate\nthe spatial reasoning abilities of MLLMs, and find that state-of-the-art models\nare not robust to complex spatial reasoning under adversarial settings. Our\nresults and findings indicate that utilizing rendering-based frameworks is an\neffective approach for developing spatially-aware generative models.\n","authors":["Agneet Chatterjee","Yiran Luo","Tejas Gokhale","Yezhou Yang","Chitta Baral"],"pdf_url":"https://arxiv.org/pdf/2408.02231v1.pdf","comment":"Accepted to ECCV 2024. Project Page :\n  https://agneetchatterjee.com/revision/"},{"id":"http://arxiv.org/abs/2408.02226v1","updated":"2024-08-05T04:10:52Z","published":"2024-08-05T04:10:52Z","title":"ProCreate, Dont Reproduce! Propulsive Energy Diffusion for Creative\n  Generation","summary":"  In this paper, we propose ProCreate, a simple and easy-to-implement method to\nimprove sample diversity and creativity of diffusion-based image generative\nmodels and to prevent training data reproduction. ProCreate operates on a set\nof reference images and actively propels the generated image embedding away\nfrom the reference embeddings during the generation process. We propose FSCG-8\n(Few-Shot Creative Generation 8), a few-shot creative generation dataset on\neight different categories -- encompassing different concepts, styles, and\nsettings -- in which ProCreate achieves the highest sample diversity and\nfidelity. Furthermore, we show that ProCreate is effective at preventing\nreplicating training data in a large-scale evaluation using training text\nprompts. Code and FSCG-8 are available at\nhttps://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The\nproject page is available at https://procreate-diffusion.github.io.\n","authors":["Jack Lu","Ryan Teehan","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2408.02226v1.pdf","comment":"Accepted for ECCV 2024. Project page:\n  https://procreate-diffusion.github.io"},{"id":"http://arxiv.org/abs/2408.02222v1","updated":"2024-08-05T03:54:40Z","published":"2024-08-05T03:54:40Z","title":"Cross-modulated Attention Transformer for RGBT Tracking","summary":"  Existing Transformer-based RGBT trackers achieve remarkable performance\nbenefits by leveraging self-attention to extract uni-modal features and\ncross-attention to enhance multi-modal feature interaction and template-search\ncorrelation computation. Nevertheless, the independent search-template\ncorrelation calculations ignore the consistency between branches, which can\nresult in ambiguous and inappropriate correlation weights. It not only limits\nthe intra-modal feature representation, but also harms the robustness of\ncross-attention for multi-modal feature interaction and search-template\ncorrelation computation. To address these issues, we propose a novel approach\ncalled Cross-modulated Attention Transformer (CAFormer), which performs\nintra-modality self-correlation, inter-modality feature interaction, and\nsearch-template correlation computation in a unified attention model, for RGBT\ntracking. In particular, we first independently generate correlation maps for\neach modality and feed them into the designed Correlation Modulated Enhancement\nmodule, modulating inaccurate correlation weights by seeking the consensus\nbetween modalities. Such kind of design unifies self-attention and\ncross-attention schemes, which not only alleviates inaccurate attention weight\ncomputation in self-attention but also eliminates redundant computation\nintroduced by extra cross-attention scheme. In addition, we propose a\ncollaborative token elimination strategy to further improve tracking inference\nefficiency and accuracy. Extensive experiments on five public RGBT tracking\nbenchmarks show the outstanding performance of the proposed CAFormer against\nstate-of-the-art methods.\n","authors":["Yun Xiao","Jiacong Zhao","Andong Lu","Chenglong Li","Yin Lin","Bing Yin","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02222v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.02214v1","updated":"2024-08-05T03:33:00Z","published":"2024-08-05T03:33:00Z","title":"More Than Positive and Negative: Communicating Fine Granularity in\n  Medical Diagnosis","summary":"  With the advance of deep learning, much progress has been made in building\npowerful artificial intelligence (AI) systems for automatic Chest X-ray (CXR)\nanalysis. Most existing AI models are trained to be a binary classifier with\nthe aim of distinguishing positive and negative cases. However, a large gap\nexists between the simple binary setting and complicated real-world medical\nscenarios. In this work, we reinvestigate the problem of automatic radiology\ndiagnosis. We first observe that there is considerable diversity among cases\nwithin the positive class, which means simply classifying them as positive\nloses many important details. This motivates us to build AI models that can\ncommunicate fine-grained knowledge from medical images like human experts. To\nthis end, we first propose a new benchmark on fine granularity learning from\nmedical images. Specifically, we devise a division rule based on medical\nknowledge to divide positive cases into two subcategories, namely atypical\npositive and typical positive. Then, we propose a new metric termed\nAUC$^\\text{FG}$ on the two subcategories for evaluation of the ability to\nseparate them apart. With the proposed benchmark, we encourage the community to\ndevelop AI diagnosis systems that could better learn fine granularity from\nmedical images. Last, we propose a simple risk modulation approach to this\nproblem by only using coarse labels in training. Empirical results show that\ndespite its simplicity, the proposed method achieves superior performance and\nthus serves as a strong baseline.\n","authors":["Xiangyu Peng","Kai Wang","Jianfei Yang","Yingying Zhu","Yang You"],"pdf_url":"https://arxiv.org/pdf/2408.02214v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02210v1","updated":"2024-08-05T03:22:10Z","published":"2024-08-05T03:22:10Z","title":"ExoViP: Step-by-step Verification and Exploration with Exoskeleton\n  Modules for Compositional Visual Reasoning","summary":"  Compositional visual reasoning methods, which translate a complex query into\na structured composition of feasible visual tasks, have exhibited a strong\npotential in complicated multi-modal tasks. Empowered by recent advances in\nlarge language models (LLMs), this multi-modal challenge has been brought to a\nnew stage by treating LLMs as few-shot/zero-shot planners, i.e.,\nvision-language (VL) programming. Such methods, despite their numerous merits,\nsuffer from challenges due to LLM planning mistakes or inaccuracy of visual\nexecution modules, lagging behind the non-compositional models. In this work,\nwe devise a \"plug-and-play\" method, ExoViP, to correct errors in both the\nplanning and execution stages through introspective verification. We employ\nverification modules as \"exoskeletons\" to enhance current VL programming\nschemes. Specifically, our proposed verification module utilizes a mixture of\nthree sub-verifiers to validate predictions after each reasoning step,\nsubsequently calibrating the visual module predictions and refining the\nreasoning trace planned by LLMs. Experimental results on two representative VL\nprogramming methods showcase consistent improvements on five compositional\nreasoning tasks on standard benchmarks. In light of this, we believe that\nExoViP can foster better performance and generalization on open-domain\nmulti-modal challenges.\n","authors":["Yuxuan Wang","Alan Yuille","Zhuowan Li","Zilong Zheng"],"pdf_url":"https://arxiv.org/pdf/2408.02210v1.pdf","comment":"To Appear at COLM 2024"},{"id":"http://arxiv.org/abs/2408.02209v1","updated":"2024-08-05T03:18:58Z","published":"2024-08-05T03:18:58Z","title":"Source-Free Domain-Invariant Performance Prediction","summary":"  Accurately estimating model performance poses a significant challenge,\nparticularly in scenarios where the source and target domains follow different\ndata distributions. Most existing performance prediction methods heavily rely\non the source data in their estimation process, limiting their applicability in\na more realistic setting where only the trained model is accessible. The few\nmethods that do not require source data exhibit considerably inferior\nperformance. In this work, we propose a source-free approach centred on\nuncertainty-based estimation, using a generative model for calibration in the\nabsence of source data. We establish connections between our approach for\nunsupervised calibration and temperature scaling. We then employ a\ngradient-based strategy to evaluate the correctness of the calibrated\npredictions. Our experiments on benchmark object recognition datasets reveal\nthat existing source-based methods fall short with limited source sample\navailability. Furthermore, our approach significantly outperforms the current\nstate-of-the-art source-free and source-based methods, affirming its\neffectiveness in domain-invariant performance estimation.\n","authors":["Ekaterina Khramtsova","Mahsa Baktashmotlagh","Guido Zuccon","Xi Wang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.02209v1.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2402.17521v3","updated":"2024-08-05T03:16:03Z","published":"2024-02-27T14:05:05Z","title":"AVS-Net: Point Sampling with Adaptive Voxel Size for 3D Scene\n  Understanding","summary":"  The recent advancements in point cloud learning have enabled intelligent\nvehicles and robots to comprehend 3D environments better. However, processing\nlarge-scale 3D scenes remains a challenging problem, such that efficient\ndownsampling methods play a crucial role in point cloud learning. Existing\ndownsampling methods either require a huge computational burden or sacrifice\nfine-grained geometric information. For such purpose, this paper presents an\nadvanced sampler that achieves both high accuracy and efficiency. The proposed\nmethod utilizes voxel centroid sampling as a foundation but effectively\naddresses the challenges regarding voxel size determination and the\npreservation of critical geometric cues. Specifically, we propose a Voxel\nAdaptation Module that adaptively adjusts voxel sizes with the reference of\npoint-based downsampling ratio. This ensures that the sampling results exhibit\na favorable distribution for comprehending various 3D objects or scenes.\nMeanwhile, we introduce a network compatible with arbitrary voxel sizes for\nsampling and feature extraction while maintaining high efficiency. The proposed\napproach is demonstrated with 3D object detection and 3D semantic segmentation.\nCompared to existing state-of-the-art methods, our approach achieves better\naccuracy on outdoor and indoor large-scale datasets, e.g. Waymo and ScanNet,\nwith promising efficiency.\n","authors":["Hongcheng Yang","Dingkang Liang","Dingyuan Zhang","Zhe Liu","Zhikang Zou","Xingyu Jiang","Yingying Zhu"],"pdf_url":"https://arxiv.org/pdf/2402.17521v3.pdf","comment":"11 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07950v2","updated":"2024-08-05T02:45:42Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v2.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2305.03713v3","updated":"2024-08-05T02:38:33Z","published":"2023-05-05T17:54:34Z","title":"Avatar Fingerprinting for Authorized Use of Synthetic Talking-Head\n  Videos","summary":"  Modern avatar generators allow anyone to synthesize photorealistic real-time\ntalking avatars, ushering in a new era of avatar-based human communication,\nsuch as with immersive AR/VR interactions or videoconferencing with limited\nbandwidths. Their safe adoption, however, requires a mechanism to verify if the\nrendered avatar is trustworthy: does it use the appearance of an individual\nwithout their consent? We term this task avatar fingerprinting. To tackle it,\nwe first introduce a large-scale dataset of real and synthetic videos of people\ninteracting on a video call, where the synthetic videos are generated using the\nfacial appearance of one person and the expressions of another. We verify the\nidentity driving the expressions in a synthetic video, by learning motion\nsignatures that are independent of the facial appearance shown. Our solution,\nthe first in this space, achieves an average AUC of 0.85. Critical to its\npractical use, it also generalizes to new generators never seen in training\n(average AUC of 0.83). The proposed dataset and other resources can be found\nat: https://research.nvidia.com/labs/nxp/avatar-fingerprinting/.\n","authors":["Ekta Prashnani","Koki Nagano","Shalini De Mello","David Luebke","Orazio Gallo"],"pdf_url":"https://arxiv.org/pdf/2305.03713v3.pdf","comment":"26 pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02192v1","updated":"2024-08-05T02:37:59Z","published":"2024-08-05T02:37:59Z","title":"Unsupervised Domain Adaption Harnessing Vision-Language Pre-training","summary":"  This paper addresses two vital challenges in Unsupervised Domain Adaptation\n(UDA) with a focus on harnessing the power of Vision-Language Pre-training\n(VLP) models. Firstly, UDA has primarily relied on ImageNet pre-trained models.\nHowever, the potential of VLP models in UDA remains largely unexplored. The\nrich representation of VLP models holds significant promise for enhancing UDA\ntasks. To address this, we propose a novel method called Cross-Modal Knowledge\nDistillation (CMKD), leveraging VLP models as teacher models to guide the\nlearning process in the target domain, resulting in state-of-the-art\nperformance. Secondly, current UDA paradigms involve training separate models\nfor each task, leading to significant storage overhead and impractical model\ndeployment as the number of transfer tasks grows. To overcome this challenge,\nwe introduce Residual Sparse Training (RST) exploiting the benefits conferred\nby VLP's extensive pre-training, a technique that requires minimal adjustment\n(approximately 0.1\\%$\\sim$0.5\\%) of VLP model parameters to achieve performance\ncomparable to fine-tuning. Combining CMKD and RST, we present a comprehensive\nsolution that effectively leverages VLP models for UDA tasks while reducing\nstorage overhead for model deployment. Furthermore, CMKD can serve as a\nbaseline in conjunction with other methods like FixMatch, enhancing the\nperformance of UDA. Our proposed method outperforms existing techniques on\nstandard benchmarks. Our code will be available at:\nhttps://github.com/Wenlve-Zhou/VLP-UDA.\n","authors":["Wenlve Zhou","Zhiheng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02192v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02191v1","updated":"2024-08-05T02:35:13Z","published":"2024-08-05T02:35:13Z","title":"Dense Feature Interaction Network for Image Inpainting Localization","summary":"  Image inpainting, which is the task of filling in missing areas in an image,\nis a common image editing technique. Inpainting can be used to conceal or alter\nimage contents in malicious manipulation of images, driving the need for\nresearch in image inpainting detection. Existing methods mostly rely on a basic\nencoder-decoder structure, which often results in a high number of false\npositives or misses the inpainted regions, especially when dealing with targets\nof varying semantics and scales. Additionally, the absence of an effective\napproach to capture boundary artifacts leads to less accurate edge\nlocalization. In this paper, we describe a new method for inpainting detection\nbased on a Dense Feature Interaction Network (DeFI-Net). DeFI-Net uses a novel\nfeature pyramid architecture to capture and amplify multi-scale representations\nacross various stages, thereby improving the detection of image inpainting by\nbetter revealing feature-level interactions. Additionally, the network can\nadaptively direct the lower-level features, which carry edge and shape\ninformation, to refine the localization of manipulated regions while\nintegrating the higher-level semantic features. Using DeFI-Net, we develop a\nmethod combining complementary representations to accurately identify inpainted\nareas. Evaluation on five image inpainting datasets demonstrate the\neffectiveness of our approach, which achieves state-of-the-art performance in\ndetecting inpainting across diverse models.\n","authors":["Ye Yao","Tingfeng Han","Shan Jia","Siwei Lyu"],"pdf_url":"https://arxiv.org/pdf/2408.02191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01355v2","updated":"2024-08-05T02:14:54Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v2.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"},{"id":"http://arxiv.org/abs/2312.10993v3","updated":"2024-08-05T01:58:58Z","published":"2023-12-18T07:44:40Z","title":"Realistic Human Motion Generation with Cross-Diffusion Models","summary":"  We introduce the Cross Human Motion Diffusion Model (CrossDiff), a novel\napproach for generating high-quality human motion based on textual\ndescriptions. Our method integrates 3D and 2D information using a shared\ntransformer network within the training of the diffusion model, unifying motion\nnoise into a single feature space. This enables cross-decoding of features into\nboth 3D and 2D motion representations, regardless of their original dimension.\nThe primary advantage of CrossDiff is its cross-diffusion mechanism, which\nallows the model to reverse either 2D or 3D noise into clean motion during\ntraining. This capability leverages the complementary information in both\nmotion representations, capturing intricate human movement details often missed\nby models relying solely on 3D information. Consequently, CrossDiff effectively\ncombines the strengths of both representations to generate more realistic\nmotion sequences. In our experiments, our model demonstrates competitive\nstate-of-the-art performance on text-to-motion benchmarks. Moreover, our method\nconsistently provides enhanced motion generation quality, capturing complex\nfull-body movement intricacies. Additionally, with a pretrained model,our\napproach accommodates using in the wild 2D motion data without 3D motion ground\ntruth during training to generate 3D motion, highlighting its potential for\nbroader applications and efficient use of available data resources. Project\npage: https://wonderno.github.io/CrossDiff-webpage/.\n","authors":["Zeping Ren","Shaoli Huang","Xiu Li"],"pdf_url":"https://arxiv.org/pdf/2312.10993v3.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.02181v1","updated":"2024-08-05T01:50:09Z","published":"2024-08-05T01:50:09Z","title":"AssemAI: Interpretable Image-Based Anomaly Detection for Manufacturing\n  Pipelines","summary":"  Anomaly detection in manufacturing pipelines remains a critical challenge,\nintensified by the complexity and variability of industrial environments. This\npaper introduces AssemAI, an interpretable image-based anomaly detection system\ntailored for smart manufacturing pipelines. Our primary contributions include\nthe creation of a tailored image dataset and the development of a custom object\ndetection model, YOLO-FF, designed explicitly for anomaly detection in\nmanufacturing assembly environments. Utilizing the preprocessed image dataset\nderived from an industry-focused rocket assembly pipeline, we address the\nchallenge of imbalanced image data and demonstrate the importance of\nimage-based methods in anomaly detection. The proposed approach leverages\ndomain knowledge in data preparation, model development and reasoning. We\ncompare our method against several baselines, including simple CNN and custom\nVisual Transformer (ViT) models, showcasing the effectiveness of our custom\ndata preparation and pretrained CNN integration. Additionally, we incorporate\nexplainability techniques at both user and model levels, utilizing ontology for\nuser-friendly explanations and SCORE-CAM for in-depth feature and model\nanalysis. Finally, the model was also deployed in a real-time setting. Our\nresults include ablation studies on the baselines, providing a comprehensive\nevaluation of the proposed system. This work highlights the broader impact of\nadvanced image-based anomaly detection in enhancing the reliability and\nefficiency of smart manufacturing processes.\n","authors":["Renjith Prasad","Chathurangi Shyalika","Ramtin Zand","Fadi El Kalach","Revathy Venkataramanan","Ramy Harik","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2408.02181v1.pdf","comment":"8 Pages, 6 Figures, 4 Tables"},{"id":"http://arxiv.org/abs/2407.20937v2","updated":"2024-08-05T01:27:39Z","published":"2024-07-30T16:19:14Z","title":"EAR: Edge-Aware Reconstruction of 3-D vertebrae structures from\n  bi-planar X-ray images","summary":"  X-ray images ease the diagnosis and treatment process due to their rapid\nimaging speed and high resolution. However, due to the projection process of\nX-ray imaging, much spatial information has been lost. To accurately provide\nefficient spinal morphological and structural information, reconstructing the\n3-D structures of the spine from the 2-D X-ray images is essential. It is\nchallenging for current reconstruction methods to preserve the edge information\nand local shapes of the asymmetrical vertebrae structures. In this study, we\npropose a new Edge-Aware Reconstruction network (EAR) to focus on the\nperformance improvement of the edge information and vertebrae shapes. In our\nnetwork, by using the auto-encoder architecture as the backbone, the edge\nattention module and frequency enhancement module are proposed to strengthen\nthe perception of the edge reconstruction. Meanwhile, we also combine four loss\nterms, including reconstruction loss, edge loss, frequency loss and projection\nloss. The proposed method is evaluated using three publicly accessible datasets\nand compared with four state-of-the-art models. The proposed method is superior\nto other methods and achieves 25.32%, 15.32%, 86.44%, 80.13%, 23.7612 and\n0.3014 with regard to MSE, MAE, Dice, SSIM, PSNR and frequency distance. Due to\nthe end-to-end and accurate reconstruction process, EAR can provide sufficient\n3-D spatial information and precise preoperative surgical planning guidance.\n","authors":["Lixing Tan","Shuang Song","Yaofeng He","Kangneng Zhou","Tong Lu","Ruoxiu Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.20937v2.pdf","comment":"13 pages, 11 figures, 3 tables"},{"id":"http://arxiv.org/abs/2311.03572v2","updated":"2024-08-05T01:05:54Z","published":"2023-11-06T22:17:18Z","title":"Unsupervised Region-Growing Network for Object Segmentation in\n  Atmospheric Turbulence","summary":"  Moving object segmentation in the presence of atmospheric turbulence is\nhighly challenging due to turbulence-induced irregular and time-varying\ndistortions. In this paper, we present an unsupervised approach for segmenting\nmoving objects in videos downgraded by atmospheric turbulence. Our key approach\nis a detect-then-grow scheme: we first identify a small set of moving object\npixels with high confidence, then gradually grow a foreground mask from those\nseeds to segment all moving objects. This method leverages rigid geometric\nconsistency among video frames to disentangle different types of motions, and\nthen uses the Sampson distance to initialize the seedling pixels. After growing\nper-frame foreground masks, we use spatial grouping loss and temporal\nconsistency loss to further refine the masks in order to ensure their\nspatio-temporal consistency. Our method is unsupervised and does not require\ntraining on labeled data. For validation, we collect and release the first\nreal-captured long-range turbulent video dataset with ground truth masks for\nmoving objects. Results show that our method achieves good accuracy in\nsegmenting moving objects and is robust for long-range videos with various\nturbulence strengths.\n","authors":["Dehao Qin","Ripon Saha","Suren Jayasuriya","Jinwei Ye","Nianyi Li"],"pdf_url":"https://arxiv.org/pdf/2311.03572v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18302v2","updated":"2024-08-05T23:48:12Z","published":"2024-02-28T12:50:16Z","title":"EchoTrack: Auditory Referring Multi-Object Tracking for Autonomous\n  Driving","summary":"  This paper introduces the task of Auditory Referring Multi-Object Tracking\n(AR-MOT), which dynamically tracks specific objects in a video sequence based\non audio expressions and appears as a challenging problem in autonomous\ndriving. Due to the lack of semantic modeling capacity in audio and video,\nexisting works have mainly focused on text-based multi-object tracking, which\noften comes at the cost of tracking quality, interaction efficiency, and even\nthe safety of assistance systems, limiting the application of such methods in\nautonomous driving. In this paper, we delve into the problem of AR-MOT from the\nperspective of audio-video fusion and audio-video tracking. We put forward\nEchoTrack, an end-to-end AR-MOT framework with dual-stream vision transformers.\nThe dual streams are intertwined with our Bidirectional Frequency-domain\nCross-attention Fusion Module (Bi-FCFM), which bidirectionally fuses audio and\nvideo features from both frequency- and spatiotemporal domains. Moreover, we\npropose the Audio-visual Contrastive Tracking Learning (ACTL) regime to extract\nhomogeneous semantic features between expressions and visual objects by\nlearning homogeneous features between different audio and video objects\neffectively. Aside from the architectural design, we establish the first set of\nlarge-scale AR-MOT benchmarks, including Echo-KITTI, Echo-KITTI+, and Echo-BDD.\nExtensive experiments on the established benchmarks demonstrate the\neffectiveness of the proposed EchoTrack and its components. The source code and\ndatasets are available at https://github.com/lab206/EchoTrack.\n","authors":["Jiacheng Lin","Jiajun Chen","Kunyu Peng","Xuan He","Zhiyong Li","Rainer Stiefelhagen","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2402.18302v2.pdf","comment":"Accepted to IEEE Transactions on Intelligent Transportation Systems\n  (T-ITS). The source code and datasets are available at\n  https://github.com/lab206/EchoTrack"},{"id":"http://arxiv.org/abs/2408.02865v1","updated":"2024-08-05T23:31:07Z","published":"2024-08-05T23:31:07Z","title":"VisionUnite: A Vision-Language Foundation Model for Ophthalmology\n  Enhanced with Clinical Knowledge","summary":"  The need for improved diagnostic methods in ophthalmology is acute,\nespecially in the less developed regions with limited access to specialists and\nadvanced equipment. Therefore, we introduce VisionUnite, a novel\nvision-language foundation model for ophthalmology enhanced with clinical\nknowledge. VisionUnite has been pretrained on an extensive dataset comprising\n1.24 million image-text pairs, and further refined using our proposed MMFundus\ndataset, which includes 296,379 high-quality fundus image-text pairs and\n889,137 simulated doctor-patient dialogue instances. Our experiments indicate\nthat VisionUnite outperforms existing generative foundation models such as\nGPT-4V and Gemini Pro. It also demonstrates diagnostic capabilities comparable\nto junior ophthalmologists. VisionUnite performs well in various clinical\nscenarios including open-ended multi-disease diagnosis, clinical explanation,\nand patient interaction, making it a highly versatile tool for initial\nophthalmic disease screening. VisionUnite can also serve as an educational aid\nfor junior ophthalmologists, accelerating their acquisition of knowledge\nregarding both common and rare ophthalmic conditions. VisionUnite represents a\nsignificant advancement in ophthalmology, with broad implications for\ndiagnostics, medical education, and understanding of disease mechanisms.\n","authors":["Zihan Li","Diping Song","Zefeng Yang","Deming Wang","Fei Li","Xiulan Zhang","Paul E. Kinahan","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.02865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02859v1","updated":"2024-08-05T22:59:50Z","published":"2024-08-05T22:59:50Z","title":"Multistain Pretraining for Slide Representation Learning in Pathology","summary":"  Developing self-supervised learning (SSL) models that can learn universal and\ntransferable representations of H&E gigapixel whole-slide images (WSIs) is\nbecoming increasingly valuable in computational pathology. These models hold\nthe potential to advance critical tasks such as few-shot classification, slide\nretrieval, and patient stratification. Existing approaches for slide\nrepresentation learning extend the principles of SSL from small images (e.g.,\n224 x 224 patches) to entire slides, usually by aligning two different\naugmentations (or views) of the slide. Yet the resulting representation remains\nconstrained by the limited clinical and biological diversity of the views.\nInstead, we postulate that slides stained with multiple markers, such as\nimmunohistochemistry, can be used as different views to form a rich\ntask-agnostic training signal. To this end, we introduce Madeleine, a\nmultimodal pretraining strategy for slide representation learning. Madeleine is\ntrained with a dual global-local cross-stain alignment objective on large\ncohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney\ntransplant samples (N=12,070 WSIs across four stains). We demonstrate the\nquality of slide representations learned by Madeleine on various downstream\nevaluations, ranging from morphological and molecular classification to\nprognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple\nmedical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.\n","authors":["Guillaume Jaume","Anurag Vaidya","Andrew Zhang","Andrew H. Song","Richard J. Chen","Sharifa Sahai","Dandan Mo","Emilio Madrigal","Long Phi Le","Faisal Mahmood"],"pdf_url":"https://arxiv.org/pdf/2408.02859v1.pdf","comment":"ECCV'24"},{"id":"http://arxiv.org/abs/2408.02855v1","updated":"2024-08-05T22:49:20Z","published":"2024-08-05T22:49:20Z","title":"Analyzing Data Efficiency and Performance of Machine Learning Algorithms\n  for Assessing Low Back Pain Physical Rehabilitation Exercises","summary":"  Analyzing human motion is an active research area, with various applications.\nIn this work, we focus on human motion analysis in the context of physical\nrehabilitation using a robot coach system. Computer-aided assessment of\nphysical rehabilitation entails evaluation of patient performance in completing\nprescribed rehabilitation exercises, based on processing movement data captured\nwith a sensory system, such as RGB and RGB-D cameras. As 2D and 3D human pose\nestimation from RGB images had made impressive improvements, we aim to compare\nthe assessment of physical rehabilitation exercises using movement data\nobtained from both RGB-D camera (Microsoft Kinect) and estimation from RGB\nvideos (OpenPose and BlazePose algorithms). A Gaussian Mixture Model (GMM) is\nemployed from position (and orientation) features, with performance metrics\ndefined based on the log-likelihood values from GMM. The evaluation is\nperformed on a medical database of clinical patients carrying out low back-pain\nrehabilitation exercises, previously coached by robot Poppy.\n","authors":["Aleksa Marusic","Louis Annabi","Sao Msi Nguyen","Adriana Tapus"],"pdf_url":"https://arxiv.org/pdf/2408.02855v1.pdf","comment":"European Conference on Mobile Robots (2023)"},{"id":"http://arxiv.org/abs/2408.02840v1","updated":"2024-08-05T21:29:33Z","published":"2024-08-05T21:29:33Z","title":"GAReT: Cross-view Video Geolocalization with Adapters and\n  Auto-Regressive Transformers","summary":"  Cross-view video geo-localization (CVGL) aims to derive GPS trajectories from\nstreet-view videos by aligning them with aerial-view images. Despite their\npromising performance, current CVGL methods face significant challenges. These\nmethods use camera and odometry data, typically absent in real-world scenarios.\nThey utilize multiple adjacent frames and various encoders for feature\nextraction, resulting in high computational costs. Moreover, these approaches\nindependently predict each street-view frame's location, resulting in\ntemporally inconsistent GPS trajectories. To address these challenges, in this\nwork, we propose GAReT, a fully transformer-based method for CVGL that does not\nrequire camera and odometry data. We introduce GeoAdapter, a\ntransformer-adapter module designed to efficiently aggregate image-level\nrepresentations and adapt them for video inputs. Specifically, we train a\ntransformer encoder on video frames and aerial images, then freeze the encoder\nto optimize the GeoAdapter module to obtain video-level representation. To\naddress temporally inconsistent trajectories, we introduce TransRetriever, an\nencoder-decoder transformer model that predicts GPS locations of street-view\nframes by encoding top-k nearest neighbor predictions per frame and\nauto-regressively decoding the best neighbor based on the previous frame's\npredictions. Our method's effectiveness is validated through extensive\nexperiments, demonstrating state-of-the-art performance on benchmark datasets.\nOur code is available at https://github.com/manupillai308/GAReT.\n","authors":["Manu S Pillai","Mamshad Nayeem Rizve","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2408.02840v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02834v1","updated":"2024-08-05T21:11:34Z","published":"2024-08-05T21:11:34Z","title":"DaCapo: a modular deep learning framework for scalable 3D image\n  segmentation","summary":"  DaCapo is a specialized deep learning library tailored to expedite the\ntraining and application of existing machine learning approaches on large,\nnear-isotropic image data. In this correspondence, we introduce DaCapo's unique\nfeatures optimized for this specific domain, highlighting its modular\nstructure, efficient experiment management tools, and scalable deployment\ncapabilities. We discuss its potential to improve access to large-scale,\nisotropic image segmentation and invite the community to explore and contribute\nto this open-source initiative.\n","authors":["William Patton","Jeff L. Rhoades","Marwan Zouinkhi","David G. Ackerman","Caroline Malin-Mayor","Diane Adjavon","Larissa Heinrich","Davis Bennett","Yurii Zubov","CellMap Project Team","Aubrey V. Weigel","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2408.02834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19082v2","updated":"2024-08-05T21:09:50Z","published":"2024-07-26T21:02:11Z","title":"Regularized Multi-Decoder Ensemble for an Error-Aware Scene\n  Representation Network","summary":"  Feature grid Scene Representation Networks (SRNs) have been applied to\nscientific data as compact functional surrogates for analysis and\nvisualization. As SRNs are black-box lossy data representations, assessing the\nprediction quality is critical for scientific visualization applications to\nensure that scientists can trust the information being visualized. Currently,\nexisting architectures do not support inference time reconstruction quality\nassessment, as coordinate-level errors cannot be evaluated in the absence of\nground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)\nensemble architecture consisting of a shared feature grid with multiple\nlightweight multi-layer perceptron decoders. MDSRN can generate a set of\nplausible predictions for a given input coordinate to compute the mean as the\nprediction of the multi-decoder ensemble and the variance as a confidence\nscore. The coordinate-level variance can be rendered along with the data to\ninform the reconstruction quality, or be integrated into uncertainty-aware\nvolume visualization algorithms. To prevent the misalignment between the\nquantified variance and the prediction quality, we propose a novel variance\nregularization loss for ensemble learning that promotes the Regularized\nmulti-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates\nclosely to the true model error. We comprehensively evaluate the quality of\nvariance quantification and data reconstruction of Monte Carlo Dropout, Mean\nField Variational Inference, Deep Ensemble, and Predicting Variance compared to\nthe proposed MDSRN and RMDSRN across diverse scalar field datasets. We\ndemonstrate that RMDSRN attains the most accurate data reconstruction and\ncompetitive variance-error correlation among uncertain SRNs under the same\nneural network parameter budgets.\n","authors":["Tianyu Xiong","Skylar W. Wurster","Hanqi Guo","Tom Peterka","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2407.19082v2.pdf","comment":"To be published in Proc. IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.02813v1","updated":"2024-08-05T20:27:45Z","published":"2024-08-05T20:27:45Z","title":"Mitigating Malicious Attacks in Federated Learning via Confidence-aware\n  Defense","summary":"  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n","authors":["Qilei Li","Ahmed M. Abdelmoniem"],"pdf_url":"https://arxiv.org/pdf/2408.02813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02803v1","updated":"2024-08-05T19:46:59Z","published":"2024-08-05T19:46:59Z","title":"SiCo: A Size-Controllable Virtual Try-On Approach for Informed\n  Decision-Making","summary":"  Virtual try-on (VTO) applications aim to improve the online shopping\nexperience by allowing users to preview garments, before making purchase\ndecisions. However, many VTO tools fail to consider the crucial relationship\nbetween a garment's size and the user's body size, often employing a\none-size-fits-all approach when visualizing a clothing item. This results in\npoor size recommendations and purchase decisions leading to increased return\nrates. To address this limitation, we introduce SiCo, an online VTO system,\nwhere users can upload images of themselves and visualize how different sizes\nof clothing would look on their body to help make better-informed purchase\ndecisions. Our user study shows SiCo's superiority over baseline VTO. The\nresults indicate that our approach significantly enhances user ability to gauge\nthe appearance of outfits on their bodies and boosts their confidence in\nselecting clothing sizes that match desired goals. Based on our evaluation, we\nbelieve our VTO design has the potential to reduce return rates and enhance the\nonline clothes shopping experience. Our code is available at\nhttps://github.com/SherryXTChen/SiCo.\n","authors":["Sherry X. Chen","Alex Christopher Lim","Yimeng Liu","Pradeep Sen","Misha Sra"],"pdf_url":"https://arxiv.org/pdf/2408.02803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02796v1","updated":"2024-08-05T19:23:45Z","published":"2024-08-05T19:23:45Z","title":"Gaussian Mixture based Evidential Learning for Stereo Matching","summary":"  In this paper, we introduce a novel Gaussian mixture based evidential\nlearning solution for robust stereo matching. Diverging from previous\nevidential deep learning approaches that rely on a single Gaussian\ndistribution, our framework posits that individual image data adheres to a\nmixture-of-Gaussian distribution in stereo matching. This assumption yields\nmore precise pixel-level predictions and more accurately mirrors the real-world\nimage distribution. By further employing the inverse-Gamma distribution as an\nintermediary prior for each mixture component, our probabilistic model achieves\nimproved depth estimation compared to its counterpart with the single Gaussian\nand effectively captures the model uncertainty, which enables a strong\ncross-domain generation ability. We evaluated our method for stereo matching by\ntraining the model using the Scene Flow dataset and testing it on KITTI 2015\nand Middlebury 2014. The experiment results consistently show that our method\nbrings improvements over the baseline methods in a trustworthy manner. Notably,\nour approach achieved new state-of-the-art results on both the in-domain\nvalidated data and the cross-domain datasets, demonstrating its effectiveness\nand robustness in stereo matching tasks.\n","authors":["Weide Liu","Xingxing Wang","Lu Wang","Jun Cheng","Fayao Liu","Xulei Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02796v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11751v2","updated":"2024-08-05T19:22:13Z","published":"2024-03-18T12:59:35Z","title":"Relational Representation Learning Network for Cross-Spectral Image\n  Patch Matching","summary":"  Recently, feature relation learning has drawn widespread attention in\ncross-spectral image patch matching. However, existing related research focuses\non extracting diverse relations between image patch features and ignores\nsufficient intrinsic feature representations of individual image patches.\nTherefore, we propose an innovative relational representation learning idea\nthat simultaneously focuses on sufficiently mining the intrinsic features of\nindividual image patches and the relations between image patch features. Based\non this, we construct a Relational Representation Learning Network (RRL-Net).\nSpecifically, we innovatively construct an autoencoder to fully characterize\nthe individual intrinsic features, and introduce a feature interaction learning\n(FIL) module to extract deep-level feature relations. To further fully mine\nindividual intrinsic features, a lightweight multi-dimensional global-to-local\nattention (MGLA) module is constructed to enhance the global feature extraction\nof individual image patches and capture local dependencies within global\nfeatures. By combining the MGLA module, we further explore the feature\nextraction network and construct an attention-based lightweight feature\nextraction (ALFE) network. In addition, we propose a multi-loss post-pruning\n(MLPP) optimization strategy, which greatly promotes network optimization while\navoiding increases in parameters and inference time. Extensive experiments\ndemonstrate that our RRL-Net achieves state-of-the-art (SOTA) performance on\nmultiple public datasets. Our code will be made public later.\n","authors":["Chuang Yu","Yunpeng Liu","Jinmiao Zhao","Dou Quan","Zelin Shi"],"pdf_url":"https://arxiv.org/pdf/2403.11751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09894v2","updated":"2024-08-05T19:21:16Z","published":"2023-12-15T15:45:52Z","title":"PathoDuet: Foundation Models for Pathological Slide Analysis of H&E and\n  IHC Stains","summary":"  Large amounts of digitized histopathological data display a promising future\nfor developing pathological foundation models via self-supervised learning\nmethods. Foundation models pretrained with these methods serve as a good basis\nfor downstream tasks. However, the gap between natural and histopathological\nimages hinders the direct application of existing methods. In this work, we\npresent PathoDuet, a series of pretrained models on histopathological images,\nand a new self-supervised learning framework in histopathology. The framework\nis featured by a newly-introduced pretext token and later task raisers to\nexplicitly utilize certain relations between images, like multiple\nmagnifications and multiple stains. Based on this, two pretext tasks,\ncross-scale positioning and cross-stain transferring, are designed to pretrain\nthe model on Hematoxylin and Eosin (H&E) images and transfer the model to\nimmunohistochemistry (IHC) images, respectively. To validate the efficacy of\nour models, we evaluate the performance over a wide variety of downstream\ntasks, including patch-level colorectal cancer subtyping and whole slide image\n(WSI)-level classification in H&E field, together with expression level\nprediction of IHC marker, tumor identification and slide-level qualitative\nanalysis in IHC field. The experimental results show the superiority of our\nmodels over most tasks and the efficacy of proposed pretext tasks. The codes\nand models are available at https://github.com/openmedlab/PathoDuet.\n","authors":["Shengyi Hua","Fang Yan","Tianle Shen","Lei Ma","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.09894v2.pdf","comment":"Accepted for Medical Image Analysis"},{"id":"http://arxiv.org/abs/2408.02792v1","updated":"2024-08-05T19:19:29Z","published":"2024-08-05T19:19:29Z","title":"Lesion Elevation Prediction from Skin Images Improves Diagnosis","summary":"  While deep learning-based computer-aided diagnosis for skin lesion image\nanalysis is approaching dermatologists' performance levels, there are several\nworks showing that incorporating additional features such as shape priors,\ntexture, color constancy, and illumination further improves the lesion\ndiagnosis performance. In this work, we look at another clinically useful\nfeature, skin lesion elevation, and investigate the feasibility of predicting\nand leveraging skin lesion elevation labels. Specifically, we use a deep\nlearning model to predict image-level lesion elevation labels from 2D skin\nlesion images. We test the elevation prediction accuracy on the derm7pt\ndataset, and use the elevation prediction model to estimate elevation labels\nfor images from five other datasets: ISIC 2016, 2017, and 2018 Challenge\ndatasets, MSK, and DermoFit. We evaluate cross-domain generalization by using\nthese estimated elevation labels as auxiliary inputs to diagnosis models, and\nshow that these improve the classification performance, with AUROC improvements\nof up to 6.29% and 2.69% for dermoscopic and clinical images, respectively. The\ncode is publicly available at https://github.com/sfu-mial/LesionElevation.\n","authors":["Kumar Abhishek","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2408.02792v1.pdf","comment":"Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2024; 12 pages, 2 tables, 4\n  figures"},{"id":"http://arxiv.org/abs/2408.02788v1","updated":"2024-08-05T19:11:46Z","published":"2024-08-05T19:11:46Z","title":"GazeXplain: Learning to Predict Natural Language Explanations of Visual\n  Scanpaths","summary":"  While exploring visual scenes, humans' scanpaths are driven by their\nunderlying attention processes. Understanding visual scanpaths is essential for\nvarious applications. Traditional scanpath models predict the where and when of\ngaze shifts without providing explanations, creating a gap in understanding the\nrationale behind fixations. To bridge this gap, we introduce GazeXplain, a\nnovel study of visual scanpath prediction and explanation. This involves\nannotating natural-language explanations for fixations across eye-tracking\ndatasets and proposing a general model with an attention-language decoder that\njointly predicts scanpaths and generates explanations. It integrates a unique\nsemantic alignment mechanism to enhance the consistency between fixations and\nexplanations, alongside a cross-dataset co-training approach for\ngeneralization. These novelties present a comprehensive and adaptable solution\nfor explainable human visual scanpath prediction. Extensive experiments on\ndiverse eye-tracking datasets demonstrate the effectiveness of GazeXplain in\nboth scanpath prediction and explanation, offering valuable insights into human\nvisual attention and cognitive processes.\n","authors":["Xianyu Chen","Ming Jiang","Qi Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.02788v1.pdf","comment":"To appear in ECCV2024"},{"id":"http://arxiv.org/abs/2408.02787v1","updated":"2024-08-05T19:11:05Z","published":"2024-08-05T19:11:05Z","title":"Segmentation Style Discovery: Application to Skin Lesion Images","summary":"  Variability in medical image segmentation, arising from annotator\npreferences, expertise, and their choice of tools, has been well documented.\nWhile the majority of multi-annotator segmentation approaches focus on modeling\nannotator-specific preferences, they require annotator-segmentation\ncorrespondence. In this work, we introduce the problem of segmentation style\ndiscovery, and propose StyleSeg, a segmentation method that learns plausible,\ndiverse, and semantically consistent segmentation styles from a corpus of\nimage-mask pairs without any knowledge of annotator correspondence. StyleSeg\nconsistently outperforms competing methods on four publicly available skin\nlesion segmentation (SLS) datasets. We also curate ISIC-MultiAnnot, the largest\nmulti-annotator SLS dataset with annotator correspondence, and our results show\na strong alignment, using our newly proposed measure AS2, between the predicted\nstyles and annotator preferences. The code and the dataset are available at\nhttps://github.com/sfu-mial/StyleSeg.\n","authors":["Kumar Abhishek","Jeremy Kawahara","Ghassan Hamarneh"],"pdf_url":"https://arxiv.org/pdf/2408.02787v1.pdf","comment":"Medical Image Computing and Computer-Assisted Intervention (MICCAI)\n  ISIC Skin Image Analysis Workshop (MICCAI ISIC) 2024; 13 pages, 2 tables, 3\n  figures"},{"id":"http://arxiv.org/abs/2309.05490v2","updated":"2024-08-05T18:57:42Z","published":"2023-09-11T14:32:04Z","title":"Learning Semantic Segmentation with Query Points Supervision on Aerial\n  Images","summary":"  Semantic segmentation is crucial in remote sensing, where high-resolution\nsatellite images are segmented into meaningful regions. Recent advancements in\ndeep learning have significantly improved satellite image segmentation.\nHowever, most of these methods are typically trained in fully supervised\nsettings that require high-quality pixel-level annotations, which are expensive\nand time-consuming to obtain. In this work, we present a weakly supervised\nlearning algorithm to train semantic segmentation algorithms that only rely on\nquery point annotations instead of full mask labels. Our proposed approach\nperforms accurate semantic segmentation and improves efficiency by\nsignificantly reducing the cost and time required for manual annotation.\nSpecifically, we generate superpixels and extend the query point labels into\nthose superpixels that group similar meaningful semantics. Then, we train\nsemantic segmentation models supervised with images partially labeled with the\nsuperpixel pseudo-labels. We benchmark our weakly supervised training approach\non an aerial image dataset and different semantic segmentation architectures,\nshowing that we can reach competitive performance compared to fully supervised\ntraining while reducing the annotation effort. The code of our proposed\napproach is publicly available at: https://github.com/santiago2205/LSSQPS.\n","authors":["Santiago Rivier","Carlos Hinojosa","Silvio Giancola","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2309.05490v2.pdf","comment":"Paper Accepted at ICIP 2024 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2408.02780v1","updated":"2024-08-05T18:57:33Z","published":"2024-08-05T18:57:33Z","title":"LR-Net: A Lightweight and Robust Network for Infrared Small Target\n  Detection","summary":"  Limited by equipment limitations and the lack of target intrinsic features,\nexisting infrared small target detection methods have difficulty meeting actual\ncomprehensive performance requirements. Therefore, we propose an innovative\nlightweight and robust network (LR-Net), which abandons the complex structure\nand achieves an effective balance between detection accuracy and resource\nconsumption. Specifically, to ensure the lightweight and robustness, on the one\nhand, we construct a lightweight feature extraction attention (LFEA) module,\nwhich can fully extract target features and strengthen information interaction\nacross channels. On the other hand, we construct a simple refined feature\ntransfer (RFT) module. Compared with direct cross-layer connections, the RFT\nmodule can improve the network's feature refinement extraction capability with\nlittle resource consumption. Meanwhile, to solve the problem of small target\nloss in high-level feature maps, on the one hand, we propose a low-level\nfeature distribution (LFD) strategy to use low-level features to supplement the\ninformation of high-level features. On the other hand, we introduce an\nefficient simplified bilinear interpolation attention module (SBAM) to promote\nthe guidance constraints of low-level features on high-level features and the\nfusion of the two. In addition, We abandon the traditional resizing method and\nadopt a new training and inference cropping strategy, which is more robust to\ndatasets with multi-scale samples. Extensive experimental results show that our\nLR-Net achieves state-of-the-art (SOTA) performance. Notably, on the basis of\nthe proposed LR-Net, we achieve 3rd place in the \"ICPR 2024 Resource-Limited\nInfrared Small Target Detection Challenge Track 2: Lightweight Infrared Small\nTarget Detection\".\n","authors":["Chuang Yu","Yunpeng Liu","Jinmiao Zhao","Zelin Shi"],"pdf_url":"https://arxiv.org/pdf/2408.02780v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02773v1","updated":"2024-08-05T18:49:58Z","published":"2024-08-05T18:49:58Z","title":"Refined Infrared Small Target Detection Scheme with Single-Point\n  Supervision","summary":"  Recently, infrared small target detection with single-point supervision has\nattracted extensive attention. However, the detection accuracy of existing\nmethods has difficulty meeting actual needs. Therefore, we propose an\ninnovative refined infrared small target detection scheme with single-point\nsupervision, which has excellent segmentation accuracy and detection rate.\nSpecifically, we introduce label evolution with single point supervision\n(LESPS) framework and explore the performance of various excellent infrared\nsmall target detection networks based on this framework. Meanwhile, to improve\nthe comprehensive performance, we construct a complete post-processing\nstrategy. On the one hand, to improve the segmentation accuracy, we use a\ncombination of test-time augmentation (TTA) and conditional random field (CRF)\nfor post-processing. On the other hand, to improve the detection rate, we\nintroduce an adjustable sensitivity (AS) strategy for post-processing, which\nfully considers the advantages of multiple detection results and reasonably\nadds some areas with low confidence to the fine segmentation image in the form\nof centroid points. In addition, to further improve the performance and explore\nthe characteristics of this task, on the one hand, we construct and find that a\nmulti-stage loss is helpful for fine-grained detection. On the other hand, we\nfind that a reasonable sliding window cropping strategy for test samples has\nbetter performance for actual multi-size samples. Extensive experimental\nresults show that the proposed scheme achieves state-of-the-art (SOTA)\nperformance. Notably, the proposed scheme won the third place in the \"ICPR 2024\nResource-Limited Infrared Small Target Detection Challenge Track 1: Weakly\nSupervised Infrared Small Target Detection\".\n","authors":["Jinmiao Zhao","Zelin Shi","Chuang Yu","Yunpeng Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.00761v4","updated":"2024-08-05T18:40:07Z","published":"2023-12-01T18:29:08Z","title":"Deep Unlearning: Fast and Efficient Gradient-free Approach to Class\n  Forgetting","summary":"  Machine unlearning is a prominent and challenging field, driven by regulatory\ndemands for user data deletion and heightened privacy awareness. Existing\napproaches involve retraining model or multiple finetuning steps for each\ndeletion request, often constrained by computational limits and restricted data\naccess. In this work, we introduce a novel class unlearning algorithm designed\nto strategically eliminate specific classes from the learned model. Our\nalgorithm first estimates the Retain and the Forget Spaces using Singular Value\nDecomposition on the layerwise activations for a small subset of samples from\nthe retain and unlearn classes, respectively. We then compute the shared\ninformation between these spaces and remove it from the forget space to isolate\nclass-discriminatory feature space. Finally, we obtain the unlearned model by\nupdating the weights to suppress the class discriminatory features from the\nactivation spaces. We demonstrate our algorithm's efficacy on ImageNet using a\nVision Transformer with only $\\sim 1.5\\%$ drop in retain accuracy compared to\nthe original model while maintaining under $1\\%$ accuracy on the unlearned\nclass samples. Furthermore, our algorithm exhibits competitive unlearning\nperformance and resilience against Membership Inference Attacks (MIA). Compared\nto baselines, it achieves an average accuracy improvement of $1.38\\%$ on the\nImageNet dataset while requiring up to $10 \\times$ fewer samples for\nunlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset\nusing a ResNet18 architecture, our approach outperforms the best baseline by\n$1.8\\%$. Our code is available at\nhttps://github.com/sangamesh-kodge/class_forgetting.\n","authors":["Sangamesh Kodge","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2312.00761v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02769v1","updated":"2024-08-05T18:38:29Z","published":"2024-08-05T18:38:29Z","title":"From Recognition to Prediction: Leveraging Sequence Reasoning for Action\n  Anticipation","summary":"  The action anticipation task refers to predicting what action will happen\nbased on observed videos, which requires the model to have a strong ability to\nsummarize the present and then reason about the future. Experience and common\nsense suggest that there is a significant correlation between different\nactions, which provides valuable prior knowledge for the action anticipation\ntask. However, previous methods have not effectively modeled this underlying\nstatistical relationship. To address this issue, we propose a novel end-to-end\nvideo modeling architecture that utilizes attention mechanisms, named\nAnticipation via Recognition and Reasoning (ARR). ARR decomposes the action\nanticipation task into action recognition and sequence reasoning tasks, and\neffectively learns the statistical relationship between actions by next action\nprediction (NAP). In comparison to existing temporal aggregation strategies,\nARR is able to extract more effective features from observable videos to make\nmore reasonable predictions. In addition, to address the challenge of\nrelationship modeling that requires extensive training data, we propose an\ninnovative approach for the unsupervised pre-training of the decoder, which\nleverages the inherent temporal dynamics of video to enhance the reasoning\ncapabilities of the network. Extensive experiments on the Epic-kitchen-100,\nEGTEA Gaze+, and 50salads datasets demonstrate the efficacy of the proposed\nmethods. The code is available at https://github.com/linuxsino/ARR.\n","authors":["Xin Liu","Chao Hao","Zitong Yu","Huanjing Yue","Jingyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02769v1.pdf","comment":"Accepted by ACM TOMM"},{"id":"http://arxiv.org/abs/2408.02766v1","updated":"2024-08-05T18:34:15Z","published":"2024-08-05T18:34:15Z","title":"ConDL: Detector-Free Dense Image Matching","summary":"  In this work, we introduce a deep-learning framework designed for estimating\ndense image correspondences. Our fully convolutional model generates dense\nfeature maps for images, where each pixel is associated with a descriptor that\ncan be matched across multiple images. Unlike previous methods, our model is\ntrained on synthetic data that includes significant distortions, such as\nperspective changes, illumination variations, shadows, and specular highlights.\nUtilizing contrastive learning, our feature maps achieve greater invariance to\nthese distortions, enabling robust matching. Notably, our method eliminates the\nneed for a keypoint detector, setting it apart from many existing\nimage-matching techniques.\n","authors":["Monika Kwiatkowski","Simon Matern","Olaf Hellwich"],"pdf_url":"https://arxiv.org/pdf/2408.02766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v1","updated":"2024-08-05T18:24:48Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v1.pdf","comment":"Expansion of \"Dimensionality Reduction for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation\" arXiv:2308.03723\n  . Submitted to the Journal for Machine Learning in Biomedical Imaging. Code\n  available at https://github.com/mckellwoodland/dimen_reduce_mahal"},{"id":"http://arxiv.org/abs/2405.01531v2","updated":"2024-08-05T18:20:39Z","published":"2024-05-02T17:59:01Z","title":"Improving Intervention Efficacy via Concept Realignment in Concept\n  Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments. Our code is available\nat: https://github.com/ExplainableML/concept_realignment.\n","authors":["Nishad Singhi","Jae Myung Kim","Karsten Roth","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2405.01531v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02750v1","updated":"2024-08-05T18:09:02Z","published":"2024-08-05T18:09:02Z","title":"Privacy-Safe Iris Presentation Attack Detection","summary":"  This paper proposes a framework for a privacy-safe iris presentation attack\ndetection (PAD) method, designed solely with synthetically-generated,\nidentity-leakage-free iris images. Once trained, the method is evaluated in a\nclassical way using state-of-the-art iris PAD benchmarks. We designed two\ngenerative models for the synthesis of ISO/IEC 19794-6-compliant iris images.\nThe first model synthesizes bona fide-looking samples. To avoid ``identity\nleakage,'' the generated samples that accidentally matched those used in the\nmodel's training were excluded. The second model synthesizes images of irises\nwith textured contact lenses and is conditioned by a given contact lens brand\nto have better control over textured contact lens appearance when forming the\ntraining set. Our experiments demonstrate that models trained solely on\nsynthetic data achieve a lower but still reasonable performance when compared\nto solutions trained with iris images collected from human subjects. This is\nthe first-of-its-kind attempt to use solely synthetic data to train a\nfully-functional iris PAD solution, and despite the performance gap between\nregular and the proposed methods, this study demonstrates that with the\nincreasing fidelity of generative models, creating such privacy-safe iris PAD\nmethods may be possible. The source codes and generative models trained for\nthis work are offered along with the paper.\n","authors":["Mahsa Mitcheff","Patrick Tinsley","Adam Czajka"],"pdf_url":"https://arxiv.org/pdf/2408.02750v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02718v1","updated":"2024-08-05T17:56:41Z","published":"2024-08-05T17:56:41Z","title":"MMIU: Multimodal Multi-image Understanding for Evaluating Large\n  Vision-Language Models","summary":"  The capability to process multiple images is crucial for Large\nVision-Language Models (LVLMs) to develop a more thorough and nuanced\nunderstanding of a scene. Recent multi-image LVLMs have begun to address this\nneed. However, their evaluation has not kept pace with their development. To\nfill this gap, we introduce the Multimodal Multi-image Understanding (MMIU)\nbenchmark, a comprehensive evaluation suite designed to assess LVLMs across a\nwide range of multi-image tasks. MMIU encompasses 7 types of multi-image\nrelationships, 52 tasks, 77K images, and 11K meticulously curated\nmultiple-choice questions, making it the most extensive benchmark of its kind.\nOur evaluation of 24 popular LVLMs, including both open-source and proprietary\nmodels, reveals significant challenges in multi-image comprehension,\nparticularly in tasks involving spatial understanding. Even the most advanced\nmodels, such as GPT-4o, achieve only 55.7% accuracy on MMIU. Through\nmulti-faceted analytical experiments, we identify key performance gaps and\nlimitations, providing valuable insights for future model and data\nimprovements. We aim for MMIU to advance the frontier of LVLM research and\ndevelopment, moving us toward achieving sophisticated multimodal multi-image\nuser interactions.\n","authors":["Fanqing Meng","Jin Wang","Chuanhao Li","Quanfeng Lu","Hao Tian","Jiaqi Liao","Xizhou Zhu","Jifeng Dai","Yu Qiao","Ping Luo","Kaipeng Zhang","Wenqi Shao"],"pdf_url":"https://arxiv.org/pdf/2408.02718v1.pdf","comment":"Project Page: https://mmiu-bench.github.io/"},{"id":"http://arxiv.org/abs/2311.13297v2","updated":"2024-08-05T17:49:58Z","published":"2023-11-22T10:27:19Z","title":"Retargeting Visual Data with Deformation Fields","summary":"  Seam carving is an image editing method that enable content-aware resizing,\nincluding operations like removing objects. However, the seam-finding strategy\nbased on dynamic programming or graph-cut limits its applications to broader\nvisual data formats and degrees of freedom for editing. Our observation is that\ndescribing the editing and retargeting of images more generally by a\ndisplacement field yields a generalisation of content-aware deformations. We\npropose to learn a deformation with a neural network that keeps the output\nplausible while trying to deform it only in places with low information\ncontent. This technique applies to different kinds of visual data, including\nimages, 3D scenes given as neural radiance fields, or even polygon meshes.\nExperiments conducted on different visual data show that our method achieves\nbetter content-aware retargeting compared to previous methods.\n","authors":["Tim Elsner","Julia Berger","Tong Wu","Victor Czech","Lin Gao","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2311.13297v2.pdf","comment":"ECCV 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.02584v1","updated":"2024-08-05T16:00:21Z","published":"2024-08-05T16:00:21Z","title":"Leveraging the Power of LLMs: A Fine-Tuning Approach for High-Quality\n  Aspect-Based Summarization","summary":"  The ever-increasing volume of digital information necessitates efficient\nmethods for users to extract key insights from lengthy documents. Aspect-based\nsummarization offers a targeted approach, generating summaries focused on\nspecific aspects within a document. Despite advancements in aspect-based\nsummarization research, there is a continuous quest for improved model\nperformance. Given that large language models (LLMs) have demonstrated the\npotential to revolutionize diverse tasks within natural language processing,\nparticularly in the problem of summarization, this paper explores the potential\nof fine-tuning LLMs for the aspect-based summarization task. We evaluate the\nimpact of fine-tuning open-source foundation LLMs, including Llama2, Mistral,\nGemma and Aya, on a publicly available domain-specific aspect based summary\ndataset. We hypothesize that this approach will enable these models to\neffectively identify and extract aspect-related information, leading to\nsuperior quality aspect-based summaries compared to the state-of-the-art. We\nestablish a comprehensive evaluation framework to compare the performance of\nfine-tuned LLMs against competing aspect-based summarization methods and\nvanilla counterparts of the fine-tuned LLMs. Our work contributes to the field\nof aspect-based summarization by demonstrating the efficacy of fine-tuning LLMs\nfor generating high-quality aspect-based summaries. Furthermore, it opens doors\nfor further exploration of using LLMs for targeted information extraction tasks\nacross various NLP domains.\n","authors":["Ankan Mullick","Sombit Bose","Rounak Saha","Ayan Kumar Bhowmick","Aditya Vempaty","Pawan Goyal","Niloy Ganguly","Prasenjit Dey","Ravi Kokku"],"pdf_url":"https://arxiv.org/pdf/2408.02584v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02545v1","updated":"2024-08-05T15:16:24Z","published":"2024-08-05T15:16:24Z","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation","summary":"  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n","authors":["Daniel Fleischer","Moshe Berchansky","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2408.02545v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2408.02535v1","updated":"2024-08-05T15:08:26Z","published":"2024-08-05T15:08:26Z","title":"Towards Coarse-grained Visual Language Navigation Task Planning Enhanced\n  by Event Knowledge Graph","summary":"  Visual language navigation (VLN) is one of the important research in embodied\nAI. It aims to enable an agent to understand the surrounding environment and\ncomplete navigation tasks. VLN instructions could be categorized into\ncoarse-grained and fine-grained commands. Fine-grained command describes a\nwhole task with subtasks step-by-step. In contrast, coarse-grained command\ngives an abstract task description, which more suites human habits. Most\nexisting work focuses on the former kind of instruction in VLN tasks, ignoring\nthe latter abstract instructions belonging to daily life scenarios. To overcome\nthe above challenge in abstract instruction, we attempt to consider\ncoarse-grained instruction in VLN by event knowledge enhancement. Specifically,\nwe first propose a prompt-based framework to extract an event knowledge graph\n(named VLN-EventKG) for VLN integrally over multiple mainstream benchmark\ndatasets. Through small and large language model collaboration, we realize\nknowledge-enhanced navigation planning (named EventNav) for VLN tasks with\ncoarse-grained instruction input. Additionally, we design a novel dynamic\nhistory backtracking module to correct potential error action planning in real\ntime. Experimental results in various public benchmarks show our\nknowledge-enhanced method has superiority in coarse-grained-instruction VLN\nusing our proposed VLN-EventKG with over $5\\%$ improvement in success rate. Our\nproject is available at https://sites.google.com/view/vln-eventkg\n","authors":["Zhao Kaichen","Song Yaoxian","Zhao Haiquan","Liu Haoyu","Li Tiefeng","Li Zhixu"],"pdf_url":"https://arxiv.org/pdf/2408.02535v1.pdf","comment":"11 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.02875v2","updated":"2024-08-05T14:01:26Z","published":"2024-03-05T11:38:48Z","title":"Enhancing Conceptual Understanding in Multimodal Contrastive Learning\n  through Hard Negative Samples","summary":"  Current multimodal models leveraging contrastive learning often face\nlimitations in developing fine-grained conceptual understanding. This is due to\nrandom negative samples during pretraining, causing almost exclusively very\ndissimilar concepts to be compared in the loss function. Consequently, the\nmodels struggle with fine-grained semantic differences. To address this\nproblem, we introduce a novel pretraining method incorporating synthetic hard\nnegative text examples. The hard negatives permute terms corresponding to\nvisual concepts, leading to a more fine-grained visual and textual concept\nalignment. Further, we introduce InpaintCOCO, a new challenging dataset for\nassessing the fine-grained alignment of colors, objects, and sizes in\nvision-language models. We created the dataset using generative inpainting from\nCOCO images by changing the visual concepts so that the images no longer match\ntheir original captions. Our results show significant improvements in\nfine-grained concept understanding across a wide range of vision-language\ndatasets, including our InpaintCOCO dataset.\n","authors":["Philipp J. Rsch","Norbert Oswald","Michaela Geierhos","Jindich Libovick"],"pdf_url":"https://arxiv.org/pdf/2403.02875v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13284v3","updated":"2024-08-05T13:49:38Z","published":"2024-07-18T08:36:28Z","title":"Semantic-aware Representation Learning for Homography Estimation","summary":"  Homography estimation is the task of determining the transformation from an\nimage pair. Our approach focuses on employing detector-free feature matching\nmethods to address this issue. Previous work has underscored the importance of\nincorporating semantic information, however there still lacks an efficient way\nto utilize semantic information. Previous methods suffer from treating the\nsemantics as a pre-processing, causing the utilization of semantics overly\ncoarse-grained and lack adaptability when dealing with different tasks. In our\nwork, we seek another way to use the semantic information, that is\nsemantic-aware feature representation learning framework.Based on this, we\npropose SRMatcher, a new detector-free feature matching method, which\nencourages the network to learn integrated semantic feature\nrepresentation.Specifically, to capture precise and rich semantics, we leverage\nthe capabilities of recently popularized vision foundation models (VFMs)\ntrained on extensive datasets. Then, a cross-images Semantic-aware Fusion Block\n(SFB) is proposed to integrate its fine-grained semantic features into the\nfeature representation space. In this way, by reducing errors stemming from\nsemantic inconsistencies in matching pairs, our proposed SRMatcher is able to\ndeliver more accurate and realistic outcomes. Extensive experiments show that\nSRMatcher surpasses solid baselines and attains SOTA results on multiple\nreal-world datasets. Compared to the previous SOTA approach GeoFormer,\nSRMatcher increases the area under the cumulative curve (AUC) by about 11% on\nHPatches. Additionally, the SRMatcher could serve as a plug-and-play framework\nfor other matching methods like LoFTR, yielding substantial precision\nimprovement.\n","authors":["Yuhan Liu","Qianxin Huang","Siqi Hui","Jingwen Fu","Sanping Zhou","Kangyi Wu","Pengna Li","Jinjun Wang"],"pdf_url":"https://arxiv.org/pdf/2407.13284v3.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.02404v1","updated":"2024-08-05T11:57:11Z","published":"2024-08-05T11:57:11Z","title":"Feedback Reciprocal Graph Collaborative Filtering","summary":"  Collaborative filtering on user-item interaction graphs has achieved success\nin the industrial recommendation. However, recommending users' truly fascinated\nitems poses a seesaw dilemma for collaborative filtering models learned from\nthe interaction graph. On the one hand, not all items that users interact with\nare equally appealing. Some items are genuinely fascinating to users, while\nothers are unfascinated. Training graph collaborative filtering models in the\nabsence of distinction between them can lead to the recommendation of\nunfascinating items to users. On the other hand, disregarding the interacted\nbut unfascinating items during graph collaborative filtering will result in an\nincomplete representation of users' interaction intent, leading to a decline in\nthe model's recommendation capabilities. To address this seesaw problem, we\npropose Feedback Reciprocal Graph Collaborative Filtering (FRGCF), which\nemphasizes the recommendation of fascinating items while attenuating the\nrecommendation of unfascinating items. Specifically, FRGCF first partitions the\nentire interaction graph into the Interacted & Fascinated (I&F) graph and the\nInteracted & Unfascinated (I&U) graph based on the user feedback. Then, FRGCF\nintroduces separate collaborative filtering on the I&F graph and the I&U graph\nwith feedback-reciprocal contrastive learning and macro-level feedback\nmodeling. This enables the I&F graph recommender to learn multi-grained\ninteraction characteristics from the I&U graph without being misdirected by it.\nExtensive experiments on four benchmark datasets and a billion-scale industrial\ndataset demonstrate that FRGCF improves the performance by recommending more\nfascinating items and fewer unfascinating items. Besides, online A/B tests on\nTaobao's recommender system verify the superiority of FRGCF.\n","authors":["Weijun Chen","Yuanchen Bei","Qijie Shen","Hao Chen","Xiao Huang","Feiran Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02404v1.pdf","comment":"9 pages, accepted by CIKM 2024"},{"id":"http://arxiv.org/abs/2408.02354v1","updated":"2024-08-05T10:02:29Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v1.pdf","comment":"5 pages, 4 figures, submitted to CIKM'24"},{"id":"http://arxiv.org/abs/2408.02304v1","updated":"2024-08-05T08:30:16Z","published":"2024-08-05T08:30:16Z","title":"Embedding Compression in Recommender Systems: A Survey","summary":"  To alleviate the problem of information explosion, recommender systems are\nwidely deployed to provide personalized information filtering services.\nUsually, embedding tables are employed in recommender systems to transform\nhigh-dimensional sparse one-hot vectors into dense real-valued embeddings.\nHowever, the embedding tables are huge and account for most of the parameters\nin industrial-scale recommender systems. In order to reduce memory costs and\nimprove efficiency, various approaches are proposed to compress the embedding\ntables. In this survey, we provide a comprehensive review of embedding\ncompression approaches in recommender systems. We first introduce deep learning\nrecommendation models and the basic concept of embedding compression in\nrecommender systems. Subsequently, we systematically organize existing\napproaches into three categories, namely low-precision, mixed-dimension, and\nweight-sharing, respectively. Lastly, we summarize the survey with some general\nsuggestions and provide future prospects for this field.\n","authors":["Shiwei Li","Huifeng Guo","Xing Tang","Ruiming Tang","Lu Hou","Ruixuan Li","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02304v1.pdf","comment":"Accepted by ACM Computing Surveys"},{"id":"http://arxiv.org/abs/2407.05502v2","updated":"2024-08-05T07:22:58Z","published":"2024-07-07T21:26:36Z","title":"Faux Polyglot: A Study on Information Disparity in Multilingual Large\n  Language Models","summary":"  With Retrieval Augmented Generation (RAG), Large Language Models (LLMs) are\nplaying a pivotal role in information search and are being adopted globally.\nAlthough the multilingual capability of LLMs offers new opportunities to bridge\nthe language barrier, do these capabilities translate into real-life scenarios\nwhere linguistic divide and knowledge conflicts between multilingual sources\nare known occurrences? In this paper, we studied LLM's linguistic preference in\na RAG-based information search setting. We found that LLMs displayed systemic\nbias towards information in the same language as the query language in both\ninformation retrieval and answer generation. Furthermore, in scenarios where\nthere is little information in the language of the query, LLMs prefer documents\nin high-resource languages, reinforcing the dominant views. Such bias exists\nfor both factual and opinion-based queries. Our results highlight the\nlinguistic divide within multilingual LLMs in information search systems. The\nseemingly beneficial multilingual capability of LLMs may backfire on\ninformation parity by reinforcing language-specific information cocoons or\nfilter bubbles further marginalizing low-resource views.\n","authors":["Nikhil Sharma","Kenton Murray","Ziang Xiao"],"pdf_url":"https://arxiv.org/pdf/2407.05502v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02215v1","updated":"2024-08-05T03:33:11Z","published":"2024-08-05T03:33:11Z","title":"Exploring Query Understanding for Amazon Product Search","summary":"  Online shopping platforms, such as Amazon, offer services to billions of\npeople worldwide. Unlike web search or other search engines, product search\nengines have their unique characteristics, primarily featuring short queries\nwhich are mostly a combination of product attributes and structured product\nsearch space. The uniqueness of product search underscores the crucial\nimportance of the query understanding component. However, there are limited\nstudies focusing on exploring this impact within real-world product search\nengines. In this work, we aim to bridge this gap by conducting a comprehensive\nstudy and sharing our year-long journey investigating how the query\nunderstanding service impacts Amazon Product Search. Firstly, we explore how\nquery understanding-based ranking features influence the ranking process. Next,\nwe delve into how the query understanding system contributes to understanding\nthe performance of a ranking model. Building on the insights gained from our\nstudy on the evaluation of the query understanding-based ranking model, we\npropose a query understanding-based multi-task learning framework for ranking.\nWe present our studies and investigations using the real-world system on Amazon\nSearch.\n","authors":["Chen Luo","Xianfeng Tang","Hanqing Lu","Yaochen Xie","Hui Liu","Zhenwei Dai","Limeng Cui","Ashutosh Joshi","Sreyashi Nag","Yang Li","Zhen Li","Rahul Goutam","Jiliang Tang","Haiyang Zhang","Qi He"],"pdf_url":"https://arxiv.org/pdf/2408.02215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00247v2","updated":"2024-08-05T03:19:58Z","published":"2024-08-01T02:54:23Z","title":"Simple but Efficient: A Multi-Scenario Nearline Retrieval Framework for\n  Recommendation on Taobao","summary":"  In recommendation systems, the matching stage is becoming increasingly\ncritical, serving as the upper limit for the entire recommendation process.\nRecently, some studies have started to explore the use of multi-scenario\ninformation for recommendations, such as model-based and data-based approaches.\nHowever, the matching stage faces significant challenges due to the need for\nultra-large-scale retrieval and meeting low latency requirements. As a result,\nthe methods applied at this stage (collaborative filtering and two-tower\nmodels) are often designed to be lightweight, hindering the full utilization of\nextensive information. On the other hand, the ranking stage features the most\nsophisticated models with the strongest scoring capabilities, but due to the\nlimited screen size of mobile devices, most of the ranked results may not gain\nexposure or be displayed. In this paper, we introduce an innovative\nmulti-scenario nearline retrieval framework. It operates by harnessing ranking\nlogs from various scenarios through Flink, allowing us to incorporate finely\nranked results from other scenarios into our matching stage in near real-time.\nBesides, we propose a streaming scoring module, which selects a crucial subset\nfrom the candidate pool. Implemented on the \"Guess You Like\" (homepage of the\nTaobao APP), China's premier e-commerce platform, our method has shown\nsubstantial improvements-most notably, a 5% uptick in product transactions.\nFurthermore, the proposed approach is not only model-free but also highly\nefficient, suggesting it can be quickly implemented in diverse scenarios and\ndemonstrate promising performance.\n","authors":["Yingcai Ma","Ziyang Wang","Yuliang Yan","Jian Wu","Yuning Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.00247v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.04625v3","updated":"2024-08-05T01:35:00Z","published":"2023-11-08T12:05:18Z","title":"A Comprehensive Summarization and Evaluation of Feature Refinement\n  Modules for CTR Prediction","summary":"  Click-through rate (CTR) prediction is widely used in academia and industry.\nMost CTR tasks fall into a feature embedding \\& feature interaction paradigm,\nwhere the accuracy of CTR prediction is mainly improved by designing practical\nfeature interaction structures. However, recent studies have argued that the\nfixed feature embedding learned only through the embedding layer limits the\nperformance of existing CTR models. Some works apply extra modules on top of\nthe embedding layer to dynamically refine feature representations in different\ninstances, making it effective and easy to integrate with existing CTR methods.\nDespite the promising results, there is a lack of a systematic review and\nsummarization of this new promising direction on the CTR task. To fill this\ngap, we comprehensively summarize and define a new module, namely\n\\textbf{feature refinement} (FR) module, that can be applied between feature\nembedding and interaction layers. We extract 14 FR modules from previous works,\nincluding instances where the FR module was proposed but not clearly defined or\nexplained. We fully assess the effectiveness and compatibility of existing FR\nmodules through comprehensive and extensive experiments with over 200 augmented\nmodels and over 4,000 runs for more than 15,000 GPU hours. The results offer\ninsightful guidelines for researchers, and all benchmarking code and\nexperimental results are open-sourced. In addition, we present a new\narchitecture of assigning independent FR modules to separate sub-networks for\nparallel CTR models, as opposed to the conventional method of inserting a\nshared FR module on top of the embedding layer. Our approach is also supported\nby comprehensive experiments demonstrating its effectiveness.\n","authors":["Fangye Wang","Hansu Gu","Dongsheng Li","Tun Lu","Peng Zhang","Li Shang","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2311.04625v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02854v1","updated":"2024-08-05T22:34:28Z","published":"2024-08-05T22:34:28Z","title":"Wiping out the limitations of Large Language Models -- A Taxonomy for\n  Retrieval Augmented Generation","summary":"  Current research on RAGs is distributed across various disciplines, and since\nthe technology is evolving very quickly, its unit of analysis is mostly on\ntechnological innovations, rather than applications in business contexts. Thus,\nin this research, we aim to create a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define RAG applications,\nfacilitating the adoption of this technology in the IS community. To the best\nof our knowledge, no RAG application taxonomies have been developed so far. We\ndescribe our methodology for developing the taxonomy, which includes the\ncriteria for selecting papers, an explanation of our rationale for employing a\nLarge Language Model (LLM)-supported approach to extract and identify initial\ncharacteristics, and a concise overview of our systematic process for\nconceptualizing the taxonomy. Our systematic taxonomy development process\nincludes four iterative phases designed to refine and enhance our understanding\nand presentation of RAG's core dimensions. We have developed a total of five\nmeta-dimensions and sixteen dimensions to comprehensively capture the concept\nof Retrieval-Augmented Generation (RAG) applications. When discussing our\nfindings, we also detail the specific research areas and pose key research\nquestions to guide future information system researchers as they explore the\nemerging topics of RAG systems.\n","authors":["Mahei Manhai Li","Irina Nikishina","zge Sevgili","Martin Semman"],"pdf_url":"https://arxiv.org/pdf/2408.02854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02795v1","updated":"2024-08-05T19:23:20Z","published":"2024-08-05T19:23:20Z","title":"Entity Retrieval for Answering Entity-Centric Questions","summary":"  The similarity between the question and indexed documents is a crucial factor\nin document retrieval for retrieval-augmented question answering. Although this\nis typically the only method for obtaining the relevant documents, it is not\nthe sole approach when dealing with entity-centric questions. In this study, we\npropose Entity Retrieval, a novel retrieval method which rather than relying on\nquestion-document similarity, depends on the salient entities within the\nquestion to identify the retrieval documents. We conduct an in-depth analysis\nof the performance of both dense and sparse retrieval methods in comparison to\nEntity Retrieval. Our findings reveal that our method not only leads to more\naccurate answers to entity-centric questions but also operates more\nefficiently.\n","authors":["Hassan S. Shavarani","Anoop Sarkar"],"pdf_url":"https://arxiv.org/pdf/2408.02795v1.pdf","comment":"17 pages total, 10 Tables, 4 Figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2405.04634v3","updated":"2024-08-05T17:53:28Z","published":"2024-05-07T19:37:22Z","title":"FRACTAL: An Ultra-Large-Scale Aerial Lidar Dataset for 3D Semantic\n  Segmentation of Diverse Landscapes","summary":"  Mapping agencies are increasingly adopting Aerial Lidar Scanning (ALS) as a\nnew tool to monitor territory and support public policies. Processing ALS data\nat scale requires efficient point classification methods that perform well over\nhighly diverse territories. To evaluate them, researchers need large annotated\nLidar datasets, however, current Lidar benchmark datasets have restricted scope\nand often cover a single urban area. To bridge this data gap, we present the\nFRench ALS Clouds from TArgeted Landscapes (FRACTAL) dataset: an\nultra-large-scale aerial Lidar dataset made of 100,000 dense point clouds with\nhigh-quality labels for 7 semantic classes and spanning 250 km$^2$. FRACTAL is\nbuilt upon France's nationwide open Lidar data. It achieves spatial and\nsemantic diversity via a sampling scheme that explicitly concentrates rare\nclasses and challenging landscapes from five French regions. It should support\nthe development of 3D deep learning approaches for large-scale land monitoring.\nWe describe the nature of the source data, the sampling workflow, the content\nof the resulting dataset, and provide an initial evaluation of segmentation\nperformance using a performant 3D neural architecture.\n","authors":["Charles Gaydon","Michel Daab","Floryne Roche"],"pdf_url":"https://arxiv.org/pdf/2405.04634v3.pdf","comment":"15 pages | 9 figures | 8 tables | Dataset is available at\n  https://huggingface.co/datasets/IGNF/FRACTAL | Trained model is available at\n  https://huggingface.co/IGNF/FRACTAL-LidarHD_7cl_randlanet | Deep learning\n  code repository is on Gihtub at https://github.com/IGNF/myria3d | Data\n  engineering code repository is on Github at https://github.com/IGNF/pacasam"},{"id":"http://arxiv.org/abs/2407.11913v2","updated":"2024-08-05T17:50:03Z","published":"2024-07-16T17:05:20Z","title":"Quantised Global Autoencoder: A Holistic Approach to Representing Visual\n  Data","summary":"  In quantised autoencoders, images are usually split into local patches, each\nencoded by one token. This representation is redundant in the sense that the\nsame number of tokens is spend per region, regardless of the visual information\ncontent in that region. Adaptive discretisation schemes like quadtrees are\napplied to allocate tokens for patches with varying sizes, but this just varies\nthe region of influence for a token which nevertheless remains a local\ndescriptor. Modern architectures add an attention mechanism to the autoencoder\nwhich infuses some degree of global information into the local tokens. Despite\nthe global context, tokens are still associated with a local image region. In\ncontrast, our method is inspired by spectral decompositions which transform an\ninput signal into a superposition of global frequencies. Taking the data-driven\nperspective, we learn custom basis functions corresponding to the codebook\nentries in our VQ-VAE setup. Furthermore, a decoder combines these basis\nfunctions in a non-linear fashion, going beyond the simple linear superposition\nof spectral decompositions. We can achieve this global description with an\nefficient transpose operation between features and channels and demonstrate our\nperformance on compression.\n","authors":["Tim Elsner","Paula Usinger","Victor Czech","Gregor Kobsik","Yanjiang He","Isaak Lim","Leif Kobbelt"],"pdf_url":"https://arxiv.org/pdf/2407.11913v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02654v1","updated":"2024-08-05T17:33:09Z","published":"2024-08-05T17:33:09Z","title":"On Using Quasirandom Sequences in Machine Learning for Model Weight\n  Initialization","summary":"  The effectiveness of training neural networks directly impacts computational\ncosts, resource allocation, and model development timelines in machine learning\napplications. An optimizer's ability to train the model adequately (in terms of\ntrained model performance) depends on the model's initial weights. Model weight\ninitialization schemes use pseudorandom number generators (PRNGs) as a source\nof randomness.\n  We investigate whether substituting PRNGs for low-discrepancy quasirandom\nnumber generators (QRNGs) -- namely Sobol' sequences -- as a source of\nrandomness for initializers can improve model performance. We examine\nMulti-Layer Perceptrons (MLP), Convolutional Neural Networks (CNN), Long\nShort-Term Memory (LSTM), and Transformer architectures trained on MNIST,\nCIFAR-10, and IMDB datasets using SGD and Adam optimizers. Our analysis uses\nten initialization schemes: Glorot, He, Lecun (both Uniform and Normal);\nOrthogonal, Random Normal, Truncated Normal, and Random Uniform. Models with\nweights set using PRNG- and QRNG-based initializers are compared pairwise for\neach combination of dataset, architecture, optimizer, and initialization\nscheme.\n  Our findings indicate that QRNG-based neural network initializers either\nreach a higher accuracy or achieve the same accuracy more quickly than\nPRNG-based initializers in 60% of the 120 experiments conducted. Thus, using\nQRNG-based initializers instead of PRNG-based initializers can speed up and\nimprove model training.\n","authors":["Andriy Miranskyy","Adam Sorrenti","Viral Thakar"],"pdf_url":"https://arxiv.org/pdf/2408.02654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02641v1","updated":"2024-08-05T17:14:35Z","published":"2024-08-05T17:14:35Z","title":"Detection of Compromised Functions in a Serverless Cloud Environment","summary":"  Serverless computing is an emerging cloud paradigm with serverless functions\nat its core. While serverless environments enable software developers to focus\non developing applications without the need to actively manage the underlying\nruntime infrastructure, they open the door to a wide variety of security\nthreats that can be challenging to mitigate with existing methods. Existing\nsecurity solutions do not apply to all serverless architectures, since they\nrequire significant modifications to the serverless infrastructure or rely on\nthird-party services for the collection of more detailed data. In this paper,\nwe present an extendable serverless security threat detection model that\nleverages cloud providers' native monitoring tools to detect anomalous behavior\nin serverless applications. Our model aims to detect compromised serverless\nfunctions by identifying post-exploitation abnormal behavior related to\ndifferent types of attacks on serverless functions, and therefore, it is a last\nline of defense. Our approach is not tied to any specific serverless\napplication, is agnostic to the type of threats, and is adaptable through model\nadjustments. To evaluate our model's performance, we developed a serverless\ncybersecurity testbed in an AWS cloud environment, which includes two different\nserverless applications and simulates a variety of attack scenarios that cover\nthe main security threats faced by serverless functions. Our evaluation\ndemonstrates our model's ability to detect all implemented attacks while\nmaintaining a negligible false alarm rate.\n","authors":["Danielle Lavi","Oleg Brodt","Dudu Mimran","Yuval Elovici","Asaf Shabtai"],"pdf_url":"https://arxiv.org/pdf/2408.02641v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02637v1","updated":"2024-08-05T17:01:33Z","published":"2024-08-05T17:01:33Z","title":"Command-line Obfuscation Detection using Small Language Models","summary":"  To avoid detection, adversaries often use command-line obfuscation. There are\nnumerous techniques of the command-line obfuscation, all designed to alter the\ncommand-line syntax without affecting its original functionality. This\nvariability forces most security solutions to create an exhaustive enumeration\nof signatures for even a single pattern. In contrast to using signatures, we\nhave implemented a scalable NLP-based detection method that leverages a\ncustom-trained, small transformer language model that can be applied to any\nsource of execution logs. The evaluation on top of real-world telemetry\ndemonstrates that our approach yields high-precision detections even on\nhigh-volume telemetry from a diverse set of environments spanning from\nuniversities and businesses to healthcare or finance. The practical value is\ndemonstrated in a case study of real-world samples detected by our model. We\nshow the model's superiority to signatures on established malware known to\nemploy obfuscation and showcase previously unseen obfuscated samples detected\nby our model.\n","authors":["Vojtech Outrata","Michael Adam Polak","Martin Kopp"],"pdf_url":"https://arxiv.org/pdf/2408.02637v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.02396v3","updated":"2024-08-05T16:49:51Z","published":"2023-12-04T23:26:12Z","title":"Unsupervised Change Detection for Space Habitats Using 3D Point Clouds","summary":"  This work presents an algorithm for scene change detection from point clouds\nto enable autonomous robotic caretaking in future space habitats. Autonomous\nrobotic systems will help maintain future deep-space habitats, such as the\nGateway space station, which will be uncrewed for extended periods. Existing\nscene analysis software used on the International Space Station (ISS) relies on\nmanually-labeled images for detecting changes. In contrast, the algorithm\npresented in this work uses raw, unlabeled point clouds as inputs. The\nalgorithm first applies modified Expectation-Maximization Gaussian Mixture\nModel (GMM) clustering to two input point clouds. It then performs change\ndetection by comparing the GMMs using the Earth Mover's Distance. The algorithm\nis validated quantitatively and qualitatively using a test dataset collected by\nan Astrobee robot in the NASA Ames Granite Lab comprising single frame depth\nimages taken directly by Astrobee and full-scene reconstructed maps built with\nRGB-D and pose data from Astrobee. The runtimes of the approach are also\nanalyzed in depth. The source code is publicly released to promote further\ndevelopment.\n","authors":["Jamie Santos","Holly Dinkel","Julia Di","Paulo V. K. Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2312.02396v3.pdf","comment":"15 pages, 7 figures, Manuscript was presented at the AIAA SciTech\n  Forum in Orlando, FL, USA, 8 - 12 January 2024. Video presentation:\n  [https://www.youtube.com/watch?v=7WHp0dQYG4Y]. Code:\n  [https://github.com/nasa/isaac/tree/master/anomaly/gmm-change-detection]"},{"id":"http://arxiv.org/abs/2303.07338v2","updated":"2024-08-05T16:34:43Z","published":"2023-03-13T17:59:02Z","title":"Revisiting Class-Incremental Learning with Pre-Trained Models:\n  Generalizability and Adaptivity are All You Need","summary":"  Class-incremental learning (CIL) aims to adapt to emerging new classes\nwithout forgetting old ones. Traditional CIL models are trained from scratch to\ncontinually acquire knowledge as data evolves. Recently, pre-training has\nachieved substantial progress, making vast pre-trained models (PTMs) accessible\nfor CIL. Contrary to traditional methods, PTMs possess generalizable\nembeddings, which can be easily transferred for CIL. In this work, we revisit\nCIL with PTMs and argue that the core factors in CIL are adaptivity for model\nupdating and generalizability for knowledge transferring. 1) We first reveal\nthat frozen PTM can already provide generalizable embeddings for CIL.\nSurprisingly, a simple baseline (SimpleCIL) which continually sets the\nclassifiers of PTM to prototype features can beat state-of-the-art even without\ntraining on the downstream task. 2) Due to the distribution gap between\npre-trained and downstream datasets, PTM can be further cultivated with\nadaptivity via model adaptation. We propose AdaPt and mERge (APER), which\naggregates the embeddings of PTM and adapted models for classifier\nconstruction. APER is a general framework that can be orthogonally combined\nwith any parameter-efficient tuning method, which holds the advantages of PTM's\ngeneralizability and adapted model's adaptivity. 3) Additionally, considering\nprevious ImageNet-based benchmarks are unsuitable in the era of PTM due to data\noverlapping, we propose four new benchmarks for assessment, namely ImageNet-A,\nObjectNet, OmniBenchmark, and VTAB. Extensive experiments validate the\neffectiveness of APER with a unified and concise framework. Code is available\nat https://github.com/zhoudw-zdw/RevisitingCIL\n","authors":["Da-Wei Zhou","Zi-Wen Cai","Han-Jia Ye","De-Chuan Zhan","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2303.07338v2.pdf","comment":"Accepted to IJCV. Code is available at:\n  https://github.com/zhoudw-zdw/RevisitingCIL"},{"id":"http://arxiv.org/abs/2408.02604v1","updated":"2024-08-05T16:27:38Z","published":"2024-08-05T16:27:38Z","title":"Learning rheological parameters of non-Newtonian fluids from velocimetry\n  data","summary":"  We solve a Bayesian inverse Navier-Stokes (N-S) problem that assimilates\nvelocimetry data in order to jointly reconstruct the flow field and learn the\nunknown N-S parameters. By incorporating a Carreau shear-thinning viscosity\nmodel into the N-S problem, we devise an algorithm that learns the most likely\nCarreau parameters of a shear-thinning fluid, and estimates their\nuncertainties, from velocimetry data alone. We then conduct a flow-MRI\nexperiment to obtain velocimetry data of an axisymmetric laminar jet through an\nidealised medical device (FDA nozzle) for a blood analogue fluid. We show that\nthe algorithm can successfully reconstruct the flow field by learning the most\nlikely Carreau parameters, and that the learned parameters are in very good\nagreement with rheometry measurements. The algorithm accepts any algebraic\neffective viscosity model, as long as the model is differentiable, and it can\nbe extended to more complicated non-Newtonian fluids (e.g. Oldroyd-B fluid) if\na viscoelastic model is incorporated into the N-S problem.\n","authors":["Alexandros Kontogiannis","Richard Hodgkinson","Emily L. Manchester"],"pdf_url":"https://arxiv.org/pdf/2408.02604v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02598v1","updated":"2024-08-05T16:15:31Z","published":"2024-08-05T16:15:31Z","title":"AI-Driven Strategies for Reducing Student Withdrawal -- A Study of EMU\n  Student Stopout","summary":"  Not everyone who enrolls in college will leave with a certificate or degree,\nbut the number of people who drop out or take a break is much higher than\nexperts previously believed. In December 2013, there were 29 million people\nwith some college education but no degree. That number jumped to 36 million by\nDecember of 2018, according to a new report from the National Student\nClearinghouse Research Center[1]. It is imperative to understand the underlying\nfactors contributing to student withdrawal and to assist decision-makers to\nidentify effective strategies to prevent it. By analyzing the characteristics\nand educational pathways of the stopout student population, our aim is to\nprovide actionable insights that can benefit institutions facing similar\nchallenges. Eastern Michigan University (EMU) faces significant challenges in\nstudent retention, with approximately 55% of its undergraduate students not\ncompleting their degrees within six years. As an institution committed to\nstudent success, EMU conducted a comprehensive study of student withdrawals to\nunderstand the influencing factors. And the paper revealed a high correlation\nbetween certain factors and withdrawals, even in the early stages of university\nattendance. Based on these findings, we developed a predictive model that\nemploys artificial intelligence techniques to assess the potential risk that\nstudents abandon their studies. These models enable universities to implement\nearly intervention strategies, support at-risk students, and improve overall\nhigher education success.\n","authors":["Yan Zhao","Amy Otteson"],"pdf_url":"https://arxiv.org/pdf/2408.02598v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2402.16517v2","updated":"2024-08-05T16:02:51Z","published":"2024-02-26T11:58:02Z","title":"Discovering Artificial Viscosity Models for Discontinuous Galerkin\n  Approximation of Conservation Laws using Physics-Informed Machine Learning","summary":"  Finite element-based high-order solvers of conservation laws offer large\naccuracy but face challenges near discontinuities due to the Gibbs phenomenon.\nArtificial viscosity is a popular and effective solution to this problem based\non physical insight. In this work, we present a physics-informed machine\nlearning algorithm to automate the discovery of artificial viscosity models in\na non-supervised paradigm. The algorithm is inspired by reinforcement learning\nand trains a neural network acting cell-by-cell (the viscosity model) by\nminimizing a loss defined as the difference with respect to a reference\nsolution thanks to automatic differentiation. This enables a dataset-free\ntraining procedure. We prove that the algorithm is effective by integrating it\ninto a state-of-the-art Runge-Kutta discontinuous Galerkin solver. We showcase\nseveral numerical tests on scalar and vectorial problems, such as Burgers' and\nEuler's equations in one and two dimensions. Results demonstrate that the\nproposed approach trains a model that is able to outperform classical viscosity\nmodels. Moreover, we show that the learnt artificial viscosity model is able to\ngeneralize across different problems and parameters.\n","authors":["Matteo Caldana","Paola F. Antonietti","Luca Dede'"],"pdf_url":"https://arxiv.org/pdf/2402.16517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02581v1","updated":"2024-08-05T15:59:36Z","published":"2024-08-05T15:59:36Z","title":"Operational range bounding of spectroscopy models with anomaly detection","summary":"  Safe operation of machine learning models requires architectures that\nexplicitly delimit their operational ranges. We evaluate the ability of anomaly\ndetection algorithms to provide indicators correlated with degraded model\nperformance. By placing acceptance thresholds over such indicators, hard\nboundaries are formed that define the model's coverage. As a use case, we\nconsider the extraction of exoplanetary spectra from transit light curves,\nspecifically within the context of ESA's upcoming Ariel mission. Isolation\nForests are shown to effectively identify contexts where prediction models are\nlikely to fail. Coverage/error trade-offs are evaluated under conditions of\ndata and concept drift. The best performance is seen when Isolation Forests\nmodel projections of the prediction model's explainability SHAP values.\n","authors":["Lus F. Simes","Pierluigi Casale","Marlia Felismino","Kai Hou Yip","Ingo P. Waldmann","Giovanna Tinetti","Theresa Lueftinger"],"pdf_url":"https://arxiv.org/pdf/2408.02581v1.pdf","comment":"To appear in \"Proceedings of SPAICE 2024: 1st ESA/IAA conference on\n  AI in and for Space\". Conference page at https://spaice.esa.int/"},{"id":"http://arxiv.org/abs/2405.06093v2","updated":"2024-08-05T15:51:50Z","published":"2024-05-09T20:45:58Z","title":"Selective Fine-tuning on LLM-labeled Data May Reduce Reliance on Human\n  Annotation: A Case Study Using Schedule-of-Event Table Detection","summary":"  Large Language Models (LLMs) have demonstrated their efficacy across a broad\nspectrum of tasks in healthcare applications. However, often LLMs need to be\nfine-tuned on task-specific expert annotated data to achieve optimal\nperformance, which can be expensive and time consuming. In this study, we\nfine-tune PaLM-2 with parameter efficient fine-tuning (PEFT) using noisy labels\nobtained from gemini-pro 1.0 for the detection of Schedule-of-Event (SoE)\ntables, which specify care plan in clinical trial protocols. We introduce a\nfiltering mechanism to select high-confidence labels for this table\nclassification task, thereby reducing the noise in the auto-generated labels.\nWe show that fine-tuned PaLM-2 with those labels achieves performance that\nexceeds the gemini-pro 1.0 and other LLMs. Furthermore, its performance is\nclose to a PaLM-2 fine-tuned on labels obtained from non-expert annotators. Our\nresults show that leveraging LLM-generated labels through powerful models like\ngemini-pro can potentially serve as a viable strategy for improving LLM\nperformance through fine-tuning in specialized tasks, particularly in domains\nwhere expert annotations are scarce, expensive, or time-consuming to obtain.\n","authors":["Bhawesh Kumar","Jonathan Amar","Eric Yang","Nan Li","Yugang Jia"],"pdf_url":"https://arxiv.org/pdf/2405.06093v2.pdf","comment":"23 pages. Published in MLHC 2024"},{"id":"http://arxiv.org/abs/2407.01281v2","updated":"2024-08-05T15:50:32Z","published":"2024-07-01T13:35:53Z","title":"Bridging Smoothness and Approximation: Theoretical Insights into\n  Over-Smoothing in Graph Neural Networks","summary":"  In this paper, we explore the approximation theory of functions defined on\ngraphs. Our study builds upon the approximation results derived from the\n$K$-functional. We establish a theoretical framework to assess the lower bounds\nof approximation for target functions using Graph Convolutional Networks (GCNs)\nand examine the over-smoothing phenomenon commonly observed in these networks.\nInitially, we introduce the concept of a $K$-functional on graphs, establishing\nits equivalence to the modulus of smoothness. We then analyze a typical type of\nGCN to demonstrate how the high-frequency energy of the output decays, an\nindicator of over-smoothing. This analysis provides theoretical insights into\nthe nature of over-smoothing within GCNs. Furthermore, we establish a lower\nbound for the approximation of target functions by GCNs, which is governed by\nthe modulus of smoothness of these functions. This finding offers a new\nperspective on the approximation capabilities of GCNs. In our numerical\nexperiments, we analyze several widely applied GCNs and observe the phenomenon\nof energy decay. These observations corroborate our theoretical results on\nexponential decay order.\n","authors":["Guangrui Yang","Jianfei Li","Ming Li","Han Feng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.01281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02575v1","updated":"2024-08-05T15:48:51Z","published":"2024-08-05T15:48:51Z","title":"Artificial Intelligence for Public Health Surveillance in Africa:\n  Applications and Opportunities","summary":"  Artificial Intelligence (AI) is revolutionizing various fields, including\npublic health surveillance. In Africa, where health systems frequently\nencounter challenges such as limited resources, inadequate infrastructure,\nfailed health information systems and a shortage of skilled health\nprofessionals, AI offers a transformative opportunity. This paper investigates\nthe applications of AI in public health surveillance across the continent,\npresenting successful case studies and examining the benefits, opportunities,\nand challenges of implementing AI technologies in African healthcare settings.\nOur paper highlights AI's potential to enhance disease monitoring and health\noutcomes, and support effective public health interventions. The findings\npresented in the paper demonstrate that AI can significantly improve the\naccuracy and timeliness of disease detection and prediction, optimize resource\nallocation, and facilitate targeted public health strategies. Additionally, our\npaper identified key barriers to the widespread adoption of AI in African\npublic health systems and proposed actionable recommendations to overcome these\nchallenges.\n","authors":["Jean Marie Tshimula","Mitterrand Kalengayi","Dieumerci Makenga","Dorcas Lilonge","Marius Asumani","Dborah Madiya","lie Nkuba Kalonji","Hugues Kanda","Ren Manass Galekwa","Josias Kumbu","Hardy Mikese","Grace Tshimula","Jean Tshibangu Muabila","Christian N. Mayemba","D'Jeff K. Nkashama","Kalonji Kalala","Steve Ataky","Tighana Wenge Basele","Mbuyi Mukendi Didier","Selain K. Kasereka","Maximilien V. Dialufuma","Godwill Ilunga Wa Kumwita","Lionel Muyuku","Jean-Paul Kimpesa","Dominique Muteba","Aaron Aruna Abedi","Lambert Mukendi Ntobo","Gloria M. Bundutidi","Dsir Kulimba Mashinda","Emmanuel Kabengele Mpinga","Nathanal M. Kasoro"],"pdf_url":"https://arxiv.org/pdf/2408.02575v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02568v1","updated":"2024-08-05T15:43:56Z","published":"2024-08-05T15:43:56Z","title":"Cross-Modality Clustering-based Self-Labeling for Multimodal Data\n  Classification","summary":"  Technological advances facilitate the ability to acquire multimodal data,\nposing a challenge for recognition systems while also providing an opportunity\nto use the heterogeneous nature of the information to increase the\ngeneralization capability of models. An often overlooked issue is the cost of\nthe labeling process, which is typically high due to the need for a significant\ninvestment in time and money associated with human experts. Existing\nsemi-supervised learning methods often focus on operating in the feature space\ncreated by the fusion of available modalities, neglecting the potential for\ncross-utilizing complementary information available in each modality. To\naddress this problem, we propose Cross-Modality Clustering-based Self-Labeling\n(CMCSL). Based on a small set of pre-labeled data, CMCSL groups instances\nbelonging to each modality in the deep feature space and then propagates known\nlabels within the resulting clusters. Next, information about the instances'\nclass membership in each modality is exchanged based on the Euclidean distance\nto ensure more accurate labeling. Experimental evaluation conducted on 20\ndatasets derived from the MM-IMDb dataset indicates that cross-propagation of\nlabels between modalities -- especially when the number of pre-labeled\ninstances is small -- can allow for more reliable labeling and thus increase\nthe classification performance in each modality.\n","authors":["Pawe Zyblewski","Leandro L. Minku"],"pdf_url":"https://arxiv.org/pdf/2408.02568v1.pdf","comment":"10 pages, 5 figures, 9 tables"},{"id":"http://arxiv.org/abs/2301.07088v3","updated":"2024-08-05T15:38:05Z","published":"2023-01-17T18:53:24Z","title":"Vision Learners Meet Web Image-Text Pairs","summary":"  Many self-supervised learning methods are pre-trained on the well-curated\nImageNet-1K dataset. In this work, given the excellent scalability of web data,\nwe consider self-supervised pre-training on noisy web sourced image-text paired\ndata. First, we conduct a benchmark study of representative self-supervised\npre-training methods on large-scale web data in a like-for-like setting. We\ncompare a range of methods, including single-modal ones that use masked\ntraining objectives and multi-modal ones that use image-text constrastive\ntraining. We observe that existing multi-modal methods do not outperform their\nsingle-modal counterparts on vision transfer learning tasks. We derive an\ninformation-theoretical view to explain these benchmark results, which provides\ninsight into how to design a novel vision learner. Inspired by this insight, we\npresent a new visual representation pre-training method, MUlti-modal\nGenerator~(MUG), that learns from scalable web sourced image-text data. MUG\nachieves state-of-the-art transfer performance on a variety of tasks and\ndemonstrates promising scaling properties. Pre-trained models and code will be\nmade public upon acceptance.\n","authors":["Bingchen Zhao","Quan Cui","Hao Wu","Osamu Yoshie","Cheng Yang","Oisin Mac Aodha"],"pdf_url":"https://arxiv.org/pdf/2301.07088v3.pdf","comment":"Project page: https://bzhao.me/MUG/"},{"id":"http://arxiv.org/abs/2408.02551v1","updated":"2024-08-05T15:26:39Z","published":"2024-08-05T15:26:39Z","title":"Process-constrained batch Bayesian approaches for yield optimization in\n  multi-reactor systems","summary":"  The optimization of yields in multi-reactor systems, which are advanced tools\nin heterogeneous catalysis research, presents a significant challenge due to\nhierarchical technical constraints. To this respect, this work introduces a\nnovel approach called process-constrained batch Bayesian optimization via\nThompson sampling (pc-BO-TS) and its generalized hierarchical extension\n(hpc-BO-TS). This method, tailored for the efficiency demands in multi-reactor\nsystems, integrates experimental constraints and balances between exploration\nand exploitation in a sequential batch optimization strategy. It offers an\nimprovement over other Bayesian optimization methods. The performance of\npc-BO-TS and hpc-BO-TS is validated in synthetic cases as well as in a\nrealistic scenario based on data obtained from high-throughput experiments done\non a multi-reactor system available in the REALCAT platform. The proposed\nmethods often outperform other sequential Bayesian optimizations and existing\nprocess-constrained batch Bayesian optimization methods. This work proposes a\nnovel approach to optimize the yield of a reaction in a multi-reactor system,\nmarking a significant step forward in digital catalysis and generally in\noptimization methods for chemical engineering.\n","authors":["Markus Grimm","Sbastien Paul","Pierre Chainais"],"pdf_url":"https://arxiv.org/pdf/2408.02551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02547v1","updated":"2024-08-05T15:17:34Z","published":"2024-08-05T15:17:34Z","title":"The Role of Functional Muscle Networks in Improving Hand Gesture\n  Perception for Human-Machine Interfaces","summary":"  Developing accurate hand gesture perception models is critical for various\nrobotic applications, enabling effective communication between humans and\nmachines and directly impacting neurorobotics and interactive robots. Recently,\nsurface electromyography (sEMG) has been explored for its rich informational\ncontext and accessibility when combined with advanced machine learning\napproaches and wearable systems. The literature presents numerous approaches to\nboost performance while ensuring robustness for neurorobots using sEMG, often\nresulting in models requiring high processing power, large datasets, and less\nscalable solutions. This paper addresses this challenge by proposing the\ndecoding of muscle synchronization rather than individual muscle activation. We\nstudy coherence-based functional muscle networks as the core of our perception\nmodel, proposing that functional synchronization between muscles and the\ngraph-based network of muscle connectivity encode contextual information about\nintended hand gestures. This can be decoded using shallow machine learning\napproaches without the need for deep temporal networks. Our technique could\nimpact myoelectric control of neurorobots by reducing computational burdens and\nenhancing efficiency. The approach is benchmarked on the Ninapro database,\nwhich contains 12 EMG signals from 40 subjects performing 17 hand gestures. It\nachieves an accuracy of 85.1%, demonstrating improved performance compared to\nexisting methods while requiring much less computational power. The results\nsupport the hypothesis that a coherence-based functional muscle network encodes\ncritical information related to gesture execution, significantly enhancing hand\ngesture perception with potential applications for neurorobotic systems and\ninteractive machines.\n","authors":["Costanza Armanini","Tuka Alhanai","Farah E. Shamout","S. Farokh Atashzar"],"pdf_url":"https://arxiv.org/pdf/2408.02547v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02545v1","updated":"2024-08-05T15:16:24Z","published":"2024-08-05T15:16:24Z","title":"RAG Foundry: A Framework for Enhancing LLMs for Retrieval Augmented\n  Generation","summary":"  Implementing Retrieval-Augmented Generation (RAG) systems is inherently\ncomplex, requiring deep understanding of data, use cases, and intricate design\ndecisions. Additionally, evaluating these systems presents significant\nchallenges, necessitating assessment of both retrieval accuracy and generative\nquality through a multi-faceted approach. We introduce RAG Foundry, an\nopen-source framework for augmenting large language models for RAG use cases.\nRAG Foundry integrates data creation, training, inference and evaluation into a\nsingle workflow, facilitating the creation of data-augmented datasets for\ntraining and evaluating large language models in RAG settings. This integration\nenables rapid prototyping and experimentation with various RAG techniques,\nallowing users to easily generate datasets and train RAG models using internal\nor specialized knowledge sources. We demonstrate the framework effectiveness by\naugmenting and fine-tuning Llama-3 and Phi-3 models with diverse RAG\nconfigurations, showcasing consistent improvements across three\nknowledge-intensive datasets. Code is released as open-source in\nhttps://github.com/IntelLabs/RAGFoundry.\n","authors":["Daniel Fleischer","Moshe Berchansky","Moshe Wasserblat","Peter Izsak"],"pdf_url":"https://arxiv.org/pdf/2408.02545v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2406.04216v3","updated":"2024-08-05T15:08:02Z","published":"2024-06-06T16:15:34Z","title":"What Do Language Models Learn in Context? The Structured Task Hypothesis","summary":"  Large language models (LLMs) exhibit an intriguing ability to learn a novel\ntask from in-context examples presented in a demonstration, termed in-context\nlearning (ICL). Understandably, a swath of research has been dedicated to\nuncovering the theories underpinning ICL. One popular hypothesis explains ICL\nby task selection. LLMs identify the task based on the demonstration and\ngeneralize it to the prompt. Another popular hypothesis is that ICL is a form\nof meta-learning, i.e., the models learn a learning algorithm at pre-training\ntime and apply it to the demonstration. Finally, a third hypothesis argues that\nLLMs use the demonstration to select a composition of tasks learned during\npre-training to perform ICL. In this paper, we empirically explore these three\nhypotheses that explain LLMs' ability to learn in context with a suite of\nexperiments derived from common text classification tasks. We invalidate the\nfirst two hypotheses with counterexamples and provide evidence in support of\nthe last hypothesis. Our results suggest an LLM could learn a novel task in\ncontext via composing tasks learned during pre-training.\n","authors":["Jiaoda Li","Yifan Hou","Mrinmaya Sachan","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2406.04216v3.pdf","comment":"This work is published in ACL 2024"},{"id":"http://arxiv.org/abs/2310.02812v2","updated":"2024-08-05T15:06:24Z","published":"2023-10-04T13:37:34Z","title":"Time-Series Classification in Smart Manufacturing Systems: An\n  Experimental Evaluation of State-of-the-Art Machine Learning Algorithms","summary":"  Manufacturing is gathering extensive amounts of diverse data, thanks to the\ngrowing number of sensors and rapid advances in sensing technologies. Among the\nvarious data types available in SMS settings, time-series data plays a pivotal\nrole. Hence, TSC emerges is crucial in this domain. The objective of this study\nis to fill this gap by providing a rigorous experimental evaluation of the SoTA\nML and DL algorithms for TSC tasks in manufacturing and industrial settings. We\nfirst explored and compiled a comprehensive list of more than 92 SoTA\nalgorithms from both TSC and manufacturing literature. Following, we selected\nthe 36 most representative algorithms from this list. To evaluate their\nperformance across various manufacturing classification tasks, we curated a set\nof 22 manufacturing datasets, representative of different characteristics that\ncover diverse manufacturing problems. Subsequently, we implemented and\nevaluated the algorithms on the manufacturing benchmark datasets, and analyzed\nthe results for each dataset. Based on the results, ResNet, DrCIF,\nInceptionTime, and ARSENAL are the top-performing algorithms, boasting an\naverage accuracy of over 96.6% across all 22 manufacturing TSC datasets. These\nfindings underscore the robustness, efficiency, scalability, and effectiveness\nof convolutional kernels in capturing temporal features in time-series data, as\nthree out of the top four performing algorithms leverage these kernels for\nfeature extraction. Additionally, LSTM, BiLSTM, and TS-LSTM algorithms deserve\nrecognition for their effectiveness in capturing features within time-series\ndata using RNN-based structures.\n","authors":["Mojtaba A. Farahani","M. R. McCormick","Ramy Harik","Thorsten Wuest"],"pdf_url":"https://arxiv.org/pdf/2310.02812v2.pdf","comment":"Published in Robotics and Computer-Integrated Manufacturing journal"},{"id":"http://arxiv.org/abs/2408.02533v1","updated":"2024-08-05T15:03:19Z","published":"2024-08-05T15:03:19Z","title":"LMEMs for post-hoc analysis of HPO Benchmarking","summary":"  The importance of tuning hyperparameters in Machine Learning (ML) and Deep\nLearning (DL) is established through empirical research and applications,\nevident from the increase in new hyperparameter optimization (HPO) algorithms\nand benchmarks steadily added by the community. However, current benchmarking\npractices using averaged performance across many datasets may obscure key\ndifferences between HPO methods, especially for pairwise comparisons. In this\nwork, we apply Linear Mixed-Effect Models-based (LMEMs) significance testing\nfor post-hoc analysis of HPO benchmarking runs. LMEMs allow flexible and\nexpressive modeling on the entire experiment data, including information such\nas benchmark meta-features, offering deeper insights than current analysis\npractices. We demonstrate this through a case study on the PriorBand paper's\nexperiment data to find insights not reported in the original work.\n","authors":["Anton Geburek","Neeratyoy Mallik","Danny Stoll","Xavier Bouthillier","Frank Hutter"],"pdf_url":"https://arxiv.org/pdf/2408.02533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.14294v2","updated":"2024-08-05T14:59:27Z","published":"2024-02-22T05:16:04Z","title":"High-arity PAC learning via exchangeability","summary":"  We develop a theory of high-arity PAC learning, which is statistical learning\nin the presence of \"structured correlation\". In this theory, hypotheses are\neither graphs, hypergraphs or, more generally, structures in finite relational\nlanguages, and i.i.d. sampling is replaced by sampling an induced substructure,\nproducing an exchangeable distribution. Our main theorems establish a\nhigh-arity (agnostic) version of the fundamental theorem of statistical\nlearning.\n","authors":["Leonardo N. Coregliano","Maryanthe Malliaris"],"pdf_url":"https://arxiv.org/pdf/2402.14294v2.pdf","comment":"150 pages, 1 figure. (This version makes expository changes to\n  Sections 1 and 2 and adds Appendix B on Bayes predictors.)"},{"id":"http://arxiv.org/abs/2408.02525v1","updated":"2024-08-05T14:46:04Z","published":"2024-08-05T14:46:04Z","title":"Single-tap Latency Reduction with Single- or Double- tap Prediction","summary":"  Touch surfaces are widely utilized for smartphones, tablet PCs, and laptops\n(touchpad), and single and double taps are the most basic and common operations\non them. The detection of single or double taps causes the single-tap latency\nproblem, which creates a bottleneck in terms of the sensitivity of touch\ninputs. To reduce the single-tap latency, we propose a novel\nmachine-learning-based tap prediction method called PredicTaps. Our method\npredicts whether a detected tap is a single tap or the first contact of a\ndouble tap without having to wait for the hundreds of milliseconds\nconventionally required. We present three evaluations and one user evaluation\nthat demonstrate its broad applicability and usability for various tap\nsituations on two form factors (touchpad and smartphone). The results showed\nPredicTaps reduces the single-tap latency from 150-500 ms to 12 ms on laptops\nand to 17.6 ms on smartphones without reducing usability.\n","authors":["Naoto Nishida","Kaori Ikematsu","Junichi Sato","Shota Yamanaka","Kota Tsubouchi"],"pdf_url":"https://arxiv.org/pdf/2408.02525v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02514v1","updated":"2024-08-05T14:34:40Z","published":"2024-08-05T14:34:40Z","title":"Stem-JEPA: A Joint-Embedding Predictive Architecture for Musical Stem\n  Compatibility Estimation","summary":"  This paper explores the automated process of determining stem compatibility\nby identifying audio recordings of single instruments that blend well with a\ngiven musical context. To tackle this challenge, we present Stem-JEPA, a novel\nJoint-Embedding Predictive Architecture (JEPA) trained on a multi-track dataset\nusing a self-supervised learning approach.\n  Our model comprises two networks: an encoder and a predictor, which are\njointly trained to predict the embeddings of compatible stems from the\nembeddings of a given context, typically a mix of several instruments. Training\na model in this manner allows its use in estimating stem compatibility -\nretrieving, aligning, or generating a stem to match a given mix - or for\ndownstream tasks such as genre or key estimation, as the training paradigm\nrequires the model to learn information related to timbre, harmony, and rhythm.\n  We evaluate our model's performance on a retrieval task on the MUSDB18\ndataset, testing its ability to find the missing stem from a mix and through a\nsubjective user study. We also show that the learned embeddings capture\ntemporal alignment information and, finally, evaluate the representations\nlearned by our model on several downstream tasks, highlighting that they\neffectively capture meaningful musical features.\n","authors":["Alain Riou","Stefan Lattner","Gatan Hadjeres","Michael Anslow","Geoffroy Peeters"],"pdf_url":"https://arxiv.org/pdf/2408.02514v1.pdf","comment":"Proceedings of the 25th International Society for Music Information\n  Retrieval Conference, ISMIR 2024"},{"id":"http://arxiv.org/abs/2408.02509v1","updated":"2024-08-05T14:31:26Z","published":"2024-08-05T14:31:26Z","title":"Practical Attacks against Black-box Code Completion Engines","summary":"  Modern code completion engines, powered by large language models, have\ndemonstrated impressive capabilities to generate functionally correct code\nbased on surrounding context. As these tools are extensively used by millions\nof developers, it is crucial to investigate their security implications. In\nthis work, we present INSEC, a novel attack that directs code completion\nengines towards generating vulnerable code. In line with most commercial\ncompletion engines, such as GitHub Copilot, INSEC assumes only black-box query\naccess to the targeted engine, without requiring any knowledge of the engine's\ninternals. Our attack works by inserting a malicious attack string as a short\ncomment in the completion input. To derive the attack string, we design a\nseries of specialized initialization schemes and an optimization procedure for\nfurther refinement. We demonstrate the strength of INSEC not only on\nstate-of-the-art open-source models but also on black-box commercial services\nsuch as the OpenAI API and GitHub Copilot. On a comprehensive set of\nsecurity-critical test cases covering 16 CWEs across 5 programming languages,\nINSEC significantly increases the likelihood of the considered completion\nengines in generating unsafe code by >50% in absolute, while maintaining the\nability in producing functionally correct code. At the same time, our attack\nhas low resource requirements, and can be developed for a cost of well under\nten USD on commodity hardware.\n","authors":["Slobodan Jenko","Jingxuan He","Niels Mndler","Mark Vero","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2408.02509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02496v1","updated":"2024-08-05T14:19:03Z","published":"2024-08-05T14:19:03Z","title":"Automatic rating of incomplete hippocampal inversions evaluated across\n  multiple cohorts","summary":"  Incomplete Hippocampal Inversion (IHI), sometimes called hippocampal\nmalrotation, is an atypical anatomical pattern of the hippocampus found in\nabout 20% of the general population. IHI can be visually assessed on coronal\nslices of T1 weighted MR images, using a composite score that combines four\nanatomical criteria. IHI has been associated with several brain disorders\n(epilepsy, schizophrenia). However, these studies were based on small samples.\nFurthermore, the factors (genetic or environmental) that contribute to the\ngenesis of IHI are largely unknown. Large-scale studies are thus needed to\nfurther understand IHI and their potential relationships to neurological and\npsychiatric disorders. However, visual evaluation is long and tedious,\njustifying the need for an automatic method. In this paper, we propose, for the\nfirst time, to automatically rate IHI. We proceed by predicting four anatomical\ncriteria, which are then summed up to form the IHI score, providing the\nadvantage of an interpretable score. We provided an extensive experimental\ninvestigation of different machine learning methods and training strategies. We\nperformed automatic rating using a variety of deep learning models (conv5-FC3,\nResNet and SECNN) as well as a ridge regression. We studied the generalization\nof our models using different cohorts and performed multi-cohort learning. We\nrelied on a large population of 2,008 participants from the IMAGEN study, 993\nand 403 participants from the QTIM/QTAB studies as well as 985 subjects from\nthe UKBiobank. We showed that deep learning models outperformed a ridge\nregression. We demonstrated that the performances of the conv5-FC3 network were\nat least as good as more complex networks while maintaining a low complexity\nand computation time. We showed that training on a single cohort may lack in\nvariability while training on several cohorts improves generalization.\n","authors":["Lisa Hemforth","Baptiste Couvy-Duchesne","Kevin De Matos","Camille Brianceau","Matthieu Joulot","Tobias Banaschewski","Arun L. W. Bokde","Sylvane Desrivires","Herta Flor","Antoine Grigis","Hugh Garavan","Penny Gowland","Andreas Heinz","Rdiger Brhl","Jean-Luc Martinot","Marie-Laure Paillre Martinot","Eric Artiges","Dimitri Papadopoulos","Herve Lemaitre","Tomas Paus","Luise Poustka","Sarah Hohmann","Nathalie Holz","Juliane H. Frhner","Michael N. Smolka","Nilakshi Vaidya","Henrik Walter","Robert Whelan","Gunter Schumann","Christian Bchel","JB Poline","Bernd Itterman","Vincent Frouin","Alexandre Martin","IMAGEN study group","Claire Cury","Olivier Colliot"],"pdf_url":"https://arxiv.org/pdf/2408.02496v1.pdf","comment":"Accepted for publication at the Journal of Machine Learning for\n  Biomedical Imaging (MELBA) https://melba-journal.org/2024:016"},{"id":"http://arxiv.org/abs/2408.02487v1","updated":"2024-08-05T14:09:30Z","published":"2024-08-05T14:09:30Z","title":"A First Look at License Compliance Capability of LLMs in Code Generation","summary":"  Recent advances in Large Language Models (LLMs) have revolutionized code\ngeneration, leading to widespread adoption of AI coding tools by developers.\nHowever, LLMs can generate license-protected code without providing the\nnecessary license information, leading to potential intellectual property\nviolations during software production. This paper addresses the critical, yet\nunderexplored, issue of license compliance in LLM-generated code by\nestablishing a benchmark to evaluate the ability of LLMs to provide accurate\nlicense information for their generated code. To establish this benchmark, we\nconduct an empirical study to identify a reasonable standard for \"striking\nsimilarity\" that excludes the possibility of independent creation, indicating a\ncopy relationship between the LLM output and certain open-source code. Based on\nthis standard, we propose an evaluation benchmark LiCoEval, to evaluate the\nlicense compliance capabilities of LLMs. Using LiCoEval, we evaluate 14 popular\nLLMs, finding that even top-performing LLMs produce a non-negligible proportion\n(0.88% to 2.01%) of code strikingly similar to existing open-source\nimplementations. Notably, most LLMs fail to provide accurate license\ninformation, particularly for code under copyleft licenses. These findings\nunderscore the urgent need to enhance LLM compliance capabilities in code\ngeneration tasks. Our study provides a foundation for future research and\ndevelopment to improve license compliance in AI-assisted software development,\ncontributing to both the protection of open-source software copyrights and the\nmitigation of legal risks for LLM users.\n","authors":["Weiwei Xu","Kai Gao","Hao He","Minghui Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02487v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02473v1","updated":"2024-08-05T13:57:32Z","published":"2024-08-05T13:57:32Z","title":"Toward Attention-based TinyML: A Heterogeneous Accelerated Architecture\n  and Automated Deployment Flow","summary":"  One of the challenges for Tiny Machine Learning (tinyML) is keeping up with\nthe evolution of Machine Learning models from Convolutional Neural Networks to\nTransformers. We address this by leveraging a heterogeneous architectural\ntemplate coupling RISC-V processors with hardwired accelerators supported by an\nautomated deployment flow. We demonstrate an Attention-based model in a tinyML\npower envelope with an octa-core cluster coupled with an accelerator for\nquantized Attention. Our deployment flow enables an end-to-end 8-bit\nMobileBERT, achieving leading-edge energy efficiency and throughput of 2960\nGOp/J and 154 GOp/s at 32.5 Inf/s consuming 52.0 mW (0.65 V, 22 nm FD-SOI\ntechnology).\n","authors":["Philip Wiese","Gamze slamolu","Moritz Scherer","Luka Macan","Victor J. B. Jung","Alessio Burrello","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2408.02473v1.pdf","comment":"Pre-print manuscript submitted for review to the IEEE Design and Test\n  Special Issue on tinyML"},{"id":"http://arxiv.org/abs/2408.02456v1","updated":"2024-08-05T13:28:51Z","published":"2024-08-05T13:28:51Z","title":"Enhancing Heterogeneous Knowledge Graph Completion with a Novel\n  GAT-based Approach","summary":"  Knowledge graphs (KGs) play a vital role in enhancing search results and\nrecommendation systems. With the rapid increase in the size of the KGs, they\nare becoming inaccuracy and incomplete. This problem can be solved by the\nknowledge graph completion methods, of which graph attention network\n(GAT)-based methods stand out since their superior performance. However,\nexisting GAT-based knowledge graph completion methods often suffer from\noverfitting issues when dealing with heterogeneous knowledge graphs, primarily\ndue to the unbalanced number of samples. Additionally, these methods\ndemonstrate poor performance in predicting the tail (head) entity that shares\nthe same relation and head (tail) entity with others. To solve these problems,\nwe propose GATH, a novel GAT-based method designed for Heterogeneous KGs. GATH\nincorporates two separate attention network modules that work synergistically\nto predict the missing entities. We also introduce novel encoding and feature\ntransformation approaches, enabling the robust performance of GATH in scenarios\nwith imbalanced samples. Comprehensive experiments are conducted to evaluate\nthe GATH's performance. Compared with the existing SOTA GAT-based model on\nHits@10 and MRR metrics, our model improves performance by 5.2% and 5.2% on the\nFB15K-237 dataset, and by 4.5% and 14.6% on the WN18RR dataset, respectively.\n","authors":["Wanxu Wei","Yitong Song","Bin Yao"],"pdf_url":"https://arxiv.org/pdf/2408.02456v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.03453v2","updated":"2024-08-05T13:01:47Z","published":"2024-04-04T13:57:44Z","title":"Conditioning of Banach Space Valued Gaussian Random Variables: An\n  Approximation Approach Based on Martingales","summary":"  In this paper we investigate the conditional distributions of two Banach\nspace valued, jointly Gaussian random variables. We show that these conditional\ndistributions are again Gaussian and that their means and covariances are\ndetermined by a general finite dimensional approximation scheme based upon a\nmartingale approach. In particular, it turns out that the covariance operators\noccurring in this scheme converge with respect to the nuclear norm and that the\nconditional probabilities converge weakly. Moreover, we discuss in detail, how\nour approximation scheme can be implemented in several classes of important\nBanach spaces such as RKHSs and $C(T)$. As an example, we then apply our\ngeneral results to the case of Gaussian processes with continuous paths\nconditioned to partial but infinite observations of their paths. Here we show\nthat conditioning on sufficiently rich, increasing sets of finitely many\nobservations leads to consistent approximations, in the sense that both the\nmean and covariance functions converge uniformly. Moreover, we discuss how\nthese results improve our understanding of the popular Gaussian processes for\nmachine learning.\n","authors":["Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2404.03453v2.pdf","comment":"52 pages plus 22 pages of supplemental material"},{"id":"http://arxiv.org/abs/2405.14244v2","updated":"2024-08-05T12:59:32Z","published":"2024-05-23T07:23:33Z","title":"Tell me why: Training preferences-based RL with human preferences and\n  step-level explanations","summary":"  Human-in-the-loop reinforcement learning allows the training of agents\nthrough various interfaces, even for non-expert humans. Recently,\npreference-based methods (PbRL), where the human has to give his preference\nover two trajectories, increased in popularity since they allow training in\ndomains where more direct feedback is hard to formulate. However, the current\nPBRL methods have limitations and do not provide humans with an expressive\ninterface for giving feedback. With this work, we propose a new\npreference-based learning method that provides humans with a more expressive\ninterface to provide their preference over trajectories and a factual\nexplanation (or annotation of why they have this preference). These\nexplanations allow the human to explain what parts of the trajectory are most\nrelevant for the preference. We allow the expression of the explanations over\nindividual trajectory steps. We evaluate our method in various simulations\nusing a simulated human oracle (with realistic restrictions), and our results\nshow that our extended feedback can improve the speed of learning.\n","authors":["Jakob Karalus"],"pdf_url":"https://arxiv.org/pdf/2405.14244v2.pdf","comment":"Workshop on Reinforcement Learning Beyond Rewards @ Reinforcement\n  Learning Conference (2024)"},{"id":"http://arxiv.org/abs/2202.04309v2","updated":"2024-08-05T12:58:37Z","published":"2022-02-09T06:56:41Z","title":"Vertical Federated Learning: Challenges, Methodologies and Experiments","summary":"  Recently, federated learning (FL) has emerged as a promising distributed\nmachine learning (ML) technology, owing to the advancing computational and\nsensing capacities of end-user devices, however with the increasing concerns on\nusers' privacy. As a special architecture in FL, vertical FL (VFL) is capable\nof constructing a hyper ML model by embracing sub-models from different\nclients. These sub-models are trained locally by vertically partitioned data\nwith distinct attributes. Therefore, the design of VFL is fundamentally\ndifferent from that of conventional FL, raising new and unique research issues.\nIn this paper, we aim to discuss key challenges in VFL with effective\nsolutions, and conduct experiments on real-life datasets to shed light on these\nissues. Specifically, we first propose a general framework on VFL, and\nhighlight the key differences between VFL and conventional FL. Then, we discuss\nresearch challenges rooted in VFL systems under four aspects, i.e., security\nand privacy risks, expensive computation and communication costs, possible\nstructural damage caused by model splitting, and system heterogeneity.\nAfterwards, we develop solutions to addressing the aforementioned challenges,\nand conduct extensive experiments to showcase the effectiveness of our proposed\nsolutions.\n","authors":["Kang Wei","Jun Li","Chuan Ma","Ming Ding","Sha Wei","Fan Wu","Guihai Chen","Thilina Ranbaduge"],"pdf_url":"https://arxiv.org/pdf/2202.04309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02433v1","updated":"2024-08-05T12:46:21Z","published":"2024-08-05T12:46:21Z","title":"On Probabilistic Embeddings in Optimal Dimension Reduction","summary":"  Dimension reduction algorithms are a crucial part of many data science\npipelines, including data exploration, feature creation and selection, and\ndenoising. Despite their wide utilization, many non-linear dimension reduction\nalgorithms are poorly understood from a theoretical perspective. In this work\nwe consider a generalized version of multidimensional scaling, which is posed\nas an optimization problem in which a mapping from a high-dimensional feature\nspace to a lower-dimensional embedding space seeks to preserve either inner\nproducts or norms of the distribution in feature space, and which encompasses\nmany commonly used dimension reduction algorithms. We analytically investigate\nthe variational properties of this problem, leading to the following insights:\n1) Solutions found using standard particle descent methods may lead to\nnon-deterministic embeddings, 2) A relaxed or probabilistic formulation of the\nproblem admits solutions with easily interpretable necessary conditions, 3) The\nglobally optimal solutions to the relaxed problem actually must give a\ndeterministic embedding. This progression of results mirrors the classical\ndevelopment of optimal transportation, and in a case relating to the\nGromov-Wasserstein distance actually gives explicit insight into the structure\nof the optimal embeddings, which are parametrically determined and\ndiscontinuous. Finally, we illustrate that a standard computational\nimplementation of this task does not learn deterministic embeddings, which\nmeans that it learns sub-optimal mappings, and that the embeddings learned in\nthat context have highly misleading clustering structure, underscoring the\ndelicate nature of solving this problem computationally.\n","authors":["Ryan Murray","Adam Pickarski"],"pdf_url":"https://arxiv.org/pdf/2408.02433v1.pdf","comment":"26 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2303.10571v2","updated":"2024-08-05T12:44:04Z","published":"2023-03-19T05:20:52Z","title":"Reinforcement Learning Friendly Vision-Language Model for Minecraft","summary":"  One of the essential missions in the AI research community is to build an\nautonomous embodied agent that can achieve high-level performance across a wide\nspectrum of tasks. However, acquiring or manually designing rewards for all\nopen-ended tasks is unrealistic. In this paper, we propose a novel cross-modal\ncontrastive learning framework architecture, CLIP4MC, aiming to learn a\nreinforcement learning (RL) friendly vision-language model (VLM) that serves as\nan intrinsic reward function for open-ended tasks. Simply utilizing the\nsimilarity between the video snippet and the language prompt is not RL-friendly\nsince standard VLMs may only capture the similarity at a coarse level. To\nachieve RL-friendliness, we incorporate the task completion degree into the VLM\ntraining objective, as this information can assist agents in distinguishing the\nimportance between different states. Moreover, we provide neat YouTube datasets\nbased on the large-scale YouTube database provided by MineDojo. Specifically,\ntwo rounds of filtering operations guarantee that the dataset covers enough\nessential information and that the video-text pair is highly correlated.\nEmpirically, we demonstrate that the proposed method achieves better\nperformance on RL tasks compared with baselines. The code and datasets are\navailable at https://github.com/PKU-RL/CLIP4MC.\n","authors":["Haobin Jiang","Junpeng Yue","Hao Luo","Ziluo Ding","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2303.10571v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2407.19858v2","updated":"2024-08-05T12:42:38Z","published":"2024-07-29T10:26:52Z","title":"AI-Powered Energy Algorithmic Trading: Integrating Hidden Markov Models\n  with Neural Networks","summary":"  In quantitative finance, machine learning methods are essential for alpha\ngeneration. This study introduces a new approach that combines Hidden Markov\nModels (HMM) and neural networks, integrated with Black-Litterman portfolio\noptimization. During the COVID period (2019-2022), this dual-model approach\nachieved a 97% return with a Sharpe ratio of 0.992. It incorporates two risk\nmodels to enhance risk management, showing efficiency during volatile periods.\nThe methodology was implemented on the QuantConnect platform, which was chosen\nfor its robust framework and experimental reproducibility. The system, which\npredicts future price movements, includes a three-year warm-up to ensure proper\nalgorithm function. It targets highly liquid, large-cap energy stocks to ensure\nstable and predictable performance while also considering broker payments. The\ndual-model alpha system utilizes log returns to select the optimal state based\non the historical performance. It combines state predictions with neural\nnetwork outputs, which are based on historical data, to generate trading\nsignals. This study examined the architecture of the trading system, data\npre-processing, training, and performance. The full code and backtesting data\nare available under the MIT license.\n","authors":["Tiago Monteiro"],"pdf_url":"https://arxiv.org/pdf/2407.19858v2.pdf","comment":"14 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02427v1","updated":"2024-08-05T12:34:49Z","published":"2024-08-05T12:34:49Z","title":"Attenuation-adjusted deep learning of pore defects in 2D radiographs of\n  additive manufacturing powders","summary":"  The presence of gas pores in metal feedstock powder for additive\nmanufacturing greatly affects the final AM product. Since current porosity\nanalysis often involves lengthy X-ray computed tomography (XCT) scans with a\nfull rotation around the sample, motivation exists to explore methods that\nallow for high throughput -- possibly enabling in-line porosity analysis during\nmanufacturing. Through labelling pore pixels on single 2D radiographs of\npowders, this work seeks to simulate such future efficient setups. High\nsegmentation accuracy is achieved by combining a model of X-ray attenuation\nthrough particles with a variant of the widely applied UNet architecture;\nnotably, F1-score increases by $11.4\\%$ compared to the baseline UNet. The\nproposed pore segmentation is enabled by: 1) pretraining on synthetic data, 2)\nmaking tight particle cutouts, and 3) subtracting an ideal particle without\npores generated from a distance map inspired by Lambert-Beers law. This paper\nexplores four image processing methods, where the fastest (yet still\nunoptimized) segments a particle in mean $0.014s$ time with F1-score $0.78$,\nand the most accurate in $0.291s$ with F1-score $0.87$. Due to their scalable\nnature, these strategies can be involved in making high throughput porosity\nanalysis of metal feedstock powder for additive manufacturing.\n","authors":["Andreas Bjerregaard","David Schumacher","Jon Sporring"],"pdf_url":"https://arxiv.org/pdf/2408.02427v1.pdf","comment":"Implementation on https://github.com/yhsure/porosity"},{"id":"http://arxiv.org/abs/2402.12198v2","updated":"2024-08-05T12:20:49Z","published":"2024-02-19T15:03:04Z","title":"Zero shot VLMs for hate meme detection: Are we there yet?","summary":"  Multimedia content on social media is rapidly evolving, with memes gaining\nprominence as a distinctive form. Unfortunately, some malicious users exploit\nmemes to target individuals or vulnerable communities, making it imperative to\nidentify and address such instances of hateful memes. Extensive research has\nbeen conducted to address this issue by developing hate meme detection models.\nHowever, a notable limitation of traditional machine/deep learning models is\nthe requirement for labeled datasets for accurate classification. Recently, the\nresearch community has witnessed the emergence of several visual language\nmodels that have exhibited outstanding performance across various tasks. In\nthis study, we aim to investigate the efficacy of these visual language models\nin handling intricate tasks such as hate meme detection. We use various prompt\nsettings to focus on zero-shot classification of hateful/harmful memes. Through\nour analysis, we observe that large VLMs are still vulnerable for zero-shot\nhate meme detection.\n","authors":["Naquee Rizwan","Paramananda Bhaskar","Mithun Das","Swadhin Satyaprakash Majhi","Punyajoy Saha","Animesh Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2402.12198v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02412v1","updated":"2024-08-05T12:11:09Z","published":"2024-08-05T12:11:09Z","title":"PENDRAM: Enabling High-Performance and Energy-Efficient Processing of\n  Deep Neural Networks through a Generalized DRAM Data Mapping Policy","summary":"  Convolutional Neural Networks (CNNs), a prominent type of Deep Neural\nNetworks (DNNs), have emerged as a state-of-the-art solution for solving\nmachine learning tasks. To improve the performance and energy efficiency of CNN\ninference, the employment of specialized hardware accelerators is prevalent.\nHowever, CNN accelerators still face performance- and energy-efficiency\nchallenges due to high off-chip memory (DRAM) access latency and energy, which\nare especially crucial for latency- and energy-constrained embedded\napplications. Moreover, different DRAM architectures have different profiles of\naccess latency and energy, thus making it challenging to optimize them for high\nperformance and energy-efficient CNN accelerators. To address this, we present\nPENDRAM, a novel design space exploration methodology that enables\nhigh-performance and energy-efficient CNN acceleration through a generalized\nDRAM data mapping policy. Specifically, it explores the impact of different\nDRAM data mapping policies and DRAM architectures across different CNN\npartitioning and scheduling schemes on the DRAM access latency and energy, then\nidentifies the pareto-optimal design choices. The experimental results show\nthat our DRAM data mapping policy improves the energy-delay-product of DRAM\naccesses in the CNN accelerator over other mapping policies by up to 96%. In\nthis manner, our PENDRAM methodology offers high-performance and\nenergy-efficient CNN acceleration under any given DRAM architectures for\ndiverse embedded AI applications.\n","authors":["Rachmad Vidya Wicaksana Putra","Muhammad Abdullah Hanif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.02412v1.pdf","comment":"11 pages, 15 figures, 2 tables. arXiv admin note: substantial text\n  overlap with arXiv:2004.10341"},{"id":"http://arxiv.org/abs/2408.02407v1","updated":"2024-08-05T12:01:42Z","published":"2024-08-05T12:01:42Z","title":"Terracorder: Sense Long and Prosper","summary":"  In-situ sensing devices need to be deployed in remote environments for long\nperiods of time; minimizing their power consumption is vital for maximising\nboth their operational lifetime and coverage. We introduce Terracorder -- a\nversatile multi-sensor device -- and showcase its exceptionally low power\nconsumption using an on-device reinforcement learning scheduler. We prototype a\nunique device setup for biodiversity monitoring and compare its battery life\nusing our scheduler against a number of fixed schedules; the scheduler captures\nmore than 80% of events at less than 50% of the number of activations of the\nbest-performing fixed schedule. We then explore how a collaborative scheduler\ncan maximise the useful operation of a network of devices, improving overall\nnetwork power consumption and robustness.\n","authors":["Josh Millar","Sarab Sethi","Hamed Haddadi","Anil Madhavapeddy"],"pdf_url":"https://arxiv.org/pdf/2408.02407v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2403.05996v3","updated":"2024-08-05T11:55:19Z","published":"2024-03-09T19:56:40Z","title":"Dissecting Deep RL with High Update Ratios: Combatting Value Divergence","summary":"  We show that deep reinforcement learning algorithms can retain their ability\nto learn without resetting network parameters in settings where the number of\ngradient updates greatly exceeds the number of environment samples by\ncombatting value function divergence. Under large update-to-data ratios, a\nrecent study by Nikishin et al. (2022) suggested the emergence of a primacy\nbias, in which agents overfit early interactions and downplay later experience,\nimpairing their ability to learn. In this work, we investigate the phenomena\nleading to the primacy bias. We inspect the early stages of training that were\nconjectured to cause the failure to learn and find that one fundamental\nchallenge is a long-standing acquaintance: value function divergence.\nOverinflated Q-values are found not only on out-of-distribution but also\nin-distribution data and can be linked to overestimation on unseen action\nprediction propelled by optimizer momentum. We employ a simple unit-ball\nnormalization that enables learning under large update ratios, show its\nefficacy on the widely used dm_control suite, and obtain strong performance on\nthe challenging dog tasks, competitive with model-based approaches. Our results\nquestion, in parts, the prior explanation for sub-optimal learning due to\noverfitting early data.\n","authors":["Marcel Hussing","Claas Voelcker","Igor Gilitschenski","Amir-massoud Farahmand","Eric Eaton"],"pdf_url":"https://arxiv.org/pdf/2403.05996v3.pdf","comment":"Accepted as a conference paper at the First Reinforcement Learning\n  Conference (RLC)"},{"id":"http://arxiv.org/abs/2407.19707v3","updated":"2024-08-05T11:22:34Z","published":"2024-07-29T05:05:13Z","title":"Neural networks for bifurcation and linear stability analysis of steady\n  states in partial differential equations","summary":"  This research introduces an extended application of neural networks for\nsolving nonlinear partial differential equations (PDEs). A neural network,\ncombined with a pseudo-arclength continuation, is proposed to construct\nbifurcation diagrams from parameterized nonlinear PDEs. Additionally, a neural\nnetwork approach is also presented for solving eigenvalue problems to analyze\nsolution linear stability, focusing on identifying the largest eigenvalue. The\neffectiveness of the proposed neural network is examined through experiments on\nthe Bratu equation and the Burgers equation. Results from a finite difference\nmethod are also presented as comparison. Varying numbers of grid points are\nemployed in each case to assess the behavior and accuracy of both the neural\nnetwork and the finite difference method. The experimental results demonstrate\nthat the proposed neural network produces better solutions, generates more\naccurate bifurcation diagrams, has reasonable computational times, and proves\neffective for linear stability analysis.\n","authors":["Muhammad Luthfi Shahab","Hadi Susanto"],"pdf_url":"https://arxiv.org/pdf/2407.19707v3.pdf","comment":"Accepted for publication in Applied Mathematics and Computation"},{"id":"http://arxiv.org/abs/2309.14857v2","updated":"2024-08-05T11:20:33Z","published":"2023-09-26T11:35:25Z","title":"Cluster Exploration using Informative Manifold Projections","summary":"  Dimensionality reduction (DR) is one of the key tools for the visual\nexploration of high-dimensional data and uncovering its cluster structure in\ntwo- or three-dimensional spaces. The vast majority of DR methods in the\nliterature do not take into account any prior knowledge a practitioner may have\nregarding the dataset under consideration. We propose a novel method to\ngenerate informative embeddings which not only factor out the structure\nassociated with different kinds of prior knowledge but also aim to reveal any\nremaining underlying structure. To achieve this, we employ a linear combination\nof two objectives: firstly, contrastive PCA that discounts the structure\nassociated with the prior information, and secondly, kurtosis projection\npursuit which ensures meaningful data separation in the obtained embeddings. We\nformulate this task as a manifold optimization problem and validate it\nempirically across a variety of datasets considering three distinct types of\nprior knowledge. Lastly, we provide an automated framework to perform iterative\nvisual exploration of high-dimensional data.\n","authors":["Stavros Gerolymatos","Xenophon Evangelopoulos","Vladimir Gusev","John Y. Goulermas"],"pdf_url":"https://arxiv.org/pdf/2309.14857v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02384v1","updated":"2024-08-05T11:16:26Z","published":"2024-08-05T11:16:26Z","title":"Strategic Federated Learning: Application to Smart Meter Data Clustering","summary":"  Federated learning (FL) involves several clients that share with a fusion\ncenter (FC), the model each client has trained with its own data. Conventional\nFL, which can be interpreted as an estimation or distortion-based approach,\nignores the final use of model information (MI) by the FC and the other\nclients. In this paper, we introduce a novel FL framework in which the FC uses\nan aggregate version of the MI to make decisions that affect the client's\nutility functions. Clients cannot choose the decisions and can only use the MI\nreported to the FC to maximize their utility. Depending on the alignment\nbetween the client and FC utilities, the client may have an individual interest\nin adding strategic noise to the model. This general framework is stated and\nspecialized to the case of clustering, in which noisy cluster representative\ninformation is reported. This is applied to the problem of power consumption\nscheduling. In this context, utility non-alignment occurs, for instance, when\nthe client wants to consume when the price of electricity is low, whereas the\nFC wants the consumption to occur when the total power is the lowest. This is\nillustrated with aggregated real data from Ausgrid \\cite{ausgrid}. Our\nnumerical analysis clearly shows that the client can increase his utility by\nadding noise to the model reported to the FC. Corresponding results and source\ncodes can be downloaded from \\cite{source-code}.\n","authors":["Hassan Mohamad","Chao Zhang","Samson Lasaulce","Vineeth S Varma","Mrouane Debbah","Mounir Ghogho"],"pdf_url":"https://arxiv.org/pdf/2408.02384v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18103v2","updated":"2024-08-05T11:13:57Z","published":"2024-07-25T15:07:35Z","title":"Fine-Tuning Large Language Models for Stock Return Prediction Using\n  Newsflow","summary":"  Large language models (LLMs) and their fine-tuning techniques have\ndemonstrated superior performance in various language understanding and\ngeneration tasks. This paper explores fine-tuning LLMs for stock return\nforecasting with financial newsflow. In quantitative investing, return\nforecasting is fundamental for subsequent tasks like stock picking, portfolio\noptimization, etc. We formulate the model to include text representation and\nforecasting modules. We propose to compare the encoder-only and decoder-only\nLLMs, considering they generate text representations in distinct ways. The\nimpact of these different representations on forecasting performance remains an\nopen question. Meanwhile, we compare two simple methods of integrating LLMs'\ntoken-level representations into the forecasting module. The experiments on\nreal news and investment universes reveal that: (1) aggregated representations\nfrom LLMs' token-level embeddings generally produce return predictions that\nenhance the performance of long-only and long-short portfolios; (2) in the\nrelatively large investment universe, the decoder LLMs-based prediction model\nleads to stronger portfolios, whereas in the small universes, there are no\nconsistent winners. Among the three LLMs studied (DeBERTa, Mistral, Llama),\nMistral performs more robustly across different universes; (3) return\npredictions derived from LLMs' text representations are a strong signal for\nportfolio construction, outperforming conventional sentiment scores.\n","authors":["Tian Guo","Emmanuel Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2407.18103v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02367v1","updated":"2024-08-05T10:32:06Z","published":"2024-08-05T10:32:06Z","title":"StoDIP: Efficient 3D MRF image reconstruction with deep image priors and\n  stochastic iterations","summary":"  Magnetic Resonance Fingerprinting (MRF) is a time-efficient approach to\nquantitative MRI for multiparametric tissue mapping. The reconstruction of\nquantitative maps requires tailored algorithms for removing aliasing artefacts\nfrom the compressed sampled MRF acquisitions. Within approaches found in the\nliterature, many focus solely on two-dimensional (2D) image reconstruction,\nneglecting the extension to volumetric (3D) scans despite their higher\nrelevance and clinical value. A reason for this is that transitioning to 3D\nimaging without appropriate mitigations presents significant challenges,\nincluding increased computational cost and storage requirements, and the need\nfor large amount of ground-truth (artefact-free) data for training. To address\nthese issues, we introduce StoDIP, a new algorithm that extends the\nground-truth-free Deep Image Prior (DIP) reconstruction to 3D MRF imaging.\nStoDIP employs memory-efficient stochastic updates across the multicoil MRF\ndata, a carefully selected neural network architecture, as well as faster\nnonuniform FFT (NUFFT) transformations. This enables a faster convergence\ncompared against a conventional DIP implementation without these features.\nTested on a dataset of whole-brain scans from healthy volunteers, StoDIP\ndemonstrated superior performance over the ground-truth-free reconstruction\nbaselines, both quantitatively and qualitatively.\n","authors":["Perla Mayo","Matteo Cencini","Carolin M. Pirkl","Marion I. Menzel","Michela Tosetti","Bjoern H. Menze","Mohammad Golbabaee"],"pdf_url":"https://arxiv.org/pdf/2408.02367v1.pdf","comment":"10 pages, 2 figures, 1 table, 1 algorithm"},{"id":"http://arxiv.org/abs/2407.08583v2","updated":"2024-08-05T10:31:24Z","published":"2024-07-11T15:08:11Z","title":"The Synergy between Data and Multi-Modal Large Language Models: A Survey\n  from Co-Development Perspective","summary":"  The rapid development of large language models (LLMs) has been witnessed in\nrecent years. Based on the powerful LLMs, multi-modal LLMs (MLLMs) extend the\nmodality from text to a broader spectrum of domains, attracting widespread\nattention due to the broader range of application scenarios. As LLMs and MLLMs\nrely on vast amounts of model parameters and data to achieve emergent\ncapabilities, the importance of data is receiving increasingly widespread\nattention and recognition. Tracing and analyzing recent data-oriented works for\nMLLMs, we find that the development of models and data is not two separate\npaths but rather interconnected. On the one hand, vaster and higher-quality\ndata contribute to better performance of MLLMs; on the other hand, MLLMs can\nfacilitate the development of data. The co-development of multi-modal data and\nMLLMs requires a clear view of 1) at which development stages of MLLMs specific\ndata-centric approaches can be employed to enhance certain MLLM capabilities,\nand 2) how MLLMs, utilizing those capabilities, can contribute to multi-modal\ndata in specific roles. To promote the data-model co-development for MLLM\ncommunity, we systematically review existing works related to MLLMs from the\ndata-model co-development perspective. A regularly maintained project\nassociated with this survey is accessible at\nhttps://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md.\n","authors":["Zhen Qin","Daoyuan Chen","Wenhao Zhang","Liuyi Yao","Yilun Huang","Bolin Ding","Yaliang Li","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2407.08583v2.pdf","comment":"Ongoing work. 21 pages. Related materials are continually maintained\n  and available at\n  https://github.com/modelscope/data-juicer/blob/main/docs/awesome_llm_data.md"},{"id":"http://arxiv.org/abs/2305.09958v2","updated":"2024-08-05T10:24:09Z","published":"2023-05-17T05:35:49Z","title":"SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous\n  Graph Neural Networks","summary":"  Graph neural networks (GNNs) realize great success in graph learning but\nsuffer from performance loss when meeting heterophily, i.e. neighboring nodes\nare dissimilar, due to their local and uniform aggregation. Existing attempts\nof heterophilous GNNs incorporate long-range or global aggregations to\ndistinguish nodes in the graph. However, these aggregations usually require\niteratively maintaining and updating full-graph information, which limits their\nefficiency when applying to large-scale graphs. In this paper, we propose\n\\aggname{}, an efficient global heterophilous GNN aggregation integrating the\nstructural similarity measurement SimRank. Our theoretical analysis illustrates\nthat \\aggname{} inherently captures distant global similarity even under\nheterophily, that conventional approaches can only achieve after iterative\naggregations. Furthermore, it enjoys efficient one-time computation with a\ncomplexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive\nevaluation demonstrates that \\aggname{} achieves state-of-the-art performance\nwith superior aggregation and overall efficiency. Notably, it obtains 5$\\times$\nacceleration on the large-scale heterophily dataset \\emph{pokec} with over 30\nmillion edges compared to the best baseline aggregation.\n","authors":["Haoyu Liu","Ningyi Liao","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2305.09958v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2405.10802v2","updated":"2024-08-05T10:20:11Z","published":"2024-05-17T14:16:40Z","title":"Reduced storage direct tensor ring decomposition for convolutional\n  neural networks compression","summary":"  Convolutional neural networks (CNNs) are among the most widely used machine\nlearning models for computer vision tasks, such as image classification. To\nimprove the efficiency of CNNs, many CNNs compressing approaches have been\ndeveloped. Low-rank methods approximate the original convolutional kernel with\na sequence of smaller convolutional kernels, which leads to reduced storage and\ntime complexities. In this study, we propose a novel low-rank CNNs compression\nmethod that is based on reduced storage direct tensor ring decomposition\n(RSDTR). The proposed method offers a higher circular mode permutation\nflexibility, and it is characterized by large parameter and FLOPS compression\nrates, while preserving a good classification accuracy of the compressed\nnetwork. The experiments, performed on the CIFAR-10 and ImageNet datasets,\nclearly demonstrate the efficiency of RSDTR in comparison to other\nstate-of-the-art CNNs compression approaches.\n","authors":["Mateusz Gabor","Rafa Zdunek"],"pdf_url":"https://arxiv.org/pdf/2405.10802v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02361v1","updated":"2024-08-05T10:10:01Z","published":"2024-08-05T10:10:01Z","title":"Dialogue Ontology Relation Extraction via Constrained Chain-of-Thought\n  Decoding","summary":"  State-of-the-art task-oriented dialogue systems typically rely on\ntask-specific ontologies for fulfilling user queries. The majority of\ntask-oriented dialogue data, such as customer service recordings, comes without\nontology and annotation. Such ontologies are normally built manually, limiting\nthe application of specialised systems. Dialogue ontology construction is an\napproach for automating that process and typically consists of two steps: term\nextraction and relation extraction. In this work, we focus on relation\nextraction in a transfer learning set-up. To improve the generalisation, we\npropose an extension to the decoding mechanism of large language models. We\nadapt Chain-of-Thought (CoT) decoding, recently developed for reasoning\nproblems, to generative relation extraction. Here, we generate multiple\nbranches in the decoding space and select the relations based on a confidence\nthreshold. By constraining the decoding to ontology terms and relations, we aim\nto decrease the risk of hallucination. We conduct extensive experimentation on\ntwo widely used datasets and find improvements in performance on target\nontology for source fine-tuned and one-shot prompted large language models.\n","authors":["Renato Vukovic","David Arps","Carel van Niekerk","Benjamin Matthias Ruppik","Hsien-Chin Lin","Michael Heck","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.02361v1.pdf","comment":"Accepted to appear at SIGDIAL 2024. 9 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02357v1","updated":"2024-08-05T10:06:53Z","published":"2024-08-05T10:06:53Z","title":"On the consistent reasoning paradox of intelligence and optimal trust in\n  AI: The power of 'I don't know'","summary":"  We introduce the Consistent Reasoning Paradox (CRP). Consistent reasoning,\nwhich lies at the core of human intelligence, is the ability to handle tasks\nthat are equivalent, yet described by different sentences ('Tell me the time!'\nand 'What is the time?'). The CRP asserts that consistent reasoning implies\nfallibility -- in particular, human-like intelligence in AI necessarily comes\nwith human-like fallibility. Specifically, it states that there are problems,\ne.g. in basic arithmetic, where any AI that always answers and strives to mimic\nhuman intelligence by reasoning consistently will hallucinate (produce wrong,\nyet plausible answers) infinitely often. The paradox is that there exists a\nnon-consistently reasoning AI (which therefore cannot be on the level of human\nintelligence) that will be correct on the same set of problems. The CRP also\nshows that detecting these hallucinations, even in a probabilistic sense, is\nstrictly harder than solving the original problems, and that there are problems\nthat an AI may answer correctly, but it cannot provide a correct logical\nexplanation for how it arrived at the answer. Therefore, the CRP implies that\nany trustworthy AI (i.e., an AI that never answers incorrectly) that also\nreasons consistently must be able to say 'I don't know'. Moreover, this can\nonly be done by implicitly computing a new concept that we introduce, termed\nthe 'I don't know' function -- something currently lacking in modern AI. In\nview of these insights, the CRP also provides a glimpse into the behaviour of\nArtificial General Intelligence (AGI). An AGI cannot be 'almost sure', nor can\nit always explain itself, and therefore to be trustworthy it must be able to\nsay 'I don't know'.\n","authors":["Alexander Bastounis","Paolo Campodonico","Mihaela van der Schaar","Ben Adcock","Anders C. Hansen"],"pdf_url":"https://arxiv.org/pdf/2408.02357v1.pdf","comment":"12 pages and 50 pages of supplementary material, 7 figures"},{"id":"http://arxiv.org/abs/2408.02355v1","updated":"2024-08-05T10:02:33Z","published":"2024-08-05T10:02:33Z","title":"Quantile Regression using Random Forest Proximities","summary":"  Due to the dynamic nature of financial markets, maintaining models that\nproduce precise predictions over time is difficult. Often the goal isn't just\npoint prediction but determining uncertainty. Quantifying uncertainty,\nespecially the aleatoric uncertainty due to the unpredictable nature of market\ndrivers, helps investors understand varying risk levels. Recently, quantile\nregression forests (QRF) have emerged as a promising solution: Unlike most\nbasic quantile regression methods that need separate models for each quantile,\nquantile regression forests estimate the entire conditional distribution of the\ntarget variable with a single model, while retaining all the salient features\nof a typical random forest. We introduce a novel approach to compute quantile\nregressions from random forests that leverages the proximity (i.e., distance\nmetric) learned by the model and infers the conditional distribution of the\ntarget variable. We evaluate the proposed methodology using publicly available\ndatasets and then apply it towards the problem of forecasting the average daily\nvolume of corporate bonds. We show that using quantile regression using Random\nForest proximities demonstrates superior performance in approximating\nconditional target distributions and prediction intervals to the original\nversion of QRF. We also demonstrate that the proposed framework is\nsignificantly more computationally efficient than traditional approaches to\nquantile regressions.\n","authors":["Mingshu Li","Bhaskarjit Sarmah","Dhruv Desai","Joshua Rosaler","Snigdha Bhagat","Philip Sommer","Dhagash Mehta"],"pdf_url":"https://arxiv.org/pdf/2408.02355v1.pdf","comment":"9 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.02354v1","updated":"2024-08-05T10:02:29Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v1.pdf","comment":"5 pages, 4 figures, submitted to CIKM'24"},{"id":"http://arxiv.org/abs/2401.13185v2","updated":"2024-08-05T10:01:48Z","published":"2024-01-24T02:16:03Z","title":"Fast Partition-Based Cross-Validation With Centering and Scaling for\n  $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$","summary":"  We present algorithms that substantially accelerate partition-based\ncross-validation for machine learning models that require matrix products\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. Our\nalgorithms have applications in model selection for, e.g., principal component\nanalysis (PCA), principal component regression (PCR), ridge regression (RR),\nordinary least squares (OLS), and partial least squares (PLS). Our algorithms\nsupport all combinations of column-wise centering and scaling of $\\mathbf{X}$\nand $\\mathbf{Y}$, and we demonstrate in our accompanying implementation that\nthis adds only a manageable, practical constant over efficient variants without\npreprocessing. We prove the correctness of our algorithms under a fold-based\npartitioning scheme and show that the running time is independent of the number\nof folds; that is, they have the same time complexity as that of computing\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ and\nspace complexity equivalent to storing $\\mathbf{X}$, $\\mathbf{Y}$,\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$, and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$.\nImportantly, unlike alternatives found in the literature, we avoid data leakage\ndue to preprocessing. We achieve these results by eliminating redundant\ncomputations in the overlap between training partitions. Concretely, we show\nhow to manipulate $\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and\n$\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$ using only samples from the validation\npartition to obtain the preprocessed training partition-wise\n$\\mathbf{X}^\\mathbf{T}\\mathbf{X}$ and $\\mathbf{X}^\\mathbf{T}\\mathbf{Y}$. To our\nknowledge, we are the first to derive correct and efficient cross-validation\nalgorithms for any of the $16$ combinations of column-wise centering and\nscaling, for which we also prove only $12$ give distinct matrix products.\n","authors":["Ole-Christian Galbo Engstrm","Martin Holm Jensen"],"pdf_url":"https://arxiv.org/pdf/2401.13185v2.pdf","comment":"31 pages, 2 tables, 1 figure, 7 algorithms"},{"id":"http://arxiv.org/abs/2408.02349v1","updated":"2024-08-05T09:54:08Z","published":"2024-08-05T09:54:08Z","title":"Active Sensing of Knee Osteoarthritis Progression with Reinforcement\n  Learning","summary":"  Osteoarthritis (OA) is the most common musculoskeletal disease, which has no\ncure. Knee OA (KOA) is one of the highest causes of disability worldwide, and\nit costs billions of United States dollars to the global community. Prediction\nof KOA progression has been of high interest to the community for years, as it\ncan advance treatment development through more efficient clinical trials and\nimprove patient outcomes through more efficient healthcare utilization.\nExisting approaches for predicting KOA, however, are predominantly static, i.e.\nconsider data from a single time point to predict progression many years into\nthe future, and knee level, i.e. consider progression in a single joint only.\nDue to these and related reasons, these methods fail to deliver the level of\npredictive performance, which is sufficient to result in cost savings and\nbetter patient outcomes. Collecting extensive data from all patients on a\nregular basis could address the issue, but it is limited by the high cost at a\npopulation level. In this work, we propose to go beyond static prediction\nmodels in OA, and bring a novel Active Sensing (AS) approach, designed to\ndynamically follow up patients with the objective of maximizing the number of\ninformative data acquisitions, while minimizing their total cost over a period\nof time. Our approach is based on Reinforcement Learning (RL), and it leverages\na novel reward function designed specifically for AS of disease progression in\nmore than one part of a human body. Our method is end-to-end, relies on\nmulti-modal Deep Learning, and requires no human input at inference time.\nThroughout an exhaustive experimental evaluation, we show that using RL can\nprovide a higher monetary benefit when compared to state-of-the-art baselines.\n","authors":["Khanh Nguyen","Huy Hoang Nguyen","Egor Panfilov","Aleksei Tiulpin"],"pdf_url":"https://arxiv.org/pdf/2408.02349v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02346v1","updated":"2024-08-05T09:45:31Z","published":"2024-08-05T09:45:31Z","title":"Exploiting Hankel-Toeplitz Structures for Fast Computation of Kernel\n  Precision Matrices","summary":"  The Hilbert-space Gaussian Process (HGP) approach offers a\nhyperparameter-independent basis function approximation for speeding up\nGaussian Process (GP) inference by projecting the GP onto M basis functions.\nThese properties result in a favorable data-independent $\\mathcal{O}(M^3)$\ncomputational complexity during hyperparameter optimization but require a\ndominating one-time precomputation of the precision matrix costing\n$\\mathcal{O}(NM^2)$ operations. In this paper, we lower this dominating\ncomputational complexity to $\\mathcal{O}(NM)$ with no additional\napproximations. We can do this because we realize that the precision matrix can\nbe split into a sum of Hankel-Toeplitz matrices, each having $\\mathcal{O}(M)$\nunique entries. Based on this realization we propose computing only these\nunique entries at $\\mathcal{O}(NM)$ costs. Further, we develop two theorems\nthat prescribe sufficient conditions for the complexity reduction to hold\ngenerally for a wide range of other approximate GP models, such as the\nVariational Fourier Feature (VFF) approach. The two theorems do this with no\nassumptions on the data and no additional approximations of the GP models\nthemselves. Thus, our contribution provides a pure speed-up of several\nexisting, widely used, GP approximations, without further approximations.\n","authors":["Frida Viset","Anton Kullberg","Frederiek Wesel","Arno Solin"],"pdf_url":"https://arxiv.org/pdf/2408.02346v1.pdf","comment":"Published in Transactions on Machine Learning (TMLR) July 2024"},{"id":"http://arxiv.org/abs/2408.02344v1","updated":"2024-08-05T09:41:34Z","published":"2024-08-05T09:41:34Z","title":"Machine Learning Applications in Medical Prognostics: A Comprehensive\n  Review","summary":"  Machine learning (ML) has revolutionized medical prognostics by integrating\nadvanced algorithms with clinical data to enhance disease prediction, risk\nassessment, and patient outcome forecasting. This comprehensive review\ncritically examines the application of various ML techniques in medical\nprognostics, focusing on their efficacy, challenges, and future directions. The\nmethodologies discussed include Random Forest (RF) for sepsis prediction,\nlogistic regression for cardiovascular risk assessment, Convolutional Neural\nNetworks (CNNs) for cancer detection, and Long Short-Term Memory (LSTM)\nnetworks for predicting clinical deterioration. RF models demonstrate robust\nperformance in handling high-dimensional data and capturing non-linear\nrelationships, making them particularly effective for sepsis prediction.\nLogistic regression remains valuable for its interpretability and ease of use\nin cardiovascular risk assessment. CNNs have shown exceptional accuracy in\ncancer detection, leveraging their ability to learn complex visual patterns\nfrom medical imaging. LSTM networks excel in analyzing temporal data, providing\naccurate predictions of clinical deterioration. The review highlights the\nstrengths and limitations of each technique, the importance of model\ninterpretability, and the challenges of data quality and privacy. Future\nresearch directions include the integration of multi-modal data sources, the\napplication of transfer learning, and the development of continuous learning\nsystems. These advancements aim to enhance the predictive power and clinical\napplicability of ML models, ultimately improving patient outcomes in healthcare\nsettings.\n","authors":["Michael Fascia"],"pdf_url":"https://arxiv.org/pdf/2408.02344v1.pdf","comment":"30 pages"},{"id":"http://arxiv.org/abs/2408.02337v1","updated":"2024-08-05T09:23:49Z","published":"2024-08-05T09:23:49Z","title":"Developing PUGG for Polish: A Modern Approach to KBQA, MRC, and IR\n  Dataset Construction","summary":"  Advancements in AI and natural language processing have revolutionized\nmachine-human language interactions, with question answering (QA) systems\nplaying a pivotal role. The knowledge base question answering (KBQA) task,\nutilizing structured knowledge graphs (KG), allows for handling extensive\nknowledge-intensive questions. However, a significant gap exists in KBQA\ndatasets, especially for low-resource languages. Many existing construction\npipelines for these datasets are outdated and inefficient in human labor, and\nmodern assisting tools like Large Language Models (LLM) are not utilized to\nreduce the workload. To address this, we have designed and implemented a\nmodern, semi-automated approach for creating datasets, encompassing tasks such\nas KBQA, Machine Reading Comprehension (MRC), and Information Retrieval (IR),\ntailored explicitly for low-resource environments. We executed this pipeline\nand introduced the PUGG dataset, the first Polish KBQA dataset, and novel\ndatasets for MRC and IR. Additionally, we provide a comprehensive\nimplementation, insightful findings, detailed statistics, and evaluation of\nbaseline models.\n","authors":["Albert Sawczyn","Katsiaryna Viarenich","Konrad Wojtasik","Aleksandra Domogaa","Marcin Oleksy","Maciej Piasecki","Tomasz Kajdanowicz"],"pdf_url":"https://arxiv.org/pdf/2408.02337v1.pdf","comment":"Accepted for ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2408.02336v1","updated":"2024-08-05T09:19:52Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v1.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.02320v1","updated":"2024-08-05T09:02:24Z","published":"2024-08-05T09:02:24Z","title":"A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion\n  Models","summary":"  Diffusion models, which convert noise into new data instances by learning to\nreverse a diffusion process, have become a cornerstone in contemporary\ngenerative modeling. In this work, we develop non-asymptotic convergence theory\nfor a popular diffusion-based sampler (i.e., the probability flow ODE sampler)\nin discrete time, assuming access to $\\ell_2$-accurate estimates of the (Stein)\nscore functions. For distributions in $\\mathbb{R}^d$, we prove that\n$d/\\varepsilon$ iterations -- modulo some logarithmic and lower-order terms --\nare sufficient to approximate the target distribution to within $\\varepsilon$\ntotal-variation distance. This is the first result establishing nearly linear\ndimension-dependency (in $d$) for the probability flow ODE sampler. Imposing\nonly minimal assumptions on the target data distribution (e.g., no smoothness\nassumption is imposed), our results also characterize how $\\ell_2$ score\nestimation errors affect the quality of the data generation processes. In\ncontrast to prior works, our theory is developed based on an elementary yet\nversatile non-asymptotic approach without the need of resorting to SDE and ODE\ntoolboxes.\n","authors":["Gen Li","Yuting Wei","Yuejie Chi","Yuxin Chen"],"pdf_url":"https://arxiv.org/pdf/2408.02320v1.pdf","comment":"This manuscript presents improved theory for probability flow ODEs\n  compared to its earlier version arXiv:2306.09251"},{"id":"http://arxiv.org/abs/2308.09605v2","updated":"2024-08-05T08:53:12Z","published":"2023-08-18T14:58:23Z","title":"Solving PDEs on Spheres with Physics-Informed Convolutional Neural\n  Networks","summary":"  Physics-informed neural networks (PINNs) have been demonstrated to be\nefficient in solving partial differential equations (PDEs) from a variety of\nexperimental perspectives. Some recent studies have also proposed PINN\nalgorithms for PDEs on surfaces, including spheres. However, theoretical\nunderstanding of the numerical performance of PINNs, especially PINNs on\nsurfaces or manifolds, is still lacking. In this paper, we establish rigorous\nanalysis of the physics-informed convolutional neural network (PICNN) for\nsolving PDEs on the sphere. By using and improving the latest approximation\nresults of deep convolutional neural networks and spherical harmonic analysis,\nwe prove an upper bound for the approximation error with respect to the Sobolev\nnorm. Subsequently, we integrate this with innovative localization complexity\nanalysis to establish fast convergence rates for PICNN. Our theoretical results\nare also confirmed and supplemented by our experiments. In light of these\nfindings, we explore potential strategies for circumventing the curse of\ndimensionality that arises when solving high-dimensional PDEs.\n","authors":["Guanhang Lei","Zhen Lei","Lei Shi","Chenyu Zeng","Ding-Xuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2308.09605v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02313v1","updated":"2024-08-05T08:46:46Z","published":"2024-08-05T08:46:46Z","title":"A Lean Transformer Model for Dynamic Malware Analysis and Detection","summary":"  Malware is a fast-growing threat to the modern computing world and existing\nlines of defense are not efficient enough to address this issue. This is mainly\ndue to the fact that many prevention solutions rely on signature-based\ndetection methods that can easily be circumvented by hackers. Therefore, there\nis a recurrent need for behavior-based analysis where a suspicious file is ran\nin a secured environment and its traces are collected to reports for analysis.\nPrevious works have shown some success leveraging Neural Networks and API calls\nsequences extracted from these execution reports.\n  Recently, Large Language Models and Generative AI have demonstrated\nimpressive capabilities mainly in Natural Language Processing tasks and\npromising applications in the cybersecurity field for both attackers and\ndefenders.\n  In this paper, we design an Encoder-Only model, based on the Transformers\narchitecture, to detect malicious files, digesting their API call sequences\ncollected by an execution emulation solution. We are also limiting the size of\nthe model architecture and the number of its parameters since it is often\nconsidered that Large Language Models may be overkill for specific tasks such\nas the one we are dealing with hereafter. In addition to achieving decent\ndetection results, this approach has the advantage of reducing our carbon\nfootprint by limiting training and inference times and facilitating technical\noperations with less hardware requirements.\n  We also carry out some analysis of our results and highlight the limits and\npossible improvements when using Transformers to analyze malicious files.\n","authors":["Tony Quertier","Benjamin Marais","Grgoire Barru","Stphane Morucci","Svan Az","Sbastien Salladin"],"pdf_url":"https://arxiv.org/pdf/2408.02313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02312v1","updated":"2024-08-05T08:45:50Z","published":"2024-08-05T08:45:50Z","title":"Optimization of Iterative Blind Detection based on Expectation\n  Maximization and Belief Propagation","summary":"  We study iterative blind symbol detection for block-fading linear\ninter-symbol interference channels. Based on the factor graph framework, we\ndesign a joint channel estimation and detection scheme that combines the\nexpectation maximization (EM) algorithm and the ubiquitous belief propagation\n(BP) algorithm. Interweaving the iterations of both schemes significantly\nreduces the EM algorithm's computational burden while retaining its excellent\nperformance. To this end, we apply simple yet effective model-based learning\nmethods to find a suitable parameter update schedule by introducing momentum in\nboth the EM parameter updates as well as in the BP message passing. Numerical\nsimulations verify that the proposed method can learn efficient schedules that\ngeneralize well and even outperform coherent BP detection in high\nsignal-to-noise scenarios.\n","authors":["Luca Schmid","Tomer Raviv","Nir Shlezinger","Laurent Schmalen"],"pdf_url":"https://arxiv.org/pdf/2408.02312v1.pdf","comment":"Accepted for presentation at Asilomar Conference on Signals, Systems,\n  and Computers 2024"},{"id":"http://arxiv.org/abs/2408.02310v1","updated":"2024-08-05T08:41:07Z","published":"2024-08-05T08:41:07Z","title":"On the Robustness of Malware Detectors to Adversarial Samples","summary":"  Adversarial examples add imperceptible alterations to inputs with the\nobjective to induce misclassification in machine learning models. They have\nbeen demonstrated to pose significant challenges in domains like image\nclassification, with results showing that an adversarially perturbed image to\nevade detection against one classifier is most likely transferable to other\nclassifiers. Adversarial examples have also been studied in malware analysis.\nUnlike images, program binaries cannot be arbitrarily perturbed without\nrendering them non-functional. Due to the difficulty of crafting adversarial\nprogram binaries, there is no consensus on the transferability of adversarially\nperturbed programs to different detectors. In this work, we explore the\nrobustness of malware detectors against adversarially perturbed malware. We\ninvestigate the transferability of adversarial attacks developed against one\ndetector, against other machine learning-based malware detectors, and code\nsimilarity techniques, specifically, locality sensitive hashing-based\ndetectors. Our analysis reveals that adversarial program binaries crafted for\none detector are generally less effective against others. We also evaluate an\nensemble of detectors and show that they can potentially mitigate the impact of\nadversarial program binaries. Finally, we demonstrate that substantial program\nchanges made to evade detection may result in the transformation technique\nbeing identified, implying that the adversary must make minimal changes to the\nprogram binary.\n","authors":["Muhammad Salman","Benjamin Zi Hao Zhao","Hassan Jameel Asghar","Muhammad Ikram","Sidharth Kaushik","Mohamed Ali Kaafar"],"pdf_url":"https://arxiv.org/pdf/2408.02310v1.pdf","comment":"This is the full version of the paper with the same title to appear\n  in the proceedings of the 2024 Workshop on Security and Artificial\n  Intelligence (SECAI 2024)"},{"id":"http://arxiv.org/abs/2408.02301v1","updated":"2024-08-05T08:23:59Z","published":"2024-08-05T08:23:59Z","title":"Network Fission Ensembles for Low-Cost Self-Ensembles","summary":"  Recent ensemble learning methods for image classification have been shown to\nimprove classification accuracy with low extra cost. However, they still\nrequire multiple trained models for ensemble inference, which eventually\nbecomes a significant burden when the model size increases. In this paper, we\npropose a low-cost ensemble learning and inference, called Network Fission\nEnsembles (NFE), by converting a conventional network itself into a multi-exit\nstructure. Starting from a given initial network, we first prune some of the\nweights to reduce the training burden. We then group the remaining weights into\nseveral sets and create multiple auxiliary paths using each set to construct\nmulti-exits. We call this process Network Fission. Through this, multiple\noutputs can be obtained from a single network, which enables ensemble learning.\nSince this process simply changes the existing network structure to multi-exits\nwithout using additional networks, there is no extra computational burden for\nensemble learning and inference. Moreover, by learning from multiple losses of\nall exits, the multi-exits improve performance via regularization, and high\nperformance can be achieved even with increased network sparsity. With our\nsimple yet effective method, we achieve significant improvement compared to\nexisting ensemble methods. The code is available at\nhttps://github.com/hjdw2/NFE.\n","authors":["Hojung Lee","Jong-Seok Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02301v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10504v2","updated":"2024-08-05T08:22:56Z","published":"2024-02-16T08:27:55Z","title":"On the resilience of the quadratic Littlewood-Offord problem","summary":"  We study the statistical resilience of the anti-concentration properties of\nRademacher polynomials in face of adversarial deterministic noise taking the\nform of sign-flips. Given a multilinear polynomial $f:\\mathbb{R}^n \\to\n\\mathbb{R}$ and a Rademacher vector $\\boldsymbol{\\xi} \\in \\{\\pm 1\\}^n$ (with\nindependent entries), our results provide probabilistic lower bound estimations\non the number of sign-flips that $\\boldsymbol{\\xi}$ can sustain without\n``inflating\" the atom probability $\\sup_{x \\in \\mathbb{R} }\n\\mathbb{P}\\{f(\\boldsymbol{\\xi}) = x\\}$ otherwise resulting in an adversarially\nbiased distribution. Special emphasis is put on bilinear and quadratic forms,\nfor which strengthened estimates are attained. From a computational\nperspective, our results in this venue are instance-bound in such a way that\nallows for an efficient computation of the statistical resilience guarantees\nfrom the quadratic polynomial itself directly. All of our probabilistic lower\nbound resilience guarantees are asymptotically tight.\n  On route, we provide a short proof for a new small-ball probability estimate\nfitting Rademacher multilinear polynomials $f: \\mathbb{R}^n \\to \\mathbb{R}$\nremoveing a polylog-factor from the classical Meka-Nguyen-Vu bound provided the\ncoefficients are independent of $n$ (dimension-free, hereafter). This removal\nwas conjectured to be possible by Meka-Nguyen-Vu regardless of our assumption.\nBilinear Rademacher forms with dimension-free coefficients arise naturally in\nCombinatorics and specifically in the dense case of the edge-statistics\nconjecture posed by Alon, Hefetz, Krivelevich, and Tyomkyn. This case of the\nconjecture was resolved by Kwan and Sauermann. Replacing the appeal to the\nMeka-Nguyen-Vu classical bound in the work of Kwan, Sudakov, and Tran with our\nshortly proved result attains an additional proof of the dense case of the\nedge-statistics conjecture.\n","authors":["Elad Aigner-Horev","Daniel Rosenberg","Roi Weiss"],"pdf_url":"https://arxiv.org/pdf/2402.10504v2.pdf","comment":"Numerous changes from the last version: 1. An oversight in the proof\n  fixed. 2. Added treatment of high degree polynomials 3. New results added"},{"id":"http://arxiv.org/abs/2408.02298v1","updated":"2024-08-05T08:14:32Z","published":"2024-08-05T08:14:32Z","title":"Backward Compatibility in Attributive Explanation and Enhanced Model\n  Training Method","summary":"  Model update is a crucial process in the operation of ML/AI systems. While\nupdating a model generally enhances the average prediction performance, it also\nsignificantly impacts the explanations of predictions. In real-world\napplications, even minor changes in explanations can have detrimental\nconsequences. To tackle this issue, this paper introduces BCX, a quantitative\nmetric that evaluates the backward compatibility of feature attribution\nexplanations between pre- and post-update models. BCX utilizes practical\nagreement metrics to calculate the average agreement between the explanations\nof pre- and post-update models, specifically among samples on which both models\naccurately predict. In addition, we propose BCXR, a BCX-aware model training\nmethod by designing surrogate losses which theoretically lower bounds agreement\nscores. Furthermore, we present a universal variant of BCXR that improves all\nagreement metrics, utilizing L2 distance among the explanations of the models.\nTo validate our approach, we conducted experiments on eight real-world\ndatasets, demonstrating that BCXR achieves superior trade-offs between\npredictive performances and BCX scores, showcasing the effectiveness of our\nBCXR methods.\n","authors":["Ryuta Matsuno"],"pdf_url":"https://arxiv.org/pdf/2408.02298v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02296v1","updated":"2024-08-05T08:14:04Z","published":"2024-08-05T08:14:04Z","title":"Heart Rate and its Variability from Short-term ECG Recordings as\n  Biomarkers for Detecting Mild Cognitive Impairment in Indian Population","summary":"  Alterations in Heart Rate (HR) and Heart Rate Variability (HRV) can reflect\nautonomic dysfunction associated with neurodegeneration. We investigate the\ninfluence of Mild Cognitive Impairment (MCI) on HR and its variability measures\nin the Indian population by designing a complete signal processing pipeline to\ndetect the R-wave peaks and compute HR and HRV features from ECG recordings of\n10 seconds, for point-of-care applications. The study cohort involves 297 urban\nparticipants, among which 48.48% are male and 51.51% are female. From the\nAddenbrooke's Cognitive Examination-III (ACE-III), MCI is detected in 19.19% of\nparticipants and the rest, 80.8% of them are cognitively healthy. Statistical\nfeatures like central tendency (mean and root mean square (RMS) of the\nNormal-to-Normal (NN) intervals) and dispersion (standard deviation (SD) of all\nNN intervals (SDNN) and root mean square of successive differences of NN\nintervals (RMSSD)) of beat-to-beat intervals are computed. The Wilcoxon rank\nsum test reveals that mean of NN intervals (p = 0.0021), the RMS of NN\nintervals (p = 0.0014), the SDNN (p = 0.0192) and the RMSSD (p = 0.0206) values\ndiffer significantly between MCI and non-MCI classes, for a level of\nsignificance, 0.05. Machine learning classifiers like, Support Vector Machine\n(SVM), Discriminant Analysis (DA) and Naive Bayes (NB) driven by mean NN\nintervals, RMS, SDNN and RMSSD, show a high accuracy of 80.80% on each\nindividual feature input. Individuals with MCI are observed to have\ncomparatively higher HR than healthy subjects. HR and its variability can be\nconsidered as potential biomarkers for detecting MCI.\n","authors":["Anjo Xavier","Sneha Noble","Justin Joseph","Thomas Gregor Issac"],"pdf_url":"https://arxiv.org/pdf/2408.02296v1.pdf","comment":"Nil"},{"id":"http://arxiv.org/abs/2408.02295v1","updated":"2024-08-05T08:12:25Z","published":"2024-08-05T08:12:25Z","title":"Generalized Gaussian Temporal Difference Error For Uncertainty-aware\n  Reinforcement Learning","summary":"  Conventional uncertainty-aware temporal difference (TD) learning methods\noften rely on simplistic assumptions, typically including a zero-mean Gaussian\ndistribution for TD errors. Such oversimplification can lead to inaccurate\nerror representations and compromised uncertainty estimation. In this paper, we\nintroduce a novel framework for generalized Gaussian error modeling in deep\nreinforcement learning, applicable to both discrete and continuous control\nsettings. Our framework enhances the flexibility of error distribution modeling\nby incorporating higher-order moments, particularly kurtosis, thereby improving\nthe estimation and mitigation of data-dependent noise, i.e., aleatoric\nuncertainty. We examine the influence of the shape parameter of the generalized\nGaussian distribution (GGD) on aleatoric uncertainty and provide a closed-form\nexpression that demonstrates an inverse relationship between uncertainty and\nthe shape parameter. Additionally, we propose a theoretically grounded\nweighting scheme to fully leverage the GGD. To address epistemic uncertainty,\nwe enhance the batch inverse variance weighting by incorporating bias reduction\nand kurtosis considerations, resulting in improved robustness. Extensive\nexperimental evaluations using policy gradient algorithms demonstrate the\nconsistent efficacy of our method, showcasing significant performance\nimprovements.\n","authors":["Seyeon Kim","Joonhun Lee","Namhoon Cho","Sungjun Han","Seungeon Baek"],"pdf_url":"https://arxiv.org/pdf/2408.02295v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.16458v2","updated":"2024-08-05T07:59:19Z","published":"2024-01-29T10:11:05Z","title":"Credit Risk Meets Large Language Models: Building a Risk Indicator from\n  Loan Descriptions in P2P Lending","summary":"  Peer-to-peer (P2P) lending has emerged as a distinctive financing mechanism,\nlinking borrowers with lenders through online platforms. However, P2P lending\nfaces the challenge of information asymmetry, as lenders often lack sufficient\ndata to assess the creditworthiness of borrowers. This paper proposes a novel\napproach to address this issue by leveraging the textual descriptions provided\nby borrowers during the loan application process. Our methodology involves\nprocessing these textual descriptions using a Large Language Model (LLM), a\npowerful tool capable of discerning patterns and semantics within the text.\nTransfer learning is applied to adapt the LLM to the specific task at hand.\n  Our results derived from the analysis of the Lending Club dataset show that\nthe risk score generated by BERT, a widely used LLM, significantly improves the\nperformance of credit risk classifiers. However, the inherent opacity of\nLLM-based systems, coupled with uncertainties about potential biases,\nunderscores critical considerations for regulatory frameworks and engenders\ntrust-related concerns among end-users, opening new avenues for future research\nin the dynamic landscape of P2P lending and artificial intelligence.\n","authors":["Mario Sanz-Guerrero","Javier Arroyo"],"pdf_url":"https://arxiv.org/pdf/2401.16458v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09780v2","updated":"2024-08-05T07:56:33Z","published":"2024-02-15T08:09:17Z","title":"TinyCL: An Efficient Hardware Architecture for Continual Learning on\n  Autonomous Systems","summary":"  The Continuous Learning (CL) paradigm consists of continuously evolving the\nparameters of the Deep Neural Network (DNN) model to progressively learn to\nperform new tasks without reducing the performance on previous tasks, i.e.,\navoiding the so-called catastrophic forgetting. However, the DNN parameter\nupdate in CL-based autonomous systems is extremely resource-hungry. The\nexisting DNN accelerators cannot be directly employed in CL because they only\nsupport the execution of the forward propagation. Only a few prior\narchitectures execute the backpropagation and weight update, but they lack the\ncontrol and management for CL. Towards this, we design a hardware architecture,\nTinyCL, to perform CL on resource-constrained autonomous systems. It consists\nof a processing unit that executes both forward and backward propagation, and a\ncontrol unit that manages memory-based CL workload. To minimize the memory\naccesses, the sliding window of the convolutional layer moves in a snake-like\nfashion. Moreover, the Multiply-and-Accumulate units can be reconfigured at\nruntime to execute different operations. As per our knowledge, our proposed\nTinyCL represents the first hardware accelerator that executes CL on autonomous\nsystems. We synthesize the complete TinyCL architecture in a 65 nm CMOS\ntechnology node with the conventional ASIC design flow. It executes 1 epoch of\ntraining on a Conv + ReLU + Dense model on the CIFAR10 dataset in 1.76 s, while\n1 training epoch of the same model using an Nvidia Tesla P100 GPU takes 103 s,\nthus achieving a 58x speedup, consuming 86 mW in a 4.74 mm2 die.\n","authors":["Eugenio Ressa","Alberto Marchisio","Maurizio Martina","Guido Masera","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09780v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.02963v3","updated":"2024-08-05T07:55:34Z","published":"2022-06-07T01:49:22Z","title":"Confidence-aware Self-Semantic Distillation on Knowledge Graph Embedding","summary":"  Knowledge Graph Embedding (KGE), which projects entities and relations into\ncontinuous vector spaces, has garnered significant attention. Although\nhigh-dimensional KGE methods offer better performance, they come at the expense\nof significant computation and memory overheads. Decreasing embedding\ndimensions significantly deteriorates model performance. While several recent\nefforts utilize knowledge distillation or non-Euclidean representation learning\nto augment the effectiveness of low-dimensional KGE, they either necessitate a\npre-trained high-dimensional teacher model or involve complex non-Euclidean\noperations, thereby incurring considerable additional computational costs. To\naddress this, this work proposes Confidence-aware Self-Knowledge Distillation\n(CSD) that learns from the model itself to enhance KGE in a low-dimensional\nspace. Specifically, CSD extracts knowledge from embeddings in previous\niterations, which would be utilized to supervise the learning of the model in\nthe next iterations. Moreover, a specific semantic module is developed to\nfilter reliable knowledge by estimating the confidence of previously learned\nembeddings. This straightforward strategy bypasses the need for time-consuming\npre-training of teacher models and can be integrated into various KGE methods\nto improve their performance. Our comprehensive experiments on six KGE\nbackbones and four datasets underscore the effectiveness of the proposed CSD.\n","authors":["Yichen Liu","Jiawei Chen","Defang Chen","Zhehui Zhou","Yan Feng","Can Wang"],"pdf_url":"https://arxiv.org/pdf/2206.02963v3.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2408.02280v1","updated":"2024-08-05T07:30:18Z","published":"2024-08-05T07:30:18Z","title":"Hardware Aware Ensemble Selection for Balancing Predictive Accuracy and\n  Cost","summary":"  Automated Machine Learning (AutoML) significantly simplifies the deployment\nof machine learning models by automating tasks from data preprocessing to model\nselection to ensembling. AutoML systems for tabular data often employ post hoc\nensembling, where multiple models are combined to improve predictive accuracy.\nThis typically results in longer inference times, a major limitation in\npractical deployments. Addressing this, we introduce a hardware-aware ensemble\nselection approach that integrates inference time into post hoc ensembling. By\nleveraging an existing framework for ensemble selection with quality diversity\noptimization, our method evaluates ensemble candidates for their predictive\naccuracy and hardware efficiency. This dual focus allows for a balanced\nconsideration of accuracy and operational efficiency. Thus, our approach\nenables practitioners to choose from a Pareto front of accurate and efficient\nensembles. Our evaluation using 83 classification datasets shows that our\napproach sustains competitive accuracy and can significantly improve ensembles'\noperational efficiency. The results of this study provide a foundation for\nextending these principles to additional hardware constraints, setting the\nstage for the development of more resource-efficient AutoML systems.\n","authors":["Jannis Maier","Felix Mller","Lennart Purucker"],"pdf_url":"https://arxiv.org/pdf/2408.02280v1.pdf","comment":"Accepted at Third International Conference on Automated Machine\n  Learning (AutoML 2024), Workshop Track; for code, see\n  https://github.com/Atraxus/HA-ES"},{"id":"http://arxiv.org/abs/2408.02279v1","updated":"2024-08-05T07:26:47Z","published":"2024-08-05T07:26:47Z","title":"DRFormer: Multi-Scale Transformer Utilizing Diverse Receptive Fields for\n  Long Time-Series Forecasting","summary":"  Long-term time series forecasting (LTSF) has been widely applied in finance,\ntraffic prediction, and other domains. Recently, patch-based transformers have\nemerged as a promising approach, segmenting data into sub-level patches that\nserve as input tokens. However, existing methods mostly rely on predetermined\npatch lengths, necessitating expert knowledge and posing challenges in\ncapturing diverse characteristics across various scales. Moreover, time series\ndata exhibit diverse variations and fluctuations across different temporal\nscales, which traditional approaches struggle to model effectively. In this\npaper, we propose a dynamic tokenizer with a dynamic sparse learning algorithm\nto capture diverse receptive fields and sparse patterns of time series data. In\norder to build hierarchical receptive fields, we develop a multi-scale\nTransformer model, coupled with multi-scale sequence extraction, capable of\ncapturing multi-resolution features. Additionally, we introduce a group-aware\nrotary position encoding technique to enhance intra- and inter-group position\nawareness among representations across different temporal scales. Our proposed\nmodel, named DRFormer, is evaluated on various real-world datasets, and\nexperimental results demonstrate its superiority compared to existing methods.\nOur code is available at: https://github.com/ruixindingECNU/DRFormer.\n","authors":["Ruixin Ding","Yuqi Chen","Yu-Ting Lan","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02279v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17900v4","updated":"2024-08-05T07:06:14Z","published":"2024-07-25T09:42:24Z","title":"The Power of Combining Data and Knowledge: GPT-4o is an Effective\n  Interpreter of Machine Learning Models in Predicting Lymph Node Metastasis of\n  Lung Cancer","summary":"  Lymph node metastasis (LNM) is a crucial factor in determining the initial\ntreatment for patients with lung cancer, yet accurate preoperative diagnosis of\nLNM remains challenging. Recently, large language models (LLMs) have garnered\nsignificant attention due to their remarkable text generation capabilities.\nLeveraging the extensive medical knowledge learned from vast corpora, LLMs can\nestimate probabilities for clinical problems, though their performance has\nhistorically been inferior to data-driven machine learning models. In this\npaper, we propose a novel ensemble method that combines the medical knowledge\nacquired by LLMs with the latent patterns identified by machine learning models\nto enhance LNM prediction performance. Initially, we developed machine learning\nmodels using patient data. We then designed a prompt template to integrate the\npatient data with the predicted probability from the machine learning model.\nSubsequently, we instructed GPT-4o, the most advanced LLM developed by OpenAI,\nto estimate the likelihood of LNM based on patient data and then adjust the\nestimate using the machine learning output. Finally, we collected three outputs\nfrom the GPT-4o using the same prompt and ensembled these results as the final\nprediction. Using the proposed method, our models achieved an AUC value of\n0.778 and an AP value of 0.426 for LNM prediction, significantly improving\npredictive performance compared to baseline machine learning models. The\nexperimental results indicate that GPT-4o can effectively leverage its medical\nknowledge and the probabilities predicted by machine learning models to achieve\nmore accurate LNM predictions. These findings demonstrate that LLMs can perform\nwell in clinical risk prediction tasks, offering a new paradigm for integrating\nmedical knowledge and patient data in clinical predictions.\n","authors":["Danqing Hu","Bing Liu","Xiaofeng Zhu","Nan Wu"],"pdf_url":"https://arxiv.org/pdf/2407.17900v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02266v1","updated":"2024-08-05T06:47:32Z","published":"2024-08-05T06:47:32Z","title":"One-Shot Collaborative Data Distillation","summary":"  Large machine-learning training datasets can be distilled into small\ncollections of informative synthetic data samples. These synthetic sets support\nefficient model learning and reduce the communication cost of data sharing.\nThus, high-fidelity distilled data can support the efficient deployment of\nmachine learning applications in distributed network environments. A naive way\nto construct a synthetic set in a distributed environment is to allow each\nclient to perform local data distillation and to merge local distillations at a\ncentral server. However, the quality of the resulting set is impaired by\nheterogeneity in the distributions of the local data held by clients. To\novercome this challenge, we introduce the first collaborative data distillation\ntechnique, called CollabDM, which captures the global distribution of the data\nand requires only a single round of communication between client and server.\nOur method outperforms the state-of-the-art one-shot learning method on skewed\ndata in distributed learning environments. We also show the promising practical\nbenefits of our method when applied to attack detection in 5G networks.\n","authors":["Rayne Holland","Chandra Thapa","Sarah Ali Siddiqui","Wei Shao","Seyit Camtepe"],"pdf_url":"https://arxiv.org/pdf/2408.02266v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02247v1","updated":"2024-08-05T05:41:16Z","published":"2024-08-05T05:41:16Z","title":"Contrastive Learning and Abstract Concepts: The Case of Natural Numbers","summary":"  Contrastive Learning (CL) has been successfully applied to classification and\nother downstream tasks related to concrete concepts, such as objects contained\nin the ImageNet dataset. No attempts seem to have been made so far in applying\nthis promising scheme to more abstract entities. A prominent example of these\ncould be the concept of (discrete) Quantity. CL can be frequently interpreted\nas a self-supervised scheme guided by some profound and ubiquitous conservation\nprinciple (e.g. conservation of identity in object classification tasks). In\nthis introductory work we apply a suitable conservation principle to the\nsemi-abstract concept of natural numbers by which discrete quantities can be\nestimated or predicted. We experimentally show, by means of a toy problem, that\ncontrastive learning can be trained to count at a glance with high accuracy\nboth at human as well as at super-human ranges.. We compare this with the\nresults of a trained-to-count at a glance supervised learning (SL) neural\nnetwork scheme of similar architecture. We show that both schemes exhibit\nsimilar good performance on baseline experiments, where the distributions of\nthe training and testing stages are equal. Importantly, we demonstrate that in\nsome generalization scenarios, where training and testing distributions differ,\nCL boasts more robust and much better error performance.\n","authors":["Daniel N. Nissani"],"pdf_url":"https://arxiv.org/pdf/2408.02247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02242v1","updated":"2024-08-05T05:27:19Z","published":"2024-08-05T05:27:19Z","title":"Methods to improve run time of hydrologic models: opportunities and\n  challenges in the machine learning era","summary":"  The application of Machine Learning (ML) to hydrologic modeling is fledgling.\nIts applicability to capture the dependencies on watersheds to forecast better\nwithin a short period is fascinating. One of the key reasons to adopt ML\nalgorithms over physics-based models is its computational efficiency advantage\nand flexibility to work with various data sets. The diverse applications,\nparticularly in emergency response and expanding over a large scale, demand the\nhydrological model in a short time and make researchers adopt data-driven\nmodeling approaches unhesitatingly. In this work, in the era of ML and deep\nlearning (DL), how it can help to improve the overall run time of physics-based\nmodel and potential constraints that should be addressed while modeling. This\npaper covers the opportunities and challenges of adopting ML for hydrological\nmodeling and subsequently how it can help to improve the simulation time of\nphysics-based models and future works that should be addressed.\n","authors":["Supath Dhital"],"pdf_url":"https://arxiv.org/pdf/2408.02242v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.03560v3","updated":"2024-08-05T04:35:48Z","published":"2022-12-07T10:25:59Z","title":"SeqLink: A Robust Neural-ODE Architecture for Modelling Partially\n  Observed Time Series","summary":"  Ordinary Differential Equations (ODE) based models have become popular as\nfoundation models for solving many time series problems. Combining neural ODEs\nwith traditional RNN models has provided the best representation for irregular\ntime series. However, ODE-based models typically require the trajectory of\nhidden states to be defined based on either the initial observed value or the\nmost recent observation, raising questions about their effectiveness when\ndealing with longer sequences and extended time intervals. In this article, we\nexplore the behaviour of the ODE models in the context of time series data with\nvarying degrees of sparsity. We introduce SeqLink, an innovative neural\narchitecture designed to enhance the robustness of sequence representation.\nUnlike traditional approaches that solely rely on the hidden state generated\nfrom the last observed value, SeqLink leverages ODE latent representations\nderived from multiple data samples, enabling it to generate robust data\nrepresentations regardless of sequence length or data sparsity level. The core\nconcept behind our model is the definition of hidden states for the unobserved\nvalues based on the relationships between samples (links between sequences).\nThrough extensive experiments on partially observed synthetic and real-world\ndatasets, we demonstrate that SeqLink improves the modelling of intermittent\ntime series, consistently outperforming state-of-the-art approaches.\n","authors":["Futoon M. Abushaqra","Hao Xue","Yongli Ren","Flora D. Salim"],"pdf_url":"https://arxiv.org/pdf/2212.03560v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02223v1","updated":"2024-08-05T03:54:52Z","published":"2024-08-05T03:54:52Z","title":"Large Language Model Aided QoS Prediction for Service Recommendation","summary":"  Large language models (LLMs) have seen rapid improvement in the recent years,\nand are used in a wider range of applications. After being trained on large\ntext corpus, LLMs obtain the capability of extracting rich features from\ntextual data. Such capability is potentially useful for the web service\nrecommendation task, where the web users and services have intrinsic attributes\nthat can be described using natural language sentences and are useful for\nrecommendation. In this paper, we explore the possibility and practicality of\nusing LLMs for web service recommendation. We propose the large language model\naided QoS prediction (llmQoS) model, which use LLMs to extract useful\ninformation from attributes of web users and services via descriptive\nsentences. This information is then used in combination with the QoS values of\nhistorical interactions of users and services, to predict QoS values for any\ngiven user-service pair. Our proposed model is shown to overcome the data\nsparsity issue for QoS prediction. We show that on the WSDream dataset, llmQoS\noutperforms comparable baseline models consistently.\n","authors":["Huiying Liu","Zekun Zhang","Qilin Wu","Yiwen Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.00861v2","updated":"2024-08-05T03:47:54Z","published":"2022-06-02T03:48:29Z","title":"Dynamic Structure Estimation from Bandit Feedback using Nonvanishing\n  Exponential Sums","summary":"  This work tackles the dynamic structure estimation problems for periodically\nbehaved discrete dynamical system in the Euclidean space. We assume the\nobservations become sequentially available in a form of bandit feedback\ncontaminated by a sub-Gaussian noise. Under such fairly general assumptions on\nthe noise distribution, we carefully identify a set of recoverable information\nof periodic structures. Our main results are the (computation and sample)\nefficient algorithms that exploit asymptotic behaviors of exponential sums to\neffectively average out the noise effect while preventing the information to be\nestimated from vanishing. In particular, the novel use of the Weyl sum, a\nvariant of exponential sums, allows us to extract spectrum information for\nlinear systems. We provide sample complexity bounds for our algorithms, and we\nexperimentally validate our theoretical claims on simulations of toy examples,\nincluding Cellular Automata.\n","authors":["Motoya Ohnishi","Isao Ishikawa","Yuko Kuroki","Masahiro Ikeda"],"pdf_url":"https://arxiv.org/pdf/2206.00861v2.pdf","comment":"35 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02217v1","updated":"2024-08-05T03:38:38Z","published":"2024-08-05T03:38:38Z","title":"Climate-Driven Doubling of Maize Loss Probability in U.S. Crop\n  Insurance: Spatiotemporal Prediction and Possible Policy Responses","summary":"  Climate change not only threatens agricultural producers but also strains\nfinancial institutions. These important food system actors include government\nentities tasked with both insuring grower livelihoods and supporting response\nto continued global warming. We use an artificial neural network to predict\nfuture maize yields in the U.S. Corn Belt, finding alarming changes to\ninstitutional risk exposure within the Federal Crop Insurance Program.\nSpecifically, our machine learning method anticipates more frequent and more\nsevere yield losses that would result in the annual probability of Yield\nProtection (YP) claims to more than double at mid-century relative to\nsimulations without continued climate change. Furthermore, our dual finding of\nrelatively unchanged average yields paired with decreasing yield stability\nreveals targeted opportunities to adjust coverage formulas to include\nvariability. This important structural shift may help regulators support grower\nadaptation to continued climate change by recognizing the value of\nrisk-reducing strategies such as regenerative agriculture. Altogether, paired\nwith open source interactive tools for deeper investigation, our risk profile\nsimulations fill an actionable gap in current understanding, bridging granular\nhistoric yield estimation and climate-informed prediction of future\ninsurer-relevant loss.\n","authors":["A Samuel Pottinger","Lawson Connor","Brookie Guzder-Williams","Maya Weltman-Fahs","Timothy Bowles"],"pdf_url":"https://arxiv.org/pdf/2408.02217v1.pdf","comment":"24 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.00657v2","updated":"2024-08-05T03:25:01Z","published":"2024-08-01T15:46:22Z","title":"Disentangling Dense Embeddings with Sparse Autoencoders","summary":"  Sparse autoencoders (SAEs) have shown promise in extracting interpretable\nfeatures from complex neural networks. We present one of the first applications\nof SAEs to dense text embeddings from large language models, demonstrating\ntheir effectiveness in disentangling semantic concepts. By training SAEs on\nembeddings of over 420,000 scientific paper abstracts from computer science and\nastronomy, we show that the resulting sparse representations maintain semantic\nfidelity while offering interpretability. We analyse these learned features,\nexploring their behaviour across different model capacities and introducing a\nnovel method for identifying ``feature families'' that represent related\nconcepts at varying levels of abstraction. To demonstrate the practical utility\nof our approach, we show how these interpretable features can be used to\nprecisely steer semantic search, allowing for fine-grained control over query\nsemantics. This work bridges the gap between the semantic richness of dense\nembeddings and the interpretability of sparse representations. We open source\nour embeddings, trained sparse autoencoders, and interpreted features, as well\nas a web app for exploring them.\n","authors":["Charles O'Neill","Christine Ye","Kartheik Iyer","John F. Wu"],"pdf_url":"https://arxiv.org/pdf/2408.00657v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.02903v4","updated":"2024-08-05T03:24:09Z","published":"2023-10-04T15:42:23Z","title":"FroSSL: Frobenius Norm Minimization for Efficient Multiview\n  Self-Supervised Learning","summary":"  Self-supervised learning (SSL) is a popular paradigm for representation\nlearning. Recent multiview methods can be classified as sample-contrastive,\ndimension-contrastive, or asymmetric network-based, with each family having its\nown approach to avoiding informational collapse. While these families converge\nto solutions of similar quality, it can be empirically shown that some methods\nare epoch-inefficient and require longer training to reach a target\nperformance. Two main approaches to improving efficiency are covariance\neigenvalue regularization and using more views. However, these two approaches\nare difficult to combine due to the computational complexity of computing\neigenvalues. We present the objective function FroSSL which reconciles both\napproaches while avoiding eigendecomposition entirely. FroSSL works by\nminimizing covariance Frobenius norms to avoid collapse and minimizing\nmean-squared error to achieve augmentation invariance. We show that FroSSL\nreaches competitive accuracies more quickly than any other SSL method and\nprovide theoretical and empirical support that this faster convergence is due\nto how FroSSL affects the eigenvalues of the embedding covariance matrices. We\nalso show that FroSSL learns competitive representations on linear probe\nevaluation when used to train a ResNet-18 on several datasets, including\nSTL-10, Tiny ImageNet, and ImageNet-100.\n","authors":["Oscar Skean","Aayush Dhakal","Nathan Jacobs","Luis Gonzalo Sanchez Giraldo"],"pdf_url":"https://arxiv.org/pdf/2310.02903v4.pdf","comment":"Accepted by ECCV2024"},{"id":"http://arxiv.org/abs/2408.02208v1","updated":"2024-08-05T03:17:44Z","published":"2024-08-05T03:17:44Z","title":"Multi-level Traffic-Responsive Tilt Camera Surveillance through\n  Predictive Correlated Online Learning","summary":"  In urban traffic management, the primary challenge of dynamically and\nefficiently monitoring traffic conditions is compounded by the insufficient\nutilization of thousands of surveillance cameras along the intelligent\ntransportation system. This paper introduces the multi-level Traffic-responsive\nTilt Camera surveillance system (TTC-X), a novel framework designed for dynamic\nand efficient monitoring and management of traffic in urban networks. By\nleveraging widely deployed pan-tilt-cameras (PTCs), TTC-X overcomes the\nlimitations of a fixed field of view in traditional surveillance systems by\nproviding mobilized and 360-degree coverage. The innovation of TTC-X lies in\nthe integration of advanced machine learning modules, including a\ndetector-predictor-controller structure, with a novel Predictive Correlated\nOnline Learning (PiCOL) methodology and the Spatial-Temporal Graph Predictor\n(STGP) for real-time traffic estimation and PTC control. The TTC-X is tested\nand evaluated under three experimental scenarios (e.g., maximum traffic flow\ncapture, dynamic route planning, traffic state estimation) based on a\nsimulation environment calibrated using real-world traffic data in Brooklyn,\nNew York. The experimental results showed that TTC-X captured over 60\\% total\nnumber of vehicles at the network level, dynamically adjusted its route\nrecommendation in reaction to unexpected full-lane closure events, and\nreconstructed link-level traffic states with best MAE less than 1.25\nvehicle/hour. Demonstrating scalability, cost-efficiency, and adaptability,\nTTC-X emerges as a powerful solution for urban traffic management in both\ncyber-physical and real-world environments.\n","authors":["Tao Li","Zilin Bian","Haozhe Lei","Fan Zuo","Ya-Ting Yang","Quanyan Zhu","Zhenning Li","Kaan Ozbay"],"pdf_url":"https://arxiv.org/pdf/2408.02208v1.pdf","comment":"Accepted to Transportation Research Part C special issue: Modelling,\n  Learning, and Control of Conventional, Cooperative and Automated Motorway and\n  Urban Traffic Systems"},{"id":"http://arxiv.org/abs/2408.02201v1","updated":"2024-08-05T03:05:02Z","published":"2024-08-05T03:05:02Z","title":"Evaluating the Performance of Large Language Models for SDG Mapping\n  (Technical Report)","summary":"  The use of large language models (LLMs) is expanding rapidly, and open-source\nversions are becoming available, offering users safer and more adaptable\noptions. These models enable users to protect data privacy by eliminating the\nneed to provide data to third parties and can be customized for specific tasks.\nIn this study, we compare the performance of various language models on the\nSustainable Development Goal (SDG) mapping task, using the output of GPT-4o as\nthe baseline. The selected open-source models for comparison include Mixtral,\nLLaMA 2, LLaMA 3, Gemma, and Qwen2. Additionally, GPT-4o-mini, a more\nspecialized version of GPT-4o, was included to extend the comparison. Given the\nmulti-label nature of the SDG mapping task, we employed metrics such as F1\nscore, precision, and recall with micro-averaging to evaluate different aspects\nof the models' performance. These metrics are derived from the confusion matrix\nto ensure a comprehensive evaluation. We provide a clear observation and\nanalysis of each model's performance by plotting curves based on F1 score,\nprecision, and recall at different thresholds. According to the results of this\nexperiment, LLaMA 2 and Gemma still have significant room for improvement. The\nother four models do not exhibit particularly large differences in performance.\nThe outputs from all seven models are available on Zenodo:\nhttps://doi.org/10.5281/zenodo.12789375.\n","authors":["Hui Yin","Amir Aryani","Nakul Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.02201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02198v1","updated":"2024-08-05T02:50:58Z","published":"2024-08-05T02:50:58Z","title":"Synergistic Learning with Multi-Task DeepONet for Efficient PDE Problem\n  Solving","summary":"  Multi-task learning (MTL) is an inductive transfer mechanism designed to\nleverage useful information from multiple tasks to improve generalization\nperformance compared to single-task learning. It has been extensively explored\nin traditional machine learning to address issues such as data sparsity and\noverfitting in neural networks. In this work, we apply MTL to problems in\nscience and engineering governed by partial differential equations (PDEs).\nHowever, implementing MTL in this context is complex, as it requires\ntask-specific modifications to accommodate various scenarios representing\ndifferent physical processes. To this end, we present a multi-task deep\noperator network (MT-DeepONet) to learn solutions across various functional\nforms of source terms in a PDE and multiple geometries in a single concurrent\ntraining session. We introduce modifications in the branch network of the\nvanilla DeepONet to account for various functional forms of a parameterized\ncoefficient in a PDE. Additionally, we handle parameterized geometries by\nintroducing a binary mask in the branch network and incorporating it into the\nloss term to improve convergence and generalization to new geometry tasks. Our\napproach is demonstrated on three benchmark problems: (1) learning different\nfunctional forms of the source term in the Fisher equation; (2) learning\nmultiple geometries in a 2D Darcy Flow problem and showcasing better transfer\nlearning capabilities to new geometries; and (3) learning 3D parameterized\ngeometries for a heat transfer problem and demonstrate the ability to predict\non new but similar geometries. Our MT-DeepONet framework offers a novel\napproach to solving PDE problems in engineering and science under a unified\numbrella based on synergistic learning that reduces the overall training cost\nfor neural operators.\n","authors":["Varun Kumar","Somdatta Goswami","Katiana Kontolati","Michael D. Shields","George Em Karniadakis"],"pdf_url":"https://arxiv.org/pdf/2408.02198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00380v2","updated":"2024-08-05T02:45:50Z","published":"2024-08-01T08:41:13Z","title":"Enhancing Whole Slide Pathology Foundation Models through Stain\n  Normalization","summary":"  Recent advancements in digital pathology have led to the development of\nnumerous foundational models that utilize self-supervised learning on patches\nextracted from gigapixel whole slide images (WSIs). While this approach\nleverages vast amounts of unlabeled data, we have discovered a significant\nissue: features extracted from these self-supervised models tend to cluster by\nindividual WSIs, a phenomenon we term WSI-specific feature collapse. This\nproblem can potentially limit the model's generalization ability and\nperformance on various downstream tasks. To address this issue, we introduce\nStain Normalized Pathology Foundational Model, a novel foundational model\ntrained on patches that have undergone stain normalization. Stain normalization\nhelps reduce color variability arising from different laboratories and\nscanners, enabling the model to learn more consistent features. Stain\nNormalized Pathology Foundational Model is trained using 285,153,903 patches\nextracted from a total of 34,795 WSIs, combining data from The Cancer Genome\nAtlas (TCGA) and the Genotype-Tissue Expression (GTEx) project. Our experiments\ndemonstrate that Stain Normalized Pathology Foundational Model significantly\nmitigates the feature collapse problem, indicating that the model has learned\nmore generalized features rather than overfitting to individual WSI\ncharacteristics. We compared Stain Normalized Pathology Foundational Model with\nstate-of-the-art models across six downstream task datasets, and our results\nshow that Stain Normalized Pathology Foundational Model achieves excellent\nperformance relative to the number of WSIs used and the model's parameter\ncount. This suggests that the application of stain normalization has\nsubstantially improved the model's efficiency and generalization capabilities.\n","authors":["Juseung Yun","Yi Hu","Jinhyung Kim","Jongseong Jang","Soonyoung Lee"],"pdf_url":"https://arxiv.org/pdf/2408.00380v2.pdf","comment":"13 pages, 8 figures"},{"id":"http://arxiv.org/abs/2404.07950v2","updated":"2024-08-05T02:45:42Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v2.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.02193v1","updated":"2024-08-05T02:38:48Z","published":"2024-08-05T02:38:48Z","title":"CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs","summary":"  Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n","authors":["Weijie Lv","Xuan Xia","Sheng-Jun Huang"],"pdf_url":"https://arxiv.org/pdf/2408.02193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00992v2","updated":"2024-08-05T02:09:58Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hour","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16020v4","updated":"2024-08-05T02:01:12Z","published":"2024-07-22T19:55:44Z","title":"Sparks of Quantum Advantage and Rapid Retraining in Machine Learning","summary":"  The advent of quantum computing holds the potential to revolutionize various\nfields by solving complex problems more efficiently than classical computers.\nDespite this promise, practical quantum advantage is hindered by current\nhardware limitations, notably the small number of qubits and high noise levels.\nIn this study, we leverage adiabatic quantum computers to optimize\nKolmogorov-Arnold Networks, a powerful neural network architecture for\nrepresenting complex functions with minimal parameters. By modifying the\nnetwork to use Bezier curves as the basis functions and formulating the\noptimization problem into a Quadratic Unconstrained Binary Optimization\nproblem, we create a fixed-sized solution space, independent of the number of\ntraining samples. This strategy allows for the optimization of an entire neural\nnetwork in a single training iteration in which, due to order of operations, a\nmajority of the processing is done using a collapsed version of the training\ndataset. This inherently creates extremely fast training speeds, which are\nvalidated experimentally, compared to classical optimizers including Adam,\nStochastic Gradient Descent, Adaptive Gradient, and simulated annealing.\nAdditionally, we introduce a novel rapid retraining capability, enabling the\nnetwork to be retrained with new data without reprocessing old samples, thus\nenhancing learning efficiency in dynamic environments. Experiments on\nretraining demonstrate a hundred times speed up using adiabatic quantum\ncomputing based optimization compared to that of the gradient descent based\noptimizers, with theoretical models allowing this speed up to be much larger!\nOur findings suggest that with further advancements in quantum hardware and\nalgorithm optimization, quantum-optimized machine learning models could have\nbroad applications across various domains, with initial focus on rapid\nretraining.\n","authors":["William Troy"],"pdf_url":"https://arxiv.org/pdf/2407.16020v4.pdf","comment":"Major updates to the paper for timings and explanations of\n  optimization strategies used. Further optimized the code and updated the\n  figures to reflect the faster timings for v3"},{"id":"http://arxiv.org/abs/2402.02977v4","updated":"2024-08-05T01:24:52Z","published":"2024-02-05T12:58:29Z","title":"Variational Flow Models: Flowing in Your Style","summary":"  We propose a systematic training-free method to transform the probability\nflow of a \"linear\" stochastic process characterized by the equation\nX_{t}=a_{t}X_{0}+\\sigma_{t}X_{1} into a straight constant-speed (SC) flow,\nreminiscent of Rectified Flow. This transformation facilitates fast sampling\nalong the original probability flow via the Euler method without training a new\nmodel of the SC flow. The flexibility of our approach allows us to extend our\ntransformation to inter-convert two posterior flows of two distinct linear\nstochastic processes. Moreover, we can easily integrate high-order numerical\nsolvers into the transformed SC flow, further enhancing the sampling accuracy\nand efficiency. Rigorous theoretical analysis and extensive experimental\nresults substantiate the advantages of our framework. Our code is available at\nthis [https://github.com/clarken92/VFM||link].\n","authors":["Kien Do","Duc Kieu","Toan Nguyen","Dang Nguyen","Hung Le","Dung Nguyen","Thin Nguyen"],"pdf_url":"https://arxiv.org/pdf/2402.02977v4.pdf","comment":"Our code is available at: https://github.com/clarken92/VFM"},{"id":"http://arxiv.org/abs/2302.00857v2","updated":"2024-08-05T01:10:35Z","published":"2023-02-02T04:02:49Z","title":"Algorithm Design for Online Meta-Learning with Task Boundary Detection","summary":"  Online meta-learning has recently emerged as a marriage between batch\nmeta-learning and online learning, for achieving the capability of quick\nadaptation on new tasks in a lifelong manner. However, most existing approaches\nfocus on the restrictive setting where the distribution of the online tasks\nremains fixed with known task boundaries. In this work, we relax these\nassumptions and propose a novel algorithm for task-agnostic online\nmeta-learning in non-stationary environments. More specifically, we first\npropose two simple but effective detection mechanisms of task switches and\ndistribution shift based on empirical observations, which serve as a key\nbuilding block for more elegant online model updates in our algorithm: the task\nswitch detection mechanism allows reusing of the best model available for the\ncurrent task at hand, and the distribution shift detection mechanism\ndifferentiates the meta model update in order to preserve the knowledge for\nin-distribution tasks and quickly learn the new knowledge for\nout-of-distribution tasks. In particular, our online meta model updates are\nbased only on the current data, which eliminates the need of storing previous\ndata as required in most existing methods. We further show that a sublinear\ntask-averaged regret can be achieved for our algorithm under mild conditions.\nEmpirical studies on three different benchmarks clearly demonstrate the\nsignificant advantage of our algorithm over related baseline approaches.\n","authors":["Daouda Sow","Sen Lin","Yingbin Liang","Junshan Zhang"],"pdf_url":"https://arxiv.org/pdf/2302.00857v2.pdf","comment":"CPAL 2024"},{"id":"http://arxiv.org/abs/2402.08225v3","updated":"2024-08-05T00:54:02Z","published":"2024-02-13T05:33:35Z","title":"Improving Black-box Robustness with In-Context Rewriting","summary":"  Machine learning models for text classification often excel on\nin-distribution (ID) data but struggle with unseen out-of-distribution (OOD)\ninputs. Most techniques for improving OOD robustness are not applicable to\nsettings where the model is effectively a black box, such as when the weights\nare frozen, retraining is costly, or the model is leveraged via an API.\nTest-time augmentation (TTA) is a simple post-hoc technique for improving\nrobustness that sidesteps black-box constraints by aggregating predictions\nacross multiple augmentations of the test input. TTA has seen limited use in\nNLP due to the challenge of generating effective natural language\naugmentations. In this work, we propose LLM-TTA, which uses LLM-generated\naugmentations as TTA's augmentation function. LLM-TTA outperforms conventional\naugmentation functions across sentiment, toxicity, and news classification\ntasks for BERT and T5 models, with BERT's OOD robustness improving by an\naverage of 4.48 percentage points without regressing average ID performance. We\nexplore selectively augmenting inputs based on prediction entropy to reduce the\nrate of expensive LLM augmentations, allowing us to maintain performance gains\nwhile reducing the average number of generated augmentations by 57.74\\%.\nLLM-TTA is agnostic to the task model architecture, does not require OOD\nlabels, and is effective across low and high-resource settings. We share our\ndata, models, and code for reproducibility.\n","authors":["Kyle O'Brien","Nathan Ng","Isha Puri","Jorge Mendez","Hamid Palangi","Yoon Kim","Marzyeh Ghassemi","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2402.08225v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02866v1","updated":"2024-08-05T23:33:24Z","published":"2024-08-05T23:33:24Z","title":"Back-Projection Diffusion: Solving the Wideband Inverse Scattering\n  Problem with Diffusion Models","summary":"  We present \\textit{Wideband back-projection diffusion}, an end-to-end\nprobabilistic framework for approximating the posterior distribution induced by\nthe inverse scattering map from wideband scattering data. This framework\nleverages conditional diffusion models coupled with the underlying physics of\nwave-propagation and symmetries in the problem, to produce highly accurate\nreconstructions. The framework introduces a factorization of the score function\ninto a physics-based latent representation inspired by the filtered\nback-propagation formula and a conditional score function conditioned on this\nlatent representation. These two steps are also constrained to obey symmetries\nin the formulation while being amenable to compression by imposing the rank\nstructure found in the filtered back-projection formula. As a result,\nempirically, our framework is able to provide sharp reconstructions\neffortlessly, even recovering sub-Nyquist features in the multiple-scattering\nregime. It has low-sample and computational complexity, its number of\nparameters scales sub-linearly with the target resolution, and it has stable\ntraining dynamics.\n","authors":["Borong Zhang","Martn Guerra","Qin Li","Leonardo Zepeda-Nez"],"pdf_url":"https://arxiv.org/pdf/2408.02866v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.09797v2","updated":"2024-08-05T23:23:14Z","published":"2023-07-19T07:31:37Z","title":"Probabilistic Forecasting with Coherent Aggregation","summary":"  Obtaining accurate probabilistic forecasts is an important operational\nchallenge in many applications, perhaps most obviously in energy management,\nclimate forecasting, supply chain planning, and resource allocation. In many of\nthese applications, there is a natural hierarchical structure over the\nforecasted quantities; and forecasting systems that adhere to this hierarchical\nstructure are said to be coherent. Furthermore, operational planning benefits\nfrom accuracy at all levels of the aggregation hierarchy. Building accurate and\ncoherent forecasting systems, however, is challenging: classic multivariate\ntime series tools and neural network methods are still being adapted for this\npurpose. In this paper, we augment an MQForecaster neural network architecture\nwith a novel deep Gaussian factor forecasting model that achieves coherence by\nconstruction, yielding a method we call the Deep Coherent Factor Model Neural\nNetwork (DeepCoFactor) model. DeepCoFactor generates samples that can be\ndifferentiated with respect to model parameters, allowing optimization on\nvarious sample-based learning objectives that align with the forecasting\nsystem's goals, including quantile loss and the scaled Continuous Ranked\nProbability Score (CRPS). In a comparison to state-of-the-art coherent\nforecasting methods, DeepCoFactor achieves significant improvements in scaled\nCRPS forecast accuracy, with gains between 4.16 and 54.40%, as measured on\nthree publicly available hierarchical forecasting datasets.\n","authors":["Kin G. Olivares","Geoffrey Ngiar","Ruijun Ma","O. Nangba Meetei","Mengfei Cao","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2307.09797v2.pdf","comment":"10 pages of main text. Updated method and results"},{"id":"http://arxiv.org/abs/2408.02861v1","updated":"2024-08-05T23:20:32Z","published":"2024-08-05T23:20:32Z","title":"A Framework for Fine-Tuning LLMs using Heterogeneous Feedback","summary":"  Large language models (LLMs) have been applied to a wide range of tasks,\nincluding text summarization, web navigation, and chatbots. They have\nbenefitted from supervised fine-tuning (SFT) and reinforcement learning from\nhuman feedback (RLHF) following an unsupervised pretraining. These datasets can\nbe difficult to collect, limited in scope, and vary in sample quality.\nAdditionally, datasets can vary extensively in supervision format, from\nnumerical to binary as well as multi-dimensional with many different values. We\npresent a framework for fine-tuning LLMs using heterogeneous feedback, which\nhas two main components. First, we combine the heterogeneous feedback data into\na single supervision format, compatible with methods like SFT and RLHF. Next,\ngiven this unified feedback dataset, we extract a high-quality and diverse\nsubset to obtain performance increases potentially exceeding the full dataset.\nWe conduct extensive experiments to understand the effectiveness of these\ntechniques for incorporating heterogeneous feedback, and demonstrate\nimprovements from using a high-quality and diverse subset of the data. We find\nthat our framework is able to improve models in multiple areas simultaneously,\nsuch as in instruction following and bias reduction.\n","authors":["Ryan Aponte","Ryan A. Rossi","Shunan Guo","Franck Dernoncourt","Tong Yu","Xiang Chen","Subrata Mitra","Nedim Lipka"],"pdf_url":"https://arxiv.org/pdf/2408.02861v1.pdf","comment":"7 pages, 1 figure"},{"id":"http://arxiv.org/abs/2405.18373v2","updated":"2024-08-05T22:25:10Z","published":"2024-05-28T17:11:34Z","title":"A Hessian-Aware Stochastic Differential Equation for Modelling SGD","summary":"  Continuous-time approximation of Stochastic Gradient Descent (SGD) is a\ncrucial tool to study its escaping behaviors from stationary points. However,\nexisting stochastic differential equation (SDE) models fail to fully capture\nthese behaviors, even for simple quadratic objectives. Built on a novel\nstochastic backward error analysis framework, we derive the Hessian-Aware\nStochastic Modified Equation (HA-SME), an SDE that incorporates Hessian\ninformation of the objective function into both its drift and diffusion terms.\nOur analysis shows that HA-SME matches the order-best approximation error\nguarantee among existing SDE models in the literature, while achieving a\nsignificantly reduced dependence on the smoothness parameter of the objective.\nFurther, for quadratic objectives, under mild conditions, HA-SME is proved to\nbe the first SDE model that recovers exactly the SGD dynamics in the\ndistributional sense. Consequently, when the local landscape near a stationary\npoint can be approximated by quadratics, HA-SME is expected to accurately\npredict the local escaping behaviors of SGD.\n","authors":["Xiang Li","Zebang Shen","Liang Zhang","Niao He"],"pdf_url":"https://arxiv.org/pdf/2405.18373v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02849v1","updated":"2024-08-05T22:19:01Z","published":"2024-08-05T22:19:01Z","title":"Active Learning for WBAN-based Health Monitoring","summary":"  We consider a novel active learning problem motivated by the need of learning\nmachine learning models for health monitoring in wireless body area network\n(WBAN). Due to the limited resources at body sensors, collecting each unlabeled\nsample in WBAN incurs a nontrivial cost. Moreover, training health monitoring\nmodels typically requires labels indicating the patient's health state that\nneed to be generated by healthcare professionals, which cannot be obtained at\nthe same pace as data collection. These challenges make our problem\nfundamentally different from classical active learning, where unlabeled samples\nare free and labels can be queried in real time. To handle these challenges, we\npropose a two-phased active learning method, consisting of an online phase\nwhere a coreset construction algorithm is proposed to select a subset of\nunlabeled samples based on their noisy predictions, and an offline phase where\nthe selected samples are labeled to train the target model. The samples\nselected by our algorithm are proved to yield a guaranteed error in\napproximating the full dataset in evaluating the loss function. Our evaluation\nbased on real health monitoring data and our own experimentation demonstrates\nthat our solution can drastically save the data curation cost without\nsacrificing the quality of the target model.\n","authors":["Cho-Chun Chiu","Tuan Nguyen","Ting He","Shiqiang Wang","Beom-Su Kim","Ki-Il Kim"],"pdf_url":"https://arxiv.org/pdf/2408.02849v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02845v1","updated":"2024-08-05T22:01:13Z","published":"2024-08-05T22:01:13Z","title":"Heterogeneous graph attention network improves cancer multiomics\n  integration","summary":"  The increase in high-dimensional multiomics data demands advanced integration\nmodels to capture the complexity of human diseases. Graph-based deep learning\nintegration models, despite their promise, struggle with small patient cohorts\nand high-dimensional features, often applying independent feature selection\nwithout modeling relationships among omics. Furthermore, conventional\ngraph-based omics models focus on homogeneous graphs, lacking multiple types of\nnodes and edges to capture diverse structures. We introduce a Heterogeneous\nGraph ATtention network for omics integration (HeteroGATomics) to improve\ncancer diagnosis. HeteroGATomics performs joint feature selection through a\nmulti-agent system, creating dedicated networks of feature and patient\nsimilarity for each omic modality. These networks are then combined into one\nheterogeneous graph for learning holistic omic-specific representations and\nintegrating predictions across modalities. Experiments on three cancer\nmultiomics datasets demonstrate HeteroGATomics' superior performance in cancer\ndiagnosis. Moreover, HeteroGATomics enhances interpretability by identifying\nimportant biomarkers contributing to the diagnosis outcomes.\n","authors":["Sina Tabakhi","Charlotte Vandermeulen","Ian Sudbery","Haiping Lu"],"pdf_url":"https://arxiv.org/pdf/2408.02845v1.pdf","comment":"29 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.16803v3","updated":"2024-08-05T21:55:11Z","published":"2024-01-30T07:50:32Z","title":"PBSCR: The Piano Bootleg Score Composer Recognition Dataset","summary":"  This article motivates, describes, and presents the PBSCR dataset for\nstudying composer recognition of classical piano music. Our goal was to design\na dataset that facilitates large-scale research on composer recognition that is\nsuitable for modern architectures and training practices. To achieve this goal,\nwe utilize the abundance of sheet music images and rich metadata on IMSLP, use\na previously proposed feature representation called a bootleg score to encode\nthe location of noteheads relative to staff lines, and present the data in an\nextremely simple format (2D binary images) to encourage rapid exploration and\niteration. The dataset itself contains 40,000 62x64 bootleg score images for a\n9-class recognition task, 100,000 62x64 bootleg score images for a 100-class\nrecognition task, and 29,310 unlabeled variable-length bootleg score images for\npretraining. The labeled data is presented in a form that mirrors MNIST images,\nin order to make it extremely easy to visualize, manipulate, and train models\nin an efficient manner. We include relevant information to connect each bootleg\nscore image with its underlying raw sheet music image, and we scrape, organize,\nand compile metadata from IMSLP on all piano works to facilitate multimodal\nresearch and allow for convenient linking to other datasets. We release\nbaseline results in a supervised and low-shot setting for future works to\ncompare against, and we discuss open research questions that the PBSCR data is\nespecially well suited to facilitate research on.\n","authors":["Arhan Jain","Alec Bunn","Austin Pham","TJ Tsai"],"pdf_url":"https://arxiv.org/pdf/2401.16803v3.pdf","comment":"19 pages, 6 figures, to be published in Transactions of the\n  International Society for Music Information Retrieval"},{"id":"http://arxiv.org/abs/2309.08569v2","updated":"2024-08-05T21:54:54Z","published":"2023-09-15T17:35:51Z","title":"Local Differential Privacy in Graph Neural Networks: a Reconstruction\n  Approach","summary":"  Graph Neural Networks have achieved tremendous success in modeling complex\ngraph data in a variety of applications. However, there are limited studies\ninvestigating privacy protection in GNNs. In this work, we propose a learning\nframework that can provide node privacy at the user level, while incurring low\nutility loss. We focus on a decentralized notion of Differential Privacy,\nnamely Local Differential Privacy, and apply randomization mechanisms to\nperturb both feature and label data at the node level before the data is\ncollected by a central server for model training. Specifically, we investigate\nthe application of randomization mechanisms in high-dimensional feature\nsettings and propose an LDP protocol with strict privacy guarantees. Based on\nfrequency estimation in statistical analysis of randomized data, we develop\nreconstruction methods to approximate features and labels from perturbed data.\nWe also formulate this learning framework to utilize frequency estimates of\ngraph clusters to supervise the training procedure at a sub-graph level.\nExtensive experiments on real-world and semi-synthetic datasets demonstrate the\nvalidity of our proposed model.\n","authors":["Karuna Bhaila","Wen Huang","Yongkai Wu","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2309.08569v2.pdf","comment":"2024 SIAM International Conference on Data Mining"},{"id":"http://arxiv.org/abs/2406.12038v2","updated":"2024-08-05T21:48:22Z","published":"2024-06-17T19:11:40Z","title":"Soft Prompting for Unlearning in Large Language Models","summary":"  The widespread popularity of Large Language Models (LLMs), partly due to\ntheir unique ability to perform in-context learning, has also brought to light\nthe importance of ethical and safety considerations when deploying these\npre-trained models. In this work, we focus on investigating machine unlearning\nfor LLMs motivated by data protection regulations. In contrast to the growing\nliterature on fine-tuning methods to achieve unlearning, we focus on a\ncomparatively lightweight alternative called soft prompting to realize the\nunlearning of a subset of training data. With losses designed to enforce\nforgetting as well as utility preservation, our framework \\textbf{S}oft\n\\textbf{P}rompting for \\textbf{U}n\\textbf{l}earning (SPUL) learns prompt tokens\nthat can be appended to an arbitrary query to induce unlearning of specific\nexamples at inference time without updating LLM parameters. We conduct a\nrigorous evaluation of the proposed method and our results indicate that SPUL\ncan significantly improve the trade-off between utility and forgetting in the\ncontext of text classification and question answering with LLMs. We further\nvalidate our method using multiple LLMs to highlight the scalability of our\nframework and provide detailed insights into the choice of hyperparameters and\nthe influence of the size of unlearning data. Our implementation is available\nat \\url{https://github.com/karuna-bhaila/llm_unlearning}.\n","authors":["Karuna Bhaila","Minh-Hao Van","Xintao Wu"],"pdf_url":"https://arxiv.org/pdf/2406.12038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02841v1","updated":"2024-08-05T21:35:51Z","published":"2024-08-05T21:35:51Z","title":"Evaluating Posterior Probabilities: Decision Theory, Proper Scoring\n  Rules, and Calibration","summary":"  Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.\n","authors":["Luciana Ferrer","Daniel Ramos"],"pdf_url":"https://arxiv.org/pdf/2408.02841v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02839v1","updated":"2024-08-05T21:25:10Z","published":"2024-08-05T21:25:10Z","title":"Optimizing Cox Models with Stochastic Gradient Descent: Theoretical\n  Foundations and Practical Guidances","summary":"  Optimizing Cox regression and its neural network variants poses substantial\ncomputational challenges in large-scale studies. Stochastic gradient descent\n(SGD), known for its scalability in model optimization, has recently been\nadapted to optimize Cox models. Unlike its conventional application, which\ntypically targets a sum of independent individual loss, SGD for Cox models\nupdates parameters based on the partial likelihood of a subset of data. Despite\nits empirical success, the theoretical foundation for optimizing Cox partial\nlikelihood with SGD is largely underexplored. In this work, we demonstrate that\nthe SGD estimator targets an objective function that is batch-size-dependent.\nWe establish that the SGD estimator for the Cox neural network (Cox-NN) is\nconsistent and achieves the optimal minimax convergence rate up to a\npolylogarithmic factor. For Cox regression, we further prove the\n$\\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with\nvariance depending on the batch size. Furthermore, we quantify the impact of\nbatch size on Cox-NN training and its effect on the SGD estimator's asymptotic\nefficiency in Cox regression. These findings are validated by extensive\nnumerical experiments and provide guidance for selecting batch sizes in SGD\napplications. Finally, we demonstrate the effectiveness of SGD in a real-world\napplication where GD is unfeasible due to the large scale of data.\n","authors":["Lang Zeng","Weijing Tang","Zhao Ren","Ying Ding"],"pdf_url":"https://arxiv.org/pdf/2408.02839v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02838v1","updated":"2024-08-05T21:22:36Z","published":"2024-08-05T21:22:36Z","title":"Interpretation of the Intent Detection Problem as Dynamics in a\n  Low-dimensional Space","summary":"  Intent detection is a text classification task whose aim is to recognize and\nlabel the semantics behind a users query. It plays a critical role in various\nbusiness applications. The output of the intent detection module strongly\nconditions the behavior of the whole system. This sequence analysis task is\nmainly tackled using deep learning techniques. Despite the widespread use of\nthese techniques, the internal mechanisms used by networks to solve the problem\nare poorly understood. Recent lines of work have analyzed the computational\nmechanisms learned by RNNs from a dynamical systems perspective. In this work,\nwe investigate how different RNN architectures solve the SNIPS intent detection\nproblem. Sentences injected into trained networks can be interpreted as\ntrajectories traversing a hidden state space. This space is constrained to a\nlow-dimensional manifold whose dimensionality is related to the embedding and\nhidden layer sizes. To generate predictions, RNN steers the trajectories\ntowards concrete regions, spatially aligned with the output layer matrix rows\ndirections. Underlying the system dynamics, an unexpected fixed point topology\nhas been identified with a limited number of attractors. Our results provide\nnew insights into the inner workings of networks that solve the intent\ndetection task.\n","authors":["Eduardo Sanchez-Karhunen","Jose F. Quesada-Moreno","Miguel A. Gutirrez-Naranjo"],"pdf_url":"https://arxiv.org/pdf/2408.02838v1.pdf","comment":"Camera-Ready version. Accepted paper at 27th European Conference on\n  Artificial Intelligence (ECAI-2024)"},{"id":"http://arxiv.org/abs/2408.02835v1","updated":"2024-08-05T21:12:12Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Ddalo Sanz-Hernndez","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v1.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02834v1","updated":"2024-08-05T21:11:34Z","published":"2024-08-05T21:11:34Z","title":"DaCapo: a modular deep learning framework for scalable 3D image\n  segmentation","summary":"  DaCapo is a specialized deep learning library tailored to expedite the\ntraining and application of existing machine learning approaches on large,\nnear-isotropic image data. In this correspondence, we introduce DaCapo's unique\nfeatures optimized for this specific domain, highlighting its modular\nstructure, efficient experiment management tools, and scalable deployment\ncapabilities. We discuss its potential to improve access to large-scale,\nisotropic image segmentation and invite the community to explore and contribute\nto this open-source initiative.\n","authors":["William Patton","Jeff L. Rhoades","Marwan Zouinkhi","David G. Ackerman","Caroline Malin-Mayor","Diane Adjavon","Larissa Heinrich","Davis Bennett","Yurii Zubov","CellMap Project Team","Aubrey V. Weigel","Jan Funke"],"pdf_url":"https://arxiv.org/pdf/2408.02834v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19082v2","updated":"2024-08-05T21:09:50Z","published":"2024-07-26T21:02:11Z","title":"Regularized Multi-Decoder Ensemble for an Error-Aware Scene\n  Representation Network","summary":"  Feature grid Scene Representation Networks (SRNs) have been applied to\nscientific data as compact functional surrogates for analysis and\nvisualization. As SRNs are black-box lossy data representations, assessing the\nprediction quality is critical for scientific visualization applications to\nensure that scientists can trust the information being visualized. Currently,\nexisting architectures do not support inference time reconstruction quality\nassessment, as coordinate-level errors cannot be evaluated in the absence of\nground truth data. We propose a parameter-efficient multi-decoder SRN (MDSRN)\nensemble architecture consisting of a shared feature grid with multiple\nlightweight multi-layer perceptron decoders. MDSRN can generate a set of\nplausible predictions for a given input coordinate to compute the mean as the\nprediction of the multi-decoder ensemble and the variance as a confidence\nscore. The coordinate-level variance can be rendered along with the data to\ninform the reconstruction quality, or be integrated into uncertainty-aware\nvolume visualization algorithms. To prevent the misalignment between the\nquantified variance and the prediction quality, we propose a novel variance\nregularization loss for ensemble learning that promotes the Regularized\nmulti-decoder SRN (RMDSRN) to obtain a more reliable variance that correlates\nclosely to the true model error. We comprehensively evaluate the quality of\nvariance quantification and data reconstruction of Monte Carlo Dropout, Mean\nField Variational Inference, Deep Ensemble, and Predicting Variance compared to\nthe proposed MDSRN and RMDSRN across diverse scalar field datasets. We\ndemonstrate that RMDSRN attains the most accurate data reconstruction and\ncompetitive variance-error correlation among uncertain SRNs under the same\nneural network parameter budgets.\n","authors":["Tianyu Xiong","Skylar W. Wurster","Hanqi Guo","Tom Peterka","Han-Wei Shen"],"pdf_url":"https://arxiv.org/pdf/2407.19082v2.pdf","comment":"To be published in Proc. IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.02833v1","updated":"2024-08-05T21:09:01Z","published":"2024-08-05T21:09:01Z","title":"Adaptive Learning for Quantum Linear Regression","summary":"  The recent availability of quantum annealers as cloud-based services has\nenabled new ways to handle machine learning problems, and several relevant\nalgorithms have been adapted to run on these devices. In a recent work, linear\nregression was formulated as a quadratic binary optimization problem that can\nbe solved via quantum annealing. Although this approach promises a\ncomputational time advantage for large datasets, the quality of the solution is\nlimited by the necessary use of a precision vector, used to approximate the\nreal-numbered regression coefficients in the quantum formulation. In this work,\nwe focus on the practical challenge of improving the precision vector encoding:\ninstead of setting an array of generic values equal for all coefficients, we\nallow each one to be expressed by its specific precision, which is tuned with a\nsimple adaptive algorithm. This approach is evaluated on synthetic datasets of\nincreasing size, and linear regression is solved using the D-Wave Advantage\nquantum annealer, as well as classical solvers. To the best of our knowledge,\nthis is the largest dataset ever evaluated for linear regression on a quantum\nannealer. The results show that our formulation is able to deliver improved\nsolution quality in all instances, and could better exploit the potential of\ncurrent quantum devices.\n","authors":["Costantino Carugno","Maurizio Ferrari Dacrema","Paolo Cremonesi"],"pdf_url":"https://arxiv.org/pdf/2408.02833v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02830v1","updated":"2024-08-05T20:55:14Z","published":"2024-08-05T20:55:14Z","title":"Setting the duration of online A/B experiments","summary":"  In designing an online A/B experiment, it is crucial to select a sample size\nand duration that ensure the resulting confidence interval (CI) for the\ntreatment effect is the right width to detect an effect of meaningful magnitude\nwith sufficient statistical power without wasting resources. While the\nrelationship between sample size and CI width is well understood, the effect of\nexperiment duration on CI width remains less clear. This paper provides an\nanalytical formula for the width of a CI based on a ratio treatment effect\nestimator as a function of both sample size (N) and duration (T). The formula\nis derived from a mixed effects model with two variance components. One\ncomponent, referred to as the temporal variance, persists over time for\nexperiments where the same users are kept in the same experiment arm across\ndifferent days. The remaining error variance component, by contrast, decays to\nzero as T gets large. The formula we derive introduces a key parameter that we\ncall the user-specific temporal correlation (UTC), which quantifies the\nrelative sizes of the two variance components and can be estimated from\nhistorical experiments. Higher UTC indicates a slower decay in CI width over\ntime. On the other hand, when the UTC is 0 -- as for experiments where users\nshuffle in and out of the experiment across days -- the CI width decays at the\nstandard parametric 1/T rate. We also study how access to pre-period data for\nthe users in the experiment affects the CI width decay. We show our formula\nclosely explains CI widths on real A/B experiments at YouTube.\n","authors":["Harrison H. Li","Chaoyu Yu"],"pdf_url":"https://arxiv.org/pdf/2408.02830v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02824v1","updated":"2024-08-05T20:46:54Z","published":"2024-08-05T20:46:54Z","title":"Wave-RVFL: A Randomized Neural Network Based on Wave Loss Function","summary":"  The random vector functional link (RVFL) network is well-regarded for its\nstrong generalization capabilities in the field of machine learning. However,\nits inherent dependencies on the square loss function make it susceptible to\nnoise and outliers. Furthermore, the calculation of RVFL's unknown parameters\nnecessitates matrix inversion of the entire training sample, which constrains\nits scalability. To address these challenges, we propose the Wave-RVFL, an RVFL\nmodel incorporating the wave loss function. We formulate and solve the proposed\noptimization problem of the Wave-RVFL using the adaptive moment estimation\n(Adam) algorithm in a way that successfully eliminates the requirement for\nmatrix inversion and significantly enhances scalability. The Wave-RVFL exhibits\nrobustness against noise and outliers by preventing over-penalization of\ndeviations, thereby maintaining a balanced approach to managing noise and\noutliers. The proposed Wave-RVFL model is evaluated on multiple UCI datasets,\nboth with and without the addition of noise and outliers, across various\ndomains and sizes. Empirical results affirm the superior performance and\nrobustness of the Wave-RVFL compared to baseline models, establishing it as a\nhighly effective and scalable classification solution.\n","authors":["M. Sajid","A. Quadir","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2408.02824v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02821v1","updated":"2024-08-05T20:39:06Z","published":"2024-08-05T20:39:06Z","title":"Continuous Monitoring via Repeated Significance","summary":"  Requiring statistical significance at multiple interim analyses to declare a\nstatistically significant result for an AB test allows less stringent\nrequirements for significance at each interim analysis. Repeated repeated\nsignificance competes well with methods built on assumptions about the test --\nassumptions that may be impossible to evaluate a priori and may require extra\ndata to evaluate empirically.\n  Instead, requiring repeated significance allows the data itself to prove\ndirectly that the required results are not due to chance alone. We explain how\nto apply tests with repeated significance to continuously monitor unbounded\ntests -- tests that do not have an a priori bound on running time or number of\nobservations. We show that it is impossible to maintain a constant requirement\nfor significance for unbounded tests, but that we can come arbitrarily close to\nthat goal.\n","authors":["Eric Bax","Arundhyoti Sarkar","Alex Shtoff"],"pdf_url":"https://arxiv.org/pdf/2408.02821v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02814v1","updated":"2024-08-05T20:27:54Z","published":"2024-08-05T20:27:54Z","title":"Pre-trained Encoder Inference: Revealing Upstream Encoders In Downstream\n  Machine Learning Services","summary":"  Though pre-trained encoders can be easily accessed online to build downstream\nmachine learning (ML) services quickly, various attacks have been designed to\ncompromise the security and privacy of these encoders. While most attacks\ntarget encoders on the upstream side, it remains unknown how an encoder could\nbe threatened when deployed in a downstream ML service. This paper unveils a\nnew vulnerability: the Pre-trained Encoder Inference (PEI) attack, which posts\nprivacy threats toward encoders hidden behind downstream ML services. By only\nproviding API accesses to a targeted downstream service and a set of candidate\nencoders, the PEI attack can infer which encoder is secretly used by the\ntargeted service based on candidate ones. We evaluate the attack performance of\nPEI against real-world encoders on three downstream tasks: image\nclassification, text classification, and text-to-image generation. Experiments\nshow that the PEI attack succeeds in revealing the hidden encoder in most cases\nand seldom makes mistakes even when the hidden encoder is not in the candidate\nset. We also conducted a case study on one of the most recent vision-language\nmodels, LLaVA, to illustrate that the PEI attack is useful in assisting other\nML attacks such as adversarial attacks. The code is available at\nhttps://github.com/fshp971/encoder-inference.\n","authors":["Shaopeng Fu","Xuexue Sun","Ke Qing","Tianhang Zheng","Di Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02813v1","updated":"2024-08-05T20:27:45Z","published":"2024-08-05T20:27:45Z","title":"Mitigating Malicious Attacks in Federated Learning via Confidence-aware\n  Defense","summary":"  Federated Learning (FL) is an emerging distributed machine learning paradigm\nthat allows multiple clients to collaboratively train a global model without\nsharing private local data. However, FL systems are vulnerable to attacks from\nmalicious clients, who can degrade the global model performance through data\npoisoning and model poisoning. Existing defense methods typically focus on a\nsingle type of attack, such as Byzantine attacks or backdoor attacks, and are\noften ineffective against potential data poisoning attacks like label flipping\nand label shuffling. Additionally, these methods often lack accuracy and\nrobustness in detecting and handling malicious updates. To address these\nissues, we propose a novel method based on model confidence scores, which\nevaluates the uncertainty of client model updates to detect and defend against\nmalicious clients. Our approach is comprehensively effective for both model\npoisoning and data poisoning attacks and is capable of accurately identifying\nand mitigating potential malicious updates from being aggregated. Experimental\nresults demonstrate that our method significantly improves the robustness of FL\nsystems against various types of attacks, also achieving higher model accuracy\nand stability across various scenarios.\n","authors":["Qilei Li","Ahmed M. Abdelmoniem"],"pdf_url":"https://arxiv.org/pdf/2408.02813v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02802v1","updated":"2024-08-05T19:45:07Z","published":"2024-08-05T19:45:07Z","title":"Deciphering Air Travel Disruptions: A Machine Learning Approach","summary":"  This research investigates flight delay trends by examining factors such as\ndeparture time, airline, and airport. It employs regression machine learning\nmethods to predict the contributions of various sources to delays. Time-series\nmodels, including LSTM, Hybrid LSTM, and Bi-LSTM, are compared with baseline\nregression models such as Multiple Regression, Decision Tree Regression, Random\nForest Regression, and Neural Network. Despite considerable errors in the\nbaseline models, the study aims to identify influential features in delay\nprediction, potentially informing flight planning strategies. Unlike previous\nwork, this research focuses on regression tasks and explores the use of\ntime-series models for predicting flight delays. It offers insights into\naviation operations by independently analyzing each delay component (e.g.,\nsecurity, weather).\n","authors":["Aravinda Jatavallabha","Jacob Gerlach","Aadithya Naresh"],"pdf_url":"https://arxiv.org/pdf/2408.02802v1.pdf","comment":"10 pages, 11 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.02801v1","updated":"2024-08-05T19:38:45Z","published":"2024-08-05T19:38:45Z","title":"Sparse Deep Learning Models with the $\\ell_1$ Regularization","summary":"  Sparse neural networks are highly desirable in deep learning in reducing its\ncomplexity. The goal of this paper is to study how choices of regularization\nparameters influence the sparsity level of learned neural networks. We first\nderive the $\\ell_1$-norm sparsity-promoting deep learning models including\nsingle and multiple regularization parameters models, from a statistical\nviewpoint. We then characterize the sparsity level of a regularized neural\nnetwork in terms of the choice of the regularization parameters. Based on the\ncharacterizations, we develop iterative algorithms for selecting regularization\nparameters so that the weight parameters of the resulting deep neural network\nenjoy prescribed sparsity levels. Numerical experiments are presented to\ndemonstrate the effectiveness of the proposed algorithms in choosing desirable\nregularization parameters and obtaining corresponding neural networks having\nboth of predetermined sparsity levels and satisfactory approximation accuracy.\n","authors":["Lixin Shen","Rui Wang","Yuesheng Xu","Mingsong Yan"],"pdf_url":"https://arxiv.org/pdf/2408.02801v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02798v1","updated":"2024-08-05T19:28:58Z","published":"2024-08-05T19:28:58Z","title":"Examining Gender and Power on Wikipedia Through Face and Politeness","summary":"  We propose a framework for analyzing discourse by combining two\ninterdependent concepts from sociolinguistic theory: face acts and politeness.\nWhile politeness has robust existing tools and data, face acts are less\nresourced. We introduce a new corpus created by annotating Wikipedia talk pages\nwith face acts and we use this to train a face act tagger. We then employ our\nframework to study how face and politeness interact with gender and power in\ndiscussions between Wikipedia editors. Among other findings, we observe that\nfemale Wikipedians are not only more polite, which is consistent with prior\nstudies, but that this difference corresponds with significantly more language\ndirected at humbling aspects of their own face. Interestingly, the distinction\nnearly vanishes once limiting to editors with administrative power.\n","authors":["Adil Soubki","Shyne Choi","Owen Rambow"],"pdf_url":"https://arxiv.org/pdf/2408.02798v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02797v1","updated":"2024-08-05T19:25:05Z","published":"2024-08-05T19:25:05Z","title":"Algorithm-Informed Graph Neural Networks for Leakage Detection and\n  Localization in Water Distribution Networks","summary":"  Detecting and localizing leakages is a significant challenge for the\nefficient and sustainable management of water distribution networks (WDN).\nLeveraging the inherent graph structure of WDNs, recent approaches have used\ngraph-based data-driven methods. However, these methods often learn shortcuts\nthat work well with in-distribution data but fail to generalize to\nout-of-distribution data. To address this limitation and inspired by the\nperfect generalization ability of classical algorithms, we propose an\nalgorithm-informed graph neural network (AIGNN). Recognizing that WDNs function\nas flow networks, incorporating max-flow information can be beneficial for\ninferring pressures. In the proposed framework, we first train AIGNN to emulate\nthe Ford-Fulkerson algorithm for solving max-flow problems. This algorithmic\nknowledge is then transferred to address the pressure estimation problem in\nWDNs. Two AIGNNs are deployed, one to reconstruct pressure based on the current\nmeasurements, and another to predict pressure based on previous measurements.\nLeakages are detected and localized by comparing the outputs of the\nreconstructor and the predictor. By pretraining AIGNNs to reason like\nalgorithms, they are expected to extract more task-relevant and generalizable\nfeatures. Experimental results demonstrate that the proposed algorithm-informed\napproach achieves superior results with better generalization ability compared\nto GNNs that do not incorporate algorithmic knowledge.\n","authors":["Zepeng Zhang","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2408.02797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.05490v2","updated":"2024-08-05T18:57:42Z","published":"2023-09-11T14:32:04Z","title":"Learning Semantic Segmentation with Query Points Supervision on Aerial\n  Images","summary":"  Semantic segmentation is crucial in remote sensing, where high-resolution\nsatellite images are segmented into meaningful regions. Recent advancements in\ndeep learning have significantly improved satellite image segmentation.\nHowever, most of these methods are typically trained in fully supervised\nsettings that require high-quality pixel-level annotations, which are expensive\nand time-consuming to obtain. In this work, we present a weakly supervised\nlearning algorithm to train semantic segmentation algorithms that only rely on\nquery point annotations instead of full mask labels. Our proposed approach\nperforms accurate semantic segmentation and improves efficiency by\nsignificantly reducing the cost and time required for manual annotation.\nSpecifically, we generate superpixels and extend the query point labels into\nthose superpixels that group similar meaningful semantics. Then, we train\nsemantic segmentation models supervised with images partially labeled with the\nsuperpixel pseudo-labels. We benchmark our weakly supervised training approach\non an aerial image dataset and different semantic segmentation architectures,\nshowing that we can reach competitive performance compared to fully supervised\ntraining while reducing the annotation effort. The code of our proposed\napproach is publicly available at: https://github.com/santiago2205/LSSQPS.\n","authors":["Santiago Rivier","Carlos Hinojosa","Silvio Giancola","Bernard Ghanem"],"pdf_url":"https://arxiv.org/pdf/2309.05490v2.pdf","comment":"Paper Accepted at ICIP 2024 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2312.00761v4","updated":"2024-08-05T18:40:07Z","published":"2023-12-01T18:29:08Z","title":"Deep Unlearning: Fast and Efficient Gradient-free Approach to Class\n  Forgetting","summary":"  Machine unlearning is a prominent and challenging field, driven by regulatory\ndemands for user data deletion and heightened privacy awareness. Existing\napproaches involve retraining model or multiple finetuning steps for each\ndeletion request, often constrained by computational limits and restricted data\naccess. In this work, we introduce a novel class unlearning algorithm designed\nto strategically eliminate specific classes from the learned model. Our\nalgorithm first estimates the Retain and the Forget Spaces using Singular Value\nDecomposition on the layerwise activations for a small subset of samples from\nthe retain and unlearn classes, respectively. We then compute the shared\ninformation between these spaces and remove it from the forget space to isolate\nclass-discriminatory feature space. Finally, we obtain the unlearned model by\nupdating the weights to suppress the class discriminatory features from the\nactivation spaces. We demonstrate our algorithm's efficacy on ImageNet using a\nVision Transformer with only $\\sim 1.5\\%$ drop in retain accuracy compared to\nthe original model while maintaining under $1\\%$ accuracy on the unlearned\nclass samples. Furthermore, our algorithm exhibits competitive unlearning\nperformance and resilience against Membership Inference Attacks (MIA). Compared\nto baselines, it achieves an average accuracy improvement of $1.38\\%$ on the\nImageNet dataset while requiring up to $10 \\times$ fewer samples for\nunlearning. Additionally, under stronger MIA attacks on the CIFAR-100 dataset\nusing a ResNet18 architecture, our approach outperforms the best baseline by\n$1.8\\%$. Our code is available at\nhttps://github.com/sangamesh-kodge/class_forgetting.\n","authors":["Sangamesh Kodge","Gobinda Saha","Kaushik Roy"],"pdf_url":"https://arxiv.org/pdf/2312.00761v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02767v1","updated":"2024-08-05T18:36:13Z","published":"2024-08-05T18:36:13Z","title":"4D-Var using Hessian approximation and backpropagation applied to\n  automatically-differentiable numerical and machine learning models","summary":"  Constraining a numerical weather prediction (NWP) model with observations via\n4D variational (4D-Var) data assimilation is often difficult to implement in\npractice due to the need to develop and maintain a software-based tangent\nlinear model and adjoint model. One of the most common 4D-Var algorithms uses\nan incremental update procedure, which has been shown to be an approximation of\nthe Gauss-Newton method. Here we demonstrate that when using a forecast model\nthat supports automatic differentiation, an efficient and in some cases more\naccurate alternative approximation of the Gauss-Newton method can be applied by\ncombining backpropagation of errors with Hessian approximation. This approach\ncan be used with either a conventional numerical model implemented within a\nsoftware framework that supports automatic differentiation, or a machine\nlearning (ML) based surrogate model. We test the new approach on a variety of\nLorenz-96 and quasi-geostrophic models. The results indicate potential for a\ndeeper integration of modeling, data assimilation, and new technologies in a\nnext-generation of operational forecast systems that leverage weather models\ndesigned to support automatic differentiation.\n","authors":["Kylen Solvik","Stephen G. Penny","Stephan Hoyer"],"pdf_url":"https://arxiv.org/pdf/2408.02767v1.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.02766v1","updated":"2024-08-05T18:34:15Z","published":"2024-08-05T18:34:15Z","title":"ConDL: Detector-Free Dense Image Matching","summary":"  In this work, we introduce a deep-learning framework designed for estimating\ndense image correspondences. Our fully convolutional model generates dense\nfeature maps for images, where each pixel is associated with a descriptor that\ncan be matched across multiple images. Unlike previous methods, our model is\ntrained on synthetic data that includes significant distortions, such as\nperspective changes, illumination variations, shadows, and specular highlights.\nUtilizing contrastive learning, our feature maps achieve greater invariance to\nthese distortions, enabling robust matching. Notably, our method eliminates the\nneed for a keypoint detector, setting it apart from many existing\nimage-matching techniques.\n","authors":["Monika Kwiatkowski","Simon Matern","Olaf Hellwich"],"pdf_url":"https://arxiv.org/pdf/2408.02766v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02761v1","updated":"2024-08-05T18:24:48Z","published":"2024-08-05T18:24:48Z","title":"Dimensionality Reduction and Nearest Neighbors for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation","summary":"  Clinically deployed deep learning-based segmentation models are known to fail\non data outside of their training distributions. While clinicians review the\nsegmentations, these models tend to perform well in most instances, which could\nexacerbate automation bias. Therefore, detecting out-of-distribution images at\ninference is critical to warn the clinicians that the model likely failed. This\nwork applied the Mahalanobis distance (MD) post hoc to the bottleneck features\nof four Swin UNETR and nnU-net models that segmented the liver on T1-weighted\nmagnetic resonance imaging and computed tomography. By reducing the dimensions\nof the bottleneck features with either principal component analysis or uniform\nmanifold approximation and projection, images the models failed on were\ndetected with high performance and minimal computational load. In addition,\nthis work explored a non-parametric alternative to the MD, a k-th nearest\nneighbors distance (KNN). KNN drastically improved scalability and performance\nover MD when both were applied to raw and average-pooled bottleneck features.\n","authors":["McKell Woodland","Nihil Patel","Austin Castelo","Mais Al Taie","Mohamed Eltaher","Joshua P. Yung","Tucker J. Netherton","Tiffany L. Calderone","Jessica I. Sanchez","Darrel W. Cleere","Ahmed Elsaiey","Nakul Gupta","David Victor","Laura Beretta","Ankit B. Patel Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2408.02761v1.pdf","comment":"Expansion of \"Dimensionality Reduction for Improving\n  Out-of-Distribution Detection in Medical Image Segmentation\" arXiv:2308.03723\n  . Submitted to the Journal for Machine Learning in Biomedical Imaging. Code\n  available at https://github.com/mckellwoodland/dimen_reduce_mahal"},{"id":"http://arxiv.org/abs/2408.02760v1","updated":"2024-08-05T18:24:09Z","published":"2024-08-05T18:24:09Z","title":"Classification of Raw MEG/EEG Data with Detach-Rocket Ensemble: An\n  Improved ROCKET Algorithm for Multivariate Time Series Analysis","summary":"  Multivariate Time Series Classification (MTSC) is a ubiquitous problem in\nscience and engineering, particularly in neuroscience, where most data\nacquisition modalities involve the simultaneous time-dependent recording of\nbrain activity in multiple brain regions. In recent years, Random Convolutional\nKernel models such as ROCKET and MiniRocket have emerged as highly effective\ntime series classification algorithms, capable of achieving state-of-the-art\naccuracy results with low computational load. Despite their success, these\ntypes of models face two major challenges when employed in neuroscience: 1)\nthey struggle to deal with high-dimensional data such as EEG and MEG, and 2)\nthey are difficult to interpret. In this work, we present a novel ROCKET-based\nalgorithm, named Detach-Rocket Ensemble, that is specifically designed to\naddress these two problems in MTSC. Our algorithm leverages pruning to provide\nan integrated estimation of channel importance, and ensembles to achieve better\naccuracy and provide a label probability. Using a synthetic multivariate time\nseries classification dataset in which we control the amount of information\ncarried by each of the channels, we first show that our algorithm is able to\ncorrectly recover the channel importance for classification. Then, using two\nreal-world datasets, a MEG dataset and an EEG dataset, we show that\nDetach-Rocket Ensemble is able to provide both interpretable channel relevance\nand competitive classification accuracy, even when applied directly to the raw\nbrain data, without the need for feature engineering.\n","authors":["Adri Solana","Erik Fransn","Gonzalo Uribarri"],"pdf_url":"https://arxiv.org/pdf/2408.02760v1.pdf","comment":"To be published in European Conference on Machine Learning and Data\n  Mining 2024, 20 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2405.01531v2","updated":"2024-08-05T18:20:39Z","published":"2024-05-02T17:59:01Z","title":"Improving Intervention Efficacy via Concept Realignment in Concept\n  Bottleneck Models","summary":"  Concept Bottleneck Models (CBMs) ground image classification on\nhuman-understandable concepts to allow for interpretable model decisions.\nCrucially, the CBM design inherently allows for human interventions, in which\nexpert users are given the ability to modify potentially misaligned concept\nchoices to influence the decision behavior of the model in an interpretable\nfashion. However, existing approaches often require numerous human\ninterventions per image to achieve strong performances, posing practical\nchallenges in scenarios where obtaining human feedback is expensive. In this\npaper, we find that this is noticeably driven by an independent treatment of\nconcepts during intervention, wherein a change of one concept does not\ninfluence the use of other ones in the model's final decision. To address this\nissue, we introduce a trainable concept intervention realignment module, which\nleverages concept relations to realign concept assignments post-intervention.\nAcross standard, real-world benchmarks, we find that concept realignment can\nsignificantly improve intervention efficacy; significantly reducing the number\nof interventions needed to reach a target classification performance or concept\nprediction accuracy. In addition, it easily integrates into existing\nconcept-based architectures without requiring changes to the models themselves.\nThis reduced cost of human-model collaboration is crucial to enhancing the\nfeasibility of CBMs in resource-constrained environments. Our code is available\nat: https://github.com/ExplainableML/concept_realignment.\n","authors":["Nishad Singhi","Jae Myung Kim","Karsten Roth","Zeynep Akata"],"pdf_url":"https://arxiv.org/pdf/2405.01531v2.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02751v1","updated":"2024-08-05T18:11:23Z","published":"2024-08-05T18:11:23Z","title":"A Novel Hybrid Approach for Tornado Prediction in the United States:\n  Kalman-Convolutional BiLSTM with Multi-Head Attention","summary":"  Tornadoes are among the most intense atmospheric vortex phenomena and pose\nsignificant challenges for detection and forecasting. Conventional methods,\nwhich heavily depend on ground-based observations and radar data, are limited\nby issues such as decreased accuracy over greater distances and a high rate of\nfalse positives. To address these challenges, this study utilizes the Seamless\nHybrid Scan Reflectivity (SHSR) dataset from the Multi-Radar Multi-Sensor\n(MRMS) system, which integrates data from multiple radar sources to enhance\naccuracy. A novel hybrid model, the Kalman-Convolutional BiLSTM with Multi-Head\nAttention, is introduced to improve dynamic state estimation and capture both\nspatial and temporal dependencies within the data. This model demonstrates\nsuperior performance in precision, recall, F1-Score, and accuracy compared to\nmethods such as K-Nearest Neighbors (KNN) and LightGBM. The results highlight\nthe considerable potential of advanced machine learning techniques to improve\ntornado prediction and reduce false alarm rates. Future research will focus on\nexpanding datasets, exploring innovative model architectures, and incorporating\nlarge language models (LLMs) to provide deeper insights. This research\nintroduces a novel model for tornado prediction, offering a robust framework\nfor enhancing forecasting accuracy and public safety.\n","authors":["Jiawei Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02751v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16778v2","updated":"2024-08-05T18:08:49Z","published":"2024-02-26T17:49:37Z","title":"On the Growth of Mistakes in Differentially Private Online Learning: A\n  Lower Bound Perspective","summary":"  In this paper, we provide lower bounds for Differentially Private (DP) Online\nLearning algorithms. Our result shows that, for a broad class of\n$(\\varepsilon,\\delta)$-DP online algorithms, for number of rounds $T$ such that\n$\\log T\\leq O(1 / \\delta)$, the expected number of mistakes incurred by the\nalgorithm grows as $\\Omega(\\log \\frac{T}{\\delta})$. This matches the upper\nbound obtained by Golowich and Livni (2021) and is in contrast to non-private\nonline learning where the number of mistakes is independent of $T$. To the best\nof our knowledge, our work is the first result towards settling lower bounds\nfor DP-Online learning and partially addresses the open question in Sanyal and\nRamponi (2022).\n","authors":["Daniil Dmitriev","Kristf Szab","Amartya Sanyal"],"pdf_url":"https://arxiv.org/pdf/2402.16778v2.pdf","comment":"Accepted at the Conference on Learning Theory (COLT) 2024, Edmonton,\n  Canada"},{"id":"http://arxiv.org/abs/2406.11714v2","updated":"2024-08-05T18:02:55Z","published":"2024-06-17T16:32:57Z","title":"Scalable Expressiveness through Preprocessed Graph Perturbations","summary":"  Graph Neural Networks (GNNs) have emerged as the predominant method for\nanalyzing graph-structured data. However, canonical GNNs have limited\nexpressive power and generalization capability, thus triggering the development\nof more expressive yet computationally intensive methods. One such approach is\nto create a series of perturbed versions of input graphs and then repeatedly\nconduct multiple message-passing operations on all variations during training.\nDespite their expressive power, this approach does not scale well on larger\ngraphs. To address this scalability issue, we introduce Scalable Expressiveness\nthrough Preprocessed Graph Perturbation (SE2P). This model offers a flexible,\nconfigurable balance between scalability and generalizability with four\ndistinct configuration classes. At one extreme, the configuration prioritizes\nscalability through minimal learnable feature extraction and extensive\npreprocessing; at the other extreme, it enhances generalizability with more\nlearnable feature extractions, though this increases scalability costs. We\nconduct extensive experiments on real-world datasets to evaluate the\ngeneralizability and scalability of SE2P variants compared to various\nstate-of-the-art benchmarks. Our results indicate that, depending on the chosen\nSE2P configuration, the model can enhance generalizability compared to\nbenchmarks while achieving significant speed improvements of up to 8-fold.\n","authors":["Danial Saber","Amirali Salehi-Abari"],"pdf_url":"https://arxiv.org/pdf/2406.11714v2.pdf","comment":"14 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.02743v1","updated":"2024-08-05T18:01:07Z","published":"2024-08-05T18:01:07Z","title":"KAN we improve on HEP classification tasks? Kolmogorov-Arnold Networks\n  applied to an LHC physics example","summary":"  Recently, Kolmogorov-Arnold Networks (KANs) have been proposed as an\nalternative to multilayer perceptrons, suggesting advantages in performance and\ninterpretability. We study a typical binary event classification task in\nhigh-energy physics including high-level features and comment on the\nperformance and interpretability of KANs in this context. We find that the\nlearned activation functions of a one-layer KAN resemble the log-likelihood\nratio of the input features. In deeper KANs, the activations in the first KAN\nlayer differ from those in the one-layer KAN, which indicates that the deeper\nKANs learn more complex representations of the data. We study KANs with\ndifferent depths and widths and we compare them to multilayer perceptrons in\nterms of performance and number of trainable parameters. For the chosen\nclassification task, we do not find that KANs are more parameter efficient.\nHowever, small KANs may offer advantages in terms of interpretability that come\nat the cost of only a moderate loss in performance.\n","authors":["Johannes Erdmann","Florian Mausolf","Jan Lukas Sph"],"pdf_url":"https://arxiv.org/pdf/2408.02743v1.pdf","comment":"25 pages, 9 figures"}],"Multimedia":[{"id":"http://arxiv.org/abs/2407.16248v3","updated":"2024-08-05T09:05:59Z","published":"2024-07-23T07:36:54Z","title":"Spatiotemporal Graph Guided Multi-modal Network for Livestreaming\n  Product Retrieval","summary":"  With the rapid expansion of e-commerce, more consumers have become accustomed\nto making purchases via livestreaming. Accurately identifying the products\nbeing sold by salespeople, i.e., livestreaming product retrieval (LPR), poses a\nfundamental and daunting challenge. The LPR task encompasses three primary\ndilemmas in real-world scenarios: 1) the recognition of intended products from\ndistractor products present in the background; 2) the video-image heterogeneity\nthat the appearance of products showcased in live streams often deviates\nsubstantially from standardized product images in stores; 3) there are numerous\nconfusing products with subtle visual nuances in the shop. To tackle these\nchallenges, we propose the Spatiotemporal Graphing Multi-modal Network (SGMN).\nFirst, we employ a text-guided attention mechanism that leverages the spoken\ncontent of salespeople to guide the model to focus toward intended products,\nemphasizing their salience over cluttered background products. Second, a\nlong-range spatiotemporal graph network is further designed to achieve both\ninstance-level interaction and frame-level matching, solving the misalignment\ncaused by video-image heterogeneity. Third, we propose a multi-modal hard\nexample mining, assisting the model in distinguishing highly similar products\nwith fine-grained features across the video-image-text domain. Through\nextensive quantitative and qualitative experiments, we demonstrate the superior\nperformance of our proposed SGMN model, surpassing the state-of-the-art methods\nby a substantial margin. The code is available at\nhttps://github.com/Huxiaowan/SGMN.\n","authors":["Xiaowan Hu","Yiyi Chen","Yan Li","Minquan Wang","Haoqian Wang","Quan Chen","Han Li","Peng Jiang"],"pdf_url":"https://arxiv.org/pdf/2407.16248v3.pdf","comment":"16 pages, 12 figures"},{"id":"http://arxiv.org/abs/2408.02272v1","updated":"2024-08-05T07:00:10Z","published":"2024-08-05T07:00:10Z","title":"COM Kitchens: An Unedited Overhead-view Video Dataset as a\n  Vision-Language Benchmark","summary":"  Procedural video understanding is gaining attention in the vision and\nlanguage community. Deep learning-based video analysis requires extensive data.\nConsequently, existing works often use web videos as training resources, making\nit challenging to query instructional contents from raw video observations. To\naddress this issue, we propose a new dataset, COM Kitchens. The dataset\nconsists of unedited overhead-view videos captured by smartphones, in which\nparticipants performed food preparation based on given recipes. Fixed-viewpoint\nvideo datasets often lack environmental diversity due to high camera setup\ncosts. We used modern wide-angle smartphone lenses to cover cooking counters\nfrom sink to cooktop in an overhead view, capturing activity without in-person\nassistance. With this setup, we collected a diverse dataset by distributing\nsmartphones to participants. With this dataset, we propose the novel\nvideo-to-text retrieval task Online Recipe Retrieval (OnRR) and new video\ncaptioning domain Dense Video Captioning on unedited Overhead-View videos\n(DVC-OV). Our experiments verified the capabilities and limitations of current\nweb-video-based SOTA methods in handling these tasks.\n","authors":["Koki Maeda","Tosho Hirasawa","Atsushi Hashimoto","Jun Harashima","Leszek Rybicki","Yusuke Fukasawa","Yoshitaka Ushiku"],"pdf_url":"https://arxiv.org/pdf/2408.02272v1.pdf","comment":"ECCV2024 accepted"},{"id":"http://arxiv.org/abs/2408.01355v2","updated":"2024-08-05T02:14:54Z","published":"2024-08-02T16:07:15Z","title":"Hallu-PI: Evaluating Hallucination in Multi-modal Large Language Models\n  within Perturbed Inputs","summary":"  Multi-modal Large Language Models (MLLMs) have demonstrated remarkable\nperformance on various visual-language understanding and generation tasks.\nHowever, MLLMs occasionally generate content inconsistent with the given\nimages, which is known as \"hallucination\". Prior works primarily center on\nevaluating hallucination using standard, unperturbed benchmarks, which overlook\nthe prevalent occurrence of perturbed inputs in real-world scenarios-such as\nimage cropping or blurring-that are critical for a comprehensive assessment of\nMLLMs' hallucination. In this paper, to bridge this gap, we propose Hallu-PI,\nthe first benchmark designed to evaluate Hallucination in MLLMs within\nPerturbed Inputs. Specifically, Hallu-PI consists of seven perturbed scenarios,\ncontaining 1,260 perturbed images from 11 object types. Each image is\naccompanied by detailed annotations, which include fine-grained hallucination\ntypes, such as existence, attribute, and relation. We equip these annotations\nwith a rich set of questions, making Hallu-PI suitable for both discriminative\nand generative tasks. Extensive experiments on 12 mainstream MLLMs, such as\nGPT-4V and Gemini-Pro Vision, demonstrate that these models exhibit significant\nhallucinations on Hallu-PI, which is not observed in unperturbed scenarios.\nFurthermore, our research reveals a severe bias in MLLMs' ability to handle\ndifferent types of hallucinations. We also design two baselines specifically\nfor perturbed scenarios, namely Perturbed-Reminder and Perturbed-ICL. We hope\nthat our study will bring researchers' attention to the limitations of MLLMs\nwhen dealing with perturbed inputs, and spur further investigations to address\nthis issue. Our code and datasets are publicly available at\nhttps://github.com/NJUNLP/Hallu-PI.\n","authors":["Peng Ding","Jingyu Wu","Jun Kuang","Dan Ma","Xuezhi Cao","Xunliang Cai","Shi Chen","Jiajun Chen","Shujian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.01355v2.pdf","comment":"Acccepted by ACM MM 2024, 14 pages, 11 figures, 9 tables"}]},"2024-08-04T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2407.21772v2","updated":"2024-08-04T22:13:39Z","published":"2024-07-31T17:48:14Z","title":"ShieldGemma: Generative AI Content Moderation Based on Gemma","summary":"  We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.\n","authors":["Wenjun Zeng","Yuchi Liu","Ryan Mullins","Ludovic Peran","Joe Fernandez","Hamza Harkous","Karthik Narasimhan","Drew Proud","Piyush Kumar","Bhaktipriya Radharapu","Olivia Sturman","Oscar Wahltinez"],"pdf_url":"https://arxiv.org/pdf/2407.21772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02152v1","updated":"2024-08-04T22:00:34Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"  Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.\n","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2408.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02143v1","updated":"2024-08-04T20:56:05Z","published":"2024-08-04T20:56:05Z","title":"Analyzing Cultural Representations of Emotions in LLMs through Mixed\n  Emotion Survey","summary":"  Large Language Models (LLMs) have gained widespread global adoption,\nshowcasing advanced linguistic capabilities across multiple of languages. There\nis a growing interest in academia to use these models to simulate and study\nhuman behaviors. However, it is crucial to acknowledge that an LLM's\nproficiency in a specific language might not fully encapsulate the norms and\nvalues associated with its culture. Concerns have emerged regarding potential\nbiases towards Anglo-centric cultures and values due to the predominance of\nWestern and US-based training data. This study focuses on analyzing the\ncultural representations of emotions in LLMs, in the specific case of\nmixed-emotion situations. Our methodology is based on the studies of Miyamoto\net al. (2010), which identified distinctive emotional indicators in Japanese\nand American human responses. We first administer their mixed emotion survey to\nfive different LLMs and analyze their outputs. Second, we experiment with\ncontextual variables to explore variations in responses considering both\nlanguage and speaker origin. Thirdly, we expand our investigation to encompass\nadditional East Asian and Western European origin languages to gauge their\nalignment with their respective cultures, anticipating a closer fit. We find\nthat (1) models have limited alignment with the evidence in the literature; (2)\nwritten language has greater effect on LLMs' response than information on\nparticipants origin; and (3) LLMs responses were found more similar for East\nAsian languages than Western European languages.\n","authors":["Shiran Dudy","Ibrahim Said Ahmad","Ryoko Kitajima","Agata Lapedriza"],"pdf_url":"https://arxiv.org/pdf/2408.02143v1.pdf","comment":"Was accepted to ACII 2024"},{"id":"http://arxiv.org/abs/2408.02128v1","updated":"2024-08-04T19:54:12Z","published":"2024-08-04T19:54:12Z","title":"Table Transformers for Imputing Textual Attributes","summary":"  Missing data in tabular dataset is a common issue as the performance of\ndownstream tasks usually depends on the completeness of the training dataset.\nPrevious missing data imputation methods focus on numeric and categorical\ncolumns, but we propose a novel end-to-end approach called Table Transformers\nfor Imputing Textual Attributes (TTITA) based on the transformer to impute\nunstructured textual columns using other columns in the table. We conduct\nextensive experiments on two Amazon Reviews datasets, and our approach shows\ncompetitive performance outperforming baseline models such as recurrent neural\nnetworks and Llama2. The performance improvement is more significant when the\ntarget sequence has a longer length. Additionally, we incorporated multi-task\nlearning to simultaneously impute for heterogeneous columns, boosting the\nperformance for text imputation. We also qualitatively compare with ChatGPT for\nrealistic applications.\n","authors":["Ting-Ruen Wei","Yuan Wang","Yoshitaka Inoue","Hsin-Tai Wu","Yi Fang"],"pdf_url":"https://arxiv.org/pdf/2408.02128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02114v1","updated":"2024-08-04T18:57:21Z","published":"2024-08-04T18:57:21Z","title":"Recent Advances in Multi-Choice Machine Reading Comprehension: A Survey\n  on Methods and Datasets","summary":"  This paper provides a thorough examination of recent developments in the\nfield of multi-choice Machine Reading Comprehension (MRC). Focused on benchmark\ndatasets, methodologies, challenges, and future trajectories, our goal is to\noffer researchers a comprehensive overview of the current landscape in\nmulti-choice MRC. The analysis delves into 30 existing cloze-style and\nmultiple-choice MRC benchmark datasets, employing a refined classification\nmethod based on attributes such as corpus style, domain, complexity, context\nstyle, question style, and answer style. This classification system enhances\nour understanding of each dataset's diverse attributes and categorizes them\nbased on their complexity. Furthermore, the paper categorizes recent\nmethodologies into Fine-tuned and Prompt-tuned methods. Fine-tuned methods\ninvolve adapting pre-trained language models (PLMs) to a specific task through\nretraining on domain-specific datasets, while prompt-tuned methods use prompts\nto guide PLM response generation, presenting potential applications in\nzero-shot or few-shot learning scenarios. By contributing to ongoing\ndiscussions, inspiring future research directions, and fostering innovations,\nthis paper aims to propel multi-choice MRC towards new frontiers of\nachievement.\n","authors":["Shima Foolad","Kourosh Kiani","Razieh Rastgoo"],"pdf_url":"https://arxiv.org/pdf/2408.02114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02103v1","updated":"2024-08-04T18:08:15Z","published":"2024-08-04T18:08:15Z","title":"Effective Demonstration Annotation for In-Context Learning via Language\n  Model-Based Determinantal Point Process","summary":"  In-context learning (ICL) is a few-shot learning paradigm that involves\nlearning mappings through input-output pairs and appropriately applying them to\nnew instances. Despite the remarkable ICL capabilities demonstrated by Large\nLanguage Models (LLMs), existing works are highly dependent on large-scale\nlabeled support sets, not always feasible in practical scenarios. To refine\nthis approach, we focus primarily on an innovative selective annotation\nmechanism, which precedes the standard demonstration retrieval. We introduce\nthe Language Model-based Determinant Point Process (LM-DPP) that simultaneously\nconsiders the uncertainty and diversity of unlabeled instances for optimal\nselection. Consequently, this yields a subset for annotation that strikes a\ntrade-off between the two factors. We apply LM-DPP to various language models,\nincluding GPT-J, LlaMA, and GPT-3. Experimental results on 9 NLU and 2\nGeneration datasets demonstrate that LM-DPP can effectively select canonical\nexamples. Further analysis reveals that LLMs benefit most significantly from\nsubsets that are both low uncertainty and high diversity.\n","authors":["Peng Wang","Xiaobin Wang","Chao Lou","Shengyu Mao","Pengjun Xie","Yong Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.02103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.07209v6","updated":"2024-08-04T17:54:15Z","published":"2023-06-12T16:12:56Z","title":"Data-Copilot: Bridging Billions of Data and Humans with Autonomous\n  Workflow","summary":"  Industries such as finance, meteorology, and energy generate vast amounts of\ndata daily. Efficiently managing, processing, and displaying this data requires\nspecialized expertise and is often tedious and repetitive. Leveraging large\nlanguage models (LLMs) to develop an automated workflow presents a highly\npromising solution. However, LLMs are not adept at handling complex numerical\ncomputations and table manipulations and are also constrained by a limited\ncontext budget. Based on this, we propose Data-Copilot, a data analysis agent\nthat autonomously performs querying, processing, and visualization of massive\ndata tailored to diverse human requests. The advancements are twofold: First,\nit is a code-centric agent that receives human requests and generates code as\nan intermediary to handle massive data, which is quite flexible for large-scale\ndata processing tasks. Second, Data-Copilot involves a data exploration phase\nin advance, which explores how to design more universal and error-free\ninterfaces for real-time response. Specifically, it actively explores data\nsources, discovers numerous common requests, and abstracts them into many\nuniversal interfaces for daily invocation. When deployed in real-time requests,\nData-Copilot only needs to invoke these pre-designed interfaces, transforming\nraw data into visualized outputs (e.g., charts, tables) that best match the\nuser's intent. Compared to generating code from scratch, invoking these\npre-designed and compiler-validated interfaces can significantly reduce errors\nduring real-time requests. Additionally, interface workflows are more efficient\nand offer greater interpretability than code. We open-sourced Data-Copilot with\nmassive Chinese financial data, such as stocks, funds, and news, demonstrating\npromising application prospects.\n","authors":["Wenqi Zhang","Yongliang Shen","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2306.07209v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.14302v2","updated":"2024-08-04T17:48:33Z","published":"2023-12-21T21:22:41Z","title":"Exploiting Novel GPT-4 APIs","summary":"  Language model attacks typically assume one of two extreme threat models:\nfull white-box access to model weights, or black-box access limited to a text\ngeneration API. However, real-world APIs are often more flexible than just text\ngeneration: these APIs expose \"gray-box\" access leading to new threat vectors.\nTo explore this, we red-team three new functionalities exposed in the GPT-4\nAPIs: fine-tuning, function calling and knowledge retrieval. We find that\nfine-tuning a model on as few as 15 harmful examples or 100 benign examples can\nremove core safeguards from GPT-4, enabling a range of harmful outputs.\nFurthermore, we find that GPT-4 Assistants readily divulge the function call\nschema and can be made to execute arbitrary function calls. Finally, we find\nthat knowledge retrieval can be hijacked by injecting instructions into\nretrieval documents. These vulnerabilities highlight that any additions to the\nfunctionality exposed by an API can create new vulnerabilities.\n","authors":["Kellin Pelrine","Mohammad Taufeeque","Micha Zajc","Euan McLean","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2312.14302v2.pdf","comment":"10 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2408.02085v1","updated":"2024-08-04T16:50:07Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v1.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2310.01469v3","updated":"2024-08-04T16:26:14Z","published":"2023-10-02T17:01:56Z","title":"LLM Lies: Hallucinations are not Bugs, but Features as Adversarial\n  Examples","summary":"  Large Language Models (LLMs), including GPT-3.5, LLaMA, and PaLM, seem to be\nknowledgeable and able to adapt to many tasks. However, we still cannot\ncompletely trust their answers, since LLMs suffer from\n\\textbf{hallucination}\\textemdash fabricating non-existent facts, deceiving\nusers with or without their awareness. However, the reasons for their existence\nand pervasiveness remain unclear. In this paper, we demonstrate that\nnonsensical prompts composed of random tokens can also elicit the LLMs to\nrespond with hallucinations. Moreover, we provide both theoretical and\nexperimental evidence that transformers can be manipulated to produce specific\npre-define tokens by perturbing its input sequence. This phenomenon forces us\nto revisit that \\emph{hallucination may be another view of adversarial\nexamples}, and it shares similar characteristics with conventional adversarial\nexamples as a basic property of LLMs. Therefore, we formalize an automatic\nhallucination triggering method as the \\textit{hallucination attack} in an\nadversarial way. Finally, we explore the basic properties of attacked\nadversarial prompts and propose a simple yet effective defense strategy. Our\ncode is released on\nGitHub\\footnote{https://github.com/PKU-YuanGroup/Hallucination-Attack}.\n","authors":["Jia-Yu Yao","Kun-Peng Ning","Zhen-Hui Liu","Mu-Nan Ning","Yu-Yang Liu","Li Yuan"],"pdf_url":"https://arxiv.org/pdf/2310.01469v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07220v2","updated":"2024-08-04T15:32:37Z","published":"2024-03-22T17:13:46Z","title":"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy\n  with Semantic Search and Hybrid Query-Based Retrievers","summary":"  Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\nprivate knowledge base of documents with Large Language Models (LLM) to build\nGenerative Q\\&A (Question-Answering) systems. However, RAG accuracy becomes\nincreasingly challenging as the corpus of documents scales up, with Retrievers\nplaying an outsized role in the overall RAG accuracy by extracting the most\nrelevant document from the corpus to provide context to the LLM. In this paper,\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\nquery strategies. Our study achieves better retrieval results and sets new\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\ndemonstrate far superior results on Generative Q\\&A datasets like SQUAD, even\nsurpassing fine-tuning performance.\n","authors":["Kunal Sawarkar","Abhilasha Mangal","Shivam Raj Solanki"],"pdf_url":"https://arxiv.org/pdf/2404.07220v2.pdf","comment":"Paper accepted by MIPR and presented at The 7th IEEE International\n  Conference on Multimedia Information. Processing and Retrieval (IEEE-MIPR\n  2024)"},{"id":"http://arxiv.org/abs/2407.16252v2","updated":"2024-08-04T15:27:28Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Chinese Law Firm Co-run by LLM Agents","summary":"  Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v2.pdf","comment":"11 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02056v1","updated":"2024-08-04T15:07:44Z","published":"2024-08-04T15:07:44Z","title":"MedSyn: LLM-based Synthetic Medical Text Generation Framework","summary":"  Generating synthetic text addresses the challenge of data availability in\nprivacy-sensitive domains such as healthcare. This study explores the\napplicability of synthetic data in real-world medical settings. We introduce\nMedSyn, a novel medical text generation framework that integrates large\nlanguage models with a Medical Knowledge Graph (MKG). We use MKG to sample\nprior medical information for the prompt and generate synthetic clinical notes\nwith GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data\nthrough application in the ICD code prediction task. Our research indicates\nthat synthetic data can increase the classification accuracy of vital and\nchallenging codes by up to 17.8% compared to settings without synthetic data.\nFurthermore, to provide new data for further research in the healthcare domain,\nwe present the largest open-source synthetic dataset of clinical notes for the\nRussian language, comprising over 41k samples covering 219 ICD-10 codes.\n","authors":["Gleb Kumichev","Pavel Blinov","Yulia Kuzkina","Vasily Goncharov","Galina Zubkova","Nikolai Zenovkin","Aleksei Goncharov","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2408.02056v1.pdf","comment":"16 pages, accepted to ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2407.16110v2","updated":"2024-08-04T14:41:42Z","published":"2024-07-23T00:52:12Z","title":"Analyzing the Polysemy Evolution using Semantic Cells","summary":"  The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.\n","authors":["Yukio Ohsawa","Dingming Xue","Kaira Sekiguchi"],"pdf_url":"https://arxiv.org/pdf/2407.16110v2.pdf","comment":"11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.14749"},{"id":"http://arxiv.org/abs/2408.02044v1","updated":"2024-08-04T14:35:30Z","published":"2024-08-04T14:35:30Z","title":"Fine-tuning multilingual language models in Twitter/X sentiment\n  analysis: a study on Eastern-European V4 languages","summary":"  The aspect-based sentiment analysis (ABSA) is a standard NLP task with\nnumerous approaches and benchmarks, where large language models (LLM) represent\nthe current state-of-the-art. We focus on ABSA subtasks based on Twitter/X data\nin underrepresented languages. On such narrow tasks, small tuned language\nmodels can often outperform universal large ones, providing available and cheap\nsolutions.\n  We fine-tune several LLMs (BERT, BERTweet, Llama2, Llama3, Mistral) for\nclassification of sentiment towards Russia and Ukraine in the context of the\nongoing military conflict. The training/testing dataset was obtained from the\nacademic API from Twitter/X during 2023, narrowed to the languages of the V4\ncountries (Czech Republic, Slovakia, Poland, Hungary). Then we measure their\nperformance under a variety of settings including translations, sentiment\ntargets, in-context learning and more, using GPT4 as a reference model. We\ndocument several interesting phenomena demonstrating, among others, that some\nmodels are much better fine-tunable on multilingual Twitter tasks than others,\nand that they can reach the SOTA level with a very small training set. Finally\nwe identify combinations of settings providing the best results.\n","authors":["Tom Filip","Martin Pavlek","Petr Sosk"],"pdf_url":"https://arxiv.org/pdf/2408.02044v1.pdf","comment":"18 pages, 4 figures"},{"id":"http://arxiv.org/abs/2309.08112v2","updated":"2024-08-04T14:15:21Z","published":"2023-09-15T02:42:03Z","title":"Empowering Private Tutoring by Chaining Large Language Models","summary":"  Artificial intelligence has been applied in various aspects of online\neducation to facilitate teaching and learning. However, few approaches has been\nmade toward a complete AI-powered tutoring system. In this work, we explore the\ndevelopment of a full-fledged intelligent tutoring system powered by\nstate-of-the-art large language models (LLMs), covering automatic course\nplanning and adjusting, tailored instruction, and flexible quiz evaluation. To\nmake the system robust to prolonged interaction and cater to individualized\neducation, the system is decomposed into three inter-connected core\nprocesses-interaction, reflection, and reaction. Each process is implemented by\nchaining LLM-powered tools along with dynamically updated memory modules. Tools\nare LLMs prompted to execute one specific task at a time, while memories are\ndata storage that gets updated during education process. Statistical results\nfrom learning logs demonstrate the effectiveness and mechanism of each tool\nusage. Subjective feedback from human users reveal the usability of each\nfunction, and comparison with ablation systems further testify the benefits of\nthe designed processes in long-term interaction.\n","authors":["Yulin Chen","Ning Ding","Hai-Tao Zheng","Zhiyuan Liu","Maosong Sun","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2309.08112v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05064v2","updated":"2024-08-04T13:57:04Z","published":"2023-10-08T08:14:03Z","title":"sign.mt: Real-Time Multilingual Sign Language Translation Application","summary":"  This demo paper presents sign.mt, an open-source application pioneering\nreal-time multilingual bi-directional translation between spoken and signed\nlanguages. Harnessing state-of-the-art open-source models, this tool aims to\naddress the communication divide between the hearing and the deaf, facilitating\nseamless translation in both spoken-to-signed and signed-to-spoken translation\ndirections.\n  Promising reliable and unrestricted communication, sign.mt offers offline\nfunctionality, crucial in areas with limited internet connectivity. It further\nenhances user engagement by offering customizable photo-realistic sign language\navatars, thereby encouraging a more personalized and authentic user experience.\n  Licensed under CC BY-NC-SA 4.0, sign.mt signifies an important stride towards\nopen, inclusive communication. The app can be used, and modified for personal\nand academic uses, and even supports a translation API, fostering integration\ninto a wider range of applications. However, it is by no means a finished\nproduct.\n  We invite the NLP community to contribute towards the evolution of sign.mt.\nWhether it be the integration of more refined models, the development of\ninnovative pipelines, or user experience improvements, your contributions can\npropel this project to new heights. Available at https://sign.mt, it stands as\na testament to what we can achieve together, as we strive to make communication\naccessible to all.\n","authors":["Amit Moryossef"],"pdf_url":"https://arxiv.org/pdf/2310.05064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01249v2","updated":"2024-08-04T12:34:29Z","published":"2023-09-03T19:24:34Z","title":"Large AI Model Empowered Multimodal Semantic Communications","summary":"  Multimodal signals, including text, audio, image, and video, can be\nintegrated into Semantic Communication (SC) systems to provide an immersive\nexperience with low latency and high quality at the semantic level. However,\nthe multimodal SC has several challenges, including data heterogeneity,\nsemantic ambiguity, and signal distortion during transmission. Recent\nadvancements in large AI models, particularly in the Multimodal Language Model\n(MLM) and Large Language Model (LLM), offer potential solutions for addressing\nthese issues. To this end, we propose a Large AI Model-based Multimodal SC\n(LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment\n(MMA) that utilizes the MLM to enable the transformation between multimodal and\nunimodal data while preserving semantic consistency. Then, a personalized\nLLM-based Knowledge Base (LKB) is proposed, which allows users to perform\npersonalized semantic extraction or recovery through the LLM. This effectively\naddresses the semantic ambiguity. Finally, we apply the Conditional Generative\nadversarial network-based channel Estimation (CGE) for estimating the wireless\nchannel state information. This approach effectively mitigates the impact of\nfading channels in SC. Finally, we conduct simulations that demonstrate the\nsuperior performance of the LAM-MSC framework.\n","authors":["Feibo Jiang","Li Dong","Yubo Peng","Kezhi Wang","Kun Yang","Cunhua Pan","Xiaohu You"],"pdf_url":"https://arxiv.org/pdf/2309.01249v2.pdf","comment":"Accepted by IEEE CM"},{"id":"http://arxiv.org/abs/2408.02006v1","updated":"2024-08-04T12:10:51Z","published":"2024-08-04T12:10:51Z","title":"LLaSA: Large Language and E-Commerce Shopping Assistant","summary":"  The e-commerce platform has evolved rapidly due to its widespread popularity\nand convenience. Developing an e-commerce shopping assistant for customers is\ncrucial to aiding them in quickly finding desired products and recommending\nprecisely what they need. However, most previous shopping assistants face two\nmain problems: (1) task-specificity, which necessitates the development of\ndifferent models for various tasks, thereby increasing development costs and\nlimiting effectiveness; and (2) poor generalization, where the trained model\nperforms inadequately on up-to-date products. To resolve these issues, we\nemploy Large Language Models (LLMs) to construct an omnipotent assistant,\nleveraging their adeptness at handling multiple tasks and their superior\ngeneralization capability. Nonetheless, LLMs lack inherent knowledge of\ne-commerce concepts. To address this, we create an instruction dataset\ncomprising 65,000 samples and diverse tasks, termed as EshopInstruct. Through\ninstruction tuning on our dataset, the assistant, named LLaSA, demonstrates the\npotential to function as an omnipotent assistant. Additionally, we propose\nvarious inference optimization strategies to enhance performance with limited\ninference resources. In the Amazon KDD Cup 2024 Challenge, our proposed method,\nLLaSA, achieved an overall ranking of 3rd place on ShopBench, including 57\ntasks and approximately 20,000 questions, and we secured top-5 rankings in each\ntrack, especially in track4, where we achieved the best performance result\namong all student teams. Our extensive practices fully demonstrate that LLMs\npossess the great potential to be competent e-commerce shopping assistants.\n","authors":["Shuo Zhang","Boci Peng","Xinping Zhao","Boren Hu","Yun Zhu","Yanjia Zeng","Xuming Hu"],"pdf_url":"https://arxiv.org/pdf/2408.02006v1.pdf","comment":"Accepted by KDD 2024 Workshop (Oral)"},{"id":"http://arxiv.org/abs/2408.01969v1","updated":"2024-08-04T09:09:13Z","published":"2024-08-04T09:09:13Z","title":"Optimal and efficient text counterfactuals using Graph Neural Networks","summary":"  As NLP models become increasingly integral to decision-making processes, the\nneed for explainability and interpretability has become paramount. In this\nwork, we propose a framework that achieves the aforementioned by generating\nsemantically edited inputs, known as counterfactual interventions, which change\nthe model prediction, thus providing a form of counterfactual explanations for\nthe model. We test our framework on two NLP tasks - binary sentiment\nclassification and topic classification - and show that the generated edits are\ncontrastive, fluent and minimal, while the whole process remains significantly\nfaster that other state-of-the-art counterfactual editors.\n","authors":["Dimitris Lymperopoulos","Maria Lymperaiou","Giorgos Filandrianos","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2408.01969v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01966v1","updated":"2024-08-04T09:04:44Z","published":"2024-08-04T09:04:44Z","title":"ML-EAT: A Multilevel Embedding Association Test for Interpretable and\n  Transparent Social Science","summary":"  This research introduces the Multilevel Embedding Association Test (ML-EAT),\na method designed for interpretable and transparent measurement of intrinsic\nbias in language technologies. The ML-EAT addresses issues of ambiguity and\ndifficulty in interpreting the traditional EAT measurement by quantifying bias\nat three levels of increasing granularity: the differential association between\ntwo target concepts with two attribute concepts; the individual effect size of\neach target concept with two attribute concepts; and the association between\neach individual target concept and each individual attribute concept. Using the\nML-EAT, this research defines a taxonomy of EAT patterns describing the nine\npossible outcomes of an embedding association test, each of which is associated\nwith a unique EAT-Map, a novel four-quadrant visualization for interpreting the\nML-EAT. Empirical analysis of static and diachronic word embeddings, GPT-2\nlanguage models, and a CLIP language-and-image model shows that EAT patterns\nadd otherwise unobservable information about the component biases that make up\nan EAT; reveal the effects of prompting in zero-shot models; and can also\nidentify situations when cosine similarity is an ineffective metric, rendering\nan EAT unreliable. Our work contributes a method for rendering bias more\nobservable and interpretable, improving the transparency of computational\ninvestigations into human minds and societies.\n","authors":["Robert Wolfe","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01966v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2402.12193v3","updated":"2024-08-04T08:56:33Z","published":"2024-02-19T14:56:18Z","title":"A Chinese Dataset for Evaluating the Safeguards in Large Language Models","summary":"  Many studies have demonstrated that large language models (LLMs) can produce\nharmful responses, exposing users to unexpected risks when LLMs are deployed.\nPrevious studies have proposed comprehensive taxonomies of the risks posed by\nLLMs, as well as corresponding prompts that can be used to examine the safety\nmechanisms of LLMs. However, the focus has been almost exclusively on English,\nand little has been explored for other languages. Here we aim to bridge this\ngap. We first introduce a dataset for the safety evaluation of Chinese LLMs,\nand then extend it to two other scenarios that can be used to better identify\nfalse negative and false positive examples in terms of risky prompt rejections.\nWe further present a set of fine-grained safety assessment criteria for each\nrisk type, facilitating both manual annotation and automatic evaluation in\nterms of LLM response harmfulness. Our experiments on five LLMs show that\nregion-specific risks are the prevalent type of risk, presenting the major\nissue with all Chinese LLMs we experimented with. Our data is available at\nhttps://github.com/Libr-AI/do-not-answer. Warning: this paper contains example\ndata that may be offensive, harmful, or biased.\n","authors":["Yuxia Wang","Zenan Zhai","Haonan Li","Xudong Han","Lizhi Lin","Zhenxuan Zhang","Jingru Zhao","Preslav Nakov","Timothy Baldwin"],"pdf_url":"https://arxiv.org/pdf/2402.12193v3.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2306.04802v4","updated":"2024-08-04T08:53:23Z","published":"2023-06-07T21:51:56Z","title":"A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises","summary":"  Healthcare knowledge graphs (HKGs) are valuable tools for organizing\nbiomedical concepts and their relationships with interpretable structures. The\nrecent advent of large language models (LLMs) has paved the way for building\nmore comprehensive and accurate HKGs. This, in turn, can improve the\nreliability of generated content and enable better evaluation of LLMs. However,\nthe challenges of HKGs such as regarding data heterogeneity and limited\ncoverage are not fully understood, highlighting the need for detailed reviews.\nThis work provides the first comprehensive review of HKGs. It summarizes the\npipeline and key techniques for HKG construction, as well as the common\nutilization approaches, i.e., model-free and model-based. The existing HKG\nresources are also organized based on the data types they capture and\napplication domains they cover, along with relevant statistical information\n(Resource available at\nhttps://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the\napplication level, we delve into the successful integration of HKGs across\nvarious health domains, ranging from fine-grained basic science research to\nhigh-level clinical decision support and public health. Lastly, the paper\nhighlights the opportunities for HKGs in the era of LLMs. This work aims to\nserve as a valuable resource for understanding the potential and opportunities\nof HKG in health research.\n","authors":["Carl Yang","Hejie Cui","Jiaying Lu","Shiyu Wang","Ran Xu","Wenjing Ma","Yue Yu","Shaojun Yu","Xuan Kan","Chen Ling","Tianfan Fu","Liang Zhao","Joyce Ho","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2306.04802v4.pdf","comment":"21 pages, preprint submitted to ACM"},{"id":"http://arxiv.org/abs/2408.01963v1","updated":"2024-08-04T08:43:09Z","published":"2024-08-04T08:43:09Z","title":"A Novel Metric for Measuring the Robustness of Large Language Models in\n  Non-adversarial Scenarios","summary":"  We evaluate the robustness of several large language models on multiple\ndatasets. Robustness here refers to the relative insensitivity of the model's\nanswers to meaning-preserving variants of their input. Benchmark datasets are\nconstructed by introducing naturally-occurring, non-malicious perturbations, or\nby generating semantically equivalent paraphrases of input questions or\nstatements. We further propose a novel metric for assessing a model robustness,\nand demonstrate its benefits in the non-adversarial scenario by empirical\nevaluation of several models on the created datasets.\n","authors":["Samuel Ackerman","Ella Rabinovich","Eitan Farchi","Ateret Anaby-Tavor"],"pdf_url":"https://arxiv.org/pdf/2408.01963v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01962v1","updated":"2024-08-04T08:41:48Z","published":"2024-08-04T08:41:48Z","title":"The Implications of Open Generative Models in Human-Centered Data\n  Science Work: A Case Study with Fact-Checking Organizations","summary":"  Calls to use open generative language models in academic research have\nhighlighted the need for reproducibility and transparency in scientific\nresearch. However, the impact of generative AI extends well beyond academia, as\ncorporations and public interest organizations have begun integrating these\nmodels into their data science pipelines. We expand this lens to include the\nimpact of open models on organizations, focusing specifically on fact-checking\norganizations, which use AI to observe and analyze large volumes of circulating\nmisinformation, yet must also ensure the reproducibility and impartiality of\ntheir work. We wanted to understand where fact-checking organizations use open\nmodels in their data science pipelines; what motivates their use of open models\nor proprietary models; and how their use of open or proprietary models can\ninform research on the societal impact of generative AI. To answer these\nquestions, we conducted an interview study with N=24 professionals at 20\nfact-checking organizations on six continents. Based on these interviews, we\noffer a five-component conceptual model of where fact-checking organizations\nemploy generative AI to support or automate parts of their data science\npipeline, including Data Ingestion, Data Analysis, Data Retrieval, Data\nDelivery, and Data Sharing. We then provide taxonomies of fact-checking\norganizations' motivations for using open models and the limitations that\nprevent them for further adopting open models, finding that they prefer open\nmodels for Organizational Autonomy, Data Privacy and Ownership, Application\nSpecificity, and Capability Transparency. However, they nonetheless use\nproprietary models due to perceived advantages in Performance, Usability, and\nSafety, as well as Opportunity Costs related to participation in emerging\ngenerative AI ecosystems. Our work provides novel perspective on open models in\ndata-driven organizations.\n","authors":["Robert Wolfe","Tanushree Mitra"],"pdf_url":"https://arxiv.org/pdf/2408.01962v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2402.16364v2","updated":"2024-08-04T08:36:08Z","published":"2024-02-26T07:33:28Z","title":"Where Do We Go from Here? Multi-scale Allocentric Relational Inference\n  from Natural Spatial Descriptions","summary":"  When communicating routes in natural language, the concept of acquired\nspatial knowledge is crucial for geographic information retrieval (GIR) and in\nspatial cognitive research. However, NLP navigation studies often overlook the\nimpact of such acquired knowledge on textual descriptions. Current navigation\nstudies concentrate on egocentric local descriptions (e.g., `it will be on your\nright') that require reasoning over the agent's local perception. These\ninstructions are typically given as a sequence of steps, with each action-step\nexplicitly mentioning and being followed by a landmark that the agent can use\nto verify they are on the right path (e.g., `turn right and then you will\nsee...'). In contrast, descriptions based on knowledge acquired through a map\nprovide a complete view of the environment and capture its overall structure.\nThese instructions (e.g., `it is south of Central Park and a block north of a\npolice station') are typically non-sequential, contain allocentric relations,\nwith multiple spatial relations and implicit actions, without any explicit\nverification. This paper introduces the Rendezvous (RVS) task and dataset,\nwhich includes 10,404 examples of English geospatial instructions for reaching\na target location using map-knowledge. Our analysis reveals that RVS exhibits a\nricher use of spatial allocentric relations, and requires resolving more\nspatial relations simultaneously compared to previous text-based navigation\nbenchmarks.\n","authors":["Tzuf Paz-Argaman","Sayali Kulkarni","John Palowitch","Jason Baldridge","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2402.16364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01961v1","updated":"2024-08-04T08:35:02Z","published":"2024-08-04T08:35:02Z","title":"Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study","summary":"  Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.\n","authors":["Robert Wolfe","Aayushi Dangol","Bill Howe","Alexis Hiniker"],"pdf_url":"https://arxiv.org/pdf/2408.01961v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01959v1","updated":"2024-08-04T08:26:58Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01950v1","updated":"2024-08-04T07:38:38Z","published":"2024-08-04T07:38:38Z","title":"Why Perturbing Symbolic Music is Necessary: Fitting the Distribution of\n  Never-used Notes through a Joint Probabilistic Diffusion Model","summary":"  Existing music generation models are mostly language-based, neglecting the\nfrequency continuity property of notes, resulting in inadequate fitting of rare\nor never-used notes and thus reducing the diversity of generated samples. We\nargue that the distribution of notes can be modeled by translational invariance\nand periodicity, especially using diffusion models to generalize notes by\ninjecting frequency-domain Gaussian noise. However, due to the low-density\nnature of music symbols, estimating the distribution of notes latent in the\nhigh-density solution space poses significant challenges. To address this\nproblem, we introduce the Music-Diff architecture, which fits a joint\ndistribution of notes and accompanying semantic information to generate\nsymbolic music conditionally. We first enhance the fragmentation module for\nextracting semantics by using event-based notations and the structural\nsimilarity index, thereby preventing boundary blurring. As a prerequisite for\nmultivariate perturbation, we introduce a joint pre-training method to\nconstruct the progressions between notes and musical semantics while avoiding\ndirect modeling of low-density notes. Finally, we recover the perturbed notes\nby a multi-branch denoiser that fits multiple noise objectives via Pareto\noptimization. Our experiments suggest that in contrast to language models,\njoint probability diffusion models perturbing at both note and semantic levels\ncan provide more sample diversity and compositional regularity. The case study\nhighlights the rhythmic advantages of our model over language- and DDPMs-based\nmodels by analyzing the hierarchical structure expressed in the self-similarity\nmetrics.\n","authors":["Shipei Liu","Xiaoya Fan","Guowei Wu"],"pdf_url":"https://arxiv.org/pdf/2408.01950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21530v2","updated":"2024-08-04T05:53:25Z","published":"2024-07-31T11:26:57Z","title":"Data Contamination Report from the 2024 CONDA Shared Task","summary":"  The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.\n","authors":["Oscar Sainz","Iker Garca-Ferrero","Alon Jacovi","Jon Ander Campos","Yanai Elazar","Eneko Agirre","Yoav Goldberg","Wei-Lin Chen","Jenny Chim","Leshem Choshen","Luca D'Amico-Wong","Melissa Dell","Run-Ze Fan","Shahriar Golchin","Yucheng Li","Pengfei Liu","Bhavish Pahwa","Ameya Prabhu","Suryansh Sharma","Emily Silcock","Kateryna Solonko","David Stap","Mihai Surdeanu","Yu-Min Tseng","Vishaal Udandarao","Zengzhi Wang","Ruijie Xu","Jinglin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.21530v2.pdf","comment":"https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database"},{"id":"http://arxiv.org/abs/2304.03531v4","updated":"2024-08-04T05:45:05Z","published":"2023-04-07T08:09:50Z","title":"From Retrieval to Generation: Efficient and Effective Entity Set\n  Expansion","summary":"  Entity Set Expansion (ESE) is a critical task aiming at expanding entities of\nthe target semantic class described by seed entities. Most existing ESE methods\nare retrieval-based frameworks that need to extract contextual features of\nentities and calculate the similarity between seed entities and candidate\nentities. To achieve the two purposes, they iteratively traverse the corpus and\nthe entity vocabulary, resulting in poor efficiency and scalability.\nExperimental results indicate that the time consumed by the retrieval-based ESE\nmethods increases linearly with entity vocabulary and corpus size. In this\npaper, we firstly propose Generative Entity Set Expansion (GenExpan) framework,\nwhich utilizes a generative pre-trained auto-regressive language model to\naccomplish ESE task. Specifically, a prefix tree is employed to guarantee the\nvalidity of entity generation, and automatically generated class names are\nadopted to guide the model to generate target entities. Moreover, we propose\nKnowledge Calibration and Generative Ranking to further bridge the gap between\ngeneric knowledge of the language model and the goal of ESE task. For\nefficiency, expansion time consumed by GenExpan is independent of entity\nvocabulary and corpus size, and GenExpan achieves an average 600% speedup\ncompared to strong baselines. For expansion effectiveness, our framework\noutperforms previous state-of-the-art ESE methods.\n","authors":["Shulin Huang","Shirong Ma","Yangning Li","Yinghui Li","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2304.03531v4.pdf","comment":"Accepted by CIKM 2024 (FULL paper)"},{"id":"http://arxiv.org/abs/2406.16464v3","updated":"2024-08-04T05:42:58Z","published":"2024-06-24T09:13:42Z","title":"InterCLIP-MEP: Interactive CLIP and Memory-Enhanced Predictor for\n  Multi-modal Sarcasm Detection","summary":"  The prevalence of sarcasm in social media, conveyed through text-image\ncombinations, presents significant challenges for sentiment analysis and\nintention mining. Existing multi-modal sarcasm detection methods have been\nproven to overestimate performance, as they struggle to effectively capture the\nintricate sarcastic cues that arise from the interaction between an image and\ntext. To address these issues, we propose InterCLIP-MEP, a novel framework for\nmulti-modal sarcasm detection. Specifically, we introduce an Interactive CLIP\n(InterCLIP) as the backbone to extract text-image representations, enhancing\nthem by embedding cross-modality information directly within each encoder,\nthereby improving the representations to capture text-image interactions\nbetter. Furthermore, an efficient training strategy is designed to adapt\nInterCLIP for our proposed Memory-Enhanced Predictor (MEP). MEP uses a dynamic,\nfixed-length dual-channel memory to store historical knowledge of valuable test\nsamples during inference. It then leverages this memory as a non-parametric\nclassifier to derive the final prediction, offering a more robust recognition\nof multi-modal sarcasm. Experiments demonstrate that InterCLIP-MEP achieves\nstate-of-the-art performance on the MMSD2.0 benchmark, with an accuracy\nimprovement of 1.08% and an F1 score improvement of 1.51% over the previous\nbest method. Code and data are available at\nhttps://github.com/CoderChen01/InterCLIP-MEP.\n","authors":["Junjie Chen","Hang Yu","Weidong Liu","Subin Huang","Sanmin Liu"],"pdf_url":"https://arxiv.org/pdf/2406.16464v3.pdf","comment":"9 pages, 6 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.01935v1","updated":"2024-08-04T05:24:32Z","published":"2024-08-04T05:24:32Z","title":"Defining and Evaluating Decision and Composite Risk in Language Models\n  Applied to Natural Language Inference","summary":"  Despite their impressive performance, large language models (LLMs) such as\nChatGPT are known to pose important risks. One such set of risks arises from\nmisplaced confidence, whether over-confidence or under-confidence, that the\nmodels have in their inference. While the former is well studied, the latter is\nnot, leading to an asymmetry in understanding the comprehensive risk of the\nmodel based on misplaced confidence. In this paper, we address this asymmetry\nby defining two types of risk (decision and composite risk), and proposing an\nexperimental framework consisting of a two-level inference architecture and\nappropriate metrics for measuring such risks in both discriminative and\ngenerative LLMs. The first level relies on a decision rule that determines\nwhether the underlying language model should abstain from inference. The second\nlevel (which applies if the model does not abstain) is the model's inference.\nDetailed experiments on four natural language commonsense reasoning datasets\nusing both an open-source ensemble-based RoBERTa model and ChatGPT, demonstrate\nthe practical utility of the evaluation framework. For example, our results\nshow that our framework can get an LLM to confidently respond to an extra 20.1%\nof low-risk inference tasks that other methods might misclassify as high-risk,\nand skip 19.8% of high-risk tasks, which would have been answered incorrectly.\n","authors":["Ke Shen","Mayank Kejriwal"],"pdf_url":"https://arxiv.org/pdf/2408.01935v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2310.03283"},{"id":"http://arxiv.org/abs/2408.01933v1","updated":"2024-08-04T05:15:02Z","published":"2024-08-04T05:15:02Z","title":"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models","summary":"  Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 521 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.\n","authors":["Bowen Wang","Jiuyang Chang","Yiming Qian","Guoxin Chen","Junhao Chen","Zhouqiang Jiang","Jiahao Zhang","Yuta Nakashima","Hajime Nagahara"],"pdf_url":"https://arxiv.org/pdf/2408.01933v1.pdf","comment":"9 pages,6 figures"},{"id":"http://arxiv.org/abs/2403.02691v3","updated":"2024-08-04T04:52:35Z","published":"2024-03-05T06:21:45Z","title":"InjecAgent: Benchmarking Indirect Prompt Injections in Tool-Integrated\n  Large Language Model Agents","summary":"  Recent work has embodied LLMs as agents, allowing them to access tools,\nperform actions, and interact with external content (e.g., emails or websites).\nHowever, external content introduces the risk of indirect prompt injection\n(IPI) attacks, where malicious instructions are embedded within the content\nprocessed by LLMs, aiming to manipulate these agents into executing detrimental\nactions against users. Given the potentially severe consequences of such\nattacks, establishing benchmarks to assess and mitigate these risks is\nimperative.\n  In this work, we introduce InjecAgent, a benchmark designed to assess the\nvulnerability of tool-integrated LLM agents to IPI attacks. InjecAgent\ncomprises 1,054 test cases covering 17 different user tools and 62 attacker\ntools. We categorize attack intentions into two primary types: direct harm to\nusers and exfiltration of private data. We evaluate 30 different LLM agents and\nshow that agents are vulnerable to IPI attacks, with ReAct-prompted GPT-4\nvulnerable to attacks 24% of the time. Further investigation into an enhanced\nsetting, where the attacker instructions are reinforced with a hacking prompt,\nshows additional increases in success rates, nearly doubling the attack success\nrate on the ReAct-prompted GPT-4. Our findings raise questions about the\nwidespread deployment of LLM Agents. Our benchmark is available at\nhttps://github.com/uiuc-kang-lab/InjecAgent.\n","authors":["Qiusi Zhan","Zhixiang Liang","Zifan Ying","Daniel Kang"],"pdf_url":"https://arxiv.org/pdf/2403.02691v3.pdf","comment":"36 pages, 6 figures, 13 tables (ACL 2024 Findings)"},{"id":"http://arxiv.org/abs/2408.01928v1","updated":"2024-08-04T04:52:21Z","published":"2024-08-04T04:52:21Z","title":"A Semi-supervised Multi-channel Graph Convolutional Network for Query\n  Classification in E-commerce","summary":"  Query intent classification is an essential module for customers to find\ndesired products on the e-commerce application quickly. Most existing query\nintent classification methods rely on the users' click behavior as a supervised\nsignal to construct training samples. However, these methods based entirely on\nposterior labels may lead to serious category imbalance problems because of the\nMatthew effect in click samples. Compared with popular categories, it is\ndifficult for products under long-tail categories to obtain traffic and user\nclicks, which makes the models unable to detect users' intent for products\nunder long-tail categories. This in turn aggravates the problem that long-tail\ncategories cannot obtain traffic, forming a vicious circle. In addition, due to\nthe randomness of the user's click, the posterior label is unstable for the\nquery with similar semantics, which makes the model very sensitive to the\ninput, leading to an unstable and incomplete recall of categories.\n  In this paper, we propose a novel Semi-supervised Multi-channel Graph\nConvolutional Network (SMGCN) to address the above problems from the\nperspective of label association and semi-supervised learning. SMGCN extends\ncategory information and enhances the posterior label by utilizing the\nsimilarity score between the query and categories. Furthermore, it leverages\nthe co-occurrence and semantic similarity graph of categories to strengthen the\nrelations among labels and weaken the influence of posterior label instability.\nWe conduct extensive offline and online A/B experiments, and the experimental\nresults show that SMGCN significantly outperforms the strong baselines, which\nshows its effectiveness and practicality.\n","authors":["Chunyuan Yuan","Ming Pang","Zheng Fang","Xue Jiang","Changping Peng","Zhangang Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01928v1.pdf","comment":"Accepted by WWW2024"},{"id":"http://arxiv.org/abs/2311.08640v4","updated":"2024-08-04T03:13:07Z","published":"2023-11-15T01:28:28Z","title":"Multistage Collaborative Knowledge Distillation from a Large Language\n  Model for Semi-Supervised Sequence Generation","summary":"  We study semi-supervised sequence generation tasks, where the few labeled\nexamples are too scarce to finetune a model, and meanwhile, few-shot prompted\nlarge language models (LLMs) exhibit room for improvement. In this paper, we\npresent the discovery that a student model distilled from a few-shot prompted\nLLM can commonly generalize better than its teacher to unseen examples on such\ntasks. We find that the student is able to learn a general pattern from the\nhigh-quality pseudolabels produced by the teacher during knowledge distillation\n(KD), and favorably not a general pattern from the low-quality pseudolables.\nLeveraging this discovery, we propose a new method, Multistage Collaborative\nKnowledge Distillation from an LLM (MCKD), for these tasks. MCKD first few-shot\nprompts an LLM to produce pseudolabels for unlabeled data. Then at each stage\nof an iterative KD process, a new pair of students is trained on disjoint\npartitions of the pseudolabeled data, and produces new and improved\npseudolabels for their unseen partitions. We conduct extensive experiments on\nfour syntactic and semantic parsing datasets and show the effectiveness of MCKD\nfor low-resource semi-supervised sequence generation. On CRAFT biomedical\nparsing, for example, 3-stage MCKD with 50 labeled examples outperforms an LLM\nteacher and vanilla KD by 7.5% and 3.7% parsing F1, respectively, and matches\nthe performance of supervised finetuning with 500 labeled examples.\n","authors":["Jiachen Zhao","Wenlong Zhao","Andrew Drozdov","Benjamin Rozonoyer","Md Arafat Sultan","Jay-Yoon Lee","Mohit Iyyer","Andrew McCallum"],"pdf_url":"https://arxiv.org/pdf/2311.08640v4.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2404.11912v3","updated":"2024-08-04T00:58:04Z","published":"2024-04-18T05:25:54Z","title":"TriForce: Lossless Acceleration of Long Sequence Generation with\n  Hierarchical Speculative Decoding","summary":"  With large language models (LLMs) widely deployed in long content generation\nrecently, there has emerged an increasing demand for efficient long-sequence\ninference support. However, key-value (KV) cache, which is stored to avoid\nre-computation, has emerged as a critical bottleneck by growing linearly in\nsize with the sequence length. Due to the auto-regressive nature of LLMs, the\nentire KV cache will be loaded for every generated token, resulting in low\nutilization of computational cores and high latency. While various compression\nmethods for KV cache have been proposed to alleviate this issue, they suffer\nfrom degradation in generation quality. We introduce TriForce, a hierarchical\nspeculative decoding system that is scalable for long sequence generation. This\napproach leverages the original model weights and dynamic sparse KV cache via\nretrieval as a draft model, which serves as an intermediate layer in the\nhierarchy and is further speculated by a smaller model to reduce its drafting\nlatency. TriForce not only facilitates impressive speedups for Llama2-7B-128K,\nachieving up to 2.31$\\times$ on an A100 GPU but also showcases scalability in\nhandling even longer contexts. For the offloading setting on two RTX 4090 GPUs,\nTriForce achieves 0.108s/token$\\unicode{x2014}$only half as slow as the\nauto-regressive baseline on an A100, which attains 7.78$\\times$ on our\noptimized offloading system. Additionally, TriForce performs 4.86$\\times$ than\nDeepSpeed-Zero-Inference on a single RTX 4090 GPU. TriForce's robustness is\nhighlighted by its consistently outstanding performance across various\ntemperatures. The code is available at\nhttps://github.com/Infini-AI-Lab/TriForce.\n","authors":["Hanshi Sun","Zhuoming Chen","Xinyu Yang","Yuandong Tian","Beidi Chen"],"pdf_url":"https://arxiv.org/pdf/2404.11912v3.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2408.01890v1","updated":"2024-08-04T00:38:34Z","published":"2024-08-04T00:38:34Z","title":"Cross-layer Attention Sharing for Large Language Models","summary":"  As large language models (LLMs) evolve, the increase in model depth and\nparameter number leads to substantial redundancy. To enhance the efficiency of\nthe attention mechanism, previous works primarily compress the KV cache or\ngroup attention heads, while largely overlooking redundancy between layers. Our\ncomprehensive analyses across various LLMs show that highly similar attention\npatterns persist within most layers. It's intuitive to save the computation by\nsharing attention weights across layers. However, further analysis reveals two\nchallenges: (1) Directly sharing the weight matrix without carefully\nrearranging the attention heads proves to be ineffective; (2) Shallow layers\nare vulnerable to small deviations in attention weights. Driven by these\ninsights, we introduce LiSA, a lightweight substitute for self-attention in\nwell-trained LLMs. LiSA employs tiny feed-forward networks to align attention\nheads between adjacent layers and low-rank matrices to approximate differences\nin layer-wise attention weights. Evaluations encompassing 13 typical benchmarks\ndemonstrate that LiSA maintains high response quality in terms of accuracy and\nperplexity while reducing redundant attention calculations within 53-84% of the\ntotal layers. Our implementations of LiSA achieve a 6X compression of Q and K,\nwith maximum throughput improvements of 19.5% for LLaMA3-8B and 32.3% for\nLLaMA2-7B.\n","authors":["Yongyu Mu","Yuzhang Wu","Yuchun Fan","Chenglong Wang","Hengyu Li","Qiaozhi He","Murun Yang","Tong Xiao","Jingbo Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01890v1.pdf","comment":"Working in process"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.02164v1","updated":"2024-08-04T23:21:46Z","published":"2024-08-04T23:21:46Z","title":"Rethinking Affect Analysis: A Protocol for Ensuring Fairness and\n  Consistency","summary":"  Evaluating affect analysis methods presents challenges due to inconsistencies\nin database partitioning and evaluation protocols, leading to unfair and biased\nresults. Previous studies claim continuous performance improvements, but our\nfindings challenge such assertions. Using these insights, we propose a unified\nprotocol for database partitioning that ensures fairness and comparability. We\nprovide detailed demographic annotations (in terms of race, gender and age),\nevaluation metrics, and a common framework for expression recognition, action\nunit detection and valence-arousal estimation. We also rerun the methods with\nthe new protocol and introduce a new leaderboards to encourage future research\nin affect recognition with a fairer comparison. Our annotations, code, and\npre-trained models are available on\n\\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.\n","authors":["Guanyu Hu","Dimitrios Kollias","Eleni Papadopoulou","Paraskevi Tzouveli","Jie Wei","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02164v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.06841"},{"id":"http://arxiv.org/abs/2408.02157v1","updated":"2024-08-04T22:23:10Z","published":"2024-08-04T22:23:10Z","title":"PanoFree: Tuning-Free Holistic Multi-view Image Generation with\n  Cross-view Self-Guidance","summary":"  Immersive scene generation, notably panorama creation, benefits significantly\nfrom the adaptation of large pre-trained text-to-image (T2I) models for\nmulti-view image generation. Due to the high cost of acquiring multi-view\nimages, tuning-free generation is preferred. However, existing methods are\neither limited to simple correspondences or require extensive fine-tuning to\ncapture complex ones. We present PanoFree, a novel method for tuning-free\nmulti-view image generation that supports an extensive array of\ncorrespondences. PanoFree sequentially generates multi-view images using\niterative warping and inpainting, addressing the key issues of inconsistency\nand artifacts from error accumulation without the need for fine-tuning. It\nimproves error accumulation by enhancing cross-view awareness and refines the\nwarping and inpainting processes via cross-view guidance, risky area estimation\nand erasing, and symmetric bidirectional guided generation for loop closure,\nalongside guidance-based semantic and density control for scene structure\npreservation. In experiments on Planar, 360{\\deg}, and Full Spherical\nPanoramas, PanoFree demonstrates significant error reduction, improves global\nconsistency, and boosts image quality without extra fine-tuning. Compared to\nexisting methods, PanoFree is up to 5x more efficient in time and 3x more\nefficient in GPU memory usage, and maintains superior diversity of results (2x\nbetter in our user study). PanoFree offers a viable alternative to costly\nfine-tuning or the use of additional pre-trained models. Project website at\nhttps://panofree.github.io/.\n","authors":["Aoming Liu","Zhong Li","Zhang Chen","Nannan Li","Yi Xu","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2408.02157v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.01996v3","updated":"2024-08-04T21:56:57Z","published":"2024-07-02T07:10:10Z","title":"ViG-Bias: Visually Grounded Bias Discovery and Mitigation","summary":"  The proliferation of machine learning models in critical decision making\nprocesses has underscored the need for bias discovery and mitigation\nstrategies. Identifying the reasons behind a biased system is not\nstraightforward, since in many occasions they are associated with hidden\nspurious correlations which are not easy to spot. Standard approaches rely on\nbias audits performed by analyzing model performance in pre-defined subgroups\nof data samples, usually characterized by common attributes like gender or\nethnicity when it comes to people, or other specific attributes defining\nsemantically coherent groups of images. However, it is not always possible to\nknow a-priori the specific attributes defining the failure modes of visual\nrecognition systems. Recent approaches propose to discover these groups by\nleveraging large vision language models, which enable the extraction of\ncross-modal embeddings and the generation of textual descriptions to\ncharacterize the subgroups where a certain model is underperforming. In this\nwork, we argue that incorporating visual explanations (e.g. heatmaps generated\nvia GradCAM or other approaches) can boost the performance of such bias\ndiscovery and mitigation frameworks. To this end, we introduce Visually\nGrounded Bias Discovery and Mitigation (ViG-Bias), a simple yet effective\ntechnique which can be integrated to a variety of existing frameworks to\nimprove both, discovery and mitigation performance. Our comprehensive\nevaluation shows that incorporating visual explanations enhances existing\ntechniques like DOMINO, FACTS and Bias-to-Text, across several challenging\ndatasets, including CelebA, Waterbirds, and NICO++.\n","authors":["Badr-Eddine Marani","Mohamed Hanini","Nihitha Malayarukil","Stergios Christodoulidis","Maria Vakalopoulou","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2407.01996v3.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2408.02146v1","updated":"2024-08-04T21:09:09Z","published":"2024-08-04T21:09:09Z","title":"Video-based Pedestrian and Vehicle Traffic Analysis During Football\n  Games","summary":"  This paper utilizes video analytics to study pedestrian and vehicle traffic\nbehavior, focusing on analyzing traffic patterns during football gamedays. The\nUniversity of Florida (UF) hosts six to seven home football games on Saturdays\nduring the college football season, attracting significant pedestrian activity.\nThrough video analytics, this study provides valuable insights into the impact\nof these events on traffic volumes and safety at intersections. Comparing\npedestrian and vehicle activities on gamedays versus non-gamedays reveals\ndiffering patterns. For example, pedestrian volume substantially increases\nduring gamedays, which is positively correlated with the probability of the\naway team winning. This correlation is likely because fans of the home team\nenjoy watching difficult games. Win probabilities as an early predictor of\npedestrian volumes at intersections can be a tool to help traffic professionals\nanticipate traffic management needs. Pedestrian-to-vehicle (P2V) conflicts\nnotably increase on gamedays, particularly a few hours before games start.\nAddressing this, a \"Barnes Dance\" movement phase within the intersection is\nrecommended. Law enforcement presence during high-activity gamedays can help\nensure pedestrian compliance and enhance safety. In contrast, we identified\nthat vehicle-to-vehicle (V2V) conflicts generally do not increase on gamedays\nand may even decrease due to heightened driver caution.\n","authors":["Jacques P. Fleischer","Ryan Pallack","Ahan Mishra","Gustavo Riente de Andrade","Subhadipto Poddar","Emmanuel Posadas","Robert Schenck","Tania Banerjee","Anand Rangarajan","Sanjay Ranka"],"pdf_url":"https://arxiv.org/pdf/2408.02146v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02140v1","updated":"2024-08-04T20:38:45Z","published":"2024-08-04T20:38:45Z","title":"VidModEx: Interpretable and Efficient Black Box Model Extraction for\n  High-Dimensional Spaces","summary":"  In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.\n","authors":["Somnath Sendhil Kumar","Yuvaraj Govindarajulu","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2408.02140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02138v1","updated":"2024-08-04T20:35:33Z","published":"2024-08-04T20:35:33Z","title":"RICA^2: Rubric-Informed, Calibrated Assessment of Actions","summary":"  The ability to quantify how well an action is carried out, also known as\naction quality assessment (AQA), has attracted recent interest in the vision\ncommunity. Unfortunately, prior methods often ignore the score rubric used by\nhuman experts and fall short of quantifying the uncertainty of the model\nprediction. To bridge the gap, we present RICA^2 - a deep probabilistic model\nthat integrates score rubric and accounts for prediction uncertainty for AQA.\nCentral to our method lies in stochastic embeddings of action steps, defined on\na graph structure that encodes the score rubric. The embeddings spread\nprobabilistic density in the latent space and allow our method to represent\nmodel uncertainty. The graph encodes the scoring criteria, based on which the\nquality scores can be decoded. We demonstrate that our method establishes new\nstate of the art on public benchmarks, including FineDiving, MTL-AQA, and\nJIGSAWS, with superior performance in score prediction and uncertainty\ncalibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/\n","authors":["Abrar Majeedi","Viswanatha Reddy Gajjala","Satya Sai Srinath Namburi GNVV","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2408.02138v1.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.02135v1","updated":"2024-08-04T20:12:38Z","published":"2024-08-04T20:12:38Z","title":"A First Look at Chebyshev-Sobolev Series for Digital Ink","summary":"  Considering digital ink as plane curves provides a valuable framework for\nvarious applications, including signature verification, note-taking, and\nmathematical handwriting recognition. These plane curves can be obtained as\nparameterized pairs of approximating truncated series (x(s), y(s)) determined\nby sampled points. Earlier work has found that representing these truncated\nseries (polynomials) in a Legendre or Legendre-Sobolev basis has a number of\ndesirable properties. These include compact data representation, meaningful\nclustering of like symbols in the vector space of polynomial coefficients,\nlinear separability of classes in this space, and highly efficient calculation\nof variation between curves. In this work, we take a first step at examining\nthe use of Chebyshev-Sobolev series for symbol recognition. The early\nindication is that this representation may be superior to Legendre-Sobolev\nrepresentation for some purposes.\n","authors":["Deepak Singh Kalhan","Stephen M. Watt"],"pdf_url":"https://arxiv.org/pdf/2408.02135v1.pdf","comment":"Accepted at MathUI 2024"},{"id":"http://arxiv.org/abs/2408.02123v1","updated":"2024-08-04T19:37:30Z","published":"2024-08-04T19:37:30Z","title":"FovEx: Human-inspired Explanations for Vision Transformers and\n  Convolutional Neural Networks","summary":"  Explainability in artificial intelligence (XAI) remains a crucial aspect for\nfostering trust and understanding in machine learning models. Current visual\nexplanation techniques, such as gradient-based or class-activation-based\nmethods, often exhibit a strong dependence on specific model architectures.\nConversely, perturbation-based methods, despite being model-agnostic, are\ncomputationally expensive as they require evaluating models on a large number\nof forward passes. In this work, we introduce Foveation-based Explanations\n(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly\nintegrates biologically inspired perturbations by iteratively creating foveated\nrenderings of the image and combines them with gradient-based visual\nexplorations to determine locations of interest efficiently. These locations\nare selected to maximize the performance of the model to be explained with\nrespect to the downstream task and then combined to generate an attribution\nmap. We provide a thorough evaluation with qualitative and quantitative\nassessments on established benchmarks. Our method achieves state-of-the-art\nperformance on both transformers (on 4 out of 5 metrics) and convolutional\nmodels (on 3 out of 5 metrics), demonstrating its versatility among various\narchitectures. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to GradCAM). This comparison enhances our confidence in\nFovEx's ability to close the interpretation gap between humans and machines.\n","authors":["Mahadev Prasad Panda","Matteo Tiezzi","Martina Vilas","Gemma Roig","Bjoern M. Eskofier","Dario Zanca"],"pdf_url":"https://arxiv.org/pdf/2408.02123v1.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2404.16845v2","updated":"2024-08-04T18:51:59Z","published":"2024-02-14T14:02:04Z","title":"HaLo-NeRF: Learning Geometry-Guided Semantics for Exploring\n  Unconstrained Photo Collections","summary":"  Internet image collections containing photos captured by crowds of\nphotographers show promise for enabling digital exploration of large-scale\ntourist landmarks. However, prior works focus primarily on geometric\nreconstruction and visualization, neglecting the key role of language in\nproviding a semantic interface for navigation and fine-grained understanding.\nIn constrained 3D domains, recent methods have leveraged vision-and-language\nmodels as a strong prior of 2D visual semantics. While these models display an\nexcellent understanding of broad visual semantics, they struggle with\nunconstrained photo collections depicting such tourist landmarks, as they lack\nexpert knowledge of the architectural domain. In this work, we present a\nlocalization system that connects neural representations of scenes depicting\nlarge-scale landmarks with text describing a semantic region within the scene,\nby harnessing the power of SOTA vision-and-language models with adaptations for\nunderstanding landmark scene semantics. To bolster such models with\nfine-grained knowledge, we leverage large-scale Internet data containing images\nof similar landmarks along with weakly-related textual information. Our\napproach is built upon the premise that images physically grounded in space can\nprovide a powerful supervision signal for localizing new concepts, whose\nsemantics may be unlocked from Internet textual metadata with large language\nmodels. We use correspondences between views of scenes to bootstrap spatial\nunderstanding of these semantics, providing guidance for 3D-compatible\nsegmentation that ultimately lifts to a volumetric scene representation. Our\nresults show that HaLo-NeRF can accurately localize a variety of semantic\nconcepts related to architectural landmarks, surpassing the results of other 3D\nmodels as well as strong 2D segmentation baselines. Our project page is at\nhttps://tau-vailab.github.io/HaLo-NeRF/.\n","authors":["Chen Dudai","Morris Alper","Hana Bezalel","Rana Hanocka","Itai Lang","Hadar Averbuch-Elor"],"pdf_url":"https://arxiv.org/pdf/2404.16845v2.pdf","comment":"Eurographics 2024. Project page:\n  https://tau-vailab.github.io/HaLo-NeRF/"},{"id":"http://arxiv.org/abs/2408.02110v1","updated":"2024-08-04T18:41:35Z","published":"2024-08-04T18:41:35Z","title":"AvatarPose: Avatar-guided 3D Pose Estimation of Close Human Interaction\n  from Sparse Multi-view Videos","summary":"  Despite progress in human motion capture, existing multi-view methods often\nface challenges in estimating the 3D pose and shape of multiple closely\ninteracting people. This difficulty arises from reliance on accurate 2D joint\nestimations, which are hard to obtain due to occlusions and body contact when\npeople are in close interaction. To address this, we propose a novel method\nleveraging the personalized implicit neural avatar of each individual as a\nprior, which significantly improves the robustness and precision of this\nchallenging pose estimation task. Concretely, the avatars are efficiently\nreconstructed via layered volume rendering from sparse multi-view videos. The\nreconstructed avatar prior allows for the direct optimization of 3D poses based\non color and silhouette rendering loss, bypassing the issues associated with\nnoisy 2D detections. To handle interpenetration, we propose a collision loss on\nthe overlapping shape regions of avatars to add penetration constraints.\nMoreover, both 3D poses and avatars are optimized in an alternating manner. Our\nexperimental results demonstrate state-of-the-art performance on several public\ndatasets.\n","authors":["Feichi Lu","Zijian Dong","Jie Song","Otmar Hilliges"],"pdf_url":"https://arxiv.org/pdf/2408.02110v1.pdf","comment":"Project Page: https://feichilu.github.io/AvatarPose/"},{"id":"http://arxiv.org/abs/2408.02100v1","updated":"2024-08-04T17:57:23Z","published":"2024-08-04T17:57:23Z","title":"View-consistent Object Removal in Radiance Fields","summary":"  Radiance Fields (RFs) have emerged as a crucial technology for 3D scene\nrepresentation, enabling the synthesis of novel views with remarkable realism.\nHowever, as RFs become more widely used, the need for effective editing\ntechniques that maintain coherence across different perspectives becomes\nevident. Current methods primarily depend on per-frame 2D image inpainting,\nwhich often fails to maintain consistency across views, thus compromising the\nrealism of edited RF scenes. In this work, we introduce a novel RF editing\npipeline that significantly enhances consistency by requiring the inpainting of\nonly a single reference image. This image is then projected across multiple\nviews using a depth-based approach, effectively reducing the inconsistencies\nobserved with per-frame inpainting. However, projections typically assume\nphotometric consistency across views, which is often impractical in real-world\nsettings. To accommodate realistic variations in lighting and viewpoint, our\npipeline adjusts the appearance of the projected views by generating multiple\ndirectional variants of the inpainted image, thereby adapting to different\nphotometric conditions. Additionally, we present an effective and robust\nmulti-view object segmentation approach as a valuable byproduct of our\npipeline. Extensive experiments demonstrate that our method significantly\nsurpasses existing frameworks in maintaining content consistency across views\nand enhancing visual quality. More results are available at\nhttps://vulab-ai.github.io/View-consistent_Object_Removal_in_Radiance_Fields.\n","authors":["Yiren Lu","Jing Ma","Yu Yin"],"pdf_url":"https://arxiv.org/pdf/2408.02100v1.pdf","comment":"Accepted to ACM Multimedia (MM) 2024. Project website is accessible\n  at\n  https://vulab-ai.github.io/View-consistent_Object_Removal_in_Radiance_Fields"},{"id":"http://arxiv.org/abs/2402.16033v2","updated":"2024-08-04T17:37:02Z","published":"2024-02-25T09:09:30Z","title":"Exploiting Regional Information Transformer for Single Image Deraining","summary":"  Transformer-based Single Image Deraining (SID) methods have achieved\nremarkable success, primarily attributed to their robust capability in\ncapturing long-range interactions. However, we've noticed that current methods\nhandle rain-affected and unaffected regions concurrently, overlooking the\ndisparities between these areas, resulting in confusion between rain streaks\nand background parts, and inabilities to obtain effective interactions,\nultimately resulting in suboptimal deraining outcomes. To address the above\nissue, we introduce the Region Transformer (Regformer), a novel SID method that\nunderlines the importance of independently processing rain-affected and\nunaffected regions while considering their combined impact for high-quality\nimage reconstruction. The crux of our method is the innovative Region\nTransformer Block (RTB), which integrates a Region Masked Attention (RMA)\nmechanism and a Mixed Gate Forward Block (MGFB). Our RTB is used for attention\nselection of rain-affected and unaffected regions and local modeling of mixed\nscales. The RMA generates attention maps tailored to these two regions and\ntheir interactions, enabling our model to capture comprehensive features\nessential for rain removal. To better recover high-frequency textures and\ncapture more local details, we develop the MGFB as a compensation module to\ncomplete local mixed scale modeling. Extensive experiments demonstrate that our\nmodel reaches state-of-the-art performance, significantly improving the image\nderaining quality. Our code and trained models are publicly available.\n","authors":["Baiang Li","Zhao Zhang","Huan Zheng","Xiaogang Xu","Yanyan Wei","Jingyi Zhang","Jicong Fan","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2402.16033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02091v1","updated":"2024-08-04T17:00:37Z","published":"2024-08-04T17:00:37Z","title":"Past Movements-Guided Motion Representation Learning for Human Motion\n  Prediction","summary":"  Human motion prediction based on 3D skeleton is a significant challenge in\ncomputer vision, primarily focusing on the effective representation of motion.\nIn this paper, we propose a self-supervised learning framework designed to\nenhance motion representation. This framework consists of two stages: first,\nthe network is pretrained through the self-reconstruction of past sequences,\nand the guided reconstruction of future sequences based on past movements. We\ndesign a velocity-based mask strategy to focus on the joints with large-scale\nmoving. Subsequently, the pretrained network undergoes finetuning for specific\ntasks. Self-reconstruction, guided by patterns of past motion, substantially\nimproves the model's ability to represent the spatiotemporal relationships\namong joints but also captures the latent relationships between past and future\nsequences. This capability is crucial for motion prediction tasks that solely\ndepend on historical motion data. By employing this straightforward yet\neffective training paradigm, our method outperforms existing\n\\textit{state-of-the-art} methods, reducing the average prediction errors by\n8.8\\% across Human3.6M, 3DPW, and AMASS datasets. The code is available at\nhttps://github.com/JunyuShi02/PMG-MRL.\n","authors":["Junyu Shi","Baoxuan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02091v1.pdf","comment":"13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.02088v1","updated":"2024-08-04T16:54:49Z","published":"2024-08-04T16:54:49Z","title":"KAN-RCBEVDepth: A multi-modal fusion algorithm in object detection for\n  autonomous driving","summary":"  Accurate 3D object detection in autonomous driving is critical yet\nchallenging due to occlusions, varying object scales, and complex urban\nenvironments. This paper introduces the RCBEV-KAN algorithm, a pioneering\nmethod designed to enhance 3D object detection by fusing multimodal sensor data\nfrom cameras, LiDAR, and millimeter-wave radar. Our innovative Bird's Eye View\n(BEV)-based approach, utilizing a Transformer architecture, significantly\nboosts detection precision and efficiency by seamlessly integrating diverse\ndata sources, improving spatial relationship handling, and optimizing\ncomputational processes. Experimental results show that the RCBEV-KAN model\ndemonstrates superior performance across most detection categories, achieving\nhigher Mean Distance AP (0.389 vs. 0.316, a 23% improvement), better ND Score\n(0.484 vs. 0.415, a 17% improvement), and faster Evaluation Time (71.28s, 8%\nfaster). These results indicate that RCBEV-KAN is more accurate, reliable, and\nefficient, making it ideal for dynamic and challenging autonomous driving\nenvironments.\n","authors":["Zhihao Lai","Chuanhao Liu","Shihui Sheng","Zhiqiang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02088v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v1","updated":"2024-08-04T16:50:07Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v1.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2405.14584v2","updated":"2024-08-04T16:26:10Z","published":"2024-05-23T13:55:11Z","title":"SE3D: A Framework For Saliency Method Evaluation In 3D Imaging","summary":"  For more than a decade, deep learning models have been dominating in various\n2D imaging tasks. Their application is now extending to 3D imaging, with 3D\nConvolutional Neural Networks (3D CNNs) being able to process LIDAR, MRI, and\nCT scans, with significant implications for fields such as autonomous driving\nand medical imaging. In these critical settings, explaining the model's\ndecisions is fundamental. Despite recent advances in Explainable Artificial\nIntelligence, however, little effort has been devoted to explaining 3D CNNs,\nand many works explain these models via inadequate extensions of 2D saliency\nmethods.\n  A fundamental limitation to the development of 3D saliency methods is the\nlack of a benchmark to quantitatively assess these on 3D data. To address this\nissue, we propose SE3D: a framework for Saliency method Evaluation in 3D\nimaging. We propose modifications to ShapeNet, ScanNet, and BraTS datasets, and\nevaluation metrics to assess saliency methods for 3D CNNs. We evaluate both\nstate-of-the-art saliency methods designed for 3D data and extensions of\npopular 2D saliency methods to 3D. Our experiments show that 3D saliency\nmethods do not provide explanations of sufficient quality, and that there is\nmargin for future improvements and safer applications of 3D CNNs in critical\nfields.\n","authors":["Mariusz Winiewski","Loris Giulivi","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2405.14584v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02079v1","updated":"2024-08-04T16:09:46Z","published":"2024-08-04T16:09:46Z","title":"Improving Neural Surface Reconstruction with Feature Priors from\n  Multi-View Image","summary":"  Recent advancements in Neural Surface Reconstruction (NSR) have significantly\nimproved multi-view reconstruction when coupled with volume rendering. However,\nrelying solely on photometric consistency in image space falls short of\naddressing complexities posed by real-world data, including occlusions and\nnon-Lambertian surfaces. To tackle these challenges, we propose an\ninvestigation into feature-level consistent loss, aiming to harness valuable\nfeature priors from diverse pretext visual tasks and overcome current\nlimitations. It is crucial to note the existing gap in determining the most\neffective pretext visual task for enhancing NSR. In this study, we\ncomprehensively explore multi-view feature priors from seven pretext visual\ntasks, comprising thirteen methods. Our main goal is to strengthen NSR training\nby considering a wide range of possibilities. Additionally, we examine the\nimpact of varying feature resolutions and evaluate both pixel-wise and\npatch-wise consistent losses, providing insights into effective strategies for\nimproving NSR performance. By incorporating pre-trained representations from\nMVSFormer and QuadTree, our approach can generate variations of MVS-NeuS and\nMatch-NeuS, respectively. Our results, analyzed on DTU and EPFL datasets,\nreveal that feature priors from image matching and multi-view stereo outperform\nother pretext tasks. Moreover, we discover that extending patch-wise\nphotometric consistency to the feature level surpasses the performance of\npixel-wise approaches. These findings underscore the effectiveness of these\ntechniques in enhancing NSR outcomes.\n","authors":["Xinlin Ren","Chenjie Cao","Yanwei Fu","Xiangyang Xue"],"pdf_url":"https://arxiv.org/pdf/2408.02079v1.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.02078v1","updated":"2024-08-04T16:09:04Z","published":"2024-08-04T16:09:04Z","title":"LDFaceNet: Latent Diffusion-based Network for High-Fidelity Deepfake\n  Generation","summary":"  Over the past decade, there has been tremendous progress in the domain of\nsynthetic media generation. This is mainly due to the powerful methods based on\ngenerative adversarial networks (GANs). Very recently, diffusion probabilistic\nmodels, which are inspired by non-equilibrium thermodynamics, have taken the\nspotlight. In the realm of image generation, diffusion models (DMs) have\nexhibited remarkable proficiency in producing both realistic and heterogeneous\nimagery through their stochastic sampling procedure. This paper proposes a\nnovel facial swapping module, termed as LDFaceNet (Latent Diffusion based Face\nSwapping Network), which is based on a guided latent diffusion model that\nutilizes facial segmentation and facial recognition modules for a conditioned\ndenoising process. The model employs a unique loss function to offer\ndirectional guidance to the diffusion process. Notably, LDFaceNet can\nincorporate supplementary facial guidance for desired outcomes without any\nretraining. To the best of our knowledge, this represents the first application\nof the latent diffusion model in the face-swapping task without prior training.\nThe results of this study demonstrate that the proposed method can generate\nextremely realistic and coherent images by leveraging the potential of the\ndiffusion model for facial swapping, thereby yielding superior visual outcomes\nand greater diversity.\n","authors":["Dwij Mehta","Aditya Mehta","Pratik Narang"],"pdf_url":"https://arxiv.org/pdf/2408.02078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19735v2","updated":"2024-08-04T15:38:41Z","published":"2024-05-30T06:31:03Z","title":"Twin Deformable Point Convolutions for Point Cloud Semantic Segmentation\n  in Remote Sensing Scenes","summary":"  Thanks to the application of deep learning technology in point cloud\nprocessing of the remote sensing field, point cloud segmentation has become a\nresearch hotspot in recent years, which can be applied to real-world 3D, smart\ncities, and other fields. Although existing solutions have made unprecedented\nprogress, they ignore the inherent characteristics of point clouds in remote\nsensing fields that are strictly arranged according to latitude, longitude, and\naltitude, which brings great convenience to the segmentation of point clouds in\nremote sensing fields. To consider this property cleverly, we propose novel\nconvolution operators, termed Twin Deformable point Convolutions (TDConvs),\nwhich aim to achieve adaptive feature learning by learning deformable sampling\npoints in the latitude-longitude plane and altitude direction, respectively.\nFirst, to model the characteristics of the latitude-longitude plane, we propose\na Cylinder-wise Deformable point Convolution (CyDConv) operator, which\ngenerates a two-dimensional cylinder map by constructing a cylinder-like grid\nin the latitude-longitude direction. Furthermore, to better integrate the\nfeatures of the latitude-longitude plane and the spatial geometric features, we\nperform a multi-scale fusion of the extracted latitude-longitude features and\nspatial geometric features, and realize it through the aggregation of adjacent\npoint features of different scales. In addition, a Sphere-wise Deformable point\nConvolution (SpDConv) operator is introduced to adaptively offset the sampling\npoints in three-dimensional space by constructing a sphere grid structure,\naiming at modeling the characteristics in the altitude direction. Experiments\non existing popular benchmarks conclude that our TDConvs achieve the best\nsegmentation performance, surpassing the existing state-of-the-art methods.\n","authors":["Yong-Qiang Mao","Hanbo Bi","Xuexue Li","Kaiqiang Chen","Zhirui Wang","Xian Sun","Kun Fu"],"pdf_url":"https://arxiv.org/pdf/2405.19735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16252v2","updated":"2024-08-04T15:27:28Z","published":"2024-07-23T07:40:41Z","title":"LawLuo: A Chinese Law Firm Co-run by LLM Agents","summary":"  Large Language Models (LLMs) demonstrate substantial potential in delivering\nlegal consultation services to users without a legal background, attributed to\ntheir superior text comprehension and generation capabilities. Nonetheless,\nexisting Chinese legal LLMs limit interaction to a single model-user dialogue,\nunlike the collaborative consultations typical of law firms, where multiple\nstaff members contribute to a single consultation. This limitation prevents an\nauthentic consultation experience. Additionally, extant Chinese legal LLMs\nsuffer from critical limitations: (1) insufficient control over the quality of\ninstruction fine-tuning data; (2) increased model hallucination resulting from\nusers' ambiguous queries; and (3) a reduction in the model's ability to follow\ninstructions over multiple dialogue turns. In response to these challenges, we\npropose a novel legal dialogue framework that leverages the collaborative\ncapabilities of multiple LLM agents, termed LawLuo. This framework encompasses\nfour agents: a receptionist, a lawyer, a secretary, and a boss, each\nresponsible for different functionalities, collaboratively providing a\ncomprehensive legal consultation to users. Additionally, we constructed two\nhigh-quality legal dialogue datasets, KINLED and MURLED, and fine-tuned\nChatGLM-3-6b using these datasets. We propose a legal query clarification\nalgorithm called ToLC. Experimental results demonstrate that LawLuo outperforms\nbaseline LLMs, including GPT-4, across three dimensions: lawyer-like language\nstyle, the usefulness of legal advice, and the accuracy of legal knowledge. Our\ncode and datasets are available at https://github.com/NEFUJing/LawLuo.\n","authors":["Jingyun Sun","Chengxiao Dai","Zhongze Luo","Yangbo Chang","Yang Li"],"pdf_url":"https://arxiv.org/pdf/2407.16252v2.pdf","comment":"11 pages, 13 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.02061v1","updated":"2024-08-04T15:20:39Z","published":"2024-08-04T15:20:39Z","title":"ParkingE2E: Camera-based End-to-end Parking Network, from Images to\n  Planning","summary":"  Autonomous parking is a crucial task in the intelligent driving field.\nTraditional parking algorithms are usually implemented using rule-based\nschemes. However, these methods are less effective in complex parking scenarios\ndue to the intricate design of the algorithms. In contrast,\nneural-network-based methods tend to be more intuitive and versatile than the\nrule-based methods. By collecting a large number of expert parking trajectory\ndata and emulating human strategy via learning-based methods, the parking task\ncan be effectively addressed. In this paper, we employ imitation learning to\nperform end-to-end planning from RGB images to path planning by imitating human\ndriving trajectories. The proposed end-to-end approach utilizes a target query\nencoder to fuse images and target features, and a transformer-based decoder to\nautoregressively predict future waypoints. We conducted extensive experiments\nin real-world scenarios, and the results demonstrate that the proposed method\nachieved an average parking success rate of 87.8% across four different\nreal-world garages. Real-vehicle experiments further validate the feasibility\nand effectiveness of the method proposed in this paper.\n","authors":["Changze Li","Ziheng Ji","Zhe Chen","Tong Qin","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02061v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02054v1","updated":"2024-08-04T15:01:23Z","published":"2024-08-04T15:01:23Z","title":"Step Saver: Predicting Minimum Denoising Steps for Diffusion Model Image\n  Generation","summary":"  In this paper, we introduce an innovative NLP model specifically fine-tuned\nto determine the minimal number of denoising steps required for any given text\nprompt. This advanced model serves as a real-time tool that recommends the\nideal denoise steps for generating high-quality images efficiently. It is\ndesigned to work seamlessly with the Diffusion model, ensuring that images are\nproduced with superior quality in the shortest possible time. Although our\nexplanation focuses on the DDIM scheduler, the methodology is adaptable and can\nbe applied to various other schedulers like Euler, Euler Ancestral, Heun, DPM2\nKarras, UniPC, and more. This model allows our customers to conserve costly\ncomputing resources by executing the fewest necessary denoising steps to\nachieve optimal quality in the produced images.\n","authors":["Jean Yu","Haim Barad"],"pdf_url":"https://arxiv.org/pdf/2408.02054v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02053v1","updated":"2024-08-04T15:01:16Z","published":"2024-08-04T15:01:16Z","title":"PanicleNeRF: low-cost, high-precision in-field phenotypingof rice\n  panicles with smartphone","summary":"  The rice panicle traits significantly influence grain yield, making them a\nprimary target for rice phenotyping studies. However, most existing techniques\nare limited to controlled indoor environments and difficult to capture the rice\npanicle traits under natural growth conditions. Here, we developed PanicleNeRF,\na novel method that enables high-precision and low-cost reconstruction of rice\npanicle three-dimensional (3D) models in the field using smartphone. The\nproposed method combined the large model Segment Anything Model (SAM) and the\nsmall model You Only Look Once version 8 (YOLOv8) to achieve high-precision\nsegmentation of rice panicle images. The NeRF technique was then employed for\n3D reconstruction using the images with 2D segmentation. Finally, the resulting\npoint clouds are processed to successfully extract panicle traits. The results\nshow that PanicleNeRF effectively addressed the 2D image segmentation task,\nachieving a mean F1 Score of 86.9% and a mean Intersection over Union (IoU) of\n79.8%, with nearly double the boundary overlap (BO) performance compared to\nYOLOv8. As for point cloud quality, PanicleNeRF significantly outperformed\ntraditional SfM-MVS (structure-from-motion and multi-view stereo) methods, such\nas COLMAP and Metashape. The panicle length was then accurately extracted with\nthe rRMSE of 2.94% for indica and 1.75% for japonica rice. The panicle volume\nestimated from 3D point clouds strongly correlated with the grain number (R2 =\n0.85 for indica and 0.82 for japonica) and grain mass (0.80 for indica and 0.76\nfor japonica). This method provides a low-cost solution for high-throughput\nin-field phenotyping of rice panicles, accelerating the efficiency of rice\nbreeding.\n","authors":["Xin Yang","Xuqi Lu","Pengyao Xie","Ziyue Guo","Hui Fang","Haowei Fu","Xiaochun Hu","Zhenbiao Sun","Haiyan Cen"],"pdf_url":"https://arxiv.org/pdf/2408.02053v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02052v1","updated":"2024-08-04T15:00:22Z","published":"2024-08-04T15:00:22Z","title":"EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier\n  Logits","summary":"  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n","authors":["Mateusz Ochal","Massimiliano Patacchiola","Malik Boudiaf","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02052v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2408.02049v1","updated":"2024-08-04T14:57:28Z","published":"2024-08-04T14:57:28Z","title":"3D Single-object Tracking in Point Clouds with High Temporal Variation","summary":"  The high temporal variation of the point clouds is the key challenge of 3D\nsingle-object tracking (3D SOT). Existing approaches rely on the assumption\nthat the shape variation of the point clouds and the motion of the objects\nacross neighboring frames are smooth, failing to cope with high temporal\nvariation data. In this paper, we present a novel framework for 3D SOT in point\nclouds with high temporal variation, called HVTrack. HVTrack proposes three\nnovel components to tackle the challenges in the high temporal variation\nscenario: 1) A Relative-Pose-Aware Memory module to handle temporal point cloud\nshape variations; 2) a Base-Expansion Feature Cross-Attention module to deal\nwith similar object distractions in expanded search areas; 3) a Contextual\nPoint Guided Self-Attention module for suppressing heavy background noise. We\nconstruct a dataset with high temporal variation (KITTI-HV) by setting\ndifferent frame intervals for sampling in the KITTI dataset. On the KITTI-HV\nwith 5 frame intervals, our HVTrack surpasses the state-of-the-art tracker\nCXTracker by 11.3%/15.7% in Success/Precision.\n","authors":["Qiao Wu","Kun Sun","Pei An","Mathieu Salzmann","Yanning Zhang","Jiaqi Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02049v1.pdf","comment":"Accepted by ECCV24"},{"id":"http://arxiv.org/abs/2408.02043v1","updated":"2024-08-04T14:30:14Z","published":"2024-08-04T14:30:14Z","title":"Deep Spectral Methods for Unsupervised Ultrasound Image Interpretation","summary":"  Ultrasound imaging is challenging to interpret due to non-uniform\nintensities, low contrast, and inherent artifacts, necessitating extensive\ntraining for non-specialists. Advanced representation with clear tissue\nstructure separation could greatly assist clinicians in mapping underlying\nanatomy and distinguishing between tissue layers. Decomposing an image into\nsemantically meaningful segments is mainly achieved using supervised\nsegmentation algorithms. Unsupervised methods are beneficial, as acquiring\nlarge labeled datasets is difficult and costly, but despite their advantages,\nthey still need to be explored in ultrasound. This paper proposes a novel\nunsupervised deep learning strategy tailored to ultrasound to obtain easily\ninterpretable tissue separations. We integrate key concepts from unsupervised\ndeep spectral methods, which combine spectral graph theory with deep learning\nmethods. We utilize self-supervised transformer features for spectral\nclustering to generate meaningful segments based on ultrasound-specific metrics\nand shape and positional priors, ensuring semantic consistency across the\ndataset. We evaluate our unsupervised deep learning strategy on three\nultrasound datasets, showcasing qualitative results across anatomical contexts\nwithout label requirements. We also conduct a comparative analysis against\nother clustering algorithms to demonstrate superior segmentation performance,\nboundary preservation, and label consistency.\n","authors":["Oleksandra Tmenova","Yordanka Velikova","Mahdi Saleh","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2408.02043v1.pdf","comment":"Accepted at International Conference on Medical Image Computing and\n  Computer Assisted Intervention, MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.02039v1","updated":"2024-08-04T14:14:54Z","published":"2024-08-04T14:14:54Z","title":"Pixel-Level Domain Adaptation: A New Perspective for Enhancing Weakly\n  Supervised Semantic Segmentation","summary":"  Recent attention has been devoted to the pursuit of learning semantic\nsegmentation models exclusively from image tags, a paradigm known as\nimage-level Weakly Supervised Semantic Segmentation (WSSS). Existing attempts\nadopt the Class Activation Maps (CAMs) as priors to mine object regions yet\nobserve the imbalanced activation issue, where only the most discriminative\nobject parts are located. In this paper, we argue that the distribution\ndiscrepancy between the discriminative and the non-discriminative parts of\nobjects prevents the model from producing complete and precise pseudo masks as\nground truths. For this purpose, we propose a Pixel-Level Domain Adaptation\n(PLDA) method to encourage the model in learning pixel-wise domain-invariant\nfeatures. Specifically, a multi-head domain classifier trained adversarially\nwith the feature extraction is introduced to promote the emergence of pixel\nfeatures that are invariant with respect to the shift between the source (i.e.,\nthe discriminative object parts) and the target (\\textit{i.e.}, the\nnon-discriminative object parts) domains. In addition, we come up with a\nConfident Pseudo-Supervision strategy to guarantee the discriminative ability\nof each pixel for the segmentation task, which serves as a complement to the\nintra-image domain adversarial training. Our method is conceptually simple,\nintuitive and can be easily integrated into existing WSSS methods. Taking\nseveral strong baseline models as instances, we experimentally demonstrate the\neffectiveness of our approach under a wide range of settings.\n","authors":["Ye Du","Zehua Fu","Qingjie Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02039v1.pdf","comment":"15 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02036v1","updated":"2024-08-04T14:07:14Z","published":"2024-08-04T14:07:14Z","title":"LEGO: Self-Supervised Representation Learning for Scene Text Images","summary":"  In recent years, significant progress has been made in scene text recognition\nby data-driven methods. However, due to the scarcity of annotated real-world\ndata, the training of these methods predominantly relies on synthetic data. The\ndistribution gap between synthetic and real data constrains the further\nperformance improvement of these methods in real-world applications. To tackle\nthis problem, a highly promising approach is to utilize massive amounts of\nunlabeled real data for self-supervised training, which has been widely proven\neffective in many NLP and CV tasks. Nevertheless, generic self-supervised\nmethods are unsuitable for scene text images due to their sequential nature. To\naddress this issue, we propose a Local Explicit and Global Order-aware\nself-supervised representation learning method (LEGO) that accounts for the\ncharacteristics of scene text images. Inspired by the human cognitive process\nof learning words, which involves spelling, reading, and writing, we propose\nthree novel pre-text tasks for LEGO to model sequential, semantic, and\nstructural features, respectively. The entire pre-training process is optimized\nby using a consistent Text Knowledge Codebook. Extensive experiments validate\nthat LEGO outperforms previous scene text self-supervised methods. The\nrecognizer incorporated with our pre-trained model achieves superior or\ncomparable performance compared to state-of-the-art scene text recognition\nmethods on six benchmarks. Furthermore, we demonstrate that LEGO can achieve\nsuperior performance in other text-related tasks.\n","authors":["Yujin Ren","Jiaxin Zhang","Lianwen Jin"],"pdf_url":"https://arxiv.org/pdf/2408.02036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02034v1","updated":"2024-08-04T13:55:58Z","published":"2024-08-04T13:55:58Z","title":"Mini-Monkey: Alleviate the Sawtooth Effect by Multi-Scale Adaptive\n  Cropping","summary":"  Recently, there has been significant interest in enhancing the capability of\nmultimodal large language models (MLLMs) to process high-resolution images.\nMost existing methods focus on adopting a cropping strategy to improve the\nability of multimodal large language models to understand image details.\nHowever, this cropping operation inevitably causes the segmentation of objects\nand connected areas, which impairs the MLLM's ability to recognize small or\nirregularly shaped objects or text. This issue is particularly evident in\nlightweight MLLMs. Addressing this issue, we propose Mini-Monkey, a lightweight\nMLLM that incorporates a plug-and-play method called multi-scale adaptive crop\nstrategy (MSAC). Mini-Monkey adaptively generates multi-scale representations,\nallowing it to select non-segmented objects from various scales. To mitigate\nthe computational overhead introduced by MSAC, we propose a Scale Compression\nMechanism (SCM), which effectively compresses image tokens. Mini-Monkey\nachieves state-of-the-art performance among 2B-parameter MLLMs. It not only\ndemonstrates leading performance on a variety of general multimodal\nunderstanding tasks but also shows consistent improvements in document\nunderstanding capabilities. On the OCRBench, Mini-Monkey achieves a score of\n802, outperforming 8B-parameter state-of-the-art model InternVL2-8B. Besides,\nour model and training strategy are very efficient, which can be trained with\nonly eight RTX 3090. The code is available at\nhttps://github.com/Yuliang-Liu/Monkey.\n","authors":["Mingxin Huang","Yuliang Liu","Dingkang Liang","Lianwen Jin","Xiang Bai"],"pdf_url":"https://arxiv.org/pdf/2408.02034v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02033v1","updated":"2024-08-04T13:51:18Z","published":"2024-08-04T13:51:18Z","title":"Enhancing Human Action Recognition and Violence Detection Through Deep\n  Learning Audiovisual Fusion","summary":"  This paper proposes a hybrid fusion-based deep learning approach based on two\ndifferent modalities, audio and video, to improve human activity recognition\nand violence detection in public places. To take advantage of audiovisual\nfusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning\n(HFBDL) are used and compared. Since the objective is to detect and recognize\nhuman violence in public places, Real-life violence situation (RLVS) dataset is\nexpanded and used. Simulating results of HFBDL show 96.67\\% accuracy on\nvalidation data, which is more accurate than the other state-of-the-art methods\non this dataset. To showcase our model's ability in real-world scenarios,\nanother dataset of 54 sounded videos of both violent and non-violent situations\nwas recorded. The model could successfully detect 52 out of 54 videos\ncorrectly. The proposed method shows a promising performance on real scenarios.\nThus, it can be used for human action recognition and violence detection in\npublic places for security purposes.\n","authors":["Pooya Janani","Amirabolfazl Suratgar","Afshin Taghvaeipour"],"pdf_url":"https://arxiv.org/pdf/2408.02033v1.pdf","comment":"This work has been submitted to the IEEE for possible publication, 10\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02032v1","updated":"2024-08-04T13:50:17Z","published":"2024-08-04T13:50:17Z","title":"Self-Introspective Decoding: Alleviating Hallucinations for Large\n  Vision-Language Models","summary":"  While Large Vision-Language Models (LVLMs) have rapidly advanced in recent\nyears, the prevalent issue known as the `hallucination' problem has emerged as\na significant bottleneck, hindering their real-world deployments. Existing\nmethods mitigate this issue mainly from two perspectives: One approach\nleverages extra knowledge like robust instruction tuning LVLMs with curated\ndatasets or employing auxiliary analysis networks, which inevitable incur\nadditional costs. Another approach, known as contrastive decoding, induces\nhallucinations by manually disturbing the vision or instruction raw inputs and\nmitigates them by contrasting the outputs of the disturbed and original LVLMs.\nHowever, these approaches rely on empirical holistic input disturbances and\ndouble the inference cost. To avoid these issues, we propose a simple yet\neffective method named Self-Introspective Decoding (SID). Our empirical\ninvestigation reveals that pretrained LVLMs can introspectively assess the\nimportance of vision tokens based on preceding vision and text (both\ninstruction and generated) tokens. We develop the Context and Text-aware Token\nSelection (CT2S) strategy, which preserves only unimportant vision tokens after\nearly layers of LVLMs to adaptively amplify text-informed hallucination during\nthe auto-regressive decoding. This approach ensures that multimodal knowledge\nabsorbed in the early layers induces multimodal contextual rather than aimless\nhallucinations. Subsequently, the original token logits subtract the amplified\nvision-and-text association hallucinations, guiding LVLMs decoding faithfully.\nExtensive experiments illustrate SID generates less-hallucination and\nhigher-quality texts across various metrics, without extra knowledge and much\nadditional computation burdens.\n","authors":["Fushuo Huo","Wenchao Xu","Zhong Zhang","Haozhao Wang","Zhicheng Chen","Peilin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.02032v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02024v1","updated":"2024-08-04T13:23:18Z","published":"2024-08-04T13:23:18Z","title":"Faster Diffusion Action Segmentation","summary":"  Temporal Action Segmentation (TAS) is an essential task in video analysis,\naiming to segment and classify continuous frames into distinct action segments.\nHowever, the ambiguous boundaries between actions pose a significant challenge\nfor high-precision segmentation. Recent advances in diffusion models have\ndemonstrated substantial success in TAS tasks due to their stable training\nprocess and high-quality generation capabilities. However, the heavy sampling\nsteps required by diffusion models pose a substantial computational burden,\nlimiting their practicality in real-time applications. Additionally, most\nrelated works utilize Transformer-based encoder architectures. Although these\narchitectures excel at capturing long-range dependencies, they incur high\ncomputational costs and face feature-smoothing issues when processing long\nvideo sequences. To address these challenges, we propose EffiDiffAct, an\nefficient and high-performance TAS algorithm. Specifically, we develop a\nlightweight temporal feature encoder that reduces computational overhead and\nmitigates the rank collapse phenomenon associated with traditional\nself-attention mechanisms. Furthermore, we introduce an adaptive skip strategy\nthat allows for dynamic adjustment of timestep lengths based on computed\nsimilarity metrics during inference, thereby further enhancing computational\nefficiency. Comprehensive experiments on the 50Salads, Breakfast, and GTEA\ndatasets demonstrated the effectiveness of the proposed algorithm.\n","authors":["Shuaibing Wang","Shunli Wang","Mingcheng Li","Dingkang Yang","Haopeng Kuang","Ziyun Qian","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02024v1.pdf","comment":"25 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.02018v1","updated":"2024-08-04T13:09:06Z","published":"2024-08-04T13:09:06Z","title":"Individualized multi-horizon MRI trajectory prediction for Alzheimer's\n  Disease","summary":"  Neurodegeneration as measured through magnetic resonance imaging (MRI) is\nrecognized as a potential biomarker for diagnosing Alzheimer's disease (AD),\nbut is generally considered less specific than amyloid or tau based biomarkers.\nDue to a large amount of variability in brain anatomy between different\nindividuals, we hypothesize that leveraging MRI time series can help improve\nspecificity, by treating each patient as their own baseline. Here we turn to\nconditional variational autoencoders to generate individualized MRI predictions\ngiven the subject's age, disease status and one previous scan. Using serial\nimaging data from the Alzheimer's Disease Neuroimaging Initiative, we train a\nnovel architecture to build a latent space distribution which can be sampled\nfrom to generate future predictions of changing anatomy. This enables us to\nextrapolate beyond the dataset and predict MRIs up to 10 years. We evaluated\nthe model on a held-out set from ADNI and an independent dataset (from Open\nAccess Series of Imaging Studies). By comparing to several alternatives, we\nshow that our model produces more individualized images with higher resolution.\nFurther, if an individual already has a follow-up MRI, we demonstrate a usage\nof our model to compute a likelihood ratio classifier for disease status. In\npractice, the model may be able to assist in early diagnosis of AD and provide\na counterfactual baseline trajectory for treatment effect estimation.\nFurthermore, it generates a synthetic dataset that can potentially be used for\ndownstream tasks such as anomaly detection and classification.\n","authors":["Rosemary He","Gabriella Ang","Daniel Tward"],"pdf_url":"https://arxiv.org/pdf/2408.02018v1.pdf","comment":"MICCAI 2024 LDTM workshop"},{"id":"http://arxiv.org/abs/2408.02014v1","updated":"2024-08-04T12:52:44Z","published":"2024-08-04T12:52:44Z","title":"Unsupervised Representation Learning by Balanced Self Attention Matching","summary":"  Many leading self-supervised methods for unsupervised representation\nlearning, in particular those for embedding image features, are built on\nvariants of the instance discrimination task, whose optimization is known to be\nprone to instabilities that can lead to feature collapse. Different techniques\nhave been devised to circumvent this issue, including the use of negative pairs\nwith different contrastive losses, the use of external memory banks, and\nbreaking of symmetry by using separate encoding networks with possibly\ndifferent structures. Our method, termed BAM, rather than directly matching\nfeatures of different views (augmentations) of input images, is based on\nmatching their self-attention vectors, which are the distributions of\nsimilarities to the entire set of augmented images of a batch. We obtain rich\nrepresentations and avoid feature collapse by minimizing a loss that matches\nthese distributions to their globally balanced and entropy regularized version,\nwhich is obtained through a simple self-optimal-transport computation. We\nablate and verify our method through a wide set of experiments that show\ncompetitive performance with leading methods on both semi-supervised and\ntransfer-learning benchmarks. Our implementation and pre-trained models are\navailable at github.com/DanielShalam/BAM .\n","authors":["Daniel Shalam","Simon Korman"],"pdf_url":"https://arxiv.org/pdf/2408.02014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02012v1","updated":"2024-08-04T12:48:20Z","published":"2024-08-04T12:48:20Z","title":"Decision Support System to triage of liver trauma","summary":"  Trauma significantly impacts global health, accounting for over 5 million\ndeaths annually, which is comparable to mortality rates from diseases such as\ntuberculosis, AIDS, and malaria. In Iran, the financial repercussions of road\ntraffic accidents represent approximately 2% of the nation's Gross National\nProduct each year. Bleeding is the leading cause of mortality in trauma\npatients within the first 24 hours following an injury, making rapid diagnosis\nand assessment of severity crucial. Trauma patients require comprehensive scans\nof all organs, generating a large volume of data. Evaluating CT images for the\nentire body is time-consuming and requires significant expertise, underscoring\nthe need for efficient time management in diagnosis. Efficient diagnostic\nprocesses can significantly reduce treatment costs and decrease the likelihood\nof secondary complications. In this context, the development of a reliable\nDecision Support System (DSS) for trauma triage, particularly focused on the\nabdominal area, is vital. This paper presents a novel method for detecting\nliver bleeding and lacerations using CT scans, utilising the GAN Pix2Pix\ntranslation model. The effectiveness of the method is quantified by Dice score\nmetrics, with the model achieving an accuracy of 97% for liver bleeding and 93%\nfor liver laceration detection. These results represent a notable improvement\nover current state-of-the-art technologies. The system's design integrates\nseamlessly with existing medical imaging technologies, making it a practical\naddition to emergency medical services. This research underscores the potential\nof advanced image translation models like GAN Pix2Pix in improving the\nprecision and speed of medical diagnostics in critical care scenarios.\n","authors":["Ali Jamali","Azadeh Nazemi","Ashkan Sami","Rosemina Bahrololoom","Shahram Paydar","Alireza Shakibafar"],"pdf_url":"https://arxiv.org/pdf/2408.02012v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02001v1","updated":"2024-08-04T11:59:09Z","published":"2024-08-04T11:59:09Z","title":"AdaCBM: An Adaptive Concept Bottleneck Model for Explainable and\n  Accurate Diagnosis","summary":"  The integration of vision-language models such as CLIP and Concept Bottleneck\nModels (CBMs) offers a promising approach to explaining deep neural network\n(DNN) decisions using concepts understandable by humans, addressing the\nblack-box concern of DNNs. While CLIP provides both explainability and\nzero-shot classification capability, its pre-training on generic image and text\ndata may limit its classification accuracy and applicability to medical image\ndiagnostic tasks, creating a transfer learning problem. To maintain\nexplainability and address transfer learning needs, CBM methods commonly design\npost-processing modules after the bottleneck module. However, this way has been\nineffective. This paper takes an unconventional approach by re-examining the\nCBM framework through the lens of its geometrical representation as a simple\nlinear classification system. The analysis uncovers that post-CBM fine-tuning\nmodules merely rescale and shift the classification outcome of the system,\nfailing to fully leverage the system's learning potential. We introduce an\nadaptive module strategically positioned between CLIP and CBM to bridge the gap\nbetween source and downstream domains. This simple yet effective approach\nenhances classification performance while preserving the explainability\nafforded by the framework. Our work offers a comprehensive solution that\nencompasses the entire process, from concept discovery to model training,\nproviding a holistic recipe for leveraging the strengths of GPT, CLIP, and CBM.\n","authors":["Townim F. Chowdhury","Vu Minh Hieu Phan","Kewen Liao","Minh-Son To","Yutong Xie","Anton van den Hengel","Johan W. Verjans","Zhibin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02001v1.pdf","comment":"Accepted at MICCAI 2024, the 27th International Conference on Medical\n  Image Computing and Computer Assisted Intervention"},{"id":"http://arxiv.org/abs/2408.01998v1","updated":"2024-08-04T11:51:00Z","published":"2024-08-04T11:51:00Z","title":"What Happens Without Background? Constructing Foreground-Only Data for\n  Fine-Grained Tasks","summary":"  Fine-grained recognition, a pivotal task in visual signal processing, aims to\ndistinguish between similar subclasses based on discriminative information\npresent in samples. However, prevailing methods often erroneously focus on\nbackground areas, neglecting the capture of genuinely effective discriminative\ninformation from the subject, thus impeding practical application. To\nfacilitate research into the impact of background noise on models and enhance\ntheir ability to concentrate on the subject's discriminative features, we\npropose an engineered pipeline that leverages the capabilities of SAM and Detic\nto create fine-grained datasets with only foreground subjects, devoid of\nbackground. Extensive cross-experiments validate this approach as a\npreprocessing step prior to training, enhancing algorithmic performance and\nholding potential for further modal expansion of the data.\n","authors":["Yuetian Wang","Wenjin Hou","Qinmu Peng","Xinge You"],"pdf_url":"https://arxiv.org/pdf/2408.01998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01986v1","updated":"2024-08-04T10:54:36Z","published":"2024-08-04T10:54:36Z","title":"DeMansia: Mamba Never Forgets Any Tokens","summary":"  This paper examines the mathematical foundations of transformer\narchitectures, highlighting their limitations particularly in handling long\nsequences. We explore prerequisite models such as Mamba, Vision Mamba (ViM),\nand LV-ViT that pave the way for our proposed architecture, DeMansia. DeMansia\nintegrates state space models with token labeling techniques to enhance\nperformance in image classification tasks, efficiently addressing the\ncomputational challenges posed by traditional transformers. The architecture,\nbenchmark, and comparisons with contemporary models demonstrate DeMansia's\neffectiveness. The implementation of this paper is available on GitHub at\nhttps://github.com/catalpaaa/DeMansia\n","authors":["Ricky Fang"],"pdf_url":"https://arxiv.org/pdf/2408.01986v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13355v2","updated":"2024-08-04T10:31:41Z","published":"2023-11-22T12:47:12Z","title":"Unified Classification and Rejection: A One-versus-All Framework","summary":"  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n","authors":["Zhen Cheng","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.13355v2.pdf","comment":"Published in Machine Intelligence Research\n  (https://link.springer.com/article/10.1007/s11633-024-1514-4)"},{"id":"http://arxiv.org/abs/2304.08386v3","updated":"2024-08-04T10:25:50Z","published":"2023-04-17T15:54:10Z","title":"Progressive Visual Prompt Learning with Contrastive Feature Re-formation","summary":"  Prompt learning has been designed as an alternative to fine-tuning for\nadapting Vision-language (V-L) models to the downstream tasks. Previous works\nmainly focus on text prompt while visual prompt works are limited for V-L\nmodels. The existing visual prompt methods endure either mediocre performance\nor unstable training process, indicating the difficulty of visual prompt\nlearning. In this paper, we propose a new Progressive Visual Prompt (ProVP)\nstructure to strengthen the interactions among prompts of different layers.\nMore importantly, our ProVP could effectively propagate the image embeddings to\ndeep layers and behave partially similar to an instance adaptive prompt method.\nTo alleviate generalization deterioration, we further propose a new contrastive\nfeature re-formation, which prevents the serious deviation of the prompted\nvisual feature from the fixed CLIP visual feature distribution. Combining both,\nour method (ProVP-Ref) is evaluated on 11 image benchmark datasets and achieves\n7/11 state-of-theart results on both few-shot and base-to-novel settings. To\nthe best of our knowledge, we are the first to demonstrate the superior\nperformance of visual prompts in V-L models to previous prompt-based methods in\ndownstream tasks. Meanwhile, it implies that our ProVP-Ref shows the best\ncapability to adapt and to generalize.\n","authors":["Chen Xu","Yuhan Zhu","Haocheng Shen","Boheng Chen","Yixuan Liao","Xiaoxin Chen","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2304.08386v3.pdf","comment":"IJCV 2024 Accepted"},{"id":"http://arxiv.org/abs/2408.01978v1","updated":"2024-08-04T09:53:50Z","published":"2024-08-04T09:53:50Z","title":"AdvQDet: Detecting Query-Based Adversarial Attacks with Adversarial\n  Contrastive Prompt Tuning","summary":"  Deep neural networks (DNNs) are known to be vulnerable to adversarial attacks\neven under a black-box setting where the adversary can only query the model.\nParticularly, query-based black-box adversarial attacks estimate adversarial\ngradients based on the returned probability vectors of the target model for a\nsequence of queries. During this process, the queries made to the target model\nare intermediate adversarial examples crafted at the previous attack step,\nwhich share high similarities in the pixel space. Motivated by this\nobservation, stateful detection methods have been proposed to detect and reject\nquery-based attacks. While demonstrating promising results, these methods\neither have been evaded by more advanced attacks or suffer from low efficiency\nin terms of the number of shots (queries) required to detect different attacks.\nArguably, the key challenge here is to assign high similarity scores for any\ntwo intermediate adversarial examples perturbed from the same clean image. To\naddress this challenge, we propose a novel Adversarial Contrastive Prompt\nTuning (ACPT) method to robustly fine-tune the CLIP image encoder to extract\nsimilar embeddings for any two intermediate adversarial queries. With ACPT, we\nfurther introduce a detection framework AdvQDet that can detect 7\nstate-of-the-art query-based attacks with $>99\\%$ detection rate within 5\nshots. We also show that ACPT is robust to 3 types of adaptive attacks. Code is\navailable at https://github.com/xinwong/AdvQDet.\n","authors":["Xin Wang","Kai Chen","Xingjun Ma","Zhineng Chen","Jingjing Chen","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.01978v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01977v1","updated":"2024-08-04T09:51:14Z","published":"2024-08-04T09:51:14Z","title":"Label Augmentation for Neural Networks Robustness","summary":"  Out-of-distribution generalization can be categorized into two types: common\nperturbations arising from natural variations in the real world and adversarial\nperturbations that are intentionally crafted to deceive neural networks. While\ndeep neural networks excel in accuracy under the assumption of identical\ndistributions between training and test data, they often encounter\nout-of-distribution scenarios resulting in a significant decline in accuracy.\nData augmentation methods can effectively enhance robustness against common\ncorruptions, but they typically fall short in improving robustness against\nadversarial perturbations. In this study, we develop Label Augmentation (LA),\nwhich enhances robustness against both common and intentional perturbations and\nimproves uncertainty estimation. Our findings indicate a Clean error rate\nimprovement of up to 23.29% when employing LA in comparisons to the baseline.\nAdditionally, it enhances robustness under common corruptions benchmark by up\nto 24.23%. When tested against FGSM and PGD attacks, improvements in\nadversarial robustness are noticeable, with enhancements of up to 53.18% for\nFGSM and 24.46% for PGD attacks.\n","authors":["Fatemeh Amerehi","Patrick Healy"],"pdf_url":"https://arxiv.org/pdf/2408.01977v1.pdf","comment":"21 pages, 4 figures, Published at 3rd Conference on Lifelong Learning\n  Agents (CoLLAs), 2024"},{"id":"http://arxiv.org/abs/2408.01976v1","updated":"2024-08-04T09:44:47Z","published":"2024-08-04T09:44:47Z","title":"Single-Point Supervised High-Resolution Dynamic Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection (IRSTD) tasks are extremely challenging for\ntwo main reasons: 1) it is difficult to obtain accurate labelling information\nthat is critical to existing methods, and 2) infrared (IR) small target\ninformation is easily lost in deep networks. To address these issues, we\npropose a single-point supervised high-resolution dynamic network (SSHD-Net).\nIn contrast to existing methods, we achieve state-of-the-art (SOTA) detection\nperformance using only single-point supervision. Specifically, we first design\na high-resolution cross-feature extraction module (HCEM), that achieves\nbi-directional feature interaction through stepped feature cascade channels\n(SFCC). It balances network depth and feature resolution to maintain deep IR\nsmall-target information. Secondly, the effective integration of global and\nlocal features is achieved through the dynamic coordinate fusion module (DCFM),\nwhich enhances the anti-interference ability in complex backgrounds. In\naddition, we introduce the high-resolution multilevel residual module (HMRM) to\nenhance the semantic information extraction capability. Finally, we design the\nadaptive target localization detection head (ATLDH) to improve detection\naccuracy. Experiments on the publicly available datasets NUDT-SIRST and\nIRSTD-1k demonstrate the effectiveness of our method. Compared to other SOTA\nmethods, our method can achieve better detection performance with only a single\npoint of supervision.\n","authors":["Jing Wu","Rixiang Ni","Feng Huang","Zhaobing Qiu","Liqiong Chen","Changhai Luo","Yunxiang Li","Youli Li"],"pdf_url":"https://arxiv.org/pdf/2408.01976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01970v1","updated":"2024-08-04T09:09:35Z","published":"2024-08-04T09:09:35Z","title":"SR-CIS: Self-Reflective Incremental System with Decoupled Memory and\n  Reasoning","summary":"  The ability of humans to rapidly learn new knowledge while retaining old\nmemories poses a significant challenge for current deep learning models. To\nhandle this challenge, we draw inspiration from human memory and learning\nmechanisms and propose the Self-Reflective Complementary Incremental System\n(SR-CIS). Comprising the deconstructed Complementary Inference Module (CIM) and\nComplementary Memory Module (CMM), SR-CIS features a small model for fast\ninference and a large model for slow deliberation in CIM, enabled by the\nConfidence-Aware Online Anomaly Detection (CA-OAD) mechanism for efficient\ncollaboration. CMM consists of task-specific Short-Term Memory (STM) region and\na universal Long-Term Memory (LTM) region. By setting task-specific Low-Rank\nAdaptive (LoRA) and corresponding prototype weights and biases, it instantiates\nexternal storage for parameter and representation memory, thus deconstructing\nthe memory module from the inference module. By storing textual descriptions of\nimages during training and combining them with the Scenario Replay Module (SRM)\npost-training for memory combination, along with periodic short-to-long-term\nmemory restructuring, SR-CIS achieves stable incremental memory with limited\nstorage requirements. Balancing model plasticity and memory stability under\nconstraints of limited storage and low data resources, SR-CIS surpasses\nexisting competitive baselines on multiple standard and few-shot incremental\nlearning benchmarks.\n","authors":["Biqing Qi","Junqi Gao","Xinquan Chen","Dong Li","Weinan Zhang","Bowen Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.01970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13309v2","updated":"2024-08-04T08:40:36Z","published":"2024-07-18T09:13:08Z","title":"Exposure Completing for Temporally Consistent Neural High Dynamic Range\n  Video Rendering","summary":"  High dynamic range (HDR) video rendering from low dynamic range (LDR) videos\nwhere frames are of alternate exposure encounters significant challenges, due\nto the exposure change and absence at each time stamp. The exposure change and\nabsence make existing methods generate flickering HDR results. In this paper,\nwe propose a novel paradigm to render HDR frames via completing the absent\nexposure information, hence the exposure information is complete and\nconsistent. Our approach involves interpolating neighbor LDR frames in the time\ndimension to reconstruct LDR frames for the absent exposures. Combining the\ninterpolated and given LDR frames, the complete set of exposure information is\navailable at each time stamp. This benefits the fusing process for HDR results,\nreducing noise and ghosting artifacts therefore improving temporal consistency.\nExtensive experimental evaluations on standard benchmarks demonstrate that our\nmethod achieves state-of-the-art performance, highlighting the importance of\nabsent exposure completing in HDR video rendering. The code is available at\nhttps://github.com/cuijiahao666/NECHDR.\n","authors":["Jiahao Cui","Wei Jiang","Zhan Peng","Zhiyu Pan","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13309v2.pdf","comment":"9 pages, 6 figures, accepted by ACM-MM 2024 (poster)"},{"id":"http://arxiv.org/abs/2408.01960v1","updated":"2024-08-04T08:33:44Z","published":"2024-08-04T08:33:44Z","title":"AnomalySD: Few-Shot Multi-Class Anomaly Detection with Stable Diffusion\n  Model","summary":"  Anomaly detection is a critical task in industrial manufacturing, aiming to\nidentify defective parts of products. Most industrial anomaly detection methods\nassume the availability of sufficient normal data for training. This assumption\nmay not hold true due to the cost of labeling or data privacy policies.\nAdditionally, mainstream methods require training bespoke models for different\nobjects, which incurs heavy costs and lacks flexibility in practice. To address\nthese issues, we seek help from Stable Diffusion (SD) model due to its\ncapability of zero/few-shot inpainting, which can be leveraged to inpaint\nanomalous regions as normal. In this paper, a few-shot multi-class anomaly\ndetection framework that adopts Stable Diffusion model is proposed, named\nAnomalySD. To adapt SD to anomaly detection task, we design different\nhierarchical text descriptions and the foreground mask mechanism for\nfine-tuning SD. In the inference stage, to accurately mask anomalous regions\nfor inpainting, we propose multi-scale mask strategy and prototype-guided mask\nstrategy to handle diverse anomalous regions. Hierarchical text prompts are\nalso utilized to guide the process of inpainting in the inference stage. The\nanomaly score is estimated based on inpainting result of all masks. Extensive\nexperiments on the MVTec-AD and VisA datasets demonstrate the superiority of\nour approach. We achieved anomaly classification and segmentation results of\n93.6%/94.8% AUROC on the MVTec-AD dataset and 86.1%/96.5% AUROC on the VisA\ndataset under multi-class and one-shot settings.\n","authors":["Zhenyu Yan","Qingqing Fang","Wenxi Lv","Qinliang Su"],"pdf_url":"https://arxiv.org/pdf/2408.01960v1.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2407.06863v4","updated":"2024-08-04T08:28:25Z","published":"2024-07-09T13:50:43Z","title":"Beyond Aesthetics: Cultural Competence in Text-to-Image Models","summary":"  Text-to-Image (T2I) models are being increasingly adopted in diverse global\ncommunities where they create visual representations of their unique cultures.\nCurrent T2I benchmarks primarily focus on faithfulness, aesthetics, and realism\nof generated images, overlooking the critical dimension of cultural competence.\nIn this work, we introduce a framework to evaluate cultural competence of T2I\nmodels along two crucial dimensions: cultural awareness and cultural diversity,\nand present a scalable approach using a combination of structured knowledge\nbases and large language models to build a large dataset of cultural artifacts\nto enable this evaluation. In particular, we apply this approach to build CUBE\n(CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to\nevaluate cultural competence of T2I models. CUBE covers cultural artifacts\nassociated with 8 countries across different geo-cultural regions and along 3\nconcepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of\nhigh-quality prompts that enable the evaluation of cultural awareness, and 2)\nCUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to\nevaluate cultural diversity. We also introduce cultural diversity as a novel\nT2I evaluation component, leveraging quality-weighted Vendi score. Our\nevaluations reveal significant gaps in the cultural awareness of existing\nmodels across countries and provide valuable insights into the cultural\ndiversity of T2I outputs for under-specified prompts. Our methodology is\nextendable to other cultural regions and concepts, and can facilitate the\ndevelopment of T2I models that better cater to the global population.\n","authors":["Nithish Kannen","Arif Ahmad","Marco Andreetto","Vinodkumar Prabhakaran","Utsav Prabhu","Adji Bousso Dieng","Pushpak Bhattacharyya","Shachi Dave"],"pdf_url":"https://arxiv.org/pdf/2407.06863v4.pdf","comment":"30 pages, 10 figures, preprint"},{"id":"http://arxiv.org/abs/2408.01959v1","updated":"2024-08-04T08:26:58Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01953v1","updated":"2024-08-04T07:59:17Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v1.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2408.01952v1","updated":"2024-08-04T07:48:12Z","published":"2024-08-04T07:48:12Z","title":"CACE-Net: Co-guidance Attention and Contrastive Enhancement for\n  Effective Audio-Visual Event Localization","summary":"  The audio-visual event localization task requires identifying concurrent\nvisual and auditory events from unconstrained videos within a network model,\nlocating them, and classifying their category. The efficient extraction and\nintegration of audio and visual modal information have always been challenging\nin this field. In this paper, we introduce CACE-Net, which differs from most\nexisting methods that solely use audio signals to guide visual information. We\npropose an audio-visual co-guidance attention mechanism that allows for\nadaptive bi-directional cross-modal attentional guidance between audio and\nvisual information, thus reducing inconsistencies between modalities. Moreover,\nwe have observed that existing methods have difficulty distinguishing between\nsimilar background and event and lack the fine-grained features for event\nclassification. Consequently, we employ background-event contrast enhancement\nto increase the discrimination of fused feature and fine-tuned pre-trained\nmodel to extract more refined and discernible features from complex multimodal\ninputs. Specifically, we have enhanced the model's ability to discern subtle\ndifferences between event and background and improved the accuracy of event\nclassification in our model. Experiments on the AVE dataset demonstrate that\nCACE-Net sets a new benchmark in the audio-visual event localization task,\nproving the effectiveness of our proposed methods in handling complex\nmultimodal learning and event localization in unconstrained videos. Code is\navailable at https://github.com/Brain-Cog-Lab/CACE-Net.\n","authors":["Xiang He","Xiangxi Liu","Yang Li","Dongcheng Zhao","Guobin Shen","Qingqun Kong","Xin Yang","Yi Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.01952v1.pdf","comment":"Accepted by ACM MM 2024. Code is available at this\n  https://github.com/Brain-Cog-Lab/CACE-Net"},{"id":"http://arxiv.org/abs/2408.01946v1","updated":"2024-08-04T07:12:59Z","published":"2024-08-04T07:12:59Z","title":"Masked Angle-Aware Autoencoder for Remote Sensing Images","summary":"  To overcome the inherent domain gap between remote sensing (RS) images and\nnatural images, some self-supervised representation learning methods have made\npromising progress. However, they have overlooked the diverse angles present in\nRS objects. This paper proposes the Masked Angle-Aware Autoencoder (MA3E) to\nperceive and learn angles during pre-training. We design a \\textit{scaling\ncenter crop} operation to create the rotated crop with random orientation on\neach original image, introducing the explicit angle variation. MA3E inputs this\ncomposite image while reconstruct the original image, aiming to effectively\nlearn rotation-invariant representations by restoring the angle variation\nintroduced on the rotated crop. To avoid biases caused by directly\nreconstructing the rotated crop, we propose an Optimal Transport (OT) loss that\nautomatically assigns similar original image patches to each rotated crop patch\nfor reconstruction. MA3E demonstrates more competitive performance than\nexisting pre-training methods on seven different RS image datasets in three\ndownstream tasks.\n","authors":["Zhihao Li","Biao Hou","Siteng Ma","Zitong Wu","Xianpeng Guo","Bo Ren","Licheng Jiao"],"pdf_url":"https://arxiv.org/pdf/2408.01946v1.pdf","comment":"This paper has been accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2407.13200v2","updated":"2024-08-04T07:06:04Z","published":"2024-07-18T06:32:45Z","title":"Adapt PointFormer: 3D Point Cloud Analysis via Adapting 2D Visual\n  Transformers","summary":"  Pre-trained large-scale models have exhibited remarkable efficacy in computer\nvision, particularly for 2D image analysis. However, when it comes to 3D point\nclouds, the constrained accessibility of data, in contrast to the vast\nrepositories of images, poses a challenge for the development of 3D pre-trained\nmodels. This paper therefore attempts to directly leverage pre-trained models\nwith 2D prior knowledge to accomplish the tasks for 3D point cloud analysis.\nAccordingly, we propose the Adaptive PointFormer (APF), which fine-tunes\npre-trained 2D models with only a modest number of parameters to directly\nprocess point clouds, obviating the need for mapping to images. Specifically,\nwe convert raw point clouds into point embeddings for aligning dimensions with\nimage tokens. Given the inherent disorder in point clouds, in contrast to the\nstructured nature of images, we then sequence the point embeddings to optimize\nthe utilization of 2D attention priors. To calibrate attention across 3D and 2D\ndomains and reduce computational overhead, a trainable PointFormer with a\nlimited number of parameters is subsequently concatenated to a frozen\npre-trained image model. Extensive experiments on various benchmarks\ndemonstrate the effectiveness of the proposed APF. The source code and more\ndetails are available at https://vcc.tech/research/2024/PointFormer.\n","authors":["Mengke Li","Da Li","Guoqing Yang","Yiu-ming Cheung","Hui Huang"],"pdf_url":"https://arxiv.org/pdf/2407.13200v2.pdf","comment":"ECAI 2024 main conference paper"},{"id":"http://arxiv.org/abs/2408.01945v1","updated":"2024-08-04T07:06:04Z","published":"2024-08-04T07:06:04Z","title":"Generalized Maximum Likelihood Estimation for Perspective-n-Point\n  Problem","summary":"  The Perspective-n-Point (PnP) problem has been widely studied in the\nliterature and applied in various vision-based pose estimation scenarios.\nHowever, existing methods ignore the anisotropy uncertainty of observations, as\ndemonstrated in several real-world datasets in this paper. This oversight may\nlead to suboptimal and inaccurate estimation, particularly in the presence of\nnoisy observations. To this end, we propose a generalized maximum likelihood\nPnP solver, named GMLPnP, that minimizes the determinant criterion by iterating\nthe GLS procedure to estimate the pose and uncertainty simultaneously. Further,\nthe proposed method is decoupled from the camera model. Results of synthetic\nand real experiments show that our method achieves better accuracy in common\npose estimation scenarios, GMLPnP improves rotation/translation accuracy by\n4.7%/2.0% on TUM-RGBD and 18.6%/18.4% on KITTI-360 dataset compared to the best\nbaseline. It is more accurate under very noisy observations in a vision-based\nUAV localization task, outperforming the best baseline by 34.4% in translation\nestimation accuracy.\n","authors":["Tian Zhan","Chunfeng Xu","Cheng Zhang","Ke Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.01945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01944v1","updated":"2024-08-04T07:04:59Z","published":"2024-08-04T07:04:59Z","title":"RobNODDI: Robust NODDI Parameter Estimation with Adaptive Sampling under\n  Continuous Representation","summary":"  Neurite Orientation Dispersion and Density Imaging (NODDI) is an important\nimaging technology used to evaluate the microstructure of brain tissue, which\nis of great significance for the discovery and treatment of various\nneurological diseases. Current deep learning-based methods perform parameter\nestimation through diffusion magnetic resonance imaging (dMRI) with a small\nnumber of diffusion gradients. These methods speed up parameter estimation and\nimprove accuracy. However, the diffusion directions used by most existing deep\nlearning models during testing needs to be strictly consistent with the\ndiffusion directions during training. This results in poor generalization and\nrobustness of deep learning models in dMRI parameter estimation. In this work,\nwe verify for the first time that the parameter estimation performance of\ncurrent mainstream methods will significantly decrease when the testing\ndiffusion directions and the training diffusion directions are inconsistent. A\nrobust NODDI parameter estimation method with adaptive sampling under\ncontinuous representation (RobNODDI) is proposed. Furthermore, long short-term\nmemory (LSTM) units and fully connected layers are selected to learn continuous\nrepresentation signals. To this end, we use a total of 100 subjects to conduct\nexperiments based on the Human Connectome Project (HCP) dataset, of which 60\nare used for training, 20 are used for validation, and 20 are used for testing.\nThe test results indicate that RobNODDI improves the generalization performance\nand robustness of the deep learning model, enhancing the stability and\nflexibility of deep learning NODDI parameter estimatimation applications.\n","authors":["Taohui Xiao","Jian Cheng","Wenxin Fan","Jing Yang","Cheng Li","Enqing Dong","Shanshan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01944v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01942v1","updated":"2024-08-04T06:34:24Z","published":"2024-08-04T06:34:24Z","title":"Visual Grounding for Object-Level Generalization in Reinforcement\n  Learning","summary":"  Generalization is a pivotal challenge for agents following natural language\ninstructions. To approach this goal, we leverage a vision-language model (VLM)\nfor visual grounding and transfer its vision-language knowledge into\nreinforcement learning (RL) for object-centric tasks, which makes the agent\ncapable of zero-shot generalization to unseen objects and instructions. By\nvisual grounding, we obtain an object-grounded confidence map for the target\nobject indicated in the instruction. Based on this map, we introduce two routes\nto transfer VLM knowledge into RL. Firstly, we propose an object-grounded\nintrinsic reward function derived from the confidence map to more effectively\nguide the agent towards the target object. Secondly, the confidence map offers\na more unified, accessible task representation for the agent's policy, compared\nto language embeddings. This enables the agent to process unseen objects and\ninstructions through comprehensible visual confidence maps, facilitating\nzero-shot object-level generalization. Single-task experiments prove that our\nintrinsic reward significantly improves performance on challenging skill\nlearning. In multi-task experiments, through testing on tasks beyond the\ntraining set, we show that the agent, when provided with the confidence map as\nthe task representation, possesses better generalization capabilities than\nlanguage-based conditioning. The code is available at\nhttps://github.com/PKU-RL/COPL.\n","authors":["Haobin Jiang","Zongqing Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01942v1.pdf","comment":"35 pages, 14 figures, 17 tables"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.02156v1","updated":"2024-08-04T22:23:09Z","published":"2024-08-04T22:23:09Z","title":"Calibration-Disentangled Learning and Relevance-Prioritized Reranking\n  for Calibrated Sequential Recommendation","summary":"  Calibrated recommendation, which aims to maintain personalized proportions of\ncategories within recommendations, is crucial in practical scenarios since it\nenhances user satisfaction by reflecting diverse interests. However, achieving\ncalibration in a sequential setting (i.e., calibrated sequential\nrecommendation) is challenging due to the need to adapt to users' evolving\npreferences. Previous methods typically leverage reranking algorithms to\ncalibrate recommendations after training a model without considering the effect\nof calibration and do not effectively tackle the conflict between relevance and\ncalibration during the reranking process. In this work, we propose LeapRec\n(Calibration-Disentangled Learning and Relevance-Prioritized Reranking), a\nnovel approach for the calibrated sequential recommendation that addresses\nthese challenges. LeapRec consists of two phases, model training phase and\nreranking phase. In the training phase, a backbone model is trained using our\nproposed calibration-disentangled learning-to-rank loss, which optimizes\npersonalized rankings while integrating calibration considerations. In the\nreranking phase, relevant items are prioritized at the top of the list, with\nitems needed for calibration following later to address potential conflicts\nbetween relevance and calibration. Through extensive experiments on four\nreal-world datasets, we show that LeapRec consistently outperforms previous\nmethods in the calibrated sequential recommendation. Our code is available at\nhttps://github.com/jeon185/LeapRec.\n","authors":["Hyunsik Jeon","Se-eun Yoon","Julian McAuley"],"pdf_url":"https://arxiv.org/pdf/2408.02156v1.pdf","comment":"Published at CIKM '24 as a full research paper"},{"id":"http://arxiv.org/abs/2408.02152v1","updated":"2024-08-04T22:00:34Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"  Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.\n","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2408.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.07220v2","updated":"2024-08-04T15:32:37Z","published":"2024-03-22T17:13:46Z","title":"Blended RAG: Improving RAG (Retriever-Augmented Generation) Accuracy\n  with Semantic Search and Hybrid Query-Based Retrievers","summary":"  Retrieval-Augmented Generation (RAG) is a prevalent approach to infuse a\nprivate knowledge base of documents with Large Language Models (LLM) to build\nGenerative Q\\&A (Question-Answering) systems. However, RAG accuracy becomes\nincreasingly challenging as the corpus of documents scales up, with Retrievers\nplaying an outsized role in the overall RAG accuracy by extracting the most\nrelevant document from the corpus to provide context to the LLM. In this paper,\nwe propose the 'Blended RAG' method of leveraging semantic search techniques,\nsuch as Dense Vector indexes and Sparse Encoder indexes, blended with hybrid\nquery strategies. Our study achieves better retrieval results and sets new\nbenchmarks for IR (Information Retrieval) datasets like NQ and TREC-COVID\ndatasets. We further extend such a 'Blended Retriever' to the RAG system to\ndemonstrate far superior results on Generative Q\\&A datasets like SQUAD, even\nsurpassing fine-tuning performance.\n","authors":["Kunal Sawarkar","Abhilasha Mangal","Shivam Raj Solanki"],"pdf_url":"https://arxiv.org/pdf/2404.07220v2.pdf","comment":"Paper accepted by MIPR and presented at The 7th IEEE International\n  Conference on Multimedia Information. Processing and Retrieval (IEEE-MIPR\n  2024)"},{"id":"http://arxiv.org/abs/2401.07769v3","updated":"2024-08-04T14:46:14Z","published":"2024-01-15T15:27:24Z","title":"Deep Evolutional Instant Interest Network for CTR Prediction in\n  Trigger-Induced Recommendation","summary":"  The recommendation has been playing a key role in many industries, e.g.,\ne-commerce, streaming media, social media, etc. Recently, a new recommendation\nscenario, called Trigger-Induced Recommendation (TIR), where users are able to\nexplicitly express their instant interests via trigger items, is emerging as an\nessential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.\nWithout explicitly modeling the user's instant interest, traditional\nrecommendation methods usually obtain sub-optimal results in TIR. Even though\nthere are a few methods considering the trigger and target items simultaneously\nto solve this problem, they still haven't taken into account temporal\ninformation of user behaviors, the dynamic change of user instant interest when\nthe user scrolls down and the interactions between the trigger and target\nitems. To tackle these problems, we propose a novel method -- Deep Evolutional\nInstant Interest Network (DEI2N), for click-through rate prediction in TIR\nscenarios. Specifically, we design a User Instant Interest Modeling Layer to\npredict the dynamic change of the intensity of instant interest when the user\nscrolls down. Temporal information is utilized in user behavior modeling.\nMoreover, an Interaction Layer is introduced to learn better interactions\nbetween the trigger and target items. We evaluate our method on several offline\nand real-world industrial datasets. Experimental results show that our proposed\nDEI2N outperforms state-of-the-art baselines. In addition, online A/B testing\ndemonstrates the superiority over the existing baseline in real-world\nproduction environments.\n","authors":["Zhibo Xiao","Luwei Yang","Tao Zhang","Wen Jiang","Wei Ning","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2401.07769v3.pdf","comment":"7 pages, 6 figures, accepted by the 17th ACM International Conference\n  on Web Search and Data Mining(WSDM'2024)"},{"id":"http://arxiv.org/abs/2403.05063v2","updated":"2024-08-04T11:49:48Z","published":"2024-03-08T05:23:27Z","title":"Aligning Large Language Models for Controllable Recommendations","summary":"  Inspired by the exceptional general intelligence of Large Language Models\n(LLMs), researchers have begun to explore their application in pioneering the\nnext generation of recommender systems - systems that are conversational,\nexplainable, and controllable. However, existing literature primarily\nconcentrates on integrating domain-specific knowledge into LLMs to enhance\naccuracy, often neglecting the ability to follow instructions. To address this\ngap, we initially introduce a collection of supervised learning tasks,\naugmented with labels derived from a conventional recommender model, aimed at\nexplicitly improving LLMs' proficiency in adhering to recommendation-specific\ninstructions. Subsequently, we develop a reinforcement learning-based alignment\nprocedure to further strengthen LLMs' aptitude in responding to users'\nintentions and mitigating formatting errors. Through extensive experiments on\ntwo real-world datasets, our method markedly advances the capability of LLMs to\ncomply with instructions within recommender systems, while sustaining a high\nlevel of accuracy performance.\n","authors":["Wensheng Lu","Jianxun Lian","Wei Zhang","Guanghua Li","Mingyang Zhou","Hao Liao","Xing Xie"],"pdf_url":"https://arxiv.org/pdf/2403.05063v2.pdf","comment":"14 pages; Accepted by ACL 2024 main conference"},{"id":"http://arxiv.org/abs/2304.03531v4","updated":"2024-08-04T05:45:05Z","published":"2023-04-07T08:09:50Z","title":"From Retrieval to Generation: Efficient and Effective Entity Set\n  Expansion","summary":"  Entity Set Expansion (ESE) is a critical task aiming at expanding entities of\nthe target semantic class described by seed entities. Most existing ESE methods\nare retrieval-based frameworks that need to extract contextual features of\nentities and calculate the similarity between seed entities and candidate\nentities. To achieve the two purposes, they iteratively traverse the corpus and\nthe entity vocabulary, resulting in poor efficiency and scalability.\nExperimental results indicate that the time consumed by the retrieval-based ESE\nmethods increases linearly with entity vocabulary and corpus size. In this\npaper, we firstly propose Generative Entity Set Expansion (GenExpan) framework,\nwhich utilizes a generative pre-trained auto-regressive language model to\naccomplish ESE task. Specifically, a prefix tree is employed to guarantee the\nvalidity of entity generation, and automatically generated class names are\nadopted to guide the model to generate target entities. Moreover, we propose\nKnowledge Calibration and Generative Ranking to further bridge the gap between\ngeneric knowledge of the language model and the goal of ESE task. For\nefficiency, expansion time consumed by GenExpan is independent of entity\nvocabulary and corpus size, and GenExpan achieves an average 600% speedup\ncompared to strong baselines. For expansion effectiveness, our framework\noutperforms previous state-of-the-art ESE methods.\n","authors":["Shulin Huang","Shirong Ma","Yangning Li","Yinghui Li","Hai-Tao Zheng"],"pdf_url":"https://arxiv.org/pdf/2304.03531v4.pdf","comment":"Accepted by CIKM 2024 (FULL paper)"},{"id":"http://arxiv.org/abs/2408.01931v1","updated":"2024-08-04T05:07:58Z","published":"2024-08-04T05:07:58Z","title":"Sharpness-Aware Cross-Domain Recommendation to Cold-Start Users","summary":"  Cross-Domain Recommendation (CDR) is a promising paradigm inspired by\ntransfer learning to solve the cold-start problem in recommender systems.\nExisting state-of-the-art CDR methods train an explicit mapping function to\ntransfer the cold-start users from a data-rich source domain to a target\ndomain. However, a limitation of these methods is that the mapping function is\ntrained on overlapping users across domains, while only a small number of\noverlapping users are available for training. By visualizing the loss landscape\nof the existing CDR model, we find that training on a small number of\noverlapping users causes the model to converge to sharp minima, leading to poor\ngeneralization. Based on this observation, we leverage loss-geometry-based\nmachine learning approach and propose a novel CDR method called Sharpness-Aware\nCDR (SCDR). Our proposed method simultaneously optimizes recommendation loss\nand loss sharpness, leading to better generalization with theoretical\nguarantees. Empirical studies on real-world datasets demonstrate that SCDR\nsignificantly outperforms the other CDR models for cold-start recommendation\ntasks, while concurrently enhancing the model's robustness to adversarial\nattacks.\n","authors":["Guohang Zeng","Qian Zhang","Guangquan Zhang","Jie Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01931v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01928v1","updated":"2024-08-04T04:52:21Z","published":"2024-08-04T04:52:21Z","title":"A Semi-supervised Multi-channel Graph Convolutional Network for Query\n  Classification in E-commerce","summary":"  Query intent classification is an essential module for customers to find\ndesired products on the e-commerce application quickly. Most existing query\nintent classification methods rely on the users' click behavior as a supervised\nsignal to construct training samples. However, these methods based entirely on\nposterior labels may lead to serious category imbalance problems because of the\nMatthew effect in click samples. Compared with popular categories, it is\ndifficult for products under long-tail categories to obtain traffic and user\nclicks, which makes the models unable to detect users' intent for products\nunder long-tail categories. This in turn aggravates the problem that long-tail\ncategories cannot obtain traffic, forming a vicious circle. In addition, due to\nthe randomness of the user's click, the posterior label is unstable for the\nquery with similar semantics, which makes the model very sensitive to the\ninput, leading to an unstable and incomplete recall of categories.\n  In this paper, we propose a novel Semi-supervised Multi-channel Graph\nConvolutional Network (SMGCN) to address the above problems from the\nperspective of label association and semi-supervised learning. SMGCN extends\ncategory information and enhances the posterior label by utilizing the\nsimilarity score between the query and categories. Furthermore, it leverages\nthe co-occurrence and semantic similarity graph of categories to strengthen the\nrelations among labels and weaken the influence of posterior label instability.\nWe conduct extensive offline and online A/B experiments, and the experimental\nresults show that SMGCN significantly outperforms the strong baselines, which\nshows its effectiveness and practicality.\n","authors":["Chunyuan Yuan","Ming Pang","Zheng Fang","Xue Jiang","Changping Peng","Zhangang Lin"],"pdf_url":"https://arxiv.org/pdf/2408.01928v1.pdf","comment":"Accepted by WWW2024"},{"id":"http://arxiv.org/abs/2407.21300v2","updated":"2024-08-04T04:04:10Z","published":"2024-07-31T03:00:59Z","title":"Implementing Streaming algorithm and k-means clusters to RAG","summary":"  Retrieval-augmented generation (RAG) has achieved great success in\ninformation retrieval to assist large language models because it builds an\nexternal knowledge database. However, it also has many problems: it consumes a\nlot of memory because of the huge database. When faced with massive streaming\ndata, it is unable to update the established index database in time. To save\nthe memory of building the database and maintain accuracy simultaneously, we\nproposed a new approach combining a streaming algorithm and k-means cluster\nwith RAG. Our approach applies a streaming algorithm to update the index and\nreduce memory consumption. Then use the k-means algorithm to cluster documents\nwith high similarities together, the query time will be shortened by doing\nthis. We conducted comparative experiments on four methods, and the results\nshow that RAG with streaming algorithm and k-means cluster performs well in\naccuracy and memory. For massive streaming data, we find that our method\nbehaves better than traditional RAG\n","authors":["Haoyu Kang","Yuzhou Zhu","Yukun Zhong","Ke Wang"],"pdf_url":"https://arxiv.org/pdf/2407.21300v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2209.03885v4","updated":"2024-08-04T23:45:40Z","published":"2022-09-08T15:41:31Z","title":"A Framework for Evaluating Privacy-Utility Trade-off in Vertical\n  Federated Learning","summary":"  Federated learning (FL) has emerged as a practical solution to tackle data\nsilo issues without compromising user privacy. One of its variants, vertical\nfederated learning (VFL), has recently gained increasing attention as the VFL\nmatches the enterprises' demands of leveraging more valuable features to build\nbetter machine learning models while preserving user privacy. Current works in\nVFL concentrate on developing a specific protection or attack mechanism for a\nparticular VFL algorithm. In this work, we propose an evaluation framework that\nformulates the privacy-utility evaluation problem. We then use this framework\nas a guide to comprehensively evaluate a broad range of protection mechanisms\nagainst most of the state-of-the-art privacy attacks for three widely deployed\nVFL algorithms. These evaluations may help FL practitioners select appropriate\nprotection mechanisms given specific requirements. Our evaluation results\ndemonstrate that: the model inversion and most of the label inference attacks\ncan be thwarted by existing protection mechanisms; the model completion (MC)\nattack is difficult to be prevented, which calls for more advanced MC-targeted\nprotection mechanisms. Based on our evaluation results, we offer concrete\nadvice on improving the privacy-preserving capability of VFL systems. The code\nis available at https://github.com/yankang18/Attack-Defense-VFL\n","authors":["Yan Kang","Jiahuan Luo","Yuanqin He","Xiaojin Zhang","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2209.03885v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02165v1","updated":"2024-08-04T23:23:48Z","published":"2024-08-04T23:23:48Z","title":"SelfBC: Self Behavior Cloning for Offline Reinforcement Learning","summary":"  Policy constraint methods in offline reinforcement learning employ additional\nregularization techniques to constrain the discrepancy between the learned\npolicy and the offline dataset. However, these methods tend to result in overly\nconservative policies that resemble the behavior policy, thus limiting their\nperformance. We investigate this limitation and attribute it to the static\nnature of traditional constraints. In this paper, we propose a novel dynamic\npolicy constraint that restricts the learned policy on the samples generated by\nthe exponential moving average of previously learned policies. By integrating\nthis self-constraint mechanism into off-policy methods, our method facilitates\nthe learning of non-conservative policies while avoiding policy collapse in the\noffline setting. Theoretical results show that our approach results in a nearly\nmonotonically improved reference policy. Extensive experiments on the D4RL\nMuJoCo domain demonstrate that our proposed method achieves state-of-the-art\nperformance among the policy constraint methods.\n","authors":["Shirong Liu","Chenjia Bai","Zixian Guo","Hao Zhang","Gaurav Sharma","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02161v1","updated":"2024-08-04T23:05:42Z","published":"2024-08-04T23:05:42Z","title":"Distilling Machine Learning's Added Value: Pareto Fronts in Atmospheric\n  Applications","summary":"  While the added value of machine learning (ML) for weather and climate\napplications is measurable, explaining it remains challenging, especially for\nlarge deep learning models. Inspired by climate model hierarchies, we propose\nthat a full hierarchy of Pareto-optimal models, defined within an appropriately\ndetermined error-complexity plane, can guide model development and help\nunderstand the models' added value. We demonstrate the use of Pareto fronts in\natmospheric physics through three sample applications, with hierarchies ranging\nfrom semi-empirical models with minimal tunable parameters (simplest) to deep\nlearning algorithms (most complex). First, in cloud cover parameterization, we\nfind that neural networks identify nonlinear relationships between cloud cover\nand its thermodynamic environment, and assimilate previously neglected features\nsuch as vertical gradients in relative humidity that improve the representation\nof low cloud cover. This added value is condensed into a ten-parameter equation\nthat rivals the performance of deep learning models. Second, we establish a ML\nmodel hierarchy for emulating shortwave radiative transfer, distilling the\nimportance of bidirectional vertical connectivity for accurately representing\nabsorption and scattering, especially for multiple cloud layers. Third, we\nemphasize the importance of convective organization information when modeling\nthe relationship between tropical precipitation and its surrounding\nenvironment. We discuss the added value of temporal memory when high-resolution\nspatial information is unavailable, with implications for precipitation\nparameterization. Therefore, by comparing data-driven models directly with\nexisting schemes using Pareto optimality, we promote process understanding by\nhierarchically unveiling system complexity, with the hope of improving the\ntrustworthiness of ML models in atmospheric applications.\n","authors":["Tom Beucler","Arthur Grundner","Sara Shamekh","Peter Ukkonen","Matthew Chantry","Ryan Lagerquist"],"pdf_url":"https://arxiv.org/pdf/2408.02161v1.pdf","comment":"18 pages, 4 figures, submitted to AMS Artificial Intelligence for the\n  Earth Systems (AIES)"},{"id":"http://arxiv.org/abs/2307.01357v3","updated":"2024-08-04T22:31:59Z","published":"2023-07-03T21:13:40Z","title":"Adaptive Principal Component Regression with Applications to Panel Data","summary":"  Principal component regression (PCR) is a popular technique for fixed-design\nerror-in-variables regression, a generalization of the linear regression\nsetting in which the observed covariates are corrupted with random noise. We\nprovide the first time-uniform finite sample guarantees for (regularized) PCR\nwhenever data is collected adaptively. Since the proof techniques for analyzing\nPCR in the fixed design setting do not readily extend to the online setting,\nour results rely on adapting tools from modern martingale concentration to the\nerror-in-variables setting. We demonstrate the usefulness of our bounds by\napplying them to the domain of panel data, a ubiquitous setting in econometrics\nand statistics. As our first application, we provide a framework for experiment\ndesign in panel data settings when interventions are assigned adaptively. Our\nframework may be thought of as a generalization of the synthetic control and\nsynthetic interventions frameworks, where data is collected via an adaptive\nintervention assignment policy. Our second application is a procedure for\nlearning such an intervention assignment policy in a setting where units arrive\nsequentially to be treated. In addition to providing theoretical performance\nguarantees (as measured by regret), we show that our method empirically\noutperforms a baseline which does not leverage error-in-variables regression.\n","authors":["Anish Agarwal","Keegan Harris","Justin Whitehouse","Zhiwei Steven Wu"],"pdf_url":"https://arxiv.org/pdf/2307.01357v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02159v1","updated":"2024-08-04T22:26:34Z","published":"2024-08-04T22:26:34Z","title":"SPINEX-TimeSeries: Similarity-based Predictions with Explainable\n  Neighbors Exploration for Time Series and Forecasting Problems","summary":"  This paper introduces a new addition to the SPINEX (Similarity-based\nPredictions with Explainable Neighbors Exploration) family, tailored\nspecifically for time series and forecasting analysis. This new algorithm\nleverages the concept of similarity and higher-order temporal interactions\nacross multiple time scales to enhance predictive accuracy and interpretability\nin forecasting. To evaluate the effectiveness of SPINEX, we present\ncomprehensive benchmarking experiments comparing it against 18 algorithms and\nacross 49 synthetic and real datasets characterized by varying trends,\nseasonality, and noise levels. Our performance assessment focused on\nforecasting accuracy and computational efficiency. Our findings reveal that\nSPINEX consistently ranks among the top 5 performers in forecasting precision\nand has a superior ability to handle complex temporal dynamics compared to\ncommonly adopted algorithms. Moreover, the algorithm's explainability features,\nPareto efficiency, and medium complexity (on the order of O(log n)) are\ndemonstrated through detailed visualizations to enhance the prediction and\ndecision-making process. We note that integrating similarity-based concepts\nopens new avenues for research in predictive analytics, promising more accurate\nand transparent decision making.\n","authors":["Ahmed Z Naser","MZ Naser"],"pdf_url":"https://arxiv.org/pdf/2408.02159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21772v2","updated":"2024-08-04T22:13:39Z","published":"2024-07-31T17:48:14Z","title":"ShieldGemma: Generative AI Content Moderation Based on Gemma","summary":"  We present ShieldGemma, a comprehensive suite of LLM-based safety content\nmoderation models built upon Gemma2. These models provide robust,\nstate-of-the-art predictions of safety risks across key harm types (sexually\nexplicit, dangerous content, harassment, hate speech) in both user input and\nLLM-generated output. By evaluating on both public and internal benchmarks, we\ndemonstrate superior performance compared to existing models, such as Llama\nGuard (+10.8\\% AU-PRC on public benchmarks) and WildCard (+4.3\\%).\nAdditionally, we present a novel LLM-based data curation pipeline, adaptable to\na variety of safety-related tasks and beyond. We have shown strong\ngeneralization performance for model trained mainly on synthetic data. By\nreleasing ShieldGemma, we provide a valuable resource to the research\ncommunity, advancing LLM safety and enabling the creation of more effective\ncontent moderation solutions for developers.\n","authors":["Wenjun Zeng","Yuchi Liu","Ryan Mullins","Ludovic Peran","Joe Fernandez","Hamza Harkous","Karthik Narasimhan","Drew Proud","Piyush Kumar","Bhaktipriya Radharapu","Olivia Sturman","Oscar Wahltinez"],"pdf_url":"https://arxiv.org/pdf/2407.21772v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18426v2","updated":"2024-08-04T22:13:16Z","published":"2024-07-25T23:04:37Z","title":"Diffusion-based subsurface multiphysics monitoring and forecasting","summary":"  Carbon capture and storage (CCS) plays a crucial role in mitigating\ngreenhouse gas emissions, particularly from industrial outputs. Using seismic\nmonitoring can aid in an accurate and robust monitoring system to ensure the\neffectiveness of CCS and mitigate associated risks. However, conventional\nseismic wave equation-based approaches are computationally demanding, which\nhinders real-time applications. In addition to efficiency, forecasting and\nuncertainty analysis are not easy to handle using such\nnumerical-simulation-based approaches. To this end, we propose a novel\nsubsurface multiphysics monitoring and forecasting framework utilizing video\ndiffusion models. This approach can generate high-quality representations of\nCO$2$ evolution and associated changes in subsurface elastic properties. With\nreconstruction guidance, forecasting and inversion can be achieved conditioned\non historical frames and/or observational data. Meanwhile, due to the\ngenerative nature of the approach, we can quantify uncertainty in the\nprediction. Tests based on the Compass model show that the proposed method\nsuccessfully captured the inherently complex physical phenomena associated with\nCO$_2$ monitoring, and it can predict and invert the subsurface elastic\nproperties and CO$_2$ saturation with consistency in their evolution.\n","authors":["Xinquan Huang","Fu Wang","Tariq Alkhalifah"],"pdf_url":"https://arxiv.org/pdf/2407.18426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02153v1","updated":"2024-08-04T22:13:14Z","published":"2024-08-04T22:13:14Z","title":"ARVO: Atlas of Reproducible Vulnerabilities for Open Source Software","summary":"  High-quality datasets of real-world vulnerabilities are enormously valuable\nfor downstream research in software security, but existing datasets are\ntypically small, require extensive manual effort to update, and are missing\ncrucial features that such research needs. In this paper, we introduce ARVO: an\nAtlas of Reproducible Vulnerabilities in Open-source software. By sourcing\nvulnerabilities from C/C++ projects that Google's OSS-Fuzz discovered and\nimplementing a reliable re-compilation system, we successfully reproduce more\nthan 5,000 memory vulnerabilities across over 250 projects, each with a\ntriggering input, the canonical developer-written patch for fixing the\nvulnerability, and the ability to automatically rebuild the project from source\nand run it at its vulnerable and patched revisions. Moreover, our dataset can\nbe automatically updated as OSS-Fuzz finds new vulnerabilities, allowing it to\ngrow over time. We provide a thorough characterization of the ARVO dataset,\nshow that it can locate fixes more accurately than Google's own OSV\nreproduction effort, and demonstrate its value for future research through two\ncase studies: firstly evaluating real-world LLM-based vulnerability repair, and\nsecondly identifying over 300 falsely patched (still-active) zero-day\nvulnerabilities from projects improperly labeled by OSS-Fuzz.\n","authors":["Xiang Mei","Pulkit Singh Singaria","Jordi Del Castillo","Haoran Xi"," Abdelouahab"," Benchikh","Tiffany Bao","Ruoyu Wang","Yan Shoshitaishvili","Adam Doup","Hammond Pearce","Brendan Dolan-Gavitt"],"pdf_url":"https://arxiv.org/pdf/2408.02153v1.pdf","comment":"14 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.02152v1","updated":"2024-08-04T22:00:34Z","published":"2024-08-04T22:00:34Z","title":"Generative Retrieval with Few-shot Indexing","summary":"  Existing generative retrieval (GR) approaches rely on training-based\nindexing, i.e., fine-tuning a model to memorise the associations between a\nquery and the document identifier (docid) of a relevant document.\nTraining-based indexing has three limitations: high training overhead,\nunder-utilization of the pre-trained knowledge of large language models (LLMs),\nand challenges in adapting to a dynamic document corpus. To address the above\nissues, we propose a novel few-shot indexing-based GR framework (Few-Shot GR).\nIt has a novel few-shot indexing process, where we prompt an LLM to generate\ndocids for all documents in a corpus, ultimately creating a docid bank for the\nentire corpus. During retrieval, we feed a query to the same LLM and constrain\nit to generate a docid within the docid bank created during indexing, and then\nmap the generated docid back to its corresponding document. Few-Shot GR relies\nsolely on prompting an LLM without requiring any training, making it more\nefficient. Moreover, we devise few-shot indexing with one-to-many mapping to\nfurther enhance Few-Shot GR. Experiments show that Few-Shot GR achieves\nsuperior performance to state-of-the-art GR methods that require heavy\ntraining.\n","authors":["Arian Askari","Chuan Meng","Mohammad Aliannejadi","Zhaochun Ren","Evangelos Kanoulas","Suzan Verberne"],"pdf_url":"https://arxiv.org/pdf/2408.02152v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.16056v2","updated":"2024-08-04T20:45:16Z","published":"2023-05-25T13:38:53Z","title":"Markov Decision Processes under External Temporal Processes","summary":"  Most reinforcement learning algorithms treat the context under which they\noperate as a stationary, isolated, and undisturbed environment. However, in\nreal world applications, environments constantly change due to a variety of\nexternal events. To address this problem, we study Markov Decision Processes\n(MDP) under the influence of an external temporal process. We formalize this\nnotion and discuss conditions under which the problem becomes tractable with\nsuitable solutions. We propose a policy iteration algorithm to solve this\nproblem and theoretically analyze its performance. We derive results on the\nsample complexity of the algorithm and study its dependency on the extent of\nnon-stationarity of the environment. We then conduct experiments to illustrate\nour results in a classic control environment.\n","authors":["Ranga Shaarad Ayyagari","Ambedkar Dukkipati"],"pdf_url":"https://arxiv.org/pdf/2305.16056v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02140v1","updated":"2024-08-04T20:38:45Z","published":"2024-08-04T20:38:45Z","title":"VidModEx: Interpretable and Efficient Black Box Model Extraction for\n  High-Dimensional Spaces","summary":"  In the domain of black-box model extraction, conventional methods reliant on\nsoft labels or surrogate datasets struggle with scaling to high-dimensional\ninput spaces and managing the complexity of an extensive array of interrelated\nclasses. In this work, we present a novel approach that utilizes SHAP (SHapley\nAdditive exPlanations) to enhance synthetic data generation. SHAP quantifies\nthe individual contributions of each input feature towards the victim model's\noutput, facilitating the optimization of an energy-based GAN towards a\ndesirable output. This method significantly boosts performance, achieving a\n16.45% increase in the accuracy of image classification models and extending to\nvideo classification models with an average improvement of 26.11% and a maximum\nof 33.36% on challenging datasets such as UCF11, UCF101, Kinetics 400, Kinetics\n600, and Something-Something V2. We further demonstrate the effectiveness and\npractical utility of our method under various scenarios, including the\navailability of top-k prediction probabilities, top-k prediction labels, and\ntop-1 labels.\n","authors":["Somnath Sendhil Kumar","Yuvaraj Govindarajulu","Pavan Kulkarni","Manojkumar Parmar"],"pdf_url":"https://arxiv.org/pdf/2408.02140v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02131v1","updated":"2024-08-04T20:02:07Z","published":"2024-08-04T20:02:07Z","title":"Model Hijacking Attack in Federated Learning","summary":"  Machine learning (ML), driven by prominent paradigms such as centralized and\nfederated learning, has made significant progress in various critical\napplications ranging from autonomous driving to face recognition. However, its\nremarkable success has been accompanied by various attacks. Recently, the model\nhijacking attack has shown that ML models can be hijacked to execute tasks\ndifferent from their original tasks, which increases both accountability and\nparasitic computational risks. Nevertheless, thus far, this attack has only\nfocused on centralized learning. In this work, we broaden the scope of this\nattack to the federated learning domain, where multiple clients collaboratively\ntrain a global model without sharing their data. Specifically, we present\nHijackFL, the first-of-its-kind hijacking attack against the global model in\nfederated learning. The adversary aims to force the global model to perform a\ndifferent task (called hijacking task) from its original task without the\nserver or benign client noticing. To accomplish this, unlike existing methods\nthat use data poisoning to modify the target model's parameters, HijackFL\nsearches for pixel-level perturbations based on their local model (without\nmodifications) to align hijacking samples with the original ones in the feature\nspace. When performing the hijacking task, the adversary applies these cloaks\nto the hijacking samples, compelling the global model to identify them as\noriginal samples and predict them accordingly. We conduct extensive experiments\non four benchmark datasets and three popular models. Empirical results\ndemonstrate that its attack performance outperforms baselines. We further\ninvestigate the factors that affect its performance and discuss possible\ndefenses to mitigate its impact.\n","authors":["Zheng Li","Siyuan Wu","Ruichuan Chen","Paarijaat Aditya","Istemi Ekin Akkus","Manohar Vanga","Min Zhang","Hao Li","Yang Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.02131v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02123v1","updated":"2024-08-04T19:37:30Z","published":"2024-08-04T19:37:30Z","title":"FovEx: Human-inspired Explanations for Vision Transformers and\n  Convolutional Neural Networks","summary":"  Explainability in artificial intelligence (XAI) remains a crucial aspect for\nfostering trust and understanding in machine learning models. Current visual\nexplanation techniques, such as gradient-based or class-activation-based\nmethods, often exhibit a strong dependence on specific model architectures.\nConversely, perturbation-based methods, despite being model-agnostic, are\ncomputationally expensive as they require evaluating models on a large number\nof forward passes. In this work, we introduce Foveation-based Explanations\n(FovEx), a novel XAI method inspired by human vision. FovEx seamlessly\nintegrates biologically inspired perturbations by iteratively creating foveated\nrenderings of the image and combines them with gradient-based visual\nexplorations to determine locations of interest efficiently. These locations\nare selected to maximize the performance of the model to be explained with\nrespect to the downstream task and then combined to generate an attribution\nmap. We provide a thorough evaluation with qualitative and quantitative\nassessments on established benchmarks. Our method achieves state-of-the-art\nperformance on both transformers (on 4 out of 5 metrics) and convolutional\nmodels (on 3 out of 5 metrics), demonstrating its versatility among various\narchitectures. Furthermore, we show the alignment between the explanation map\nproduced by FovEx and human gaze patterns (+14\\% in NSS compared to RISE,\n+203\\% in NSS compared to GradCAM). This comparison enhances our confidence in\nFovEx's ability to close the interpretation gap between humans and machines.\n","authors":["Mahadev Prasad Panda","Matteo Tiezzi","Martina Vilas","Gemma Roig","Bjoern M. Eskofier","Dario Zanca"],"pdf_url":"https://arxiv.org/pdf/2408.02123v1.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2408.02117v1","updated":"2024-08-04T19:14:36Z","published":"2024-08-04T19:14:36Z","title":"Value-Based Rationales Improve Social Experience: A Multiagent\n  Simulation Study","summary":"  We propose Exanna, a framework to realize agents that incorporate values in\ndecision making. An Exannaagent considers the values of itself and others when\nproviding rationales for its actions and evaluating the rationales provided by\nothers. Via multiagent simulation, we demonstrate that considering values in\ndecision making and producing rationales, especially for norm-deviating\nactions, leads to (1) higher conflict resolution, (2) better social experience,\n(3) higher privacy, and (4) higher flexibility.\n","authors":["Sz-Ting Tzeng","Nirav Ajmeri","Munindar P. Singh"],"pdf_url":"https://arxiv.org/pdf/2408.02117v1.pdf","comment":"13 pages, 13 figures, 13 tables (and supplementary material with\n  reproducibility and additional results), accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2408.02114v1","updated":"2024-08-04T18:57:21Z","published":"2024-08-04T18:57:21Z","title":"Recent Advances in Multi-Choice Machine Reading Comprehension: A Survey\n  on Methods and Datasets","summary":"  This paper provides a thorough examination of recent developments in the\nfield of multi-choice Machine Reading Comprehension (MRC). Focused on benchmark\ndatasets, methodologies, challenges, and future trajectories, our goal is to\noffer researchers a comprehensive overview of the current landscape in\nmulti-choice MRC. The analysis delves into 30 existing cloze-style and\nmultiple-choice MRC benchmark datasets, employing a refined classification\nmethod based on attributes such as corpus style, domain, complexity, context\nstyle, question style, and answer style. This classification system enhances\nour understanding of each dataset's diverse attributes and categorizes them\nbased on their complexity. Furthermore, the paper categorizes recent\nmethodologies into Fine-tuned and Prompt-tuned methods. Fine-tuned methods\ninvolve adapting pre-trained language models (PLMs) to a specific task through\nretraining on domain-specific datasets, while prompt-tuned methods use prompts\nto guide PLM response generation, presenting potential applications in\nzero-shot or few-shot learning scenarios. By contributing to ongoing\ndiscussions, inspiring future research directions, and fostering innovations,\nthis paper aims to propel multi-choice MRC towards new frontiers of\nachievement.\n","authors":["Shima Foolad","Kourosh Kiani","Razieh Rastgoo"],"pdf_url":"https://arxiv.org/pdf/2408.02114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02111v1","updated":"2024-08-04T18:47:55Z","published":"2024-08-04T18:47:55Z","title":"Understanding Deep Learning via Notions of Rank","summary":"  Despite the extreme popularity of deep learning in science and industry, its\nformal understanding is limited. This thesis puts forth notions of rank as key\nfor developing a theory of deep learning, focusing on the fundamental aspects\nof generalization and expressiveness. In particular, we establish that\ngradient-based training can induce an implicit regularization towards low rank\nfor several neural network architectures, and demonstrate empirically that this\nphenomenon may facilitate an explanation of generalization over natural data\n(e.g., audio, images, and text). Then, we characterize the ability of graph\nneural networks to model interactions via a notion of rank, which is commonly\nused for quantifying entanglement in quantum physics. A central tool underlying\nthese results is a connection between neural networks and tensor\nfactorizations. Practical implications of our theory for designing explicit\nregularization schemes and data preprocessing algorithms are presented.\n","authors":["Noam Razin"],"pdf_url":"https://arxiv.org/pdf/2408.02111v1.pdf","comment":"PhD thesis"},{"id":"http://arxiv.org/abs/2405.01013v2","updated":"2024-08-04T18:09:39Z","published":"2024-05-02T05:29:22Z","title":"Non-clairvoyant Scheduling with Partial Predictions","summary":"  The non-clairvoyant scheduling problem has gained new interest within\nlearning-augmented algorithms, where the decision-maker is equipped with\npredictions without any quality guarantees. In practical settings, access to\npredictions may be reduced to specific instances, due to cost or data\nlimitations. Our investigation focuses on scenarios where predictions for only\n$B$ job sizes out of $n$ are available to the algorithm. We first establish\nnear-optimal lower bounds and algorithms in the case of perfect predictions.\nSubsequently, we present a learning-augmented algorithm satisfying the\nrobustness, consistency, and smoothness criteria, and revealing a novel\ntradeoff between consistency and smoothness inherent in the scenario with a\nrestricted number of predictions.\n","authors":["Ziyad Benomar","Vianney Perchet"],"pdf_url":"https://arxiv.org/pdf/2405.01013v2.pdf","comment":"Accepted as a conference paper at ICML 2024"},{"id":"http://arxiv.org/abs/2402.12668v2","updated":"2024-08-04T18:07:51Z","published":"2024-02-20T02:36:26Z","title":"Randomization Can Reduce Both Bias and Variance: A Case Study in Random\n  Forests","summary":"  We study the often overlooked phenomenon, first noted in\n\\cite{breiman2001random}, that random forests appear to reduce bias compared to\nbagging. Motivated by an interesting paper by \\cite{mentch2020randomization},\nwhere the authors argue that random forests reduce effective degrees of freedom\nand only outperform bagging ensembles in low signal-to-noise ratio (SNR)\nsettings, we explore how random forests can uncover patterns in the data missed\nby bagging. We empirically demonstrate that in the presence of such patterns,\nrandom forests reduce bias along with variance and increasingly outperform\nbagging ensembles when SNR is high. Our observations offer insights into the\nreal-world success of random forests across a range of SNRs and enhance our\nunderstanding of the difference between random forests and bagging ensembles\nwith respect to the randomization injected into each split. Our investigations\nalso yield practical insights into the importance of tuning $mtry$ in random\nforests.\n","authors":["Brian Liu","Rahul Mazumder"],"pdf_url":"https://arxiv.org/pdf/2402.12668v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00927v2","updated":"2024-08-04T18:04:50Z","published":"2024-07-01T03:17:34Z","title":"Learnability of Parameter-Bounded Bayes Nets","summary":"  Bayes nets are extensively used in practice to efficiently represent joint\nprobability distributions over a set of random variables and capture dependency\nrelations. In a seminal paper, Chickering et al. (JMLR 2004) showed that given\na distribution $\\mathbb{P}$, that is defined as the marginal distribution of a\nBayes net, it is $\\mathsf{NP}$-hard to decide whether there is a\nparameter-bounded Bayes net that represents $\\mathbb{P}$. They called this\nproblem LEARN. In this work, we extend the $\\mathsf{NP}$-hardness result of\nLEARN and prove the $\\mathsf{NP}$-hardness of a promise search variant of\nLEARN, whereby the Bayes net in question is guaranteed to exist and one is\nasked to find such a Bayes net. We complement our hardness result with a\npositive result about the sample complexity that is sufficient to recover a\nparameter-bounded Bayes net that is close (in TV distance) to a given\ndistribution $\\mathbb{P}$, that is represented by some parameter-bounded Bayes\nnet, generalizing a degree-bounded sample complexity result of Brustle et al.\n(EC 2020).\n","authors":["Arnab Bhattacharyya","Davin Choo","Sutanu Gayen","Dimitrios Myrisiotis"],"pdf_url":"https://arxiv.org/pdf/2407.00927v2.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2312.14302v2","updated":"2024-08-04T17:48:33Z","published":"2023-12-21T21:22:41Z","title":"Exploiting Novel GPT-4 APIs","summary":"  Language model attacks typically assume one of two extreme threat models:\nfull white-box access to model weights, or black-box access limited to a text\ngeneration API. However, real-world APIs are often more flexible than just text\ngeneration: these APIs expose \"gray-box\" access leading to new threat vectors.\nTo explore this, we red-team three new functionalities exposed in the GPT-4\nAPIs: fine-tuning, function calling and knowledge retrieval. We find that\nfine-tuning a model on as few as 15 harmful examples or 100 benign examples can\nremove core safeguards from GPT-4, enabling a range of harmful outputs.\nFurthermore, we find that GPT-4 Assistants readily divulge the function call\nschema and can be made to execute arbitrary function calls. Finally, we find\nthat knowledge retrieval can be hijacked by injecting instructions into\nretrieval documents. These vulnerabilities highlight that any additions to the\nfunctionality exposed by an API can create new vulnerabilities.\n","authors":["Kellin Pelrine","Mohammad Taufeeque","Micha Zajc","Euan McLean","Adam Gleave"],"pdf_url":"https://arxiv.org/pdf/2312.14302v2.pdf","comment":"10 pages, 1 figure, 4 tables"},{"id":"http://arxiv.org/abs/2307.00575v2","updated":"2024-08-04T16:28:21Z","published":"2023-07-02T13:59:47Z","title":"Mode-wise Principal Subspace Pursuit and Matrix Spiked Covariance Model","summary":"  This paper introduces a novel framework called Mode-wise Principal Subspace\nPursuit (MOP-UP) to extract hidden variations in both the row and column\ndimensions for matrix data. To enhance the understanding of the framework, we\nintroduce a class of matrix-variate spiked covariance models that serve as\ninspiration for the development of the MOP-UP algorithm. The MOP-UP algorithm\nconsists of two steps: Average Subspace Capture (ASC) and Alternating\nProjection (AP). These steps are specifically designed to capture the row-wise\nand column-wise dimension-reduced subspaces which contain the most informative\nfeatures of the data. ASC utilizes a novel average projection operator as\ninitialization and achieves exact recovery in the noiseless setting. We analyze\nthe convergence and non-asymptotic error bounds of MOP-UP, introducing a\nblockwise matrix eigenvalue perturbation bound that proves the desired bound,\nwhere classic perturbation bounds fail. The effectiveness and practical merits\nof the proposed framework are demonstrated through experiments on both\nsimulated and real datasets. Lastly, we discuss generalizations of our approach\nto higher-order data.\n","authors":["Runshi Tang","Ming Yuan","Anru R. Zhang"],"pdf_url":"https://arxiv.org/pdf/2307.00575v2.pdf","comment":"Journal of the Royal Statistical Society, Series B, to appear"},{"id":"http://arxiv.org/abs/2404.14462v3","updated":"2024-08-04T16:24:15Z","published":"2024-04-22T06:19:46Z","title":"Towards smaller, faster decoder-only transformers: Architectural\n  variants and their implications","summary":"  In recent times, the research on Large Language Models (LLMs) has grown\nexponentially, predominantly focusing on models underpinned by the transformer\narchitecture, as established by [1], and further developed through the\ndecoder-only variations by [2]. Contemporary efforts in this field primarily\naim to enhance model capabilities by scaling up both the architecture and data\nvolumes utilized during training. However, the exploration into reduce these\nmodel sizes while preserving their efficacy remains scant. In this study, we\nintroduce three modifications to the decoder-only transformer architecture,\nnamely ParallelGPT (pgpt), LinearGPT (lgpt), and ConvGPT (cgpt). These variants\ndemonstrate comparable performance to the conventional architecture in language\ngeneration, yet benefit from reduced model sizes and faster training processes.\nWe open-source the model weights and the complete codebase for these\nimplementation for further research.\n","authors":["Sathya Krishnan Suresh","Shunmugapriya P"],"pdf_url":"https://arxiv.org/pdf/2404.14462v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2407.02461v2","updated":"2024-08-04T15:41:23Z","published":"2024-07-02T17:40:06Z","title":"Decentralized Intelligence Network (DIN)","summary":"  Decentralized Intelligence Network (DIN) is a theoretical framework\naddressing data fragmentation and siloing challenges, enabling scalable AI\nthrough data sovereignty. It facilitates effective AI utilization within\nsovereign networks by overcoming barriers to accessing diverse data sources,\nleveraging: 1) personal data stores to ensure data sovereignty, where data\nremains securely within Participants' control; 2) a scalable federated learning\nprotocol implemented on a public blockchain for decentralized AI training,\nwhere only model parameter updates are shared, keeping data within the personal\ndata stores; and 3) a scalable, trustless cryptographic rewards mechanism on a\npublic blockchain to incentivize participation and ensure fair reward\ndistribution through a decentralized auditing protocol. This approach\nguarantees that no entity can prevent or control access to training data or\ninfluence financial benefits, as coordination and reward distribution are\nmanaged on the public blockchain with an immutable record. The framework\nsupports effective AI training by allowing Participants to maintain control\nover their data, benefit financially, and contribute to a decentralized,\nscalable ecosystem that leverages collective AI to develop beneficial\nalgorithms.\n","authors":["Abraham Nash"],"pdf_url":"https://arxiv.org/pdf/2407.02461v2.pdf","comment":"14 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.02065v1","updated":"2024-08-04T15:30:15Z","published":"2024-08-04T15:30:15Z","title":"A Multi-class Ride-hailing Service Subsidy System Utilizing Deep Causal\n  Networks","summary":"  In the ride-hailing industry, subsidies are predominantly employed to\nincentivize consumers to place more orders, thereby fostering market growth.\nCausal inference techniques are employed to estimate the consumer elasticity\nwith different subsidy levels. However, the presence of confounding effects\nposes challenges in achieving an unbiased estimate of the uplift effect. We\nintroduce a consumer subsidizing system to capture relationships between\nsubsidy propensity and the treatment effect, which proves effective while\nmaintaining a lightweight online environment.\n","authors":["Zhe Yu","Chi Xia","Shaosheng Cao","Lin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02063v3","updated":"2024-08-04T15:16:27Z","published":"2024-05-03T12:48:21Z","title":"Few-sample Variational Inference of Bayesian Neural Networks with\n  Arbitrary Nonlinearities","summary":"  Bayesian Neural Networks (BNNs) extend traditional neural networks to provide\nuncertainties associated with their outputs. On the forward pass through a BNN,\npredictions (and their uncertainties) are made either by Monte Carlo sampling\nnetwork weights from the learned posterior or by analytically propagating\nstatistical moments through the network. Though flexible, Monte Carlo sampling\nis computationally expensive and can be infeasible or impractical under\nresource constraints or for large networks. While moment propagation can\nameliorate the computational costs of BNN inference, it can be difficult or\nimpossible for networks with arbitrary nonlinearities, thereby restricting the\npossible set of network layers permitted with such a scheme. In this work, we\ndemonstrate a simple yet effective approach for propagating statistical moments\nthrough arbitrary nonlinearities with only 3 deterministic samples, enabling\nfew-sample variational inference of BNNs without restricting the set of network\nlayers used. Furthermore, we leverage this approach to demonstrate a novel\nnonlinear activation function that we use to inject physics-informed prior\ninformation into output nodes of a BNN.\n","authors":["David J. Schodt"],"pdf_url":"https://arxiv.org/pdf/2405.02063v3.pdf","comment":"Comment 1: Fixed plot markers in figure 6 to match legend and to\n  improve grayscale appearance"},{"id":"http://arxiv.org/abs/2408.02056v1","updated":"2024-08-04T15:07:44Z","published":"2024-08-04T15:07:44Z","title":"MedSyn: LLM-based Synthetic Medical Text Generation Framework","summary":"  Generating synthetic text addresses the challenge of data availability in\nprivacy-sensitive domains such as healthcare. This study explores the\napplicability of synthetic data in real-world medical settings. We introduce\nMedSyn, a novel medical text generation framework that integrates large\nlanguage models with a Medical Knowledge Graph (MKG). We use MKG to sample\nprior medical information for the prompt and generate synthetic clinical notes\nwith GPT-4 and fine-tuned LLaMA models. We assess the benefit of synthetic data\nthrough application in the ICD code prediction task. Our research indicates\nthat synthetic data can increase the classification accuracy of vital and\nchallenging codes by up to 17.8% compared to settings without synthetic data.\nFurthermore, to provide new data for further research in the healthcare domain,\nwe present the largest open-source synthetic dataset of clinical notes for the\nRussian language, comprising over 41k samples covering 219 ICD-10 codes.\n","authors":["Gleb Kumichev","Pavel Blinov","Yulia Kuzkina","Vasily Goncharov","Galina Zubkova","Nikolai Zenovkin","Aleksei Goncharov","Andrey Savchenko"],"pdf_url":"https://arxiv.org/pdf/2408.02056v1.pdf","comment":"16 pages, accepted to ECML PKDD 2024"},{"id":"http://arxiv.org/abs/2408.02052v1","updated":"2024-08-04T15:00:22Z","published":"2024-08-04T15:00:22Z","title":"EOL: Transductive Few-Shot Open-Set Recognition by Enhancing Outlier\n  Logits","summary":"  In Few-Shot Learning (FSL), models are trained to recognise unseen objects\nfrom a query set, given a few labelled examples from a support set. In standard\nFSL, models are evaluated on query instances sampled from the same class\ndistribution of the support set. In this work, we explore the more nuanced and\npractical challenge of Open-Set Few-Shot Recognition (OSFSL). Unlike standard\nFSL, OSFSL incorporates unknown classes into the query set, thereby requiring\nthe model not only to classify known classes but also to identify outliers.\nBuilding on the groundwork laid by previous studies, we define a novel\ntransductive inference technique that leverages the InfoMax principle to\nexploit the unlabelled query set. We called our approach the Enhanced Outlier\nLogit (EOL) method. EOL refines class prototype representations through model\ncalibration, effectively balancing the inlier-outlier ratio. This calibration\nenhances pseudo-label accuracy for the query set and improves the optimisation\nobjective within the transductive inference process. We provide a comprehensive\nempirical evaluation demonstrating that EOL consistently surpasses traditional\nmethods, recording performance improvements ranging from approximately $+1.3%$\nto $+6.3%$ across a variety of classification and outlier detection metrics and\nbenchmarks, even in the presence of inlier-outlier imbalance.\n","authors":["Mateusz Ochal","Massimiliano Patacchiola","Malik Boudiaf","Sen Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02052v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2408.02050v1","updated":"2024-08-04T14:57:44Z","published":"2024-08-04T14:57:44Z","title":"Recovering the state and dynamics of autonomous system with partial\n  states solution using neural networks","summary":"  In this paper we explore the performance of deep hidden physics model (M.\nRaissi 2018) for autonomous system, this systems do not explicitly depend on\ntime. The dynamics of states are dependent on states itself. Such systems can\nbe found in nature and have applications\n  in modeling chemical concentrations, population dynamics, n-body problems in\nphysics etc. In this work we are going to see how we can obtain dynamics of\nstates based on solution of limited partial states. The proposed method can\nfind the state and dynamics of which the data is provided in the training,\nalthough we do not claim to accurately find the solution of states whose data\nis not utilized while training.\n","authors":["Vijay Kag"],"pdf_url":"https://arxiv.org/pdf/2408.02050v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07769v3","updated":"2024-08-04T14:46:14Z","published":"2024-01-15T15:27:24Z","title":"Deep Evolutional Instant Interest Network for CTR Prediction in\n  Trigger-Induced Recommendation","summary":"  The recommendation has been playing a key role in many industries, e.g.,\ne-commerce, streaming media, social media, etc. Recently, a new recommendation\nscenario, called Trigger-Induced Recommendation (TIR), where users are able to\nexplicitly express their instant interests via trigger items, is emerging as an\nessential role in many e-commerce platforms, e.g., Alibaba.com and Amazon.\nWithout explicitly modeling the user's instant interest, traditional\nrecommendation methods usually obtain sub-optimal results in TIR. Even though\nthere are a few methods considering the trigger and target items simultaneously\nto solve this problem, they still haven't taken into account temporal\ninformation of user behaviors, the dynamic change of user instant interest when\nthe user scrolls down and the interactions between the trigger and target\nitems. To tackle these problems, we propose a novel method -- Deep Evolutional\nInstant Interest Network (DEI2N), for click-through rate prediction in TIR\nscenarios. Specifically, we design a User Instant Interest Modeling Layer to\npredict the dynamic change of the intensity of instant interest when the user\nscrolls down. Temporal information is utilized in user behavior modeling.\nMoreover, an Interaction Layer is introduced to learn better interactions\nbetween the trigger and target items. We evaluate our method on several offline\nand real-world industrial datasets. Experimental results show that our proposed\nDEI2N outperforms state-of-the-art baselines. In addition, online A/B testing\ndemonstrates the superiority over the existing baseline in real-world\nproduction environments.\n","authors":["Zhibo Xiao","Luwei Yang","Tao Zhang","Wen Jiang","Wei Ning","Yujiu Yang"],"pdf_url":"https://arxiv.org/pdf/2401.07769v3.pdf","comment":"7 pages, 6 figures, accepted by the 17th ACM International Conference\n  on Web Search and Data Mining(WSDM'2024)"},{"id":"http://arxiv.org/abs/2408.02045v1","updated":"2024-08-04T14:45:26Z","published":"2024-08-04T14:45:26Z","title":"DNA-SE: Towards Deep Neural-Nets Assisted Semiparametric Estimation","summary":"  Semiparametric statistics play a pivotal role in a wide range of domains,\nincluding but not limited to missing data, causal inference, and transfer\nlearning, to name a few. In many settings, semiparametric theory leads to\n(nearly) statistically optimal procedures that yet involve numerically solving\nFredholm integral equations of the second kind. Traditional numerical methods,\nsuch as polynomial or spline approximations, are difficult to scale to\nmulti-dimensional problems. Alternatively, statisticians may choose to\napproximate the original integral equations by ones with closed-form solutions,\nresulting in computationally more efficient, but statistically suboptimal or\neven incorrect procedures. To bridge this gap, we propose a novel framework by\nformulating the semiparametric estimation problem as a bi-level optimization\nproblem; and then we develop a scalable algorithm called Deep Neural-Nets\nAssisted Semiparametric Estimation (DNA-SE) by leveraging the universal\napproximation property of Deep Neural-Nets (DNN) to streamline semiparametric\nprocedures. Through extensive numerical experiments and a real data analysis,\nwe demonstrate the numerical and statistical advantages of $\\dnase$ over\ntraditional methods. To the best of our knowledge, we are the first to bring\nDNN into semiparametric statistics as a numerical solver of integral equations\nin our proposed general framework.\n","authors":["Qinshuo Liu","Zixin Wang","Xi-An Li","Xinyao Ji","Lei Zhang","Lin Liu","Zhonghua Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02045v1.pdf","comment":"semiparametric statistics, missing data, causal inference, Fredholm\n  integral equations of the second kind, bi-level optimization, deep learning,\n  AI for science"},{"id":"http://arxiv.org/abs/2408.02033v1","updated":"2024-08-04T13:51:18Z","published":"2024-08-04T13:51:18Z","title":"Enhancing Human Action Recognition and Violence Detection Through Deep\n  Learning Audiovisual Fusion","summary":"  This paper proposes a hybrid fusion-based deep learning approach based on two\ndifferent modalities, audio and video, to improve human activity recognition\nand violence detection in public places. To take advantage of audiovisual\nfusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning\n(HFBDL) are used and compared. Since the objective is to detect and recognize\nhuman violence in public places, Real-life violence situation (RLVS) dataset is\nexpanded and used. Simulating results of HFBDL show 96.67\\% accuracy on\nvalidation data, which is more accurate than the other state-of-the-art methods\non this dataset. To showcase our model's ability in real-world scenarios,\nanother dataset of 54 sounded videos of both violent and non-violent situations\nwas recorded. The model could successfully detect 52 out of 54 videos\ncorrectly. The proposed method shows a promising performance on real scenarios.\nThus, it can be used for human action recognition and violence detection in\npublic places for security purposes.\n","authors":["Pooya Janani","Amirabolfazl Suratgar","Afshin Taghvaeipour"],"pdf_url":"https://arxiv.org/pdf/2408.02033v1.pdf","comment":"This work has been submitted to the IEEE for possible publication, 10\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2408.02022v1","updated":"2024-08-04T13:19:45Z","published":"2024-08-04T13:19:45Z","title":"Scenario-based Thermal Management Parametrization Through Deep\n  Reinforcement Learning","summary":"  The thermal system of battery electric vehicles demands advanced control. Its\nthermal management needs to effectively control active components across\nvarying operating conditions. While robust control function parametrization is\nrequired, current methodologies show significant drawbacks. They consume\nconsiderable time, human effort, and extensive real-world testing.\nConsequently, there is a need for innovative and intelligent solutions that are\ncapable of autonomously parametrizing embedded controllers. Addressing this\nissue, our paper introduces a learning-based tuning approach. We propose a\nmethodology that benefits from automated scenario generation for increased\nrobustness across vehicle usage scenarios. Our deep reinforcement learning\nagent processes the tuning task context and incorporates an image-based\ninterpretation of embedded parameter sets. We demonstrate its applicability to\na valve controller parametrization task and verify it in real-world vehicle\ntesting. The results highlight the competitive performance to baseline methods.\nThis novel approach contributes to the shift towards virtual development of\nthermal management functions, with promising potential of large-scale parameter\ntuning in the automotive industry.\n","authors":["Thomas Rudolf","Philip Muhl","Sren Hohmann","Lutz Eckstein"],"pdf_url":"https://arxiv.org/pdf/2408.02022v1.pdf","comment":"8 pages, 7 figures, 2 tables, 1 algorithm, 10 equations, conference"},{"id":"http://arxiv.org/abs/2408.02019v1","updated":"2024-08-04T13:11:49Z","published":"2024-08-04T13:11:49Z","title":"Personalized Federated Learning on Heterogeneous and Long-Tailed Data\n  via Expert Collaborative Learning","summary":"  Personalized Federated Learning (PFL) aims to acquire customized models for\neach client without disclosing raw data by leveraging the collective knowledge\nof distributed clients. However, the data collected in real-world scenarios is\nlikely to follow a long-tailed distribution. For example, in the medical\ndomain, it is more common for the number of general health notes to be much\nlarger than those specifically relatedto certain diseases. The presence of\nlong-tailed data can significantly degrade the performance of PFL models.\nAdditionally, due to the diverse environments in which each client operates,\ndata heterogeneity is also a classic challenge in federated learning. In this\npaper, we explore the joint problem of global long-tailed distribution and data\nheterogeneity in PFL and propose a method called Expert Collaborative Learning\n(ECL) to tackle this problem. Specifically, each client has multiple experts,\nand each expert has a different training subset, which ensures that each class,\nespecially the minority classes, receives sufficient training. Multiple experts\ncollaborate synergistically to produce the final prediction output. Without\nspecial bells and whistles, the vanilla ECL outperforms other state-of-the-art\nPFL methods on several benchmark datasets under different degrees of data\nheterogeneity and long-tailed distribution.\n","authors":["Fengling Lv","Xinyi Shang","Yang Zhou","Yiqun Zhang","Mengke Li","Yang Lu"],"pdf_url":"https://arxiv.org/pdf/2408.02019v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02014v1","updated":"2024-08-04T12:52:44Z","published":"2024-08-04T12:52:44Z","title":"Unsupervised Representation Learning by Balanced Self Attention Matching","summary":"  Many leading self-supervised methods for unsupervised representation\nlearning, in particular those for embedding image features, are built on\nvariants of the instance discrimination task, whose optimization is known to be\nprone to instabilities that can lead to feature collapse. Different techniques\nhave been devised to circumvent this issue, including the use of negative pairs\nwith different contrastive losses, the use of external memory banks, and\nbreaking of symmetry by using separate encoding networks with possibly\ndifferent structures. Our method, termed BAM, rather than directly matching\nfeatures of different views (augmentations) of input images, is based on\nmatching their self-attention vectors, which are the distributions of\nsimilarities to the entire set of augmented images of a batch. We obtain rich\nrepresentations and avoid feature collapse by minimizing a loss that matches\nthese distributions to their globally balanced and entropy regularized version,\nwhich is obtained through a simple self-optimal-transport computation. We\nablate and verify our method through a wide set of experiments that show\ncompetitive performance with leading methods on both semi-supervised and\ntransfer-learning benchmarks. Our implementation and pre-trained models are\navailable at github.com/DanielShalam/BAM .\n","authors":["Daniel Shalam","Simon Korman"],"pdf_url":"https://arxiv.org/pdf/2408.02014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01249v2","updated":"2024-08-04T12:34:29Z","published":"2023-09-03T19:24:34Z","title":"Large AI Model Empowered Multimodal Semantic Communications","summary":"  Multimodal signals, including text, audio, image, and video, can be\nintegrated into Semantic Communication (SC) systems to provide an immersive\nexperience with low latency and high quality at the semantic level. However,\nthe multimodal SC has several challenges, including data heterogeneity,\nsemantic ambiguity, and signal distortion during transmission. Recent\nadvancements in large AI models, particularly in the Multimodal Language Model\n(MLM) and Large Language Model (LLM), offer potential solutions for addressing\nthese issues. To this end, we propose a Large AI Model-based Multimodal SC\n(LAM-MSC) framework, where we first present the MLM-based Multimodal Alignment\n(MMA) that utilizes the MLM to enable the transformation between multimodal and\nunimodal data while preserving semantic consistency. Then, a personalized\nLLM-based Knowledge Base (LKB) is proposed, which allows users to perform\npersonalized semantic extraction or recovery through the LLM. This effectively\naddresses the semantic ambiguity. Finally, we apply the Conditional Generative\nadversarial network-based channel Estimation (CGE) for estimating the wireless\nchannel state information. This approach effectively mitigates the impact of\nfading channels in SC. Finally, we conduct simulations that demonstrate the\nsuperior performance of the LAM-MSC framework.\n","authors":["Feibo Jiang","Li Dong","Yubo Peng","Kezhi Wang","Kun Yang","Cunhua Pan","Xiaohu You"],"pdf_url":"https://arxiv.org/pdf/2309.01249v2.pdf","comment":"Accepted by IEEE CM"},{"id":"http://arxiv.org/abs/2312.16019v3","updated":"2024-08-04T12:12:36Z","published":"2023-12-26T12:18:31Z","title":"Robust Survival Analysis with Adversarial Regularization","summary":"  Survival Analysis (SA) models the time until an event occurs, with\napplications in fields like medicine, defense, finance, and aerospace. Recent\nwork shows that Neural Networks (NNs) can capture complex relationships in SA.\nHowever, dataset uncertainties (e.g., noisy measurements, human error) can\ndegrade model performance. To address this, we leverage NN verification\nadvances to create algorithms for robust, fully-parametric survival models. We\nintroduce a robust loss function and use CROWN-IBP regularization to handle\ncomputational challenges in the Min-Max problem. Evaluating our approach on\nSurvSet datasets, we find that our Survival Analysis with Adversarial\nRegularization (SAWAR) method consistently outperforms baselines under various\nperturbations with respect to Negative Log Likelihood (NegLL), Integrated Brier\nScore (IBS), and Concordance Index (CI). This demonstrates that adversarial\nregularization enhances SA performance and calibration, mitigating data\nuncertainty and improving generalization across diverse datasets up to 150%\nacross all perturbation magnitudes.\n","authors":["Michael Potter","Stefano Maxenti","Michael Everett"],"pdf_url":"https://arxiv.org/pdf/2312.16019v3.pdf","comment":"12 pages, 2 figures, submission to IEEE Journal of Biomedical and\n  Health Informatics"},{"id":"http://arxiv.org/abs/2408.01993v1","updated":"2024-08-04T11:25:07Z","published":"2024-08-04T11:25:07Z","title":"Towards Automatic Hands-on-Keyboard Attack Detection Using LLMs in EDR\n  Solutions","summary":"  Endpoint Detection and Remediation (EDR) platforms are essential for\nidentifying and responding to cyber threats. This study presents a novel\napproach using Large Language Models (LLMs) to detect Hands-on-Keyboard (HOK)\ncyberattacks. Our method involves converting endpoint activity data into\nnarrative forms that LLMs can analyze to distinguish between normal operations\nand potential HOK attacks. We address the challenges of interpreting endpoint\ndata by segmenting narratives into windows and employing a dual training\nstrategy. The results demonstrate that LLM-based models have the potential to\noutperform traditional machine learning methods, offering a promising direction\nfor enhancing EDR capabilities and apply LLMs in cybersecurity.\n","authors":["Amit Portnoy","Ehud Azikri","Shay Kels"],"pdf_url":"https://arxiv.org/pdf/2408.01993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01988v1","updated":"2024-08-04T11:00:43Z","published":"2024-08-04T11:00:43Z","title":"MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few\n  Shots","summary":"  Wearable systems provide continuous health monitoring and can lead to early\ndetection of potential health issues. However, the lifecycle of wearable\nsystems faces several challenges. First, effective model training for new\nwearable devices requires substantial labeled data from various subjects\ncollected directly by the wearable. Second, subsequent model updates require\nfurther extensive labeled data for retraining. Finally, frequent model updating\non the wearable device can decrease the battery life in long-term data\nmonitoring. Addressing these challenges, in this paper, we propose MetaWearS, a\nmeta-learning method to reduce the amount of initial data collection required.\nMoreover, our approach incorporates a prototypical updating mechanism,\nsimplifying the update process by modifying the class prototype rather than\nretraining the entire model. We explore the performance of MetaWearS in two\ncase studies, namely, the detection of epileptic seizures and the detection of\natrial fibrillation. We show that by fine-tuning with just a few samples, we\nachieve 70% and 82% AUC for the detection of epileptic seizures and the\ndetection of atrial fibrillation, respectively. Compared to a conventional\napproach, our proposed method performs better with up to 45% AUC. Furthermore,\nupdating the model with only 16 minutes of additional labeled data increases\nthe AUC by up to 5.3%. Finally, MetaWearS reduces the energy consumption for\nmodel updates by 456x and 418x for epileptic seizure and AF detection,\nrespectively.\n","authors":["Alireza Amirshahi","Maedeh H. Toosi","Siamak Mohammadi","Stefano Albini","Pasquale Davide Schiavone","Giovanni Ansaloni","Amir Aminifar","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2408.01988v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.08442v2","updated":"2024-08-04T10:42:42Z","published":"2022-06-16T20:48:19Z","title":"A Look at Value-Based Decision-Time vs. Background Planning Methods\n  Across Different Settings","summary":"  In model-based reinforcement learning (RL), an agent can leverage a learned\nmodel to improve its way of behaving in different ways. Two of the prevalent\nways to do this are through decision-time and background planning methods. In\nthis study, we are interested in understanding how the value-based versions of\nthese two planning methods will compare against each other across different\nsettings. Towards this goal, we first consider the simplest instantiations of\nvalue-based decision-time and background planning methods and provide\ntheoretical results on which one will perform better in the regular RL and\ntransfer learning settings. Then, we consider the modern instantiations of them\nand provide hypotheses on which one will perform better in the same settings.\nFinally, we perform illustrative experiments to validate these theoretical\nresults and hypotheses. Overall, our findings suggest that even though\nvalue-based versions of the two planning methods perform on par in their\nsimplest instantiations, the modern instantiations of value-based decision-time\nplanning methods can perform on par or better than the modern instantiations of\nvalue-based background planning methods in both the regular RL and transfer\nlearning settings.\n","authors":["Safa Alver","Doina Precup"],"pdf_url":"https://arxiv.org/pdf/2206.08442v2.pdf","comment":"Accepted to EWRL 2024"},{"id":"http://arxiv.org/abs/2311.13355v2","updated":"2024-08-04T10:31:41Z","published":"2023-11-22T12:47:12Z","title":"Unified Classification and Rejection: A One-versus-All Framework","summary":"  Classifying patterns of known classes and rejecting ambiguous and novel (also\ncalled as out-of-distribution (OOD)) inputs are involved in open world pattern\nrecognition. Deep neural network models usually excel in closed-set\nclassification while performs poorly in rejecting OOD inputs. To tackle this\nproblem, numerous methods have been designed to perform open set recognition\n(OSR) or OOD rejection/detection tasks. Previous methods mostly take\npost-training score transformation or hybrid models to ensure low scores on OOD\ninputs while separating known classes. In this paper, we attempt to build a\nunified framework for building open set classifiers for both classification and\nOOD rejection. We formulate the open set recognition of $ K $-known-class as a\n$ (K+1) $-class classification problem with model trained on known-class\nsamples only. By decomposing the $ K $-class problem into $ K $ one-versus-all\n(OVA) binary classification tasks and binding some parameters, we show that\ncombining the scores of OVA classifiers can give $ (K+1) $-class posterior\nprobabilities, which enables classification and OOD rejection in a unified\nframework. To maintain the closed-set classification accuracy of the OVA\ntrained classifier, we propose a hybrid training strategy combining OVA loss\nand multi-class cross-entropy loss. We implement the OVA framework and hybrid\ntraining strategy on the recently proposed convolutional prototype network and\nprototype classifier on vision transformer (ViT) backbone. Experiments on\npopular OSR and OOD detection datasets demonstrate that the proposed framework,\nusing a single multi-class classifier, yields competitive performance in\nclosed-set classification, OOD detection, and misclassification detection.\n","authors":["Zhen Cheng","Xu-Yao Zhang","Cheng-Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2311.13355v2.pdf","comment":"Published in Machine Intelligence Research\n  (https://link.springer.com/article/10.1007/s11633-024-1514-4)"},{"id":"http://arxiv.org/abs/2408.01981v1","updated":"2024-08-04T10:16:11Z","published":"2024-08-04T10:16:11Z","title":"Multiview learning with twin parametric margin SVM","summary":"  Multiview learning (MVL) seeks to leverage the benefits of diverse\nperspectives to complement each other, effectively extracting and utilizing the\nlatent information within the dataset. Several twin support vector\nmachine-based MVL (MvTSVM) models have been introduced and demonstrated\noutstanding performance in various learning tasks. However, MvTSVM-based models\nface significant challenges in the form of computational complexity due to four\nmatrix inversions, the need to reformulate optimization problems in order to\nemploy kernel-generated surfaces for handling non-linear cases, and the\nconstraint of uniform noise assumption in the training data. Particularly in\ncases where the data possesses a heteroscedastic error structure, these\nchallenges become even more pronounced. In view of the aforementioned\nchallenges, we propose multiview twin parametric margin support vector machine\n(MvTPMSVM). MvTPMSVM constructs parametric hyperplanes with the goal of\nmaximizing the parametric margin between two classes, aiming to regulate and\nmanage the impact of the heteroscedastic noise structure existing within the\ndata. The proposed MvTPMSVM model avoids the explicit computation of matrix\ninversions in the dual formulation, leading to enhanced computational\nefficiency. We perform an extensive assessment of the MvTPMSVM model using\nbenchmark datasets such as UCI, KEEL, synthetic, and Animals with Attributes\n(AwA). Our experimental results, coupled with rigorous statistical analyses,\nconfirm the superior generalization capabilities of the proposed MvTPMSVM model\ncompared to the baseline models. The source code of the proposed MvTPMSVM model\nis available at \\url{https://github.com/mtanveer1/MvTPMSVM}.\n","authors":["A. Quadir","M. Tanveer"],"pdf_url":"https://arxiv.org/pdf/2408.01981v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01979v1","updated":"2024-08-04T09:53:57Z","published":"2024-08-04T09:53:57Z","title":"Shaping Rewards, Shaping Routes: On Multi-Agent Deep Q-Networks for\n  Routing in Satellite Constellation Networks","summary":"  Effective routing in satellite mega-constellations has become crucial to\nfacilitate the handling of increasing traffic loads, more complex network\narchitectures, as well as the integration into 6G networks. To enhance\nadaptability as well as robustness to unpredictable traffic demands, and to\nsolve dynamic routing environments efficiently, machine learning-based\nsolutions are being considered. For network control problems, such as\noptimizing packet forwarding decisions according to Quality of Service\nrequirements and maintaining network stability, deep reinforcement learning\ntechniques have demonstrated promising results. For this reason, we investigate\nthe viability of multi-agent deep Q-networks for routing in satellite\nconstellation networks. We focus specifically on reward shaping and quantifying\ntraining convergence for joint optimization of latency and load balancing in\nstatic and dynamic scenarios. To address identified drawbacks, we propose a\nnovel hybrid solution based on centralized learning and decentralized control.\n","authors":["Manuel M. H. Roth","Anupama Hegde","Thomas Delamotte","Andreas Knopp"],"pdf_url":"https://arxiv.org/pdf/2408.01979v1.pdf","comment":"5 pages, 5 figures, to be published in proceedings of European Space\n  Agency SPAICE Conference 2024, https://spaice.esa.int/"},{"id":"http://arxiv.org/abs/2408.01972v1","updated":"2024-08-04T09:26:00Z","published":"2024-08-04T09:26:00Z","title":"RVI-SAC: Average Reward Off-Policy Deep Reinforcement Learning","summary":"  In this paper, we propose an off-policy deep reinforcement learning (DRL)\nmethod utilizing the average reward criterion. While most existing DRL methods\nemploy the discounted reward criterion, this can potentially lead to a\ndiscrepancy between the training objective and performance metrics in\ncontinuing tasks, making the average reward criterion a recommended\nalternative. We introduce RVI-SAC, an extension of the state-of-the-art\noff-policy DRL method, Soft Actor-Critic (SAC), to the average reward\ncriterion. Our proposal consists of (1) Critic updates based on RVI Q-learning,\n(2) Actor updates introduced by the average reward soft policy improvement\ntheorem, and (3) automatic adjustment of Reset Cost enabling the average reward\nreinforcement learning to be applied to tasks with termination. We apply our\nmethod to the Gymnasium's Mujoco tasks, a subset of locomotion tasks, and\ndemonstrate that RVI-SAC shows competitive performance compared to existing\nmethods.\n","authors":["Yukinari Hisaki","Isao Ono"],"pdf_url":"https://arxiv.org/pdf/2408.01972v1.pdf","comment":"Accepted at ICML 2024; Code:\n  https://github.com/yhisaki/average-reward-drl"},{"id":"http://arxiv.org/abs/2408.01967v1","updated":"2024-08-04T09:08:55Z","published":"2024-08-04T09:08:55Z","title":"A multi-task deep learning approach for lane-level pavement performance\n  prediction with segment-level data","summary":"  The elaborate pavement performance prediction is an important premise of\nimplementing preventive maintenance. Our survey reveals that in practice, the\npavement performance is usually measured at segment-level, where an unique\nperformance value is obtained for all lanes within one segment of 1km length.\nIt still lacks more elaborate performance analysis at lane-level due to costly\ndata collection and difficulty in prediction modeling. Therefore, this study\ndeveloped a multi-task deep learning approach to predict the lane-level\npavement performance with a large amount of historical segment-level\nperformance measurement data. The unified prediction framework can effectively\naddress inherent correlation and differences across lanes. In specific, the\nprediction framework firstly employed an Long Short-Term Memory (LSTM) layer to\ncapture the segment-level pavement deterioration pattern. Then multiple\ntask-specific LSTM layers were designed based on number of lanes to capture\nlane-level differences in pavement performance. Finally, we concatenated\nmultiple task-specific LSTM outputs with auxiliary features for further\ntraining and obtained the lane-level predictions after fully connected layer.\nThe aforementioned prediction framework was validated with a real case in\nChina. It revealed a better model performance regardless of one-way 2-lane,\n3-lane, and 4-lane scenarios, all lower than 10% in terms of mean absolute\npercentage error. The proposed prediction framework also outperforms other\nensemble learning and shallow machine learning methods in almost every lane.\n","authors":["Bo Wang","Wenbo Zhang","Yunpeng LI"],"pdf_url":"https://arxiv.org/pdf/2408.01967v1.pdf","comment":"24 pages, 8 figures, 4 tables"},{"id":"http://arxiv.org/abs/2407.18569v3","updated":"2024-08-04T09:01:00Z","published":"2024-07-26T07:51:11Z","title":"PP-TIL: Personalized Planning for Autonomous Driving with Instance-based\n  Transfer Imitation Learning","summary":"  Personalized motion planning holds significant importance within urban\nautomated driving, catering to the unique requirements of individual users.\nNevertheless, prior endeavors have frequently encountered difficulties in\nsimultaneously addressing two crucial aspects: personalized planning within\nintricate urban settings and enhancing planning performance through data\nutilization. The challenge arises from the expensive and limited nature of user\ndata, coupled with the scene state space tending towards infinity. These\nfactors contribute to overfitting and poor generalization problems during model\ntraining. Henceforth, we propose an instance-based transfer imitation learning\napproach. This method facilitates knowledge transfer from extensive expert\ndomain data to the user domain, presenting a fundamental resolution to these\nissues. We initially train a pre-trained model using large-scale expert data.\nSubsequently, during the fine-tuning phase, we feed the batch data, which\ncomprises expert and user data. Employing the inverse reinforcement learning\ntechnique, we extract the style feature distribution from user demonstrations,\nconstructing the regularization term for the approximation of user style. In\nour experiments, we conducted extensive evaluations of the proposed method.\nCompared to the baseline methods, our approach mitigates the overfitting issue\ncaused by sparse user data. Furthermore, we discovered that integrating the\ndriving model with a differentiable nonlinear optimizer as a safety protection\nlayer for end-to-end personalized fine-tuning results in superior planning\nperformance.\n","authors":["Fangze Lin","Ying He","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2407.18569v3.pdf","comment":"IROS 2024 Accepted"},{"id":"http://arxiv.org/abs/2306.04802v4","updated":"2024-08-04T08:53:23Z","published":"2023-06-07T21:51:56Z","title":"A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises","summary":"  Healthcare knowledge graphs (HKGs) are valuable tools for organizing\nbiomedical concepts and their relationships with interpretable structures. The\nrecent advent of large language models (LLMs) has paved the way for building\nmore comprehensive and accurate HKGs. This, in turn, can improve the\nreliability of generated content and enable better evaluation of LLMs. However,\nthe challenges of HKGs such as regarding data heterogeneity and limited\ncoverage are not fully understood, highlighting the need for detailed reviews.\nThis work provides the first comprehensive review of HKGs. It summarizes the\npipeline and key techniques for HKG construction, as well as the common\nutilization approaches, i.e., model-free and model-based. The existing HKG\nresources are also organized based on the data types they capture and\napplication domains they cover, along with relevant statistical information\n(Resource available at\nhttps://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase). At the\napplication level, we delve into the successful integration of HKGs across\nvarious health domains, ranging from fine-grained basic science research to\nhigh-level clinical decision support and public health. Lastly, the paper\nhighlights the opportunities for HKGs in the era of LLMs. This work aims to\nserve as a valuable resource for understanding the potential and opportunities\nof HKG in health research.\n","authors":["Carl Yang","Hejie Cui","Jiaying Lu","Shiyu Wang","Ran Xu","Wenjing Ma","Yue Yu","Shaojun Yu","Xuan Kan","Chen Ling","Tianfan Fu","Liang Zhao","Joyce Ho","Fei Wang"],"pdf_url":"https://arxiv.org/pdf/2306.04802v4.pdf","comment":"21 pages, preprint submitted to ACM"},{"id":"http://arxiv.org/abs/2408.01964v1","updated":"2024-08-04T08:44:00Z","published":"2024-08-04T08:44:00Z","title":"Top K Enhanced Reinforcement Learning Attacks on Heterogeneous Graph\n  Node Classification","summary":"  Graph Neural Networks (GNNs) have attracted substantial interest due to their\nexceptional performance on graph-based data. However, their robustness,\nespecially on heterogeneous graphs, remains underexplored, particularly against\nadversarial attacks. This paper proposes HeteroKRLAttack, a targeted evasion\nblack-box attack method for heterogeneous graphs. By integrating reinforcement\nlearning with a Top-K algorithm to reduce the action space, our method\nefficiently identifies effective attack strategies to disrupt node\nclassification tasks. We validate the effectiveness of HeteroKRLAttack through\nexperiments on multiple heterogeneous graph datasets, showing significant\nreductions in classification accuracy compared to baseline methods. An ablation\nstudy underscores the critical role of the Top-K algorithm in enhancing attack\nperformance. Our findings highlight potential vulnerabilities in current models\nand provide guidance for future defense strategies against adversarial attacks\non heterogeneous graphs.\n","authors":["Honglin Gao","Gaoxi Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.01964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16364v2","updated":"2024-08-04T08:36:08Z","published":"2024-02-26T07:33:28Z","title":"Where Do We Go from Here? Multi-scale Allocentric Relational Inference\n  from Natural Spatial Descriptions","summary":"  When communicating routes in natural language, the concept of acquired\nspatial knowledge is crucial for geographic information retrieval (GIR) and in\nspatial cognitive research. However, NLP navigation studies often overlook the\nimpact of such acquired knowledge on textual descriptions. Current navigation\nstudies concentrate on egocentric local descriptions (e.g., `it will be on your\nright') that require reasoning over the agent's local perception. These\ninstructions are typically given as a sequence of steps, with each action-step\nexplicitly mentioning and being followed by a landmark that the agent can use\nto verify they are on the right path (e.g., `turn right and then you will\nsee...'). In contrast, descriptions based on knowledge acquired through a map\nprovide a complete view of the environment and capture its overall structure.\nThese instructions (e.g., `it is south of Central Park and a block north of a\npolice station') are typically non-sequential, contain allocentric relations,\nwith multiple spatial relations and implicit actions, without any explicit\nverification. This paper introduces the Rendezvous (RVS) task and dataset,\nwhich includes 10,404 examples of English geospatial instructions for reaching\na target location using map-knowledge. Our analysis reveals that RVS exhibits a\nricher use of spatial allocentric relations, and requires resolving more\nspatial relations simultaneously compared to previous text-based navigation\nbenchmarks.\n","authors":["Tzuf Paz-Argaman","Sayali Kulkarni","John Palowitch","Jason Baldridge","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2402.16364v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01961v1","updated":"2024-08-04T08:35:02Z","published":"2024-08-04T08:35:02Z","title":"Representation Bias of Adolescents in AI: A Bilingual, Bicultural Study","summary":"  Popular and news media often portray teenagers with sensationalism, as both a\nrisk to society and at risk from society. As AI begins to absorb some of the\nepistemic functions of traditional media, we study how teenagers in two\ncountries speaking two languages: 1) are depicted by AI, and 2) how they would\nprefer to be depicted. Specifically, we study the biases about teenagers\nlearned by static word embeddings (SWEs) and generative language models (GLMs),\ncomparing these with the perspectives of adolescents living in the U.S. and\nNepal. We find English-language SWEs associate teenagers with societal\nproblems, and more than 50% of the 1,000 words most associated with teenagers\nin the pretrained GloVe SWE reflect such problems. Given prompts about\nteenagers, 30% of outputs from GPT2-XL and 29% from LLaMA-2-7B GLMs discuss\nsocietal problems, most commonly violence, but also drug use, mental illness,\nand sexual taboo. Nepali models, while not free of such associations, are less\ndominated by social problems. Data from workshops with N=13 U.S. adolescents\nand N=18 Nepalese adolescents show that AI presentations are disconnected from\nteenage life, which revolves around activities like school and friendship.\nParticipant ratings of how well 20 trait words describe teens are decorrelated\nfrom SWE associations, with Pearson's r=.02, n.s. in English FastText and\nr=.06, n.s. in GloVe; and r=.06, n.s. in Nepali FastText and r=-.23, n.s. in\nGloVe. U.S. participants suggested AI could fairly present teens by\nhighlighting diversity, while Nepalese participants centered positivity.\nParticipants were optimistic that, if it learned from adolescents, rather than\nmedia sources, AI could help mitigate stereotypes. Our work offers an\nunderstanding of the ways SWEs and GLMs misrepresent a developmentally\nvulnerable group and provides a template for less sensationalized\ncharacterization.\n","authors":["Robert Wolfe","Aayushi Dangol","Bill Howe","Alexis Hiniker"],"pdf_url":"https://arxiv.org/pdf/2408.01961v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01959v1","updated":"2024-08-04T08:26:58Z","published":"2024-08-04T08:26:58Z","title":"Dataset Scale and Societal Consistency Mediate Facial Impression Bias in\n  Vision-Language AI","summary":"  Multimodal AI models capable of associating images and text hold promise for\nnumerous domains, ranging from automated image captioning to accessibility\napplications for blind and low-vision users. However, uncertainty about bias\nhas in some cases limited their adoption and availability. In the present work,\nwe study 43 CLIP vision-language models to determine whether they learn\nhuman-like facial impression biases, and we find evidence that such biases are\nreflected across three distinct CLIP model families. We show for the first time\nthat the the degree to which a bias is shared across a society predicts the\ndegree to which it is reflected in a CLIP model. Human-like impressions of\nvisually unobservable attributes, like trustworthiness and sexuality, emerge\nonly in models trained on the largest dataset, indicating that a better fit to\nuncurated cultural data results in the reproduction of increasingly subtle\nsocial biases. Moreover, we use a hierarchical clustering approach to show that\ndataset size predicts the extent to which the underlying structure of facial\nimpression bias resembles that of facial impression bias in humans. Finally, we\nshow that Stable Diffusion models employing CLIP as a text encoder learn facial\nimpression biases, and that these biases intersect with racial biases in Stable\nDiffusion XL-Turbo. While pretrained CLIP models may prove useful for\nscientific studies of bias, they will also require significant dataset curation\nwhen intended for use as general-purpose models in a zero-shot setting.\n","authors":["Robert Wolfe","Aayushi Dangol","Alexis Hiniker","Bill Howe"],"pdf_url":"https://arxiv.org/pdf/2408.01959v1.pdf","comment":"Accepted at Artificial Intelligence, Ethics, and Society 2024"},{"id":"http://arxiv.org/abs/2408.01953v1","updated":"2024-08-04T07:59:17Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v1.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2407.10768v5","updated":"2024-08-04T07:53:03Z","published":"2024-07-15T14:50:15Z","title":"ISMRNN: An Implicitly Segmented RNN Method with Mamba for Long-Term Time\n  Series Forecasting","summary":"  Long time series forecasting aims to utilize historical information to\nforecast future states over extended horizons. Traditional RNN-based series\nforecasting methods struggle to effectively address long-term dependencies and\ngradient issues in long time series problems. Recently, SegRNN has emerged as a\nleading RNN-based model tailored for long-term series forecasting,\ndemonstrating state-of-the-art performance while maintaining a streamlined\narchitecture through innovative segmentation and parallel decoding techniques.\nNevertheless, SegRNN has several limitations: its fixed segmentation disrupts\ndata continuity and fails to effectively leverage information across different\nsegments, the segmentation strategy employed by SegRNN does not fundamentally\naddress the issue of information loss within the recurrent structure. To\naddress these issues, we propose the ISMRNN method with three key enhancements:\nwe introduce an implicit segmentation structure to decompose the time series\nand map it to segmented hidden states, resulting in denser information exchange\nduring the segmentation phase. Additionally, we incorporate residual structures\nin the encoding layer to mitigate information loss within the recurrent\nstructure. To extract information more effectively, we further integrate the\nMamba architecture to enhance time series information extraction. Experiments\non several real-world long time series forecasting datasets demonstrate that\nour model surpasses the performance of current state-of-the-art models.\n","authors":["GaoXiang Zhao","Li Zhou","XiaoQiang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.10768v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11060v2","updated":"2024-08-04T05:59:13Z","published":"2023-10-17T08:06:08Z","title":"Privacy-Preserving Graph Embedding based on Local Differential Privacy","summary":"  Graph embedding has become a powerful tool for learning latent\nrepresentations of nodes in a graph. Despite its superior performance in\nvarious graph-based machine learning tasks, serious privacy concerns arise when\nthe graph data contains personal or sensitive information. To address this\nissue, we investigate and develop graph embedding algorithms that satisfy local\ndifferential privacy (LDP). We introduce a novel privacy-preserving graph\nembedding framework, named PrivGE, to protect node data privacy. Specifically,\nwe propose an LDP mechanism to obfuscate node data and utilize personalized\nPageRank as the proximity measure to learn node representations. Furthermore,\nwe provide a theoretical analysis of the privacy guarantees and utility offered\nby the PrivGE framework. Extensive experiments on several real-world graph\ndatasets demonstrate that PrivGE achieves an optimal balance between privacy\nand utility, and significantly outperforms existing methods in node\nclassification and link prediction tasks.\n","authors":["Zening Li","Rong-Hua Li","Meihao Liao","Fusheng Jin","Guoren Wang"],"pdf_url":"https://arxiv.org/pdf/2310.11060v2.pdf","comment":"to be published in CIKM 2024"},{"id":"http://arxiv.org/abs/2407.15100v2","updated":"2024-08-04T05:55:40Z","published":"2024-07-21T09:32:34Z","title":"A General Framework for Data-Use Auditing of ML Models","summary":"  Auditing the use of data in training machine-learning (ML) models is an\nincreasingly pressing challenge, as myriad ML practitioners routinely leverage\nthe effort of content creators to train models without their permission. In\nthis paper, we propose a general method to audit an ML model for the use of a\ndata-owner's data in training, without prior knowledge of the ML task for which\nthe data might be used. Our method leverages any existing black-box membership\ninference method, together with a sequential hypothesis test of our own design,\nto detect data use with a quantifiable, tunable false-detection rate. We show\nthe effectiveness of our proposed framework by applying it to audit data use in\ntwo types of ML models, namely image classifiers and foundation models.\n","authors":["Zonghao Huang","Neil Zhenqiang Gong","Michael K. Reiter"],"pdf_url":"https://arxiv.org/pdf/2407.15100v2.pdf","comment":"The full paper of \"A General Framework for Data-Use Auditing of ML\n  Models\" accepted by CCS 2024"},{"id":"http://arxiv.org/abs/2407.21530v2","updated":"2024-08-04T05:53:25Z","published":"2024-07-31T11:26:57Z","title":"Data Contamination Report from the 2024 CONDA Shared Task","summary":"  The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant\naspects of data contamination in natural language processing, where data\ncontamination is understood as situations where evaluation data is included in\npre-training corpora used to train large scale models, compromising evaluation\nresults. The workshop fostered a shared task to collect evidence on data\ncontamination in current available datasets and models. The goal of the shared\ntask and associated database is to assist the community in understanding the\nextent of the problem and to assist researchers in avoiding reporting\nevaluation results on known contaminated resources. The shared task provides a\nstructured, centralized public database for the collection of contamination\nevidence, open to contributions from the community via GitHub pool requests.\nThis first compilation paper is based on 566 reported entries over 91\ncontaminated sources from a total of 23 contributors. The details of the\nindividual contamination events are available in the platform. The platform\ncontinues to be online, open to contributions from the community.\n","authors":["Oscar Sainz","Iker Garca-Ferrero","Alon Jacovi","Jon Ander Campos","Yanai Elazar","Eneko Agirre","Yoav Goldberg","Wei-Lin Chen","Jenny Chim","Leshem Choshen","Luca D'Amico-Wong","Melissa Dell","Run-Ze Fan","Shahriar Golchin","Yucheng Li","Pengfei Liu","Bhavish Pahwa","Ameya Prabhu","Suryansh Sharma","Emily Silcock","Kateryna Solonko","David Stap","Mihai Surdeanu","Yu-Min Tseng","Vishaal Udandarao","Zengzhi Wang","Ruijie Xu","Jinglin Yang"],"pdf_url":"https://arxiv.org/pdf/2407.21530v2.pdf","comment":"https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.02113v1","updated":"2024-08-04T18:54:59Z","published":"2024-08-04T18:54:59Z","title":"Diseo de sonido para producciones audiovisuales e historias sonoras\n  en el aula. Hacia una docencia creativa mediante el uso de herramientas\n  inteligentes","summary":"  This study aims to share a teaching experience teaching sound design for\naudiovisual productions and compares different projects tackled by students. It\nis not intended to be a comparative analysis of different types of teaching but\nrather an analysis of different problems observed in different profiles of\nstudents of the subject who study it in different grades. The world of audio\ncan be very interesting for a large part of the students, both those with\ncreative and technical inclinations. Musical creation and production,\nsynchronization with images, dubbing, etc. They are disciplines that are\ngenerally interesting but can have a very high barrier to entry due to their\ngreat technical complexity. Sometimes it can take weeks or even months for the\nuninitiated to begin to use audio editing programs with the necessary ease,\nwhich are not always particularly intuitive for students. Learning through the\nuse of PBL methodologies generates, in our experience, results much superior to\nthose that can be observed through the use of other teaching methods such as\nmaster classes. Students acquire technical skills while developing creative\nprojects in which they get personally involved. Despite everything mentioned\nabove, most interactions between teachers and students focus on aspects of\ntechnical correction. From different parameters in reverbs (such as pre-delay,\ndecay, modulation...) to how to correctly adjust compressors, noise gates,\netc.; The number of tools with which to work with audio is incredibly\nextensive, as well as many of its features that can present serious differences\ndepending on their manufacturers.\n","authors":["Miguel Civit","Francisco Cuadrado"],"pdf_url":"https://arxiv.org/pdf/2408.02113v1.pdf","comment":"11 pages, in Spanish language. 1 figure. In La nueva era del\n  p\\'odcast"},{"id":"http://arxiv.org/abs/2408.02033v1","updated":"2024-08-04T13:51:18Z","published":"2024-08-04T13:51:18Z","title":"Enhancing Human Action Recognition and Violence Detection Through Deep\n  Learning Audiovisual Fusion","summary":"  This paper proposes a hybrid fusion-based deep learning approach based on two\ndifferent modalities, audio and video, to improve human activity recognition\nand violence detection in public places. To take advantage of audiovisual\nfusion, late fusion, intermediate fusion, and hybrid fusion-based deep learning\n(HFBDL) are used and compared. Since the objective is to detect and recognize\nhuman violence in public places, Real-life violence situation (RLVS) dataset is\nexpanded and used. Simulating results of HFBDL show 96.67\\% accuracy on\nvalidation data, which is more accurate than the other state-of-the-art methods\non this dataset. To showcase our model's ability in real-world scenarios,\nanother dataset of 54 sounded videos of both violent and non-violent situations\nwas recorded. The model could successfully detect 52 out of 54 videos\ncorrectly. The proposed method shows a promising performance on real scenarios.\nThus, it can be used for human action recognition and violence detection in\npublic places for security purposes.\n","authors":["Pooya Janani","Amirabolfazl Suratgar","Afshin Taghvaeipour"],"pdf_url":"https://arxiv.org/pdf/2408.02033v1.pdf","comment":"This work has been submitted to the IEEE for possible publication, 10\n  pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.13309v2","updated":"2024-08-04T08:40:36Z","published":"2024-07-18T09:13:08Z","title":"Exposure Completing for Temporally Consistent Neural High Dynamic Range\n  Video Rendering","summary":"  High dynamic range (HDR) video rendering from low dynamic range (LDR) videos\nwhere frames are of alternate exposure encounters significant challenges, due\nto the exposure change and absence at each time stamp. The exposure change and\nabsence make existing methods generate flickering HDR results. In this paper,\nwe propose a novel paradigm to render HDR frames via completing the absent\nexposure information, hence the exposure information is complete and\nconsistent. Our approach involves interpolating neighbor LDR frames in the time\ndimension to reconstruct LDR frames for the absent exposures. Combining the\ninterpolated and given LDR frames, the complete set of exposure information is\navailable at each time stamp. This benefits the fusing process for HDR results,\nreducing noise and ghosting artifacts therefore improving temporal consistency.\nExtensive experimental evaluations on standard benchmarks demonstrate that our\nmethod achieves state-of-the-art performance, highlighting the importance of\nabsent exposure completing in HDR video rendering. The code is available at\nhttps://github.com/cuijiahao666/NECHDR.\n","authors":["Jiahao Cui","Wei Jiang","Zhan Peng","Zhiyu Pan","Zhiguo Cao"],"pdf_url":"https://arxiv.org/pdf/2407.13309v2.pdf","comment":"9 pages, 6 figures, accepted by ACM-MM 2024 (poster)"},{"id":"http://arxiv.org/abs/2402.16364v2","updated":"2024-08-04T08:36:08Z","published":"2024-02-26T07:33:28Z","title":"Where Do We Go from Here? Multi-scale Allocentric Relational Inference\n  from Natural Spatial Descriptions","summary":"  When communicating routes in natural language, the concept of acquired\nspatial knowledge is crucial for geographic information retrieval (GIR) and in\nspatial cognitive research. However, NLP navigation studies often overlook the\nimpact of such acquired knowledge on textual descriptions. Current navigation\nstudies concentrate on egocentric local descriptions (e.g., `it will be on your\nright') that require reasoning over the agent's local perception. These\ninstructions are typically given as a sequence of steps, with each action-step\nexplicitly mentioning and being followed by a landmark that the agent can use\nto verify they are on the right path (e.g., `turn right and then you will\nsee...'). In contrast, descriptions based on knowledge acquired through a map\nprovide a complete view of the environment and capture its overall structure.\nThese instructions (e.g., `it is south of Central Park and a block north of a\npolice station') are typically non-sequential, contain allocentric relations,\nwith multiple spatial relations and implicit actions, without any explicit\nverification. This paper introduces the Rendezvous (RVS) task and dataset,\nwhich includes 10,404 examples of English geospatial instructions for reaching\na target location using map-knowledge. Our analysis reveals that RVS exhibits a\nricher use of spatial allocentric relations, and requires resolving more\nspatial relations simultaneously compared to previous text-based navigation\nbenchmarks.\n","authors":["Tzuf Paz-Argaman","Sayali Kulkarni","John Palowitch","Jason Baldridge","Reut Tsarfaty"],"pdf_url":"https://arxiv.org/pdf/2402.16364v2.pdf","comment":null}]},"2024-08-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2402.08831v2","updated":"2024-08-03T23:29:26Z","published":"2024-02-13T22:26:24Z","title":"eCeLLM: Generalizing Large Language Models for E-commerce from\n  Large-scale, High-quality Instruction Data","summary":"  With tremendous efforts on developing effective e-commerce models,\nconventional e-commerce models show limited success in generalist e-commerce\nmodeling, and suffer from unsatisfactory performance on new users and new\nproducts - a typical out-of-domain generalization challenge. Meanwhile, large\nlanguage models (LLMs) demonstrate outstanding performance in generalist\nmodeling and out-of-domain generalizability in many fields. Toward fully\nunleashing their power for e-commerce, in this paper, we construct ECInstruct,\nthe first open-sourced, large-scale, and high-quality benchmark instruction\ndataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of\ne-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive\nexperiments and evaluation demonstrate that eCeLLM models substantially\noutperform baseline models, including the most advanced GPT-4, and the\nstate-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM\nexhibits excellent generalizability to out-of-domain settings, including unseen\nproducts and unseen instructions, highlighting its superiority as a generalist\ne-commerce model. Both the ECInstruct dataset and the eCeLLM models show great\npotential in empowering versatile and effective LLMs for e-commerce. ECInstruct\nand eCeLLM models are publicly accessible through\nhttps://ninglab.github.io/eCeLLM.\n","authors":["Bo Peng","Xinyi Ling","Ziru Chen","Huan Sun","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2402.08831v2.pdf","comment":"ICML 2024; Bo Peng and Xinyi Ling contributed equally to this paper"},{"id":"http://arxiv.org/abs/2408.01875v1","updated":"2024-08-03T22:49:27Z","published":"2024-08-03T22:49:27Z","title":"Re-Invoke: Tool Invocation Rewriting for Zero-Shot Tool Retrieval","summary":"  Recent advances in large language models (LLMs) have enabled autonomous\nagents with complex reasoning and task-fulfillment capabilities using a wide\nrange of tools. However, effectively identifying the most relevant tools for a\ngiven task becomes a key bottleneck as the toolset size grows, hindering\nreliable tool utilization. To address this, we introduce Re-Invoke, an\nunsupervised tool retrieval method designed to scale effectively to large\ntoolsets without training. Specifically, we first generate a diverse set of\nsynthetic queries that comprehensively cover different aspects of the query\nspace associated with each tool document during the tool indexing phase.\nSecond, we leverage LLM's query understanding capabilities to extract key\ntool-related context and underlying intents from user queries during the\ninference phase. Finally, we employ a novel multi-view similarity ranking\nstrategy based on intents to pinpoint the most relevant tools for each query.\nOur evaluation demonstrates that Re-Invoke significantly outperforms\nstate-of-the-art alternatives in both single-tool and multi-tool scenarios, all\nwithin a fully unsupervised setting. Notably, on the ToolE datasets, we achieve\na 20% relative improvement in nDCG@5 for single-tool retrieval and a 39%\nimprovement for multi-tool retrieval.\n","authors":["Yanfei Chen","Jinsung Yoon","Devendra Singh Sachan","Qingze Wang","Vincent Cohen-Addad","Mohammadhossein Bateni","Chen-Yu Lee","Tomas Pfister"],"pdf_url":"https://arxiv.org/pdf/2408.01875v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01869v1","updated":"2024-08-03T22:14:13Z","published":"2024-08-03T22:14:13Z","title":"MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented\n  Generation for Pharmacovigilance","summary":"  In the era of Large Language Models (LLMs), given their remarkable text\nunderstanding and generation abilities, there is an unprecedented opportunity\nto develop new, LLM-based methods for trustworthy medical knowledge synthesis,\nextraction and summarization. This paper focuses on the problem of\nPharmacovigilance (PhV), where the significance and challenges lie in\nidentifying Adverse Drug Events (ADEs) from diverse text sources, such as\nmedical literature, clinical notes, and drug labels. Unfortunately, this task\nis hindered by factors including variations in the terminologies of drugs and\noutcomes, and ADE descriptions often being buried in large amounts of narrative\ntext. We present MALADE, the first effective collaborative multi-agent system\npowered by LLM with Retrieval Augmented Generation for ADE extraction from drug\nlabel data. This technique involves augmenting a query to an LLM with relevant\ninformation extracted from text resources, and instructing the LLM to compose a\nresponse consistent with the augmented data. MALADE is a general LLM-agnostic\narchitecture, and its unique capabilities are: (1) leveraging a variety of\nexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,\nOpenFDA drug information API), (2) extracting drug-outcome association in a\nstructured format along with the strength of the association, and (3) providing\nexplanations for established associations. Instantiated with GPT-4 Turbo or\nGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area\nUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our\nimplementation leverages the Langroid multi-agent LLM framework and can be\nfound at https://github.com/jihyechoi77/malade.\n","authors":["Jihye Choi","Nils Palumbo","Prasad Chalasani","Matthew M. Engelhard","Somesh Jha","Anivarya Kumar","David Page"],"pdf_url":"https://arxiv.org/pdf/2408.01869v1.pdf","comment":"Paper published at Machine Learning for Healthcare 2024 (MLHC'24)"},{"id":"http://arxiv.org/abs/2408.01866v1","updated":"2024-08-03T21:31:34Z","published":"2024-08-03T21:31:34Z","title":"Efficient Solutions For An Intriguing Failure of LLMs: Long Context\n  Window Does Not Mean LLMs Can Analyze Long Sequences Flawlessly","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\ncomprehending and analyzing lengthy sequential inputs, owing to their extensive\ncontext windows that allow processing millions of tokens in a single forward\npass. However, this paper uncovers a surprising limitation: LLMs fall short\nwhen handling long input sequences. We investigate this issue using three\ndatasets and two tasks (sentiment analysis and news categorization) across\nvarious LLMs, including Claude 3, Gemini Pro, GPT 3.5 Turbo, Llama 3 Instruct,\nand Mistral Instruct models. To address this limitation, we propose and\nevaluate ad-hoc solutions that substantially enhance LLMs' performance on long\ninput sequences by up to 50%, while reducing API cost and latency by up to 93%\nand 50%, respectively.\n","authors":["Peyman Hosseini","Ignacio Castro","Iacopo Ghinassi","Matthew Purver"],"pdf_url":"https://arxiv.org/pdf/2408.01866v1.pdf","comment":"11 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2401.11305v2","updated":"2024-08-03T20:22:29Z","published":"2024-01-20T19:32:56Z","title":"A Narrative Review of Identity, Data, and Location Privacy Techniques in\n  Edge Computing and Mobile Crowdsourcing","summary":"  As digital technology advances, the proliferation of connected devices poses\nsignificant challenges and opportunities in mobile crowdsourcing and edge\ncomputing. This narrative review focuses on the need for privacy protection in\nthese fields, emphasizing the increasing importance of data security in a\ndata-driven world. Through an analysis of contemporary academic literature,\nthis review provides an understanding of the current trends and privacy\nconcerns in mobile crowdsourcing and edge computing. We present insights and\nhighlight advancements in privacy-preserving techniques, addressing identity,\ndata, and location privacy. This survey serves as a useful resource for\nresearchers, industry professionals, and policymakers, offering an overview of\nprivacy challenges and potential solutions in these interconnected domains.\n","authors":["Syed Raza Bashir","Shaina Raza","Vojislav Misic"],"pdf_url":"https://arxiv.org/pdf/2401.11305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01852v1","updated":"2024-08-03T19:33:33Z","published":"2024-08-03T19:33:33Z","title":"Slo Escchame: Spanish Emotional Accompaniment Chatbot","summary":"  According to the World Health Organization (WHO), suicide was the fourth\nleading cause of death in the world for individuals aged 15 to 29 in 2019.\nGiven the rapid increase in mental health issues, providing psychological\nsupport is both crucial and urgent. In this paper: (1) we propose S\\'olo\nEsc\\'uchame, the first open-source Spanish emotional assistance chatbot, based\non LLaMA-2-7b-Chat. (2) We introduced the HEAR (Hispanic Emotional\nAccompaniment Responses) dataset, compiled from multiple English sources\ntranslated into Spanish, as well as generic data generated using\nChatGPT-3.5-Turbo. Finally, (3) we propose an evaluation metric based on two\nsemi-automatic assessment methods. Our system outperforms a range of\nstate-of-the-art models in providing psychological assistance in Spanish. Our\nmodels and datasets are publicly available to facilitate reproducibility.\n","authors":["Bruno Gil Ramrez","Jessica Lpez Espejel","Mara del Carmen Santiago Daz","Gustavo Trinidad Rubn Linares"],"pdf_url":"https://arxiv.org/pdf/2408.01852v1.pdf","comment":"Accepted at the 23rd Mexican International Conference on Artificial\n  Intelligence (MICAI) 2024"},{"id":"http://arxiv.org/abs/2408.01838v1","updated":"2024-08-03T18:28:31Z","published":"2024-08-03T18:28:31Z","title":"Tracking Emotional Dynamics in Chat Conversations: A Hybrid Approach\n  using DistilBERT and Emoji Sentiment Analysis","summary":"  Computer-mediated communication has become more important than face-to-face\ncommunication in many contexts. Tracking emotional dynamics in chat\nconversations can enhance communication, improve services, and support\nwell-being in various contexts. This paper explores a hybrid approach to\ntracking emotional dynamics in chat conversations by combining DistilBERT-based\ntext emotion detection and emoji sentiment analysis. A Twitter dataset was\nanalyzed using various machine learning algorithms, including SVM, Random\nForest, and AdaBoost. We contrasted their performance with DistilBERT. Results\nreveal DistilBERT's superior performance in emotion recognition. Our approach\naccounts for emotive expressions conveyed through emojis to better understand\nparticipants' emotions during chats. We demonstrate how this approach can\neffectively capture and analyze emotional shifts in real-time conversations.\nOur findings show that integrating text and emoji analysis is an effective way\nof tracking chat emotion, with possible applications in customer service, work\nchats, and social media interactions.\n","authors":["Ayan Igali","Abdulkhak Abdrakhman","Yerdaut Torekhan","Pakizar Shamoi"],"pdf_url":"https://arxiv.org/pdf/2408.01838v1.pdf","comment":"This work has been submitted to the Springer journal for possible\n  publication. Copyright may be transferred without notice, after which this\n  version may no longer be accessible"},{"id":"http://arxiv.org/abs/2407.00569v4","updated":"2024-08-03T17:52:43Z","published":"2024-06-30T03:04:11Z","title":"Investigating and Mitigating the Multimodal Hallucination Snowballing in\n  Large Vision-Language Models","summary":"  Though advanced in understanding visual information with human languages,\nLarge Vision-Language Models (LVLMs) still suffer from multimodal\nhallucinations. A natural concern is that during multimodal interaction, the\ngenerated hallucinations could influence the LVLMs' subsequent generation.\nThus, we raise a question: When presented with a query relevant to the\npreviously generated hallucination, will LVLMs be misled and respond\nincorrectly, even though the ground visual information exists? To answer this,\nwe propose a framework called MMHalSnowball to evaluate LVLMs' behaviors when\nencountering generated hallucinations, where LVLMs are required to answer\nspecific visual questions within a curated hallucinatory conversation.\nCrucially, our experiment shows that the performance of open-source LVLMs drops\nby at least $31\\%$, indicating that LVLMs are prone to accept the generated\nhallucinations and make false claims that they would not have supported without\ndistractions. We term this phenomenon Multimodal Hallucination Snowballing. To\nmitigate this, we further propose a training-free method called Residual Visual\nDecoding, where we revise the output distribution of LVLMs with the one derived\nfrom the residual visual input, providing models with direct access to the\nvisual information. Experiments show that our method can mitigate more than\n$24\\%$ of the snowballed multimodal hallucination while maintaining\ncapabilities.\n","authors":["Weihong Zhong","Xiaocheng Feng","Liang Zhao","Qiming Li","Lei Huang","Yuxuan Gu","Weitao Ma","Yuan Xu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2407.00569v4.pdf","comment":"Accepted to ACL 2024 Main Conference. 21 pages, 20 figures"},{"id":"http://arxiv.org/abs/2309.16804v2","updated":"2024-08-03T17:13:24Z","published":"2023-09-28T19:14:18Z","title":"Curriculum-Driven Edubot: A Framework for Developing Language Learning\n  Chatbots Through Synthesizing Conversational Data","summary":"  Chatbots have become popular in educational settings, revolutionizing how\nstudents interact with material and how teachers teach. We present\nCurriculum-Driven EduBot, a framework for developing a chatbot that combines\nthe interactive features of chatbots with the systematic material of English\ntextbooks to assist students in enhancing their conversational skills. We begin\nby extracting pertinent topics from textbooks and using large language models\nto generate dialogues related to these topics. We then fine-tune an open-source\nmodel using our generated conversational data to create our curriculum-driven\nchatbot. User studies demonstrate that EduBot outperforms ChatGPT in leading\ncurriculum-based dialogues and adapting its dialogue to match the user's\nEnglish proficiency level. By combining traditional textbook methodologies with\nconversational AI, our approach offers learners an interactive tool that aligns\nwith their curriculum and provides user-tailored conversation practice. This\nfacilitates meaningful student-bot dialogues and enriches the overall learning\nexperience within the curriculum's pedagogical framework.\n","authors":["Yu Li","Shang Qu","Jili Shen","Shangchao Min","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2309.16804v2.pdf","comment":"SIGDIAL 2024, 23 pages"},{"id":"http://arxiv.org/abs/2406.06399v3","updated":"2024-08-03T15:12:02Z","published":"2024-06-10T15:52:49Z","title":"Should We Fine-Tune or RAG? Evaluating Different Techniques to Adapt\n  LLMs for Dialogue","summary":"  We study the limitations of Large Language Models (LLMs) for the task of\nresponse generation in human-machine dialogue. Several techniques have been\nproposed in the literature for different dialogue types (e.g., Open-Domain).\nHowever, the evaluations of these techniques have been limited in terms of base\nLLMs, dialogue types and evaluation metrics. In this work, we extensively\nanalyze different LLM adaptation techniques when applied to different dialogue\ntypes. We have selected two base LLMs, Llama-2 and Mistral, and four dialogue\ntypes Open-Domain, Knowledge-Grounded, Task-Oriented, and Question Answering.\nWe evaluate the performance of in-context learning and fine-tuning techniques\nacross datasets selected for each dialogue type. We assess the impact of\nincorporating external knowledge to ground the generation in both scenarios of\nRetrieval-Augmented Generation (RAG) and gold knowledge. We adopt consistent\nevaluation and explainability criteria for automatic metrics and human\nevaluation protocols. Our analysis shows that there is no universal\nbest-technique for adapting large language models as the efficacy of each\ntechnique depends on both the base LLM and the specific type of dialogue. Last\nbut not least, the assessment of the best adaptation technique should include\nhuman evaluation to avoid false expectations and outcomes derived from\nautomatic metrics.\n","authors":["Simone Alghisi","Massimo Rizzoli","Gabriel Roccabruna","Seyed Mahed Mousavi","Giuseppe Riccardi"],"pdf_url":"https://arxiv.org/pdf/2406.06399v3.pdf","comment":"Accepted at INLG 2024"},{"id":"http://arxiv.org/abs/2408.01803v1","updated":"2024-08-03T15:07:44Z","published":"2024-08-03T15:07:44Z","title":"STBLLM: Breaking the 1-Bit Barrier with Structured Binary LLMs","summary":"  In this paper, we present STBLLM, the first structural binarization framework\nfor compressing Large Language Models (LLMs) to less than 1-bit precision. LLMs\nhave achieved remarkable performance, but their heavy memory requirements have\nhindered widespread adoption, particularly on resource-constrained devices.\nBinarization, which quantifies weights to a mere 1-bit, achieves a milestone in\nincreasing computational efficiency. However, we observe that some weights in\nbinarized LLMs can be randomly flipped without significant performance\ndegradation, indicating the potential for further compression. To exploit this,\nour STBLLM employs an N:M sparsity to perform structural binarization of the\nweights. First, we introduce a new Standardized Importance (SI) metric that\nconsiders weight magnitude and input feature norm to better evaluate weight\nsignificance. Then, we propose a layer-wise approach where different layers of\nthe LLM can be sparsified with varying N:M ratios, balancing compression and\naccuracy. Finally, we use residual approximation with double binarization to\npreserve information for salient weights. In addition, we utilize a\nfine-grained grouping strategy for less important weights that applies\ndifferent quantization schemes to sparse, intermediate, and dense regions. We\nconduct extensive experiments on various language models, including the\nLLaMA-1/2/3, OPT family, and Mistral, to evaluate the effectiveness of STBLLM.\nThe results demonstrate that our approach performs better than other compressed\nbinarization LLM methods while significantly reducing memory requirements.\n","authors":["Peijie Dong","Lujun Li","Dayou Du","Yuhan Chen","Zhenheng Tang","Qiang Wang","Wei Xue","Wenhan Luo","Qifeng Liu","Yike Guo","Xiaowen Chu"],"pdf_url":"https://arxiv.org/pdf/2408.01803v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.14770v2","updated":"2024-08-03T13:34:24Z","published":"2023-09-26T09:03:25Z","title":"KERMIT: Knowledge Graph Completion of Enhanced Relation Modeling with\n  Inverse Transformation","summary":"  Knowledge graph completion (KGC) revolves around populating missing triples\nin a knowledge graph using available information. Text-based methods, which\ndepend on textual descriptions of triples, often encounter difficulties when\nthese descriptions lack sufficient information for accurate prediction-an issue\ninherent to the datasets and not easily resolved through modeling alone. To\naddress this and ensure data consistency, we first use large language models\n(LLMs) to generate coherent descriptions, bridging the semantic gap between\nqueries and answers. Secondly, we utilize inverse relations to create a\nsymmetric graph, thereby providing augmented training samples for KGC.\nAdditionally, we employ the label information inherent in knowledge graphs\n(KGs) to enhance the existing contrastive framework, making it fully\nsupervised. These efforts have led to significant performance improvements on\nthe WN18RR and FB15k-237 datasets. According to standard evaluation metrics,\nour approach achieves a 4.2% improvement in Hit@1 on WN18RR and a 3.4%\nimprovement in Hit@3 on FB15k-237, demonstrating superior performance.\n","authors":["Haotian Li","Bin Yu","Yuliang Wei","Kai Wang","Richard Yi Da Xu","Bailing Wang"],"pdf_url":"https://arxiv.org/pdf/2309.14770v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01779v1","updated":"2024-08-03T13:28:19Z","published":"2024-08-03T13:28:19Z","title":"MathLearner: A Large Language Model Agent Framework for Learning to\n  Solve Mathematical Problems","summary":"  With the development of artificial intelligence (AI), large language models\n(LLM) are widely used in many fields. However, the reasoning ability of LLM is\nstill very limited when it comes to mathematical reasoning. Mathematics plays\nan important role in all aspects of human society and is a technical guarantee\nin the fields of healthcare, transport and aerospace, for this reason, the\ndevelopment of AI big language models in the field of mathematics has great\npotential significance. To improve the mathematical reasoning ability of large\nlanguage models, we proposed an agent framework for learning to solve\nmathematical problems based on inductive reasoning. By emulating the human\nlearning process of generalization of learned information and effective\napplication of previous knowledge in new reasoning tasks, this framework has\ngreat performance in the mathematical reasoning process. It improves global\naccuracy over the baseline method (chain-of-thought) by 20.96% and solves\n17.54% of the mathematical problems that the baseline cannot solve. Benefiting\nfrom the efficient RETRIEVAL method, our model improves the ability of large\nlanguage models to efficiently use external knowledge, i.e., the mathematical\ncomputation of the model can be based on written procedures. In education, our\nmodel can be used as a personalised learning aid, thus reducing the inequality\nof educational resources.\n","authors":["Wenbei Xie","Donglin Liu","Haoran Yan","Wenjie Wu","Zongyang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.01779v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00823v2","updated":"2024-08-03T12:41:29Z","published":"2024-05-01T19:07:03Z","title":"WorkBench: a Benchmark Dataset for Agents in a Realistic Workplace\n  Setting","summary":"  We introduce WorkBench: a benchmark dataset for evaluating agents' ability to\nexecute tasks in a workplace setting. WorkBench contains a sandbox environment\nwith five databases, 26 tools, and 690 tasks. These tasks represent common\nbusiness activities, such as sending emails and scheduling meetings. The tasks\nin WorkBench are challenging as they require planning, tool selection, and\noften multiple actions. If a task has been successfully executed, one (or more)\nof the database values may change. The correct outcome for each task is unique\nand unambiguous, which allows for robust, automated evaluation. We call this\nkey contribution outcome-centric evaluation. We evaluate five existing ReAct\nagents on WorkBench, finding they successfully complete as few as 3% of tasks\n(Llama2-70B), and just 43% for the best-performing (GPT-4). We further find\nthat agents' errors can result in the wrong action being taken, such as an\nemail being sent to the wrong person. WorkBench reveals weaknesses in agents'\nability to undertake common business activities, raising questions about their\nuse in high-stakes workplace settings. WorkBench is publicly available as a\nfree resource at https://github.com/olly-styles/WorkBench.\n","authors":["Olly Styles","Sam Miller","Patricio Cerda-Mardini","Tanaya Guha","Victor Sanchez","Bertie Vidgen"],"pdf_url":"https://arxiv.org/pdf/2405.00823v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01748v1","updated":"2024-08-03T11:08:53Z","published":"2024-08-03T11:08:53Z","title":"Discovery of Rare Causal Knowledge from Financial Statement Summaries","summary":"  What would happen if temperatures were subdued and result in a cool summer?\nOne can easily imagine that air conditioner, ice cream or beer sales would be\nsuppressed as a result of this. Less obvious is that agricultural shipments\nmight be delayed, or that sound proofing material sales might decrease. The\nability to extract such causal knowledge is important, but it is also important\nto distinguish between cause-effect pairs that are known and those that are\nlikely to be unknown, or rare. Therefore, in this paper, we propose a method\nfor extracting rare causal knowledge from Japanese financial statement\nsummaries produced by companies. Our method consists of three steps. First, it\nextracts sentences that include causal knowledge from the summaries using a\nmachine learning method based on an extended language ontology. Second, it\nobtains causal knowledge from the extracted sentences using syntactic patterns.\nFinally, it extracts the rarest causal knowledge from the knowledge it has\nobtained.\n","authors":["Hiroki Sakaji","Jason Bennett","Risa Murono","Kiyoshi Izumi","Hiroyuki Sakai"],"pdf_url":"https://arxiv.org/pdf/2408.01748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01745v1","updated":"2024-08-03T11:05:41Z","published":"2024-08-03T11:05:41Z","title":"Indexing and Visualization of Climate Change Narratives Using BERT and\n  Causal Extraction","summary":"  In this study, we propose a methodology to extract, index, and visualize\n``climate change narratives'' (stories about the connection between causal and\nconsequential events related to climate change). We use two natural language\nprocessing methods, BERT (Bidirectional Encoder Representations from\nTransformers) and causal extraction, to textually analyze newspaper articles on\nclimate change to extract ``climate change narratives.'' The novelty of the\nmethodology could extract and quantify the causal relationships assumed by the\nnewspaper's writers. Looking at the extracted climate change narratives over\ntime, we find that since 2018, an increasing number of narratives suggest the\nimpact of the development of climate change policy discussion and the\nimplementation of climate change-related policies on corporate behaviors,\nmacroeconomics, and price dynamics. We also observed the recent emergence of\nnarratives focusing on the linkages between climate change-related policies and\nmonetary policy. Furthermore, there is a growing awareness of the negative\nimpacts of natural disasters (e.g., abnormal weather and severe floods) related\nto climate change on economic activities, and this issue might be perceived as\na new challenge for companies and governments. The methodology of this study is\nexpected to be applied to a wide range of fields, as it can analyze causal\nrelationships among various economic topics, including analysis of inflation\nexpectation or monetary policy communication strategy.\n","authors":["Hiroki Sakaji","Noriyasu Kaneda"],"pdf_url":"https://arxiv.org/pdf/2408.01745v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01744v1","updated":"2024-08-03T11:04:04Z","published":"2024-08-03T11:04:04Z","title":"Summarization of Investment Reports Using Pre-trained Model","summary":"  In this paper, we attempt to summarize monthly reports as investment reports.\nFund managers have a wide range of tasks, one of which is the preparation of\ninvestment reports. In addition to preparing monthly reports on fund\nmanagement, fund managers prepare management reports that summarize these\nmonthly reports every six months or once a year. The preparation of fund\nreports is a labor-intensive and time-consuming task. Therefore, in this paper,\nwe tackle investment summarization from monthly reports using transformer-based\nmodels. There are two main types of summarization methods: extractive\nsummarization and abstractive summarization, and this study constructs both\nmethods and examines which is more useful in summarizing investment reports.\n","authors":["Hiroki Sakaji","Ryotaro Kobayashi","Kiyoshi Izumi","Hiroyuki Mitsugi","Wataru Kuramoto"],"pdf_url":"https://arxiv.org/pdf/2408.01744v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.17807v3","updated":"2024-08-03T07:59:33Z","published":"2024-06-23T11:58:26Z","title":"Enhancing Commentary Strategies for Imperfect Information Card Games: A\n  Study of Large Language Models in Guandan Commentary","summary":"  Recent advancements in large language models (LLMs) have unlocked the\npotential for generating high-quality game commentary. However, producing\ninsightful and engaging commentary for complex games with incomplete\ninformation remains a significant challenge. In this paper, we introduce a\nnovel commentary method that combine Reinforcement Learning (RL) and LLMs,\ntailored specifically for the Chinese card game \\textit{Guandan}. Our system\nleverages RL to generate intricate card-playing scenarios and employs LLMs to\ngenerate corresponding commentary text, effectively emulating the strategic\nanalysis and narrative prowess of professional commentators. The framework\ncomprises a state commentary guide, a Theory of Mind (ToM)-based strategy\nanalyzer, and a style retrieval module, which seamlessly collaborate to deliver\ndetailed and context-relevant game commentary in the Chinese language\nenvironment. We empower LLMs with ToM capabilities and refine both retrieval\nand information filtering mechanisms. This facilitates the generation of\npersonalized commentary content. Our experimental results showcase the\nsubstantial enhancement in performance achieved by the proposed commentary\nframework when applied to open-source LLMs, surpassing the performance of GPT-4\nacross multiple evaluation metrics.\n","authors":["Meiling Tao","Xuechen Liang","Ziyi Wang","Yiling Tao","Tianyu Shi"],"pdf_url":"https://arxiv.org/pdf/2406.17807v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01700v1","updated":"2024-08-03T07:42:53Z","published":"2024-08-03T07:42:53Z","title":"Integrating Large Language Models and Knowledge Graphs for Extraction\n  and Validation of Textual Test Data","summary":"  Aerospace manufacturing companies, such as Thales Alenia Space, design,\ndevelop, integrate, verify, and validate products characterized by high\ncomplexity and low volume. They carefully document all phases for each product\nbut analyses across products are challenging due to the heterogeneity and\nunstructured nature of the data in documents. In this paper, we propose a\nhybrid methodology that leverages Knowledge Graphs (KGs) in conjunction with\nLarge Language Models (LLMs) to extract and validate data contained in these\ndocuments. We consider a case study focused on test data related to electronic\nboards for satellites. To do so, we extend the Semantic Sensor Network\nontology. We store the metadata of the reports in a KG, while the actual test\nresults are stored in parquet accessible via a Virtual Knowledge Graph. The\nvalidation process is managed using an LLM-based approach. We also conduct a\nbenchmarking study to evaluate the performance of state-of-the-art LLMs in\nexecuting this task. Finally, we analyze the costs and benefits of automating\npreexisting processes of manual data extraction and validation for subsequent\ncross-report analyses.\n","authors":["Antonio De Santis","Marco Balduini","Federico De Santis","Andrea Proia","Arsenio Leo","Marco Brambilla","Emanuele Della Valle"],"pdf_url":"https://arxiv.org/pdf/2408.01700v1.pdf","comment":"Paper Accepted at ISWC 2024 In-Use Track"},{"id":"http://arxiv.org/abs/2408.01682v1","updated":"2024-08-03T06:40:00Z","published":"2024-08-03T06:40:00Z","title":"Multi-Frame Vision-Language Model for Long-form Reasoning in Driver\n  Behavior Analysis","summary":"  Identifying risky driving behavior in real-world situations is essential for\nthe safety of both drivers and pedestrians. However, integrating natural\nlanguage models in this field remains relatively untapped. To address this, we\ncreated a novel multi-modal instruction tuning dataset and driver coaching\ninference system. Our primary use case is dashcam-based coaching for commercial\ndrivers. The North American Dashcam Market is expected to register a CAGR of\n15.4 percent from 2022 to 2027. Our dataset enables language models to learn\nvisual instructions across various risky driving scenarios, emphasizing\ndetailed reasoning crucial for effective driver coaching and managerial\ncomprehension. Our model is trained on road-facing and driver-facing RGB camera\nfootage, capturing the comprehensive scope of driving behavior in vehicles\nequipped with dashcams.\n","authors":["Hiroshi Takato","Hiroshi Tsutsui","Komei Soda","Hidetaka Kamigaito"],"pdf_url":"https://arxiv.org/pdf/2408.01682v1.pdf","comment":"On-going work"},{"id":"http://arxiv.org/abs/2401.09002v5","updated":"2024-08-03T06:39:25Z","published":"2024-01-17T06:42:44Z","title":"AttackEval: How to Evaluate the Effectiveness of Jailbreak Attacking on\n  Large Language Models","summary":"  Ensuring the security of large language models (LLMs) against attacks has\nbecome increasingly urgent, with jailbreak attacks representing one of the most\nsophisticated threats. To deal with such risks, we introduce an innovative\nframework that can help evaluate the effectiveness of jailbreak attacks on\nLLMs. Unlike traditional binary evaluations focusing solely on the robustness\nof LLMs, our method assesses the effectiveness of the attacking prompts\nthemselves. We present two distinct evaluation frameworks: a coarse-grained\nevaluation and a fine-grained evaluation. Each framework uses a scoring range\nfrom 0 to 1, offering unique perspectives and allowing for the assessment of\nattack effectiveness in different scenarios. Additionally, we develop a\ncomprehensive ground truth dataset specifically tailored for jailbreak prompts.\nThis dataset serves as a crucial benchmark for our current study and provides a\nfoundational resource for future research. By comparing with traditional\nevaluation methods, our study shows that the current results align with\nbaseline metrics while offering a more nuanced and fine-grained assessment. It\nalso helps identify potentially harmful attack prompts that might appear\nharmless in traditional evaluations. Overall, our work establishes a solid\nfoundation for assessing a broader range of attack prompts in the area of\nprompt injection.\n","authors":["Dong shu","Mingyu Jin","Chong Zhang","Liangyao Li","Zihao Zhou","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2401.09002v5.pdf","comment":"34 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.01679v1","updated":"2024-08-03T06:35:54Z","published":"2024-08-03T06:35:54Z","title":"MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal\n  Knowledge Graph","summary":"  Multi-modal knowledge graphs have emerged as a powerful approach for\ninformation representation, combining data from different modalities such as\ntext, images, and videos. While several such graphs have been constructed and\nhave played important roles in applications like visual question answering and\nrecommendation systems, challenges persist in their development. These include\nthe scarcity of high-quality Chinese knowledge graphs and limited domain\ncoverage in existing multi-modal knowledge graphs. This paper introduces\nMMPKUBase, a robust and extensive Chinese multi-modal knowledge graph that\ncovers diverse domains, including birds, mammals, ferns, and more, comprising\nover 50,000 entities and over 1 million filtered images. To ensure data\nquality, we employ Prototypical Contrastive Learning and the Isolation Forest\nalgorithm to refine the image data. Additionally, we have developed a\nuser-friendly platform to facilitate image attribute exploration.\n","authors":["Xuan Yi","Yanzeng Li","Lei Zou"],"pdf_url":"https://arxiv.org/pdf/2408.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10051v4","updated":"2024-08-03T03:52:48Z","published":"2024-05-16T12:40:01Z","title":"MarkLLM: An Open-Source Toolkit for LLM Watermarking","summary":"  LLM watermarking, which embeds imperceptible yet algorithmically detectable\nsignals in model outputs to identify LLM-generated text, has become crucial in\nmitigating the potential misuse of large language models. However, the\nabundance of LLM watermarking algorithms, their intricate mechanisms, and the\ncomplex evaluation procedures and perspectives pose challenges for researchers\nand the community to easily experiment with, understand, and assess the latest\nadvancements. To address these issues, we introduce MarkLLM, an open-source\ntoolkit for LLM watermarking. MarkLLM offers a unified and extensible framework\nfor implementing LLM watermarking algorithms, while providing user-friendly\ninterfaces to ensure ease of access. Furthermore, it enhances understanding by\nsupporting automatic visualization of the underlying mechanisms of these\nalgorithms. For evaluation, MarkLLM offers a comprehensive suite of 12 tools\nspanning three perspectives, along with two types of automated evaluation\npipelines. Through MarkLLM, we aim to support researchers while improving the\ncomprehension and involvement of the general public in LLM watermarking\ntechnology, fostering consensus and driving further advancements in research\nand application. Our code is available at https://github.com/THU-BPM/MarkLLM.\n","authors":["Leyi Pan","Aiwei Liu","Zhiwei He","Zitian Gao","Xuandong Zhao","Yijian Lu","Binglin Zhou","Shuliang Liu","Xuming Hu","Lijie Wen","Irwin King","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2405.10051v4.pdf","comment":"17 pages, 5 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.01638v1","updated":"2024-08-03T02:41:10Z","published":"2024-08-03T02:41:10Z","title":"Transforming Slot Schema Induction with Generative Dialogue State\n  Inference","summary":"  The challenge of defining a slot schema to represent the state of a\ntask-oriented dialogue system is addressed by Slot Schema Induction (SSI),\nwhich aims to automatically induce slots from unlabeled dialogue data. Whereas\nprevious approaches induce slots by clustering value spans extracted directly\nfrom the dialogue text, we demonstrate the power of discovering slots using a\ngenerative approach. By training a model to generate slot names and values that\nsummarize key dialogue information with no prior task knowledge, our SSI method\ndiscovers high-quality candidate information for representing dialogue state.\nThese discovered slot-value candidates can be easily clustered into unified\nslot schemas that align well with human-authored schemas. Experimental\ncomparisons on the MultiWOZ and SGD datasets demonstrate that Generative\nDialogue State Inference (GenDSI) outperforms the previous state-of-the-art on\nmultiple aspects of the SSI task.\n","authors":["James D. Finch","Boxin Zhao","Jinho D. Choi"],"pdf_url":"https://arxiv.org/pdf/2408.01638v1.pdf","comment":"Accepted to SIGDIAL 2024"},{"id":"http://arxiv.org/abs/2408.01633v1","updated":"2024-08-03T02:11:48Z","published":"2024-08-03T02:11:48Z","title":"Self-Emotion Blended Dialogue Generation in Social Simulation Agents","summary":"  When engaging in conversations, dialogue agents in a virtual simulation\nenvironment may exhibit their own emotional states that are unrelated to the\nimmediate conversational context, a phenomenon known as self-emotion. This\nstudy explores how such self-emotion affects the agents' behaviors in dialogue\nstrategies and decision-making within a large language model (LLM)-driven\nsimulation framework. In a dialogue strategy prediction experiment, we analyze\nthe dialogue strategy choices employed by agents both with and without\nself-emotion, comparing them to those of humans. The results show that\nincorporating self-emotion helps agents exhibit more human-like dialogue\nstrategies. In an independent experiment comparing the performance of models\nfine-tuned on GPT-4 generated dialogue datasets, we demonstrate that\nself-emotion can lead to better overall naturalness and humanness. Finally, in\na virtual simulation environment where agents have discussions on multiple\ntopics, we show that self-emotion of agents can significantly influence the\ndecision-making process of the agents, leading to approximately a 50% change in\ndecisions.\n","authors":["Qiang Zhang","Jason Naradowsky","Yusuke Miyao"],"pdf_url":"https://arxiv.org/pdf/2408.01633v1.pdf","comment":"Accepted in SIGDIAL 2024"},{"id":"http://arxiv.org/abs/2401.05695v2","updated":"2024-08-03T01:52:51Z","published":"2024-01-11T06:42:45Z","title":"Integrating Physician Diagnostic Logic into Large Language Models:\n  Preference Learning from Process Feedback","summary":"  The use of large language models in medical dialogue generation has garnered\nsignificant attention, with a focus on improving response quality and fluency.\nWhile previous studies have made progress in optimizing model performance for\nsingle-round medical Q&A tasks, there is a need to enhance the model's\ncapability for multi-round conversations to avoid logical inconsistencies. To\naddress this, we propose an approach called preference learning from process\nfeedback~(PLPF), which integrates the doctor's diagnostic logic into LLMs. PLPF\ninvolves rule modeling, preference data generation, and preference alignment to\ntrain the model to adhere to the diagnostic process. Experimental results using\nStandardized Patient Testing show that PLPF enhances the diagnostic accuracy of\nthe baseline model in medical conversations by 17.6%, outperforming traditional\nreinforcement learning from human feedback. Additionally, PLPF demonstrates\neffectiveness in both multi-round and single-round dialogue tasks, showcasing\nits potential for improving medical dialogue generation.\n","authors":["Chengfeng Dou","Zhi Jin","Wenpin Jiao","Haiyan Zhao","Yongqiang Zhao","Zhenwei Tao"],"pdf_url":"https://arxiv.org/pdf/2401.05695v2.pdf","comment":"Accepted by ACL2024 Findings"},{"id":"http://arxiv.org/abs/2408.01623v1","updated":"2024-08-03T01:15:50Z","published":"2024-08-03T01:15:50Z","title":"Dialog Flow Induction for Constrainable LLM-Based Chatbots","summary":"  LLM-driven dialog systems are used in a diverse set of applications, ranging\nfrom healthcare to customer service. However, given their generalization\ncapability, it is difficult to ensure that these chatbots stay within the\nboundaries of the specialized domains, potentially resulting in inaccurate\ninformation and irrelevant responses. This paper introduces an unsupervised\napproach for automatically inducing domain-specific dialog flows that can be\nused to constrain LLM-based chatbots. We introduce two variants of dialog flow\nbased on the availability of in-domain conversation instances. Through human\nand automatic evaluation over various dialog domains, we demonstrate that our\nhigh-quality data-guided dialog flows achieve better domain coverage, thereby\novercoming the need for extensive manual crafting of such flows.\n","authors":["Stuti Agrawal","Nishi Uppuluri","Pranav Pillai","Revanth Gangi Reddy","Zoey Li","Gokhan Tur","Dilek Hakkani-Tur","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2408.01623v1.pdf","comment":"Accepted at SIGDIAL 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.08831v2","updated":"2024-08-03T23:29:26Z","published":"2024-02-13T22:26:24Z","title":"eCeLLM: Generalizing Large Language Models for E-commerce from\n  Large-scale, High-quality Instruction Data","summary":"  With tremendous efforts on developing effective e-commerce models,\nconventional e-commerce models show limited success in generalist e-commerce\nmodeling, and suffer from unsatisfactory performance on new users and new\nproducts - a typical out-of-domain generalization challenge. Meanwhile, large\nlanguage models (LLMs) demonstrate outstanding performance in generalist\nmodeling and out-of-domain generalizability in many fields. Toward fully\nunleashing their power for e-commerce, in this paper, we construct ECInstruct,\nthe first open-sourced, large-scale, and high-quality benchmark instruction\ndataset for e-commerce. Leveraging ECInstruct, we develop eCeLLM, a series of\ne-commerce LLMs, by instruction-tuning general-purpose LLMs. Our comprehensive\nexperiments and evaluation demonstrate that eCeLLM models substantially\noutperform baseline models, including the most advanced GPT-4, and the\nstate-of-the-art task-specific models in in-domain evaluation. Moreover, eCeLLM\nexhibits excellent generalizability to out-of-domain settings, including unseen\nproducts and unseen instructions, highlighting its superiority as a generalist\ne-commerce model. Both the ECInstruct dataset and the eCeLLM models show great\npotential in empowering versatile and effective LLMs for e-commerce. ECInstruct\nand eCeLLM models are publicly accessible through\nhttps://ninglab.github.io/eCeLLM.\n","authors":["Bo Peng","Xinyi Ling","Ziru Chen","Huan Sun","Xia Ning"],"pdf_url":"https://arxiv.org/pdf/2402.08831v2.pdf","comment":"ICML 2024; Bo Peng and Xinyi Ling contributed equally to this paper"},{"id":"http://arxiv.org/abs/2408.01869v1","updated":"2024-08-03T22:14:13Z","published":"2024-08-03T22:14:13Z","title":"MALADE: Orchestration of LLM-powered Agents with Retrieval Augmented\n  Generation for Pharmacovigilance","summary":"  In the era of Large Language Models (LLMs), given their remarkable text\nunderstanding and generation abilities, there is an unprecedented opportunity\nto develop new, LLM-based methods for trustworthy medical knowledge synthesis,\nextraction and summarization. This paper focuses on the problem of\nPharmacovigilance (PhV), where the significance and challenges lie in\nidentifying Adverse Drug Events (ADEs) from diverse text sources, such as\nmedical literature, clinical notes, and drug labels. Unfortunately, this task\nis hindered by factors including variations in the terminologies of drugs and\noutcomes, and ADE descriptions often being buried in large amounts of narrative\ntext. We present MALADE, the first effective collaborative multi-agent system\npowered by LLM with Retrieval Augmented Generation for ADE extraction from drug\nlabel data. This technique involves augmenting a query to an LLM with relevant\ninformation extracted from text resources, and instructing the LLM to compose a\nresponse consistent with the augmented data. MALADE is a general LLM-agnostic\narchitecture, and its unique capabilities are: (1) leveraging a variety of\nexternal sources, such as medical literature, drug labels, and FDA tools (e.g.,\nOpenFDA drug information API), (2) extracting drug-outcome association in a\nstructured format along with the strength of the association, and (3) providing\nexplanations for established associations. Instantiated with GPT-4 Turbo or\nGPT-4o, and FDA drug label data, MALADE demonstrates its efficacy with an Area\nUnder ROC Curve of 0.90 against the OMOP Ground Truth table of ADEs. Our\nimplementation leverages the Langroid multi-agent LLM framework and can be\nfound at https://github.com/jihyechoi77/malade.\n","authors":["Jihye Choi","Nils Palumbo","Prasad Chalasani","Matthew M. Engelhard","Somesh Jha","Anivarya Kumar","David Page"],"pdf_url":"https://arxiv.org/pdf/2408.01869v1.pdf","comment":"Paper published at Machine Learning for Healthcare 2024 (MLHC'24)"},{"id":"http://arxiv.org/abs/2408.01784v1","updated":"2024-08-03T13:37:40Z","published":"2024-08-03T13:37:40Z","title":"Graph Stochastic Neural Process for Inductive Few-shot Knowledge Graph\n  Completion","summary":"  Knowledge graphs (KGs) store enormous facts as relationships between\nentities. Due to the long-tailed distribution of relations and the\nincompleteness of KGs, there is growing interest in few-shot knowledge graph\ncompletion (FKGC). Existing FKGC methods often assume the existence of all\nentities in KGs, which may not be practical since new relations and entities\ncan emerge over time. Therefore, we focus on a more challenging task called\ninductive few-shot knowledge graph completion (I-FKGC), where both relations\nand entities during the test phase are unknown before. Inspired by the idea of\ninductive reasoning, we cast I-FKGC as an inductive reasoning problem.\nSpecifically, we propose a novel Graph Stochastic Neural Process approach\n(GS-NP), which consists of two major modules. In the first module, to obtain a\ngeneralized hypothesis (e.g., shared subgraph), we present a neural\nprocess-based hypothesis extractor that models the joint distribution of\nhypothesis, from which we can sample a hypothesis for predictions. In the\nsecond module, based on the hypothesis, we propose a graph stochastic\nattention-based predictor to test if the triple in the query set aligns with\nthe extracted hypothesis. Meanwhile, the predictor can generate an explanatory\nsubgraph identified by the hypothesis. Finally, the training of these two\nmodules is seamlessly combined into a unified objective function, of which the\neffectiveness is verified by theoretical analyses as well as empirical studies.\nExtensive experiments on three public datasets demonstrate that our method\noutperforms existing methods and derives new state-of-the-art performance.\n","authors":["Zicheng Zhao","Linhao Luo","Shirui Pan","Chengqi Zhang","Chen Gong"],"pdf_url":"https://arxiv.org/pdf/2408.01784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01723v1","updated":"2024-08-03T09:27:57Z","published":"2024-08-03T09:27:57Z","title":"A Novel Evaluation Framework for Image2Text Generation","summary":"  Evaluating the quality of automatically generated image descriptions is\nchallenging, requiring metrics that capture various aspects such as\ngrammaticality, coverage, correctness, and truthfulness. While human evaluation\noffers valuable insights, its cost and time-consuming nature pose limitations.\nExisting automated metrics like BLEU, ROUGE, METEOR, and CIDEr aim to bridge\nthis gap but often show weak correlations with human judgment. We address this\nchallenge by introducing a novel evaluation framework rooted in a modern large\nlanguage model (LLM), such as GPT-4 or Gemini, capable of image generation. In\nour proposed framework, we begin by feeding an input image into a designated\nimage captioning model, chosen for evaluation, to generate a textual\ndescription. Using this description, an LLM then creates a new image. By\nextracting features from both the original and LLM-created images, we measure\ntheir similarity using a designated similarity metric. A high similarity score\nsuggests that the image captioning model has accurately generated textual\ndescriptions, while a low similarity score indicates discrepancies, revealing\npotential shortcomings in the model's performance. Human-annotated reference\ncaptions are not required in our proposed evaluation framework, which serves as\na valuable tool for evaluating the effectiveness of image captioning models.\nIts efficacy is confirmed through human evaluation.\n","authors":["Jia-Hong Huang","Hongyi Zhu","Yixian Shen","Stevan Rudinac","Alessio M. Pacces","Evangelos Kanoulas"],"pdf_url":"https://arxiv.org/pdf/2408.01723v1.pdf","comment":"The paper has been accepted for presentation at the 47th\n  International ACM SIGIR Conference on Research and Development in Information\n  Retrieval, specifically in the Large Language Model for Evaluation in IR\n  (LLM4Eval) Workshop in 2024"},{"id":"http://arxiv.org/abs/2407.19937v2","updated":"2024-08-03T05:40:20Z","published":"2024-07-29T12:17:48Z","title":"AOTree: Aspect Order Tree-based Model for Explainable Recommendation","summary":"  Recent recommender systems aim to provide not only accurate recommendations\nbut also explanations that help users understand them better. However, most\nexisting explainable recommendations only consider the importance of content in\nreviews, such as words or aspects, and ignore the ordering relationship among\nthem. This oversight neglects crucial ordering dimensions in the human\ndecision-making process, leading to suboptimal performance. Therefore, in this\npaper, we propose Aspect Order Tree-based (AOTree) explainable recommendation\nmethod, inspired by the Order Effects Theory from cognitive and decision\npsychology, in order to capture the dependency relationships among decisive\nfactors. We first validate the theory in the recommendation scenario by\nanalyzing the reviews of the users. Then, according to the theory, the proposed\nAOTree expands the construction of the decision tree to capture aspect orders\nin users' decision-making processes, and use attention mechanisms to make\npredictions based on the aspect orders. Extensive experiments demonstrate our\nmethod's effectiveness on rating predictions, and our approach aligns more\nconsistently with the user' s decision-making process by displaying\nexplanations in a particular order, thereby enhancing interpretability.\n","authors":["Wenxin Zhao","Peng Zhang","Hansu Gu","Dongsheng Li","Tun Lu","Ning Gu"],"pdf_url":"https://arxiv.org/pdf/2407.19937v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04264v3","updated":"2024-08-03T01:06:20Z","published":"2024-03-17T17:01:45Z","title":"Logic Query of Thoughts: Guiding Large Language Models to Answer Complex\n  Logic Queries with Knowledge Graphs","summary":"  Despite the superb performance in many tasks, large language models (LLMs)\nbear the risk of generating hallucination or even wrong answers when confronted\nwith tasks that demand the accuracy of knowledge. The issue becomes even more\nnoticeable when addressing logic queries that require multiple logic reasoning\nsteps. On the other hand, knowledge graph (KG) based question answering methods\nare capable of accurately identifying the correct answers with the help of\nknowledge graph, yet its accuracy could quickly deteriorate when the knowledge\ngraph itself is sparse and incomplete. It remains a critical challenge on how\nto integrate knowledge graph reasoning with LLMs in a mutually beneficial way\nso as to mitigate both the hallucination problem of LLMs as well as the\nincompleteness issue of knowledge graphs. In this paper, we propose\n'Logic-Query-of-Thoughts' (LGOT) which is the first of its kind to combine LLMs\nwith knowledge graph based logic query reasoning. LGOT seamlessly combines\nknowledge graph reasoning and LLMs, effectively breaking down complex logic\nqueries into easy to answer subquestions. Through the utilization of both\nknowledge graph reasoning and LLMs, it successfully derives answers for each\nsubquestion. By aggregating these results and selecting the highest quality\ncandidate answers for each step, LGOT achieves accurate results to complex\nquestions. Our experimental findings demonstrate substantial performance\nenhancements, with up to 20% improvement over ChatGPT.\n","authors":["Lihui Liu","Zihao Wang","Ruizhong Qiu","Yikun Ban","Eunice Chan","Yangqiu Song","Jingrui He","Hanghang Tong"],"pdf_url":"https://arxiv.org/pdf/2404.04264v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02691v1","updated":"2024-08-03T06:58:07Z","published":"2024-08-03T06:58:07Z","title":"Symmetric Graph Contrastive Learning against Noisy Views for\n  Recommendation","summary":"  Graph Contrastive Learning (GCL) leverages data augmentation techniques to\nproduce contrasting views, enhancing the accuracy of recommendation systems\nthrough learning the consistency between contrastive views. However, existing\naugmentation methods, such as directly perturbing interaction graph (e.g.,\nnode/edge dropout), may interfere with the original connections and generate\npoor contrasting views, resulting in sub-optimal performance. In this paper, we\ndefine the views that share only a small amount of information with the\noriginal graph due to poor data augmentation as noisy views (i.e., the last 20%\nof the views with a cosine similarity value less than 0.1 to the original\nview). We demonstrate through detailed experiments that noisy views will\nsignificantly degrade recommendation performance. Further, we propose a\nmodel-agnostic Symmetric Graph Contrastive Learning (SGCL) method with\ntheoretical guarantees to address this issue. Specifically, we introduce\nsymmetry theory into graph contrastive learning, based on which we propose a\nsymmetric form and contrast loss resistant to noisy interference. We provide\ntheoretical proof that our proposed SGCL method has a high tolerance to noisy\nviews. Further demonstration is given by conducting extensive experiments on\nthree real-world datasets. The experimental results demonstrate that our\napproach substantially increases recommendation accuracy, with relative\nimprovements reaching as high as 12.25% over nine other competing models. These\nresults highlight the efficacy of our method.\n","authors":["Chu Zhao","Enneng Yang","Yuliang Liang","Jianzhe Zhao","Guibing Guo","Xingwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02691v1.pdf","comment":"24 pages, submitted to TOIS"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.01690v1","updated":"2024-08-03T07:05:40Z","published":"2024-08-03T07:05:40Z","title":"IDNet: A Novel Dataset for Identity Document Analysis and Fraud\n  Detection","summary":"  Effective fraud detection and analysis of government-issued identity\ndocuments, such as passports, driver's licenses, and identity cards, are\nessential in thwarting identity theft and bolstering security on online\nplatforms. The training of accurate fraud detection and analysis tools depends\non the availability of extensive identity document datasets. However, current\npublicly available benchmark datasets for identity document analysis, including\nMIDV-500, MIDV-2020, and FMIDV, fall short in several respects: they offer a\nlimited number of samples, cover insufficient varieties of fraud patterns, and\nseldom include alterations in critical personal identifying fields like\nportrait images, limiting their utility in training models capable of detecting\nrealistic frauds while preserving privacy.\n  In response to these shortcomings, our research introduces a new benchmark\ndataset, IDNet, designed to advance privacy-preserving fraud detection efforts.\nThe IDNet dataset comprises 837,060 images of synthetically generated identity\ndocuments, totaling approximately 490 gigabytes, categorized into 20 types from\n$10$ U.S. states and 10 European countries. We evaluate the utility and present\nuse cases of the dataset, illustrating how it can aid in training\nprivacy-preserving fraud detection methods, facilitating the generation of\ncamera and video capturing of identity documents, and testing schema\nunification and other identity document management functionalities.\n","authors":["Hong Guan","Yancheng Wang","Lulu Xie","Soham Nag","Rajeev Goel","Niranjan Erappa Narayana Swamy","Yingzhen Yang","Chaowei Xiao","Jonathan Prisby","Ross Maciejewski","Jia Zou"],"pdf_url":"https://arxiv.org/pdf/2408.01690v1.pdf","comment":"40 pages"},{"id":"http://arxiv.org/abs/2408.01679v1","updated":"2024-08-03T06:35:54Z","published":"2024-08-03T06:35:54Z","title":"MMPKUBase: A Comprehensive and High-quality Chinese Multi-modal\n  Knowledge Graph","summary":"  Multi-modal knowledge graphs have emerged as a powerful approach for\ninformation representation, combining data from different modalities such as\ntext, images, and videos. While several such graphs have been constructed and\nhave played important roles in applications like visual question answering and\nrecommendation systems, challenges persist in their development. These include\nthe scarcity of high-quality Chinese knowledge graphs and limited domain\ncoverage in existing multi-modal knowledge graphs. This paper introduces\nMMPKUBase, a robust and extensive Chinese multi-modal knowledge graph that\ncovers diverse domains, including birds, mammals, ferns, and more, comprising\nover 50,000 entities and over 1 million filtered images. To ensure data\nquality, we employ Prototypical Contrastive Learning and the Isolation Forest\nalgorithm to refine the image data. Additionally, we have developed a\nuser-friendly platform to facilitate image attribute exploration.\n","authors":["Xuan Yi","Yanzeng Li","Lei Zou"],"pdf_url":"https://arxiv.org/pdf/2408.01679v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01669v1","updated":"2024-08-03T05:35:13Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v1.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.01668v1","updated":"2024-08-03T05:34:53Z","published":"2024-08-03T05:34:53Z","title":"Multiple Contexts and Frequencies Aggregation Network forDeepfake\n  Detection","summary":"  Deepfake detection faces increasing challenges since the fast growth of\ngenerative models in developing massive and diverse Deepfake technologies.\nRecent advances rely on introducing heuristic features from spatial or\nfrequency domains rather than modeling general forgery features within\nbackbones. To address this issue, we turn to the backbone design with two\nintuitive priors from spatial and frequency detectors, \\textit{i.e.,} learning\nrobust spatial attributes and frequency distributions that are discriminative\nfor real and fake samples. To this end, we propose an efficient network for\nface forgery detection named MkfaNet, which consists of two core modules. For\nspatial contexts, we design a Multi-Kernel Aggregator that adaptively selects\norgan features extracted by multiple convolutions for modeling subtle facial\ndifferences between real and fake faces. For the frequency components, we\npropose a Multi-Frequency Aggregator to process different bands of frequency\ncomponents by adaptively reweighing high-frequency and low-frequency features.\nComprehensive experiments on seven popular deepfake detection benchmarks\ndemonstrate that our proposed MkfaNet variants achieve superior performances in\nboth within-domain and across-domain evaluations with impressive efficiency of\nparameter usage.\n","authors":["Zifeng Li","Wenzhong Tang","Shijun Gao","Shuai Wang","Yanxiang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01668v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01651v1","updated":"2024-08-03T03:30:57Z","published":"2024-08-03T03:30:57Z","title":"Music2P: A Multi-Modal AI-Driven Tool for Simplifying Album Cover Design","summary":"  In today's music industry, album cover design is as crucial as the music\nitself, reflecting the artist's vision and brand. However, many AI-driven album\ncover services require subscriptions or technical expertise, limiting\naccessibility. To address these challenges, we developed Music2P, an\nopen-source, multi-modal AI-driven tool that streamlines album cover creation,\nmaking it efficient, accessible, and cost-effective through Ngrok. Music2P\nautomates the design process using techniques such as Bootstrapping Language\nImage Pre-training (BLIP), music-to-text conversion (LP-music-caps), image\nsegmentation (LoRA), and album cover and QR code generation (ControlNet). This\npaper demonstrates the Music2P interface, details our application of these\ntechnologies, and outlines future improvements. Our ultimate goal is to provide\na tool that empowers musicians and producers, especially those with limited\nresources or expertise, to create compelling album covers.\n","authors":["Joong Ho Choi","Geonyeong Choi","Ji-Eun Han","Wonjin Yang","Zhi-Qi Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.01651v1.pdf","comment":"Accepted at CIKM 2024 Demo Paper track. Project available at\n  https://github.com/JC-78/Music2P"},{"id":"http://arxiv.org/abs/2407.09026v2","updated":"2024-08-03T02:22:34Z","published":"2024-07-12T06:34:24Z","title":"HPC: Hierarchical Progressive Coding Framework for Volumetric Video","summary":"  Volumetric video based on Neural Radiance Field (NeRF) holds vast potential\nfor various 3D applications, but its substantial data volume poses significant\nchallenges for compression and transmission. Current NeRF compression lacks the\nflexibility to adjust video quality and bitrate within a single model for\nvarious network and device capacities. To address these issues, we propose HPC,\na novel hierarchical progressive volumetric video coding framework achieving\nvariable bitrate using a single model. Specifically, HPC introduces a\nhierarchical representation with a multi-resolution residual radiance field to\nreduce temporal redundancy in long-duration sequences while simultaneously\ngenerating various levels of detail. Then, we propose an end-to-end progressive\nlearning approach with a multi-rate-distortion loss function to jointly\noptimize both hierarchical representation and compression. Our HPC trained only\nonce can realize multiple compression levels, while the current methods need to\ntrain multiple fixed-bitrate models for different rate-distortion (RD)\ntradeoffs. Extensive experiments demonstrate that HPC achieves flexible quality\nlevels with variable bitrate by a single model and exhibits competitive RD\nperformance, even outperforming fixed-bitrate models across various datasets.\n","authors":["Zihan Zheng","Houqiang Zhong","Qiang Hu","Xiaoyun Zhang","Li Song","Ya Zhang","Yanfeng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.09026v2.pdf","comment":"11 pages, 7 figures, ACM Multimedia 24"}]},"2024-08-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.03326v1","updated":"2024-08-06T17:59:44Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v1.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2408.03325v1","updated":"2024-08-06T17:58:53Z","published":"2024-08-06T17:58:53Z","title":"CoverBench: A Challenging Benchmark for Complex Claim Verification","summary":"  There is a growing line of research on verifying the correctness of language\nmodels' outputs. At the same time, LMs are being used to tackle complex queries\nthat require reasoning. We introduce CoverBench, a challenging benchmark\nfocused on verifying LM outputs in complex reasoning settings. Datasets that\ncan be used for this purpose are often designed for other complex reasoning\ntasks (e.g., QA) targeting specific use-cases (e.g., financial tables),\nrequiring transformations, negative sampling and selection of hard examples to\ncollect such a benchmark. CoverBench provides a diversified evaluation for\ncomplex claim verification in a variety of domains, types of reasoning,\nrelatively long inputs, and a variety of standardizations, such as multiple\nrepresentations for tables where available, and a consistent schema. We\nmanually vet the data for quality to ensure low levels of label noise. Finally,\nwe report a variety of competitive baseline results to show CoverBench is\nchallenging and has very significant headroom. The data is available at\nhttps://huggingface.co/datasets/google/coverbench .\n","authors":["Alon Jacovi","Moran Ambar","Eyal Ben-David","Uri Shaham","Amir Feder","Mor Geva","Dror Marcus","Avi Caciularu"],"pdf_url":"https://arxiv.org/pdf/2408.03325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03319v1","updated":"2024-08-06T17:51:42Z","published":"2024-08-06T17:51:42Z","title":"Training LLMs to Recognize Hedges in Spontaneous Narratives","summary":"  Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.\n","authors":["Amie J. Paige","Adil Soubki","John Murzaku","Owen Rambow","Susan E. Brennan"],"pdf_url":"https://arxiv.org/pdf/2408.03319v1.pdf","comment":"Amie Paige, Adil Soubki, and John Murzaku contributed equally to this\n  study"},{"id":"http://arxiv.org/abs/2408.03314v1","updated":"2024-08-06T17:35:05Z","published":"2024-08-06T17:35:05Z","title":"Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling Model Parameters","summary":"  Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.\n","authors":["Charlie Snell","Jaehoon Lee","Kelvin Xu","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2408.03314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04492v2","updated":"2024-08-06T17:31:33Z","published":"2024-02-07T00:31:49Z","title":"ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation","summary":"  This paper introduces the ColorSwap dataset, designed to assess and improve\nthe proficiency of multimodal models in matching objects with their colors. The\ndataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000\nexamples. Each example includes a caption-image pair, along with a\n``color-swapped'' pair. We follow the Winoground schema: the two captions in an\nexample have the same words, but the color words have been rearranged to modify\ndifferent objects. The dataset was created through a novel blend of automated\ncaption and image generation with humans in the loop. We evaluate image-text\nmatching (ITM) and visual language models (VLMs) and find that even the latest\nones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on\nour main VLM metric, although they may improve with more advanced prompting\ntechniques. On the main ITM metric, contrastive models such as CLIP and SigLIP\nperform close to chance (at 12% and 30%, respectively), although the\nnon-contrastive BLIP ITM model is stronger (87%). We also find that finetuning\non fewer than 2,000 examples yields significant performance gains on this\nout-of-distribution word-order understanding task. The dataset is here:\nhttps://github.com/Top34051/colorswap and here:\nhttps://huggingface.co/datasets/stanfordnlp/colorswap.\n","authors":["Jirayu Burapacheep","Ishan Gaur","Agam Bhatia","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2402.04492v2.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2408.03297v1","updated":"2024-08-06T16:55:54Z","published":"2024-08-06T16:55:54Z","title":"KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge\n  Selection in Retrieval-Augmented Language Models","summary":"  By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.\n","authors":["Ruizhe Zhang","Yongxin Xu","Yuzhen Xiao","Runchuan Zhu","Xinke Jiang","Xu Chu","Junfeng Zhao","Yasha Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03297v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03290v1","updated":"2024-08-06T16:39:42Z","published":"2024-08-06T16:39:42Z","title":"SARA: Singular-Value Based Adaptive Low-Rank Adaption","summary":"  With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.\n","authors":["Jihao Gu","Shuai Chen","Zelin Wang","Yibo Zhang","Ping Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03281v1","updated":"2024-08-06T16:28:30Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v1.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval;Leaderboard\n  at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03256v1","updated":"2024-08-06T15:40:32Z","published":"2024-08-06T15:40:32Z","title":"Synthesizing Text-to-SQL Data from Weak and Strong LLMs","summary":"  The capability gap between open-source and closed-source large language\nmodels (LLMs) remains a challenge in text-to-SQL tasks. In this paper, we\nintroduce a synthetic data approach that combines data produced by larger, more\npowerful models (strong models) with error information data generated by\nsmaller, not well-aligned models (weak models). The method not only enhances\nthe domain generalization of text-to-SQL models but also explores the potential\nof error data supervision through preference learning. Furthermore, we employ\nthe synthetic data approach for instruction tuning on open-source LLMs,\nresulting SENSE, a specialized text-to-SQL model. The effectiveness of SENSE is\ndemonstrated through state-of-the-art results on the SPIDER and BIRD\nbenchmarks, bridging the performance gap between open-source models and methods\nprompted by closed-source models.\n","authors":["Jiaxi Yang","Binyuan Hui","Min Yang","Jian Yang","Junyang Lin","Chang Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03256v1.pdf","comment":"12 pages, 7 figures, ACL 2024"},{"id":"http://arxiv.org/abs/2402.15745v2","updated":"2024-08-06T15:28:30Z","published":"2024-02-24T06:57:15Z","title":"GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models\n  Evaluation","summary":"  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.\n","authors":["Yi Zong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2402.15745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08354v3","updated":"2024-08-06T15:14:36Z","published":"2023-04-17T15:16:10Z","title":"Tool Learning with Foundation Models","summary":"  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n","authors":["Yujia Qin","Shengding Hu","Yankai Lin","Weize Chen","Ning Ding","Ganqu Cui","Zheni Zeng","Yufei Huang","Chaojun Xiao","Chi Han","Yi Ren Fung","Yusheng Su","Huadong Wang","Cheng Qian","Runchu Tian","Kunlun Zhu","Shihao Liang","Xingyu Shen","Bokai Xu","Zhen Zhang","Yining Ye","Bowen Li","Ziwei Tang","Jing Yi","Yuzhang Zhu","Zhenning Dai","Lan Yan","Xin Cong","Yaxi Lu","Weilin Zhao","Yuxiang Huang","Junxi Yan","Xu Han","Xian Sun","Dahai Li","Jason Phang","Cheng Yang","Tongshuang Wu","Heng Ji","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.08354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03247v1","updated":"2024-08-06T15:07:08Z","published":"2024-08-06T15:07:08Z","title":"Unveiling Factual Recall Behaviors of Large Language Models through\n  Knowledge Neurons","summary":"  In this paper, we investigate whether Large Language Models (LLMs) actively\nrecall or retrieve their internal repositories of factual knowledge when faced\nwith reasoning tasks. Through an analysis of LLMs' internal factual recall at\neach reasoning step via Knowledge Neurons, we reveal that LLMs fail to harness\nthe critical factual associations under certain circumstances. Instead, they\ntend to opt for alternative, shortcut-like pathways to answer reasoning\nquestions. By manually manipulating the recall process of parametric knowledge\nin LLMs, we demonstrate that enhancing this recall process directly improves\nreasoning performance whereas suppressing it leads to notable degradation.\nFurthermore, we assess the effect of Chain-of-Thought (CoT) prompting, a\npowerful technique for addressing complex reasoning tasks. Our findings\nindicate that CoT can intensify the recall of factual knowledge by encouraging\nLLMs to engage in orderly and reliable reasoning. Furthermore, we explored how\ncontextual conflicts affect the retrieval of facts during the reasoning process\nto gain a comprehensive understanding of the factual recall behaviors of LLMs.\nCode and data will be available soon.\n","authors":["Yifei Wang","Yuheng Chen","Wanting Wen","Yu Sheng","Linjing Li","Daniel Dajun Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.03247v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03246v1","updated":"2024-08-06T15:06:40Z","published":"2024-08-06T15:06:40Z","title":"Making Long-Context Language Models Better Multi-Hop Reasoners","summary":"  Recent advancements in long-context modeling have enhanced language models\n(LMs) for complex tasks across multiple NLP applications. Despite this\nprogress, we find that these models struggle with multi-hop reasoning and\nexhibit decreased performance in the presence of noisy contexts. In this paper,\nwe introduce Reasoning with Attributions, a novel approach that prompts LMs to\nsupply attributions for each assertion during their reasoning. We validate our\napproach through experiments on three multi-hop datasets, employing both\nproprietary and open-source models, and demonstrate its efficacy and\nresilience. Furthermore, we explore methods to augment reasoning capabilities\nvia fine-tuning and offer an attribution-annotated dataset and a specialized\ntraining strategy. Our fine-tuned model achieves competitive performance on\nmulti-hop reasoning benchmarks, closely paralleling proprietary LMs such as\nChatGPT and Claude-instant.\n","authors":["Yanyang Li","Shuo Liang","Michael R. Lyu","Liwei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03246v1.pdf","comment":"ACL 2024 Main Conference Camera Ready; Dataset, model, and code are\n  available at https://github.com/LaVi-Lab/LongContextReasoner"},{"id":"http://arxiv.org/abs/2404.03623v2","updated":"2024-08-06T15:02:33Z","published":"2024-04-04T17:45:59Z","title":"Unveiling LLMs: The Evolution of Latent Representations in a Dynamic\n  Knowledge Graph","summary":"  Large Language Models (LLMs) demonstrate an impressive capacity to recall a\nvast range of factual knowledge. However, understanding their underlying\nreasoning and internal mechanisms in exploiting this knowledge remains a key\nresearch area. This work unveils the factual information an LLM represents\ninternally for sentence-level claim verification. We propose an end-to-end\nframework to decode factual knowledge embedded in token representations from a\nvector space to a set of ground predicates, showing its layer-wise evolution\nusing a dynamic knowledge graph. Our framework employs activation patching, a\nvector-level technique that alters a token representation during inference, to\nextract encoded knowledge. Accordingly, we neither rely on training nor\nexternal models. Using factual and common-sense claims from two claim\nverification datasets, we showcase interpretability analyses at local and\nglobal levels. The local analysis highlights entity centrality in LLM\nreasoning, from claim-related information and multi-hop reasoning to\nrepresentation errors causing erroneous evaluation. On the other hand, the\nglobal reveals trends in the underlying evolution, such as word-based knowledge\nevolving into claim-related facts. By interpreting semantics from LLM latent\nrepresentations and enabling graph-related analyses, this work enhances the\nunderstanding of the factual knowledge resolution process.\n","authors":["Marco Bronzini","Carlo Nicolini","Bruno Lepri","Jacopo Staiano","Andrea Passerini"],"pdf_url":"https://arxiv.org/pdf/2404.03623v2.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2403.03823v7","updated":"2024-08-06T14:47:11Z","published":"2024-03-06T16:10:01Z","title":"A Modular Approach for Multimodal Summarization of TV Shows","summary":"  In this paper we address the task of summarizing television shows, which\ntouches key areas in AI research: complex reasoning, multiple modalities, and\nlong narratives. We present a modular approach where separate components\nperform specialized sub-tasks which we argue affords greater flexibility\ncompared to end-to-end methods. Our modules involve detecting scene boundaries,\nreordering scenes so as to minimize the number of cuts between different\nevents, converting visual information to text, summarizing the dialogue in each\nscene, and fusing the scene summaries into a final summary for the entire\nepisode. We also present a new metric, PRISMA (Precision and Recall EvaluatIon\nof Summary FActs), to measure both precision and recall of generated summaries,\nwhich we decompose into atomic facts. Tested on the recently released\nSummScreen3D dataset, our method produces higher quality summaries than\ncomparison models, as measured with ROUGE and our new fact-based metric, and as\nassessed by human evaluators.\n","authors":["Louis Mahon","Mirella Lapata"],"pdf_url":"https://arxiv.org/pdf/2403.03823v7.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20089v2","updated":"2024-08-06T14:46:29Z","published":"2024-05-30T14:25:56Z","title":"The Fine-Tuning Paradox: Boosting Translation Quality Without\n  Sacrificing LLM Abilities","summary":"  Fine-tuning large language models (LLMs) for machine translation has shown\nimprovements in overall translation quality. However, it is unclear what is the\nimpact of fine-tuning on desirable LLM behaviors that are not present in neural\nmachine translation models, such as steerability, inherent document-level\ntranslation abilities, and the ability to produce less literal translations. We\nperform an extensive translation evaluation on the LLaMA and Falcon family of\nmodels with model size ranging from 7 billion up to 65 billion parameters. Our\nresults show that while fine-tuning improves the general translation quality of\nLLMs, several abilities degrade. In particular, we observe a decline in the\nability to perform formality steering, to produce technical translations\nthrough few-shot examples, and to perform document-level translation. On the\nother hand, we observe that the model produces less literal translations after\nfine-tuning on parallel data. We show that by including monolingual data as\npart of the fine-tuning data we can maintain the abilities while simultaneously\nenhancing overall translation quality. Our findings emphasize the need for\nfine-tuning strategies that preserve the benefits of LLMs for machine\ntranslation.\n","authors":["David Stap","Eva Hasler","Bill Byrne","Christof Monz","Ke Tran"],"pdf_url":"https://arxiv.org/pdf/2405.20089v2.pdf","comment":"Accepted to ACL 2024 (long, main). Latest version includes link to\n  the IdiomsInCtx-MT dataset"},{"id":"http://arxiv.org/abs/2404.05530v2","updated":"2024-08-06T14:30:31Z","published":"2024-04-08T13:59:02Z","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.\n","authors":["Tim Baumgrtner","Yang Gao","Dana Alon","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2404.05530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03202v1","updated":"2024-08-06T14:00:23Z","published":"2024-08-06T14:00:23Z","title":"A Debiased Nearest Neighbors Framework for Multi-Label Text\n  Classification","summary":"  Multi-Label Text Classification (MLTC) is a practical yet challenging task\nthat involves assigning multiple non-exclusive labels to each document.\nPrevious studies primarily focus on capturing label correlations to assist\nlabel prediction by introducing special labeling schemes, designing specific\nmodel structures, or adding auxiliary tasks. Recently, the $k$ Nearest Neighbor\n($k$NN) framework has shown promise by retrieving labeled samples as references\nto mine label co-occurrence information in the embedding space. However, two\ncritical biases, namely embedding alignment bias and confidence estimation\nbias, are often overlooked, adversely affecting prediction performance. In this\npaper, we introduce a DEbiased Nearest Neighbors (DENN) framework for MLTC,\nspecifically designed to mitigate these biases. To address embedding alignment\nbias, we propose a debiased contrastive learning strategy, enhancing neighbor\nconsistency on label co-occurrence. For confidence estimation bias, we present\na debiased confidence estimation strategy, improving the adaptive combination\nof predictions from $k$NN and inductive binary classifications. Extensive\nexperiments conducted on four public benchmark datasets (i.e., AAPD, RCV1-V2,\nAmazon-531, and EUR-LEX57K) showcase the effectiveness of our proposed method.\nBesides, our method does not introduce any extra parameters.\n","authors":["Zifeng Cheng","Zhiwei Jiang","Yafeng Yin","Zhaoling Chen","Cong Wang","Shiping Ge","Qiguo Huang","Qing Gu"],"pdf_url":"https://arxiv.org/pdf/2408.03202v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00655v3","updated":"2024-08-06T13:38:50Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context","summary":"  Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aimed at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a\nSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectively\ncondense the information within a sentence into a singular token, while the\nSentence Decoder can reconstruct this compressed token back into sentence. By\nintegrating SentenceVAE into the input and output layers of LLMs, we develop\nSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference\nmethod. In addition, the SentenceVAE module of SLLMs can maintain the integrity\nof the original semantic content by segmenting the context into sentences,\nthereby improving accuracy while boosting inference speed. Moreover, compared\nto previous LLMs, SLLMs process fewer tokens over equivalent context length,\nsignificantly reducing memory demands for self-attention computation and\nfacilitating the handling of longer context. Extensive experiments on Wanjuan\ndataset have reveal that the proposed method can accelerate inference speed by\n204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and\ndecrease memory overhead by 86~91% for the equivalent context length, compared\nto the token-by-token method.\n","authors":["Hongjun An","Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v3.pdf","comment":"update the article"},{"id":"http://arxiv.org/abs/2408.03172v1","updated":"2024-08-06T13:16:16Z","published":"2024-08-06T13:16:16Z","title":"Leveraging Parameter Efficient Training Methods for Low Resource Text\n  Classification: A Case Study in Marathi","summary":"  With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.\n","authors":["Pranita Deshmukh","Nikita Kulkarni","Sanhita Kulkarni","Kareena Manghani","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2408.03172v1.pdf","comment":"Accepted at I2CT 2024"},{"id":"http://arxiv.org/abs/2408.03150v1","updated":"2024-08-06T12:49:33Z","published":"2024-08-06T12:49:33Z","title":"Conditioning LLMs with Emotion in Neural Machine Translation","summary":"  Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.\n","authors":["Charles Brazier","Jean-Luc Rouas"],"pdf_url":"https://arxiv.org/pdf/2408.03150v1.pdf","comment":"6 pages, In Proceedings of the 21st International Conference on\n  Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024"},{"id":"http://arxiv.org/abs/2408.03149v1","updated":"2024-08-06T12:45:56Z","published":"2024-08-06T12:45:56Z","title":"Leveraging Entity Information for Cross-Modality Correlation Learning:\n  The Entity-Guided Multimodal Summarization","summary":"  The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.\n","authors":["Yanghai Zhang","Ye Liu","Shiwei Wu","Kai Zhang","Xukai Liu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03149v1.pdf","comment":"In ACL-Findings 2024"},{"id":"http://arxiv.org/abs/2408.03130v1","updated":"2024-08-06T12:07:32Z","published":"2024-08-06T12:07:32Z","title":"Inference Optimizations for Large Language Models: Effects, Challenges,\n  and Practical Considerations","summary":"  Large language models are ubiquitous in natural language processing because\nthey can adapt to new tasks without retraining. However, their sheer scale and\ncomplexity present unique challenges and opportunities, prompting researchers\nand practitioners to explore novel model training, optimization, and deployment\nmethods. This literature review focuses on various techniques for reducing\nresource requirements and compressing large language models, including\nquantization, pruning, knowledge distillation, and architectural optimizations.\nThe primary objective is to explore each method in-depth and highlight its\nunique challenges and practical applications. The discussed methods are\ncategorized into a taxonomy that presents an overview of the optimization\nlandscape and helps navigate it to understand the research trajectory better.\n","authors":["Leo Donisch","Sigurd Schacht","Carsten Lanquillon"],"pdf_url":"https://arxiv.org/pdf/2408.03130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03127v1","updated":"2024-08-06T11:59:09Z","published":"2024-08-06T11:59:09Z","title":"Lisbon Computational Linguists at SemEval-2024 Task 2: Using A Mistral\n  7B Model and Data Augmentation","summary":"  This paper describes our approach to the SemEval-2024 safe biomedical Natural\nLanguage Inference for Clinical Trials (NLI4CT) task, which concerns\nclassifying statements about Clinical Trial Reports (CTRs). We explored the\ncapabilities of Mistral-7B, a generalist open-source Large Language Model\n(LLM). We developed a prompt for the NLI4CT task, and fine-tuned a quantized\nversion of the model using an augmented version of the training dataset. The\nexperimental results show that this approach can produce notable results in\nterms of the macro F1-score, while having limitations in terms of faithfulness\nand consistency. All the developed code is publicly available on a GitHub\nrepository\n","authors":["Artur Guimares","Bruno Martins","Joo Magalhes"],"pdf_url":"https://arxiv.org/pdf/2408.03127v1.pdf","comment":"8 pages, 1 figure, submitted and accepted into the \"18th\n  International Workshop on Semantic Evaluation (SemEval-2024)\""},{"id":"http://arxiv.org/abs/2404.01869v2","updated":"2024-08-06T11:58:53Z","published":"2024-04-02T11:46:31Z","title":"Beyond Accuracy: Evaluating the Reasoning Behavior of Large Language\n  Models -- A Survey","summary":"  Large language models (LLMs) have recently shown impressive performance on\ntasks involving reasoning, leading to a lively debate on whether these models\npossess reasoning capabilities similar to humans. However, despite these\nsuccesses, the depth of LLMs' reasoning abilities remains uncertain. This\nuncertainty partly stems from the predominant focus on task performance,\nmeasured through shallow accuracy metrics, rather than a thorough investigation\nof the models' reasoning behavior. This paper seeks to address this gap by\nproviding a comprehensive review of studies that go beyond task accuracy,\noffering deeper insights into the models' reasoning processes. Furthermore, we\nsurvey prevalent methodologies to evaluate the reasoning behavior of LLMs,\nemphasizing current trends and efforts towards more nuanced reasoning analyses.\nOur review suggests that LLMs tend to rely on surface-level patterns and\ncorrelations in their training data, rather than on sophisticated reasoning\nabilities. Additionally, we identify the need for further research that\ndelineates the key differences between human and LLM-based reasoning. Through\nthis survey, we aim to shed light on the complex reasoning processes within\nLLMs.\n","authors":["Philipp Mondorf","Barbara Plank"],"pdf_url":"https://arxiv.org/pdf/2404.01869v2.pdf","comment":"COLM 2024, 27 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.03125v1","updated":"2024-08-06T11:56:26Z","published":"2024-08-06T11:56:26Z","title":"COMMENTATOR: A Code-mixed Multilingual Text Annotation Framework","summary":"  As the NLP community increasingly addresses challenges associated with\nmultilingualism, robust annotation tools are essential to handle multilingual\ndatasets efficiently. In this paper, we introduce a code-mixed multilingual\ntext annotation framework, COMMENTATOR, specifically designed for annotating\ncode-mixed text. The tool demonstrates its effectiveness in token-level and\nsentence-level language annotation tasks for Hinglish text. We perform robust\nqualitative human-based evaluations to showcase COMMENTATOR led to 5x faster\nannotations than the best baseline. Our code is publicly available at\n\\url{https://github.com/lingo-iitgn/commentator}. The demonstration video is\navailable at \\url{https://bit.ly/commentator_video}.\n","authors":["Rajvee Sheth","Shubh Nisar","Heenaben Prajapati","Himanshu Beniwal","Mayank Singh"],"pdf_url":"https://arxiv.org/pdf/2408.03125v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03119v1","updated":"2024-08-06T11:49:11Z","published":"2024-08-06T11:49:11Z","title":"Evaluating the Translation Performance of Large Language Models Based on\n  Euas-20","summary":"  In recent years, with the rapid development of deep learning technology,\nlarge language models (LLMs) such as BERT and GPT have achieved breakthrough\nresults in natural language processing tasks. Machine translation (MT), as one\nof the core tasks of natural language processing, has also benefited from the\ndevelopment of large language models and achieved a qualitative leap. Despite\nthe significant progress in translation performance achieved by large language\nmodels, machine translation still faces many challenges. Therefore, in this\npaper, we construct the dataset Euas-20 to evaluate the performance of large\nlanguage models on translation tasks, the translation ability on different\nlanguages, and the effect of pre-training data on the translation ability of\nLLMs for researchers and developers.\n","authors":["Yan Huang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03119v1.pdf","comment":"15 pages, 8 figures"},{"id":"http://arxiv.org/abs/2401.06824v3","updated":"2024-08-06T11:46:37Z","published":"2024-01-12T00:50:04Z","title":"Rethinking Jailbreaking through the Lens of Representation Engineering","summary":"  The recent surge in jailbreaking methods has revealed the vulnerability of\nLarge Language Models (LLMs) to malicious inputs. While earlier research has\nprimarily concentrated on increasing the success rates of jailbreaking attacks,\nthe underlying mechanism for safeguarding LLMs remains underexplored. This\nstudy investigates the vulnerability of safety-aligned LLMs by uncovering\nspecific activity patterns within the representation space generated by LLMs.\nSuch ``safety patterns'' can be identified with only a few pairs of contrastive\nqueries in a simple method and function as ``keys'' (used as a metaphor for\nsecurity defense capability) that can be used to open or lock Pandora's Box of\nLLMs. Extensive experiments demonstrate that the robustness of LLMs against\njailbreaking can be lessened or augmented by attenuating or strengthening the\nidentified safety patterns. These findings deepen our understanding of\njailbreaking phenomena and call for the LLM community to address the potential\nmisuse of open-source LLMs.\n","authors":["Tianlong Li","Shihan Dou","Wenhao Liu","Muling Wu","Changze Lv","Rui Zheng","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2401.06824v3.pdf","comment":"21 pages, 20 figures, 6 tables"},{"id":"http://arxiv.org/abs/2407.03234v3","updated":"2024-08-06T11:15:00Z","published":"2024-07-03T16:03:42Z","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","summary":"  We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","authors":["Hannah Brown","Leon Lin","Kenji Kawaguchi","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2407.03234v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2305.11326v3","updated":"2024-08-06T11:14:04Z","published":"2023-05-18T22:23:40Z","title":"Automatic Generation of Conversational Interfaces for Tabular Data\n  Analysis","summary":"  Tabular data is the most common format to publish and exchange structured\ndata online. A clear example is the growing number of open data portals\npublished by public administrations. However, exploitation of these data\nsources is currently limited to technical people able to programmatically\nmanipulate and digest such data. As an alternative, we propose the use of\nchatbots to offer a conversational interface to facilitate the exploration of\ntabular data sources, including support for data analytics questions that are\nresponded via charts rendered by the chatbot. Moreover, our chatbots are\nautomatically generated from the data source itself thanks to the instantiation\nof a configurable collection of conversation patterns matched to the chatbot\nintents and entities.\n","authors":["Marcos Gomez-Vazquez","Jordi Cabot","Robert Claris"],"pdf_url":"https://arxiv.org/pdf/2305.11326v3.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03099v1","updated":"2024-08-06T11:04:07Z","published":"2024-08-06T11:04:07Z","title":"Topic Modeling with Fine-tuning LLMs and Bag of Sentences","summary":"  Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.03099v1.pdf","comment":"This is the submitted journal version of enhanced with the novel\n  fine-tuning part of \"Efficient and Flexible Topic Modeling using Pretrained\n  Embeddings and Bag of Sentences'' which appeared at the International\n  Conference on Agents and Artificial Intelligence(ICAART) in 2024"},{"id":"http://arxiv.org/abs/2408.03094v1","updated":"2024-08-06T10:51:47Z","published":"2024-08-06T10:51:47Z","title":"500xCompressor: Generalized Prompt Compression for Large Language Models","summary":"  Prompt compression is crucial for enhancing inference speed, reducing costs,\nand improving user experience. However, current methods face challenges such as\nlow compression ratios and potential data leakage during evaluation. To address\nthese issues, we propose 500xCompressor, a method that compresses extensive\nnatural language contexts into a minimum of one single special token. The\n500xCompressor introduces approximately 0.3% additional parameters and achieves\ncompression ratios ranging from 6x to 480x. It is designed to compress any\ntext, answer various types of questions, and could be utilized by the original\nlarge language model (LLM) without requiring fine-tuning. Initially,\n500xCompressor was pretrained on the Arxiv Corpus, followed by fine-tuning on\nthe ArxivQA dataset, and subsequently evaluated on strictly unseen and\nclassical question answering (QA) datasets. The results demonstrate that the\nLLM retained 62.26-72.89% of its capabilities compared to using non-compressed\nprompts. This study also shows that not all the compressed tokens are equally\nutilized and that K V values have significant advantages over embeddings in\npreserving information at high compression ratios. The highly compressive\nnature of natural language prompts, even for fine-grained complex information,\nsuggests promising potential for future applications and further research into\ndeveloping a new LLM language.\n","authors":["Zongqian Li","Yixuan Su","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2408.03094v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03092v1","updated":"2024-08-06T10:46:46Z","published":"2024-08-06T10:46:46Z","title":"Extend Model Merging from Fine-Tuned to Pre-Trained Large Language\n  Models via Weight Disentanglement","summary":"  Merging Large Language Models (LLMs) aims to amalgamate multiple homologous\nLLMs into one with all the capabilities. Ideally, any LLMs sharing the same\nbackbone should be mergeable, irrespective of whether they are Fine-Tuned (FT)\nwith minor parameter changes or Pre-Trained (PT) with substantial parameter\nshifts. However, existing methods often manually assign the model importance,\nrendering them feasible only for LLMs with similar parameter alterations, such\nas multiple FT LLMs. The diverse parameter changed ranges between FT and PT\nLLMs pose challenges for current solutions in empirically determining the\noptimal combination. In this paper, we make a pioneering effort to broaden the\napplicability of merging techniques from FT to PT LLMs. We initially examine\nthe efficacy of current methods in merging FT and PT LLMs, discovering that\nthey struggle to deal with PT LLMs. Subsequently, we introduce an approach\nbased on WeIght DisENtanglement (WIDEN) to effectively extend the merging\nscope, which first disentangles model weights into magnitude and direction\ncomponents, and then performs adaptive fusion by considering their respective\ncontributions. In the experiments, we merge Qwen1.5-Chat (an FT LLM with\ninstruction-following skills) with Sailor (a PT LLM with multilingual\nabilities) across 7B and 14B model scales. Results reveal that: (1) existing\nsolutions usually fail when merging Sailor, either losing both abilities or\nonly retaining instruction-following skills; (2) WIDEN successfully injects the\nmultilingual abilities of Sailor into Qwen1.5-Chat and make it proficient in\nSoutheast Asian languages, achieving enhancements in the fundamental\ncapabilities. In light of previous research, we also merge multiple 13B FT LLMs\nand observe that WIDEN achieves a balanced amalgamation of instruction\nfollowing, mathematical reasoning, and code generation skills.\n","authors":["Le Yu","Bowen Yu","Haiyang Yu","Fei Huang","Yongbin Li"],"pdf_url":"https://arxiv.org/pdf/2408.03092v1.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2405.09948v2","updated":"2024-08-06T10:41:25Z","published":"2024-05-16T09:52:21Z","title":"Mitigating Text Toxicity with Counterfactual Generation","summary":"  Toxicity mitigation consists in rephrasing text in order to remove offensive\nor harmful meaning. Neural natural language processing (NLP) models have been\nwidely used to target and mitigate textual toxicity. However, existing methods\nfail to detoxify text while preserving the initial non-toxic meaning at the\nsame time. In this work, we propose to apply counterfactual generation methods\nfrom the eXplainable AI (XAI) field to target and mitigate textual toxicity. In\nparticular, we perform text detoxification by applying local feature importance\nand counterfactual generation methods to a toxicity classifier distinguishing\nbetween toxic and non-toxic texts. We carry out text detoxification through\ncounterfactual generation on three datasets and compare our approach to three\ncompetitors. Automatic and human evaluations show that recently developed NLP\ncounterfactual generators can mitigate toxicity accurately while better\npreserving the meaning of the initial text as compared to classical\ndetoxification methods. Finally, we take a step back from using automated\ndetoxification tools, and discuss how to manage the polysemous nature of\ntoxicity and the risk of malicious use of detoxification tools. This work is\nthe first to bridge the gap between counterfactual generation and text\ndetoxification and paves the way towards more practical application of XAI\nmethods.\n","authors":["Milan Bhan","Jean-Noel Vittaut","Nina Achache","Victor Legrand","Nicolas Chesneau","Annabelle Blangero","Juliette Murris","Marie-Jeanne Lesot"],"pdf_url":"https://arxiv.org/pdf/2405.09948v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03079v1","updated":"2024-08-06T10:15:15Z","published":"2024-08-06T10:15:15Z","title":"Enhancing Complex Causality Extraction via Improved Subtask Interaction\n  and Knowledge Fusion","summary":"  Event Causality Extraction (ECE) aims at extracting causal event pairs from\ntexts. Despite ChatGPT's recent success, fine-tuning small models remains the\nbest approach for the ECE task. However, existing fine-tuning based ECE methods\ncannot address all three key challenges in ECE simultaneously: 1) Complex\nCausality Extraction, where multiple causal-effect pairs occur within a single\nsentence; 2) Subtask~ Interaction, which involves modeling the mutual\ndependence between the two subtasks of ECE, i.e., extracting events and\nidentifying the causal relationship between extracted events; and 3) Knowledge\nFusion, which requires effectively fusing the knowledge in two modalities,\ni.e., the expressive pretrained language models and the structured knowledge\ngraphs. In this paper, we propose a unified ECE framework (UniCE to address all\nthree issues in ECE simultaneously. Specifically, we design a subtask\ninteraction mechanism to enable mutual interaction between the two ECE\nsubtasks. Besides, we design a knowledge fusion mechanism to fuse knowledge in\nthe two modalities. Furthermore, we employ separate decoders for each subtask\nto facilitate complex causality extraction. Experiments on three benchmark\ndatasets demonstrate that our method achieves state-of-the-art performance and\noutperforms ChatGPT with a margin of at least 30% F1-score. More importantly,\nour model can also be used to effectively improve the ECE performance of\nChatGPT via in-context learning.\n","authors":["Jinglong Gao","Chen Lu","Xiao Ding","Zhongyang Li","Ting Liu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2408.03079v1.pdf","comment":"NLPCC 2024 Oral"},{"id":"http://arxiv.org/abs/2406.02265v3","updated":"2024-08-06T10:10:58Z","published":"2024-06-04T12:41:54Z","title":"Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning","summary":"  Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.\n","authors":["Wenyan Li","Jiaang Li","Rita Ramos","Raphael Tang","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2406.02265v3.pdf","comment":"9 pages, long paper at ACL 2024"},{"id":"http://arxiv.org/abs/2408.03074v1","updated":"2024-08-06T10:02:05Z","published":"2024-08-06T10:02:05Z","title":"Towards an Analysis of Discourse and Interactional Pragmatic Reasoning\n  Capabilities of Large Language Models","summary":"  In this work, we want to give an overview on which pragmatic abilities have\nbeen tested in LLMs so far and how these tests have been carried out. To do\nthis, we first discuss the scope of the field of pragmatics and suggest a\nsubdivision into discourse pragmatics and interactional pragmatics. We give a\nnon-exhaustive overview of the phenomena of those two subdomains and the\nmethods traditionally used to analyze them. We subsequently consider the\nresulting heterogeneous set of phenomena and methods as a starting point for\nour survey of work on discourse pragmatics and interactional pragmatics in the\ncontext of LLMs.\n","authors":["Amelie Robrecht","Judith Sieker","Clara Lachenmaier","Sina Zarie","Stefan Kopp"],"pdf_url":"https://arxiv.org/pdf/2408.03074v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03070v1","updated":"2024-08-06T09:54:49Z","published":"2024-08-06T09:54:49Z","title":"Probing structural constraints of negation in Pretrained Language Models","summary":"  Contradictory results about the encoding of the semantic impact of negation\nin pretrained language models (PLMs). have been drawn recently (e.g. Kassner\nand Sch{\\\"u}tze (2020); Gubelmann and Handschuh (2022)). In this paper we focus\nrather on the way PLMs encode negation and its formal impact, through the\nphenomenon of the Negative Polarity Item (NPI) licensing in English. More\nprecisely, we use probes to identify which contextual representations best\nencode 1) the presence of negation in a sentence, and 2) the polarity of a\nneighboring masked polarity item. We find that contextual representations of\ntokens inside the negation scope do allow for (i) a better prediction of the\npresence of not compared to those outside the scope and (ii) a better\nprediction of the right polarity of a masked polarity item licensed by not,\nalthough the magnitude of the difference varies from PLM to PLM. Importantly,\nin both cases the trend holds even when controlling for distance to not. This\ntends to indicate that the embeddings of these models do reflect the notion of\nnegation scope, and do encode the impact of negation on NPI licensing. Yet,\nfurther control experiments reveal that the presence of other lexical items is\nalso better captured when using the contextual representation of a token within\nthe same syntactic clause than outside from it, suggesting that PLMs simply\ncapture the more general notion of syntactic clause.\n","authors":["David Kletz","Marie Candito","Pascal Amsili"],"pdf_url":"https://arxiv.org/pdf/2408.03070v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14972v2","updated":"2024-08-06T09:45:27Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate that BDoG is able to achieve state-of-the-art\nresults in ScienceQA and MMBench with significant improvements over previous\nmethods. The source code can be accessed at https://github.com/thecharm/BDoG.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v2.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.03062v1","updated":"2024-08-06T09:27:41Z","published":"2024-08-06T09:27:41Z","title":"Analysis of Argument Structure Constructions in a Deep Recurrent\n  Language Model","summary":"  Understanding how language and linguistic constructions are processed in the\nbrain is a fundamental question in cognitive computational neuroscience. In\nthis study, we explore the representation and processing of Argument Structure\nConstructions (ASCs) in a recurrent neural language model. We trained a Long\nShort-Term Memory (LSTM) network on a custom-made dataset consisting of 2000\nsentences, generated using GPT-4, representing four distinct ASCs: transitive,\nditransitive, caused-motion, and resultative constructions.\n  We analyzed the internal activations of the LSTM model's hidden layers using\nMultidimensional Scaling (MDS) and t-Distributed Stochastic Neighbor Embedding\n(t-SNE) to visualize the sentence representations. The Generalized\nDiscrimination Value (GDV) was calculated to quantify the degree of clustering\nwithin these representations. Our results show that sentence representations\nform distinct clusters corresponding to the four ASCs across all hidden layers,\nwith the most pronounced clustering observed in the last hidden layer before\nthe output layer. This indicates that even a relatively simple,\nbrain-constrained recurrent neural network can effectively differentiate\nbetween various construction types.\n  These findings are consistent with previous studies demonstrating the\nemergence of word class and syntax rule representations in recurrent language\nmodels trained on next word prediction tasks. In future work, we aim to\nvalidate these results using larger language models and compare them with\nneuroimaging data obtained during continuous speech perception. This study\nhighlights the potential of recurrent neural language models to mirror\nlinguistic processing in the human brain, providing valuable insights into the\ncomputational and neural mechanisms underlying language understanding.\n","authors":["Pegah Ramezani","Achim Schilling","Patrick Krauss"],"pdf_url":"https://arxiv.org/pdf/2408.03062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.10512v3","updated":"2024-08-06T09:24:01Z","published":"2023-06-18T09:54:33Z","title":"From Static Benchmarks to Adaptive Testing: Psychometrics in AI\n  Evaluation","summary":"  As AI systems continue to grow, particularly generative models like Large\nLanguage Models (LLMs), their rigorous evaluation is crucial for development\nand deployment. To determine their adequacy, researchers have developed various\nlarge-scale benchmarks against a so-called gold-standard test set and report\nmetrics averaged across all items. However, this static evaluation paradigm\nincreasingly shows its limitations, including high computational costs, data\ncontamination, and the impact of low-quality or erroneous items on evaluation\nreliability and efficiency. In this Perspective, drawing from human\npsychometrics, we discuss a paradigm shift from static evaluation methods to\nadaptive testing. This involves estimating the characteristics and value of\neach test item in the benchmark and dynamically adjusting items in real-time,\ntailoring the evaluation based on the model's ongoing performance instead of\nrelying on a fixed test set. This paradigm not only provides a more robust\nability estimation but also significantly reduces the number of test items\nrequired. We analyze the current approaches, advantages, and underlying reasons\nfor adopting psychometrics in AI evaluation. We propose that adaptive testing\nwill become the new norm in AI model evaluation, enhancing both the efficiency\nand effectiveness of assessing advanced intelligence systems.\n","authors":["Yan Zhuang","Qi Liu","Yuting Ning","Weizhe Huang","Zachary A. Pardos","Patrick C. Kyllonen","Jiyun Zu","Qingyang Mao","Rui Lv","Zhenya Huang","Guanhao Zhao","Zheng Zhang","Shijin Wang","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2306.10512v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11757v3","updated":"2024-08-06T09:17:59Z","published":"2024-06-17T17:16:45Z","title":"STAR: SocioTechnical Approach to Red Teaming Language Models","summary":"  This research introduces STAR, a sociotechnical framework that improves on\ncurrent best practices for red teaming safety of large language models. STAR\nmakes two key contributions: it enhances steerability by generating\nparameterised instructions for human red teamers, leading to improved coverage\nof the risk surface. Parameterised instructions also provide more detailed\ninsights into model failures at no increased cost. Second, STAR improves signal\nquality by matching demographics to assess harms for specific groups, resulting\nin more sensitive annotations. STAR further employs a novel step of arbitration\nto leverage diverse viewpoints and improve label reliability, treating\ndisagreement not as noise but as a valuable contribution to signal quality.\n","authors":["Laura Weidinger","John Mellor","Bernat Guillen Pegueroles","Nahema Marchal","Ravin Kumar","Kristian Lum","Canfer Akbulut","Mark Diaz","Stevie Bergman","Mikel Rodriguez","Verena Rieser","William Isaac"],"pdf_url":"https://arxiv.org/pdf/2406.11757v3.pdf","comment":"8 pages, 5 figures, 5 pages appendix. * denotes equal contribution"},{"id":"http://arxiv.org/abs/2408.03047v1","updated":"2024-08-06T09:02:53Z","published":"2024-08-06T09:02:53Z","title":"OpenOmni: A Collaborative Open Source Tool for Building Future-Ready\n  Multimodal Conversational Agents","summary":"  Multimodal conversational agents are highly desirable because they offer\nnatural and human-like interaction. However, there is a lack of comprehensive\nend-to-end solutions to support collaborative development and benchmarking.\nWhile proprietary systems like GPT-4o and Gemini demonstrating impressive\nintegration of audio, video, and text with response times of 200-250ms,\nchallenges remain in balancing latency, accuracy, cost, and data privacy. To\nbetter understand and quantify these issues, we developed OpenOmni, an\nopen-source, end-to-end pipeline benchmarking tool that integrates advanced\ntechnologies such as Speech-to-Text, Emotion Detection, Retrieval Augmented\nGeneration, Large Language Models, along with the ability to integrate\ncustomized models. OpenOmni supports local and cloud deployment, ensuring data\nprivacy and supporting latency and accuracy benchmarking. This flexible\nframework allows researchers to customize the pipeline, focusing on real\nbottlenecks and facilitating rapid proof-of-concept development. OpenOmni can\nsignificantly enhance applications like indoor assistance for visually impaired\nindividuals, advancing human-computer interaction. Our demonstration video is\navailable https://www.youtube.com/watch?v=zaSiT3clWqY, demo is available via\nhttps://openomni.ai4wa.com, code is available via\nhttps://github.com/AI4WA/OpenOmniFramework.\n","authors":["Qiang Sun","Yuanyi Luo","Sirui Li","Wenxiao Zhang","Wei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03047v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.14423v4","updated":"2024-08-06T08:44:47Z","published":"2023-08-28T09:04:03Z","title":"GADePo: Graph-Assisted Declarative Pooling Transformers for\n  Document-Level Relation Extraction","summary":"  Document-level relation extraction typically relies on text-based encoders\nand hand-coded pooling heuristics to aggregate information learned by the\nencoder. In this paper, we leverage the intrinsic graph processing capabilities\nof the Transformer model and propose replacing hand-coded pooling methods with\nnew tokens in the input, which are designed to aggregate information via\nexplicit graph relations in the computation of attention weights. We introduce\na joint text-graph Transformer model and a graph-assisted declarative pooling\n(GADePo) specification of the input, which provides explicit and high-level\ninstructions for information aggregation. GADePo allows the pooling process to\nbe guided by domain-specific knowledge or desired outcomes but still learned by\nthe Transformer, leading to more flexible and customisable pooling strategies.\nWe evaluate our method across diverse datasets and models and show that our\napproach yields promising results that are consistently better than those\nachieved by the hand-coded pooling functions.\n","authors":["Andrei C. Coman","Christos Theodoropoulos","Marie-Francine Moens","James Henderson"],"pdf_url":"https://arxiv.org/pdf/2308.14423v4.pdf","comment":"Accepted to KnowledgeNLP workshop at ACL 2024"},{"id":"http://arxiv.org/abs/2408.03033v1","updated":"2024-08-06T08:25:49Z","published":"2024-08-06T08:25:49Z","title":"L3iTC at the FinLLM Challenge Task: Quantization for Financial Text\n  Classification & Summarization","summary":"  This article details our participation (L3iTC) in the FinLLM Challenge Task\n2024, focusing on two key areas: Task 1, financial text classification, and\nTask 2, financial text summarization. To address these challenges, we\nfine-tuned several large language models (LLMs) to optimize performance for\neach task. Specifically, we used 4-bit quantization and LoRA to determine which\nlayers of the LLMs should be trained at a lower precision. This approach not\nonly accelerated the fine-tuning process on the training data provided by the\norganizers but also enabled us to run the models on low GPU memory. Our\nfine-tuned models achieved third place for the financial classification task\nwith an F1-score of 0.7543 and secured sixth place in the financial\nsummarization task on the official test datasets.\n","authors":["Elvys Linhares Pontes","Carlos-Emiliano Gonzlez-Gallardo","Mohamed Benjannet","Caryn Qu","Antoine Doucet"],"pdf_url":"https://arxiv.org/pdf/2408.03033v1.pdf","comment":"Joint Workshop of the 8th Financial Technology and Natural Language\n  Processing (FinNLP) and the 1st Agent AI for Scenario Planning (AgentScen),\n  2024"},{"id":"http://arxiv.org/abs/2401.10660v2","updated":"2024-08-06T08:23:30Z","published":"2024-01-19T12:26:57Z","title":"Accelerating Multilingual Language Model for Excessively Tokenized\n  Languages","summary":"  Recent advancements in large language models (LLMs) have remarkably enhanced\nperformances on a variety of tasks in multiple languages. However, tokenizers\nin LLMs trained primarily on English-centric corpora often overly fragment a\ntext into character or Unicode-level tokens in non-Roman alphabetic languages,\nleading to inefficient text generation. We introduce a simple yet effective\nframework to accelerate text generation in such languages. Our approach\ninvolves employing a new language model head with a vocabulary set tailored to\na specific target language for a pre-trained LLM. This is followed by\nfine-tuning the new head while incorporating a verification step to ensure the\nmodel's performance is preserved. We show that this targeted fine-tuning, while\nfreezing other model parameters, effectively reduces token fragmentation for\nthe target language. Our extensive experiments demonstrate that the proposed\nframework increases the generation speed by a factor of 1.7 while maintaining\nthe performance of pre-trained multilingual models on target monolingual tasks.\n","authors":["Jimin Hong","Gibbeum Lee","Jaewoong Cho"],"pdf_url":"https://arxiv.org/pdf/2401.10660v2.pdf","comment":"Accepted to ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.03010v1","updated":"2024-08-06T07:45:05Z","published":"2024-08-06T07:45:05Z","title":"Fact Finder -- Enhancing Domain Expertise of Large Language Models by\n  Incorporating Knowledge Graphs","summary":"  Recent advancements in Large Language Models (LLMs) have showcased their\nproficiency in answering natural language queries. However, their effectiveness\nis hindered by limited domain-specific knowledge, raising concerns about the\nreliability of their responses. We introduce a hybrid system that augments LLMs\nwith domain-specific knowledge graphs (KGs), thereby aiming to enhance factual\ncorrectness using a KG-based retrieval approach. We focus on a medical KG to\ndemonstrate our methodology, which includes (1) pre-processing, (2) Cypher\nquery generation, (3) Cypher query processing, (4) KG retrieval, and (5)\nLLM-enhanced response generation. We evaluate our system on a curated dataset\nof 69 samples, achieving a precision of 78\\% in retrieving correct KG nodes.\nOur findings indicate that the hybrid system surpasses a standalone LLM in\naccuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.\nThis positions the system as a promising tool for applications that demand\nfactual correctness and completeness, such as target identification -- a\ncritical process in pinpointing biological entities for disease treatment or\ncrop enhancement. Moreover, its intuitive search interface and ability to\nprovide accurate responses within seconds make it well-suited for\ntime-sensitive, precision-focused research contexts. We publish the source code\ntogether with the dataset and the prompt templates used.\n","authors":["Daniel Steinigen","Roman Teucher","Timm Heine Ruland","Max Rudat","Nicolas Flores-Herr","Peter Fischer","Nikola Milosevic","Christopher Schymura","Angelo Ziletti"],"pdf_url":"https://arxiv.org/pdf/2408.03010v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2403.01988v2","updated":"2024-08-06T07:40:11Z","published":"2024-03-04T12:35:09Z","title":"FKA-Owl: Advancing Multimodal Fake News Detection through\n  Knowledge-Augmented LVLMs","summary":"  The massive generation of multimodal fake news involving both text and images\nexhibits substantial distribution discrepancies, prompting the need for\ngeneralized detectors. However, the insulated nature of training restricts the\ncapability of classical detectors to obtain open-world facts. While Large\nVision-Language Models (LVLMs) have encoded rich world knowledge, they are not\ninherently tailored for combating fake news and struggle to comprehend local\nforgery details. In this paper, we propose FKA-Owl, a novel framework that\nleverages forgery-specific knowledge to augment LVLMs, enabling them to reason\nabout manipulations effectively. The augmented forgery-specific knowledge\nincludes semantic correlation between text and images, and artifact trace in\nimage manipulation. To inject these two kinds of knowledge into the LVLM, we\ndesign two specialized modules to establish their representations,\nrespectively. The encoded knowledge embeddings are then incorporated into\nLVLMs. Extensive experiments on the public benchmark demonstrate that FKA-Owl\nachieves superior cross-domain performance compared to previous methods. Code\nis publicly available at https://liuxuannan.github.io/FKA_Owl.github.io/.\n","authors":["Xuannan Liu","Peipei Li","Huaibo Huang","Zekun Li","Xing Cui","Jiahao Liang","Lixiong Qin","Weihong Deng","Zhaofeng He"],"pdf_url":"https://arxiv.org/pdf/2403.01988v2.pdf","comment":"Accepted by ACM MM 2024.Project page:\n  https://liuxuannan.github.io/FKA_Owl.github.io/"},{"id":"http://arxiv.org/abs/2407.10805v3","updated":"2024-08-06T06:47:06Z","published":"2024-07-15T15:20:40Z","title":"Think-on-Graph 2.0: Deep and Interpretable Large Language Model\n  Reasoning with Knowledge Graph-guided Retrieval","summary":"  Retrieval-augmented generation (RAG) has significantly advanced large\nlanguage models (LLMs) by enabling dynamic information retrieval to mitigate\nknowledge gaps and hallucinations in generated content. However, these systems\noften falter with complex reasoning and consistency across diverse queries. In\nthis work, we present Think-on-Graph 2.0, an enhanced RAG framework that aligns\nquestions with the knowledge graph and uses it as a navigational tool, which\ndeepens and refines the RAG paradigm for information collection and\nintegration. The KG-guided navigation fosters deep and long-range associations\nto uphold logical consistency and optimize the scope of retrieval for precision\nand interoperability. In conjunction, factual consistency can be better ensured\nthrough semantic similarity guided by precise directives. ToG${2.0}$ not only\nimproves the accuracy and reliability of LLMs' responses but also demonstrates\nthe potential of hybrid structured knowledge systems to significantly advance\nLLM reasoning, aligning it closer to human-like performance. We conducted\nextensive experiments on four public datasets to demonstrate the advantages of\nour method compared to the baseline.\n","authors":["Shengjie Ma","Chengjin Xu","Xuhui Jiang","Muzhi Li","Huaren Qu","Jian Guo"],"pdf_url":"https://arxiv.org/pdf/2407.10805v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.02514v3","updated":"2024-08-06T06:39:02Z","published":"2024-06-22T12:50:41Z","title":"LOGIC-LM++: Multi-Step Refinement for Symbolic Formulations","summary":"  In this paper we examine the limitations of Large Language Models (LLMs) for\ncomplex reasoning tasks. Although recent works have started to employ formal\nlanguages as an intermediate representation for reasoning tasks, they often\nface challenges in accurately generating and refining these formal\nspecifications to ensure correctness. To address these issues, this paper\nproposes Logic-LM++, an improvement on Logic-LM . It uses the ability of LLMs\nto do pairwise comparisons, allowing the evaluation of the refinements\nsuggested by the LLM. The paper demonstrates that Logic-LM++ outperforms\nLogic-LM and other contemporary techniques across natural language reasoning\ntasks on three datasets, FOLIO, ProofWriter and AR-LSAT, with an average\nimprovement of 18.5% on standard prompting, 12.3% on chain of thought prompting\nand 5% on Logic-LM.\n","authors":["Shashank Kirtania","Priyanshu Gupta","Arjun Radhakirshna"],"pdf_url":"https://arxiv.org/pdf/2407.02514v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02976v1","updated":"2024-08-06T06:16:00Z","published":"2024-08-06T06:16:00Z","title":"Empathy Level Alignment via Reinforcement Learning for Empathetic\n  Response Generation","summary":"  Empathetic response generation, aiming at understanding the user's situation\nand feelings and respond empathically, is crucial in building human-like\ndialogue systems. Previous methods mainly focus on using maximum likelihood\nestimation as the optimization objective for training response generation\nmodels, without taking into account the empathy level alignment between\ngenerated responses and target responses. To this end, we propose an empathetic\nresponse generation using reinforcement learning (EmpRL) framework. The\nframework designs an effective empathy reward function and generates empathetic\nresponses by maximizing the expected reward through reinforcement learning.\nGiven the powerful text generation capability of pre-trained language models,\nEmpRL utilizes the pre-trained T5 model as the generator and conducts further\ntraining to initialize the policy. To align the empathy level between generated\nresponses and target responses in the context, an empathy reward function\ncontaining three empathy communication mechanisms, i.e., emotional reaction,\ninterpretation, and exploration, is constructed using pre-designed and\npre-trained empathy identifiers. Finally, the proximal policy optimization\nalgorithm is used to further train the policy to produce empathetic responses.\nBoth automatic and manual evaluations demonstrate that the proposed EmpRL\nframework can improve the quality of generated responses, enhance the empathy\nlevel similarity between generated and target responses, and produce empathetic\nresponses covering both affective and cognitive aspects.\n","authors":["Hui Ma","Bo Zhang","Bo Xu","Jian Wang","Hongfei Lin","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02976v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02970v1","updated":"2024-08-06T05:50:41Z","published":"2024-08-06T05:50:41Z","title":"EC-Guide: A Comprehensive E-Commerce Guide for Instruction Tuning and\n  Quantization","summary":"  Large language models (LLMs) have attracted considerable attention in various\nfields for their cost-effective solutions to diverse challenges, especially\nwith advancements in instruction tuning and quantization. E-commerce, with its\ncomplex tasks and extensive product-user interactions, presents a promising\napplication area for LLMs. However, the domain-specific concepts and knowledge\ninherent in e-commerce pose significant challenges for adapting general LLMs.\nTo address this issue, we developed EC-Guide\n\\href{https://github.com/fzp0424/EC-Guide-KDDUP-2024}, a comprehensive\ne-commerce guide for instruction tuning and quantization of LLMs. We also\nheuristically integrated Chain-of-Thought (CoT) during inference to enhance\narithmetic performance. Our approach achieved the 2nd place in Track 2 and 5th\nplace in Track 5 at the Amazon KDD Cup'24\n\\href{https://www.aicrowd.com/challenges/amazon-kdd-cup-2024-multi-task-online-shopping-challenge-for-llms}.\nAdditionally, our solution is model-agnostic, enabling effective scalability\nacross larger systems.\n","authors":["Zhaopeng Feng","Zijie Meng","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2408.02970v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.08229v2","updated":"2024-08-06T05:41:59Z","published":"2024-03-13T04:14:33Z","title":"Boosting Disfluency Detection with Large Language Model as Disfluency\n  Generator","summary":"  Current disfluency detection methods heavily rely on costly and scarce\nhuman-annotated data. To tackle this issue, some approaches employ heuristic or\nstatistical features to generate disfluent sentences, partially improving\ndetection performance. However, these sentences often deviate from real-life\nscenarios, constraining overall model enhancement. In this study, we propose a\nlightweight data augmentation approach for disfluency detection, utilizing the\nsuperior generative and semantic understanding capabilities of large language\nmodel (LLM) to generate disfluent sentences as augmentation data. We leverage\nLLM to generate diverse and more realistic sentences guided by specific\nprompts, without the need for fine-tuning the LLM. Subsequently, we apply an\nuncertainty-aware data filtering approach to improve the quality of the\ngenerated sentences, utilized in training a small detection model for improved\nperformance. Experiments using enhanced data yielded state-of-the-art results.\nThe results showed that using a small amount of LLM-generated enhanced data can\nsignificantly improve performance, thereby further enhancing\ncost-effectiveness. Our code is available here.\n","authors":["Zhenrong Cheng","Jiayan Guo","Hao Sun","Yan Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.08229v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02964v1","updated":"2024-08-06T05:21:13Z","published":"2024-08-06T05:21:13Z","title":"Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The\n  Impact of Prompt Engineering and Knowledge Retrieval","summary":"  Large language models (LLMs) are fundamentally transforming human-facing\napplications in the health and well-being domains: boosting patient engagement,\naccelerating clinical decision-making, and facilitating medical education.\nAlthough state-of-the-art LLMs have shown superior performance in several\nconversational applications, evaluations within nutrition and diet applications\nare still insufficient. In this paper, we propose to employ the Registered\nDietitian (RD) exam to conduct a standard and comprehensive evaluation of\nstate-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing\nboth accuracy and consistency in nutrition queries. Our evaluation includes\n1050 RD exam questions encompassing several nutrition topics and proficiency\nlevels. In addition, for the first time, we examine the impact of Zero-Shot\n(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),\nand Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the\nresponses. Our findings revealed that while these LLMs obtained acceptable\noverall performance, their results varied considerably with different prompts\nand question domains. GPT-4o with CoT-SC prompting outperformed the other\napproaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.\nFor GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both\naccuracy and consistency. RAP was particularly effective for GPT-4o to answer\nExpert level questions. Consequently, choosing the appropriate LLM and\nprompting technique, tailored to the proficiency level and specific domain, can\nmitigate errors and potential risks in diet and nutrition chatbots.\n","authors":["Iman Azimi","Mohan Qi","Li Wang","Amir M. Rahmani","Youlin Li"],"pdf_url":"https://arxiv.org/pdf/2408.02964v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01933v2","updated":"2024-08-06T04:28:01Z","published":"2024-08-04T05:15:02Z","title":"DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language\n  Models","summary":"  Large language models (LLMs) have recently showcased remarkable capabilities,\nspanning a wide range of tasks and applications, including those in the medical\ndomain. Models like GPT-4 excel in medical question answering but may face\nchallenges in the lack of interpretability when handling complex tasks in real\nclinical settings. We thus introduce the diagnostic reasoning dataset for\nclinical notes (DiReCT), aiming at evaluating the reasoning ability and\ninterpretability of LLMs compared to human doctors. It contains 511 clinical\nnotes, each meticulously annotated by physicians, detailing the diagnostic\nreasoning process from observations in a clinical note to the final diagnosis.\nAdditionally, a diagnostic knowledge graph is provided to offer essential\nknowledge for reasoning, which may not be covered in the training data of\nexisting LLMs. Evaluations of leading LLMs on DiReCT bring out a significant\ngap between their reasoning ability and that of human doctors, highlighting the\ncritical need for models that can reason effectively in real-world clinical\nscenarios.\n","authors":["Bowen Wang","Jiuyang Chang","Yiming Qian","Guoxin Chen","Junhao Chen","Zhouqiang Jiang","Jiahao Zhang","Yuta Nakashima","Hajime Nagahara"],"pdf_url":"https://arxiv.org/pdf/2408.01933v2.pdf","comment":"9 pages,6 figures"},{"id":"http://arxiv.org/abs/2408.02948v1","updated":"2024-08-06T04:19:23Z","published":"2024-08-06T04:19:23Z","title":"Are Female Carpenters like Blue Bananas? A Corpus Investigation of\n  Occupation Gender Typicality","summary":"  People tend to use language to mention surprising properties of events: for\nexample, when a banana is blue, we are more likely to mention color than when\nit is yellow. This fact is taken to suggest that yellowness is somehow a\ntypical feature of bananas, and blueness is exceptional. Similar to how a\nyellow color is typical of bananas, there may also be genders that are typical\nof occupations. In this work, we explore this question using information\ntheoretic techniques coupled with corpus statistic analysis. In two distinct\nlarge corpora, we do not find strong evidence that occupations and gender\ndisplay the same patterns of mentioning as do bananas and color. Instead, we\nfind that gender mentioning is correlated with femaleness of occupation in\nparticular, suggesting perhaps that woman-dominated occupations are seen as\nsomehow ``more gendered'' than male-dominated ones, and thereby they encourage\nmore gender mentioning overall.\n","authors":["Da Ju","Karen Ulrich","Adina Williams"],"pdf_url":"https://arxiv.org/pdf/2408.02948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02945v1","updated":"2024-08-06T04:12:31Z","published":"2024-08-06T04:12:31Z","title":"Self-Supervised Learning for Multi-Channel Neural Transducer","summary":"  Self-supervised learning, such as with the wav2vec 2.0 framework\nsignificantly improves the accuracy of end-to-end automatic speech recognition\n(ASR). Wav2vec 2.0 has been applied to single-channel end-to-end ASR models. In\nthis work, we explored a self-supervised learning method for a multi-channel\nend-to-end ASR model based on the wav2vec 2.0 framework. As the multi-channel\nend-to-end ASR model, we focused on a multi-channel neural transducer. In\npre-training, we compared three different methods for feature quantization to\ntrain a multi-channel conformer audio encoder: joint quantization, feature-wise\nquantization and channel-wise quantization. In fine-tuning, we trained the\nmulti-channel conformer-transducer. All experiments were conducted using the\nfar-field in-house and CHiME-4 datasets. The results of the experiments showed\nthat feature-wise quantization was the most effective among the methods. We\nobserved a 66% relative reduction in character error rate compared with the\nmodel without any pre-training for the far-field in-house dataset.\n","authors":["Atsushi Kojima"],"pdf_url":"https://arxiv.org/pdf/2408.02945v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.05356v4","updated":"2024-08-06T03:57:33Z","published":"2023-12-08T20:28:08Z","title":"Neuron Patching: Semantic-based Neuron-level Language Model Repair for\n  Code Generation","summary":"  Large Language Models (LLMs) have already gained widespread adoption in\nsoftware engineering, particularly in code generation tasks. However, updating\nthese models with new knowledge can be prohibitively expensive, yet it is\nessential to maximize their utility, such as implementing a hotfix technique to\naddress urgent or critical LLM errors. In this paper, we propose \\textsc{MENT},\na novel and effective model editing approach to repair LLMs in coding tasks.\n\\textsc{MENT} is effective, efficient, and reliable, capable of correcting a\nneural model by patching just one or two neurons. As pioneering work on\nneuron-level model editing of generative models, we formalize the editing\nprocess and introduce the involved concepts. We also introduce new measures to\nevaluate its generalization ability and establish a benchmark for further\nstudy. Our approach is evaluated on three coding tasks: line-level code\ngeneration, shellcode generation, and intent-to-bash translation. The\nexperimental results demonstrate that the proposed approach significantly\noutperforms the state-of-the-art in both effectiveness and efficiency measures.\nFurthermore, we showcase the applications of \\textsc{MENT} for LLM reasoning in\nsoftware engineering. By editing LLM knowledge, the directly or indirectly\ndependent behaviors of API invocation in the chain-of-thought change\naccordingly. This illustrates the significance of repairing LLMs in the context\nof software engineering.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v4.pdf","comment":"12 pages, 7 figures, 7 tables, under peer-review"},{"id":"http://arxiv.org/abs/2402.12022v2","updated":"2024-08-06T03:34:06Z","published":"2024-02-19T10:31:53Z","title":"Distilling Large Language Models for Text-Attributed Graph Learning","summary":"  Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.\n","authors":["Bo Pan","Zheng Zhang","Yifei Zhang","Yuntong Hu","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12022v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2407.21276v2","updated":"2024-08-06T03:34:03Z","published":"2024-07-31T01:51:24Z","title":"Multi-Level Querying using A Knowledge Pyramid","summary":"  This paper addresses the need for improved precision in existing\nRetrieval-Augmented Generation (RAG) methods that primarily focus on enhancing\nrecall. We propose a multi-layer knowledge pyramid approach within the RAG\nframework to achieve a better balance between precision and recall. The\nknowledge pyramid consists of three layers: Ontologies, Knowledge Graphs (KGs),\nand chunk-based raw text. We employ cross-layer augmentation techniques for\ncomprehensive knowledge coverage and dynamic updates of the Ontology schema and\ninstances. To ensure compactness, we utilize cross-layer filtering methods for\nknowledge condensation in KGs. Our approach, named PolyRAG, follows a waterfall\nmodel for retrieval, starting from the top of the pyramid and progressing down\nuntil a confident answer is obtained. We introduce two benchmarks for\ndomain-specific knowledge retrieval, one in the academic domain and the other\nin the financial domain. The effectiveness of the methods has been validated\nthrough comprehensive experiments by outperforming 19 SOTA methods. An\nencouraging observation is that the proposed method has augmented the GPT-4,\nproviding 395\\% F1 gain by improving its performance from 0.1636 to 0.8109.\n","authors":["Rubing Chen","Xulu Zhang","Jiaxin Wu","Wenqi Fan","Xiao-Yong Wei","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.21276v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02927v1","updated":"2024-08-06T03:21:13Z","published":"2024-08-06T03:21:13Z","title":"HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection","summary":"  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n","authors":["Yuxin Wang","Duanyu Feng","Yongfu Dai","Zhengyu Chen","Jimin Huang","Sophia Ananiadou","Qianqian Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v2","updated":"2024-08-06T03:19:25Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v2.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02923v1","updated":"2024-08-06T03:16:09Z","published":"2024-08-06T03:16:09Z","title":"Intermediate direct preference optimization","summary":"  We propose the intermediate direct preference optimization (DPO) method to\ncalculate the DPO loss at selected intermediate layers as an auxiliary loss for\nfinetuning large language models (LLMs). The conventional DPO method fine-tunes\na supervised fine-tuning (SFT) model by calculating the DPO loss using logits\nfrom the final layer. In our intermediate DPO approach, DPO losses are\ncalculated using the logits from K-selected intermediate layers and averaged to\nobtain the intermediate DPO loss. For training the intermediate DPO model, the\nfinal loss is obtained by calculating the weighted sum of the DPO and\nintermediate DPO losses. During inference, the intermediate DPO model decodes\nusing the final layer logits similarly to the conventional DPO model. In\nexperiments using the ultrafeedback dataset, the performance of the\nintermediate DPO model was evaluated using GPT-4. As a result, the intermediate\nDPO model trained using the intermediate DPO loss calculated at the 22nd layer\nof a 32-layer SFT model achieved win rates of 52.5% and 67.5% against the\nconventional DPO and SFT models, respectively, demonstrating the effectiveness\nof the proposed method. Furthermore, we report the relationships among the\nposition of the selected intermediate layers, the number of layers, and\nperformance.\n","authors":["Atsushi Kojima"],"pdf_url":"https://arxiv.org/pdf/2408.02923v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02919v1","updated":"2024-08-06T03:08:36Z","published":"2024-08-06T03:08:36Z","title":"Data Checklist: On Unit-Testing Datasets with Usable Information","summary":"  Model checklists (Ribeiro et al., 2020) have emerged as a useful tool for\nunderstanding the behavior of LLMs, analogous to unit-testing in software\nengineering. However, despite datasets being a key determinant of model\nbehavior, evaluating datasets, e.g., for the existence of annotation artifacts,\nis largely done ad hoc, once a problem in model behavior has already been found\ndownstream. In this work, we take a more principled approach to unit-testing\ndatasets by proposing a taxonomy based on the V-information literature. We call\na collection of such unit tests a data checklist. Using a checklist, not only\nare we able to recover known artifacts in well-known datasets such as SNLI, but\nwe also discover previously unknown artifacts in preference datasets for LLM\nalignment. Data checklists further enable a new kind of data filtering, which\nwe use to improve the efficacy and data efficiency of preference alignment.\n","authors":["Heidi C. Zhang","Shabnam Behzad","Kawin Ethayarajh","Dan Jurafsky"],"pdf_url":"https://arxiv.org/pdf/2408.02919v1.pdf","comment":"17 pages, 4 figures. COLM 2024"},{"id":"http://arxiv.org/abs/2407.17379v2","updated":"2024-08-06T02:44:44Z","published":"2024-07-24T15:59:01Z","title":"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image\n  Relational Association Capabilities in Large Visual Language Models","summary":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.\n","authors":["Siwei Wu","Kang Zhu","Yu Bai","Yiming Liang","Yizhi Li","Haoning Wu","J. H. Liu","Ruibo Liu","Xingwei Qu","Xuxin Cheng","Ge Zhang","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17379v2.pdf","comment":"VLMs, Multi-Image Association"},{"id":"http://arxiv.org/abs/2408.02907v1","updated":"2024-08-06T02:39:55Z","published":"2024-08-06T02:39:55Z","title":"Leveraging Inter-Chunk Interactions for Enhanced Retrieval in Large\n  Language Model-Based Question Answering","summary":"  Retrieving external knowledge and prompting large language models with\nrelevant information is an effective paradigm to enhance the performance of\nquestion-answering tasks. Previous research typically handles paragraphs from\nexternal documents in isolation, resulting in a lack of context and ambiguous\nreferences, particularly in multi-document and complex tasks. To overcome these\nchallenges, we propose a new retrieval framework IIER, that leverages\nInter-chunk Interactions to Enhance Retrieval. This framework captures the\ninternal connections between document chunks by considering three types of\ninteractions: structural, keyword, and semantic. We then construct a unified\nChunk-Interaction Graph to represent all external documents comprehensively.\nAdditionally, we design a graph-based evidence chain retriever that utilizes\nprevious paths and chunk interactions to guide the retrieval process. It\nidentifies multiple seed nodes based on the target question and iteratively\nsearches for relevant chunks to gather supporting evidence. This retrieval\nprocess refines the context and reasoning chain, aiding the large language\nmodel in reasoning and answer generation. Extensive experiments demonstrate\nthat IIER outperforms strong baselines across four datasets, highlighting its\neffectiveness in improving retrieval and reasoning capabilities.\n","authors":["Tiezheng Guo","Chen Wang","Yanyi Liu","Jiawei Tang","Pan Li","Sai Xu","Qingwen Yang","Xianlin Gao","Zhi Li","Yingyou Wen"],"pdf_url":"https://arxiv.org/pdf/2408.02907v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16110v3","updated":"2024-08-06T02:37:51Z","published":"2024-07-23T00:52:12Z","title":"Analyzing Polysemy Evolution Using Semantic Cells","summary":"  The senses of words evolve. The sense of the same word may change from today\nto tomorrow, and multiple senses of the same word may be the result of the\nevolution of each other, that is, they may be parents and children. If we view\nJuba as an evolving ecosystem, the paradigm of learning the correct answer,\nwhich does not move with the sense of a word, is no longer valid. This paper is\na case study that shows that word polysemy is an evolutionary consequence of\nthe modification of Semantic Cells, which has al-ready been presented by the\nauthor, by introducing a small amount of diversity in its initial state as an\nexample of analyzing the current set of short sentences. In particular, the\nanalysis of a sentence sequence of 1000 sentences in some order for each of the\nfour senses of the word Spring, collected using Chat GPT, shows that the word\nacquires the most polysemy monotonically in the analysis when the senses are\narranged in the order in which they have evolved. In other words, we present a\nmethod for analyzing the dynamism of a word's acquiring polysemy with evolution\nand, at the same time, a methodology for viewing polysemy from an evolutionary\nframework rather than a learning-based one.\n","authors":["Yukio Ohsawa","Dingming Xue","Kaira Sekiguchi"],"pdf_url":"https://arxiv.org/pdf/2407.16110v3.pdf","comment":"11 pages, 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.14749"},{"id":"http://arxiv.org/abs/2408.02901v1","updated":"2024-08-06T02:15:12Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v1.pdf","comment":"6 pages; library tech report"},{"id":"http://arxiv.org/abs/2408.02899v1","updated":"2024-08-06T02:07:37Z","published":"2024-08-06T02:07:37Z","title":"SETN: Stock Embedding Enhanced with Textual and Network Information","summary":"  Stock embedding is a method for vector representation of stocks. There is a\ngrowing demand for vector representations of stock, i.e., stock embedding, in\nwealth management sectors, and the method has been applied to various tasks\nsuch as stock price prediction, portfolio optimization, and similar fund\nidentifications. Stock embeddings have the advantage of enabling the\nquantification of relative relationships between stocks, and they can extract\nuseful information from unstructured data such as text and network data. In\nthis study, we propose stock embedding enhanced with textual and network\ninformation (SETN) using a domain-adaptive pre-trained transformer-based model\nto embed textual information and a graph neural network model to grasp network\ninformation. We evaluate the performance of our proposed model on related\ncompany information extraction tasks. We also demonstrate that stock embeddings\nobtained from the proposed model perform better in creating thematic funds than\nthose obtained from baseline methods, providing a promising pathway for various\napplications in the wealth management industry.\n","authors":["Takehiro Takayanagi","Hiroki Sakaji","Kiyoshi Izumi"],"pdf_url":"https://arxiv.org/pdf/2408.02899v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01964v4","updated":"2024-08-06T02:06:20Z","published":"2024-07-02T05:43:15Z","title":"Enabling Discriminative Reasoning in LLMs for Legal Judgment Prediction","summary":"  Legal judgment prediction is essential for enhancing judicial efficiency. In\nthis work, we identify that existing large language models (LLMs) underperform\nin this domain due to challenges in understanding case complexities and\ndistinguishing between similar charges. To adapt LLMs for effective legal\njudgment prediction, we introduce the Ask-Discriminate-Predict (ADAPT)\nreasoning framework inspired by human judicial reasoning. ADAPT involves\ndecomposing case facts, discriminating among potential charges, and predicting\nthe final judgment. We further enhance LLMs through fine-tuning with multi-task\nsynthetic trajectories to improve legal judgment prediction accuracy and\nefficiency under our ADAPT framework. Extensive experiments conducted on two\nwidely-used datasets demonstrate the superior performance of our framework in\nlegal judgment prediction, particularly when dealing with complex and confusing\ncharges.\n","authors":["Chenlong Deng","Kelong Mao","Yuyao Zhang","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2407.01964v4.pdf","comment":"repo: https://github.com/ChenlongDeng/ADAPT"},{"id":"http://arxiv.org/abs/2406.14643v2","updated":"2024-08-06T02:04:35Z","published":"2024-06-20T18:07:19Z","title":"Holistic Evaluation for Interleaved Text-and-Image Generation","summary":"  Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.\n","authors":["Minqian Liu","Zhiyang Xu","Zihao Lin","Trevor Ashby","Joy Rimchala","Jiaxin Zhang","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2406.14643v2.pdf","comment":"13 pages, 6 figures, 6 tables. Website:\n  https://vt-nlp.github.io/InterleavedEval/. Dataset:\n  https://huggingface.co/mqliu/InterleavedBench"},{"id":"http://arxiv.org/abs/2306.12587v2","updated":"2024-08-06T02:02:27Z","published":"2023-06-21T22:00:03Z","title":"ARIES: A Corpus of Scientific Paper Edits Made in Response to Peer\n  Reviews","summary":"  We introduce the task of automatically revising scientific papers based on\npeer feedback and release ARIES, a dataset of review comments and their\ncorresponding paper edits. The data is drawn from real reviewer-author\ninteractions from computer science, and we provide labels linking each reviewer\ncomment to the specific paper edits made by the author in response. We\nautomatically create a high-precision silver training set, as well as an\nexpert-labeled test set that shows high inter-annotator agreement. In\nexperiments with 10 models covering the state of the art, we find that they\nstruggle even to identify which edits correspond to a comment -- especially\nwhen the relationship between the edit and the comment is indirect and requires\nreasoning to uncover. We also extensively analyze GPT-4's ability to generate\nedits given a comment and the original paper. We find that it often succeeds on\na superficial level, but tends to rigidly follow the wording of the feedback\nrather than the underlying intent, and lacks technical details compared to\nhuman-written edits.\n","authors":["Mike D'Arcy","Alexis Ross","Erin Bransom","Bailey Kuehl","Jonathan Bragg","Tom Hope","Doug Downey"],"pdf_url":"https://arxiv.org/pdf/2306.12587v2.pdf","comment":"ACL 2024, 10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2406.11704v2","updated":"2024-08-06T22:37:06Z","published":"2024-06-17T16:25:04Z","title":"Nemotron-4 340B Technical Report","summary":"  We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,\nNemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open\naccess under the NVIDIA Open Model License Agreement, a permissive model\nlicense that allows distribution, modification, and use of the models and its\noutputs. These models perform competitively to open access models on a wide\nrange of evaluation benchmarks, and were sized to fit on a single DGX H100 with\n8 GPUs when deployed in FP8 precision. We believe that the community can\nbenefit from these models in various research studies and commercial\napplications, especially for generating synthetic data to train smaller\nlanguage models. Notably, over 98% of data used in our model alignment process\nis synthetically generated, showcasing the effectiveness of these models in\ngenerating synthetic data. To further support open research and facilitate\nmodel development, we are also open-sourcing the synthetic data generation\npipeline used in our model alignment process.\n","authors":[" Nvidia"," :","Bo Adler","Niket Agarwal","Ashwath Aithal","Dong H. Anh","Pallab Bhattacharya","Annika Brundyn","Jared Casper","Bryan Catanzaro","Sharon Clay","Jonathan Cohen","Sirshak Das","Ayush Dattagupta","Olivier Delalleau","Leon Derczynski","Yi Dong","Daniel Egert","Ellie Evans","Aleksander Ficek","Denys Fridman","Shaona Ghosh","Boris Ginsburg","Igor Gitman","Tomasz Grzegorzek","Robert Hero","Jining Huang","Vibhu Jawa","Joseph Jennings","Aastha Jhunjhunwala","John Kamalu","Sadaf Khan","Oleksii Kuchaiev","Patrick LeGresley","Hui Li","Jiwei Liu","Zihan Liu","Eileen Long","Ameya Sunil Mahabaleshwarkar","Somshubra Majumdar","James Maki","Miguel Martinez","Maer Rodrigues de Melo","Ivan Moshkov","Deepak Narayanan","Sean Narenthiran","Jesus Navarro","Phong Nguyen","Osvald Nitski","Vahid Noroozi","Guruprasad Nutheti","Christopher Parisien","Jupinder Parmar","Mostofa Patwary","Krzysztof Pawelec","Wei Ping","Shrimai Prabhumoye","Rajarshi Roy","Trisha Saar","Vasanth Rao Naik Sabavat","Sanjeev Satheesh","Jane Polak Scowcroft","Jason Sewall","Pavel Shamis","Gerald Shen","Mohammad Shoeybi","Dave Sizer","Misha Smelyanskiy","Felipe Soares","Makesh Narsimhan Sreedhar","Dan Su","Sandeep Subramanian","Shengyang Sun","Shubham Toshniwal","Hao Wang","Zhilin Wang","Jiaxuan You","Jiaqi Zeng","Jimmy Zhang","Jing Zhang","Vivienne Zhang","Yian Zhang","Chen Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.11704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15750v2","updated":"2024-08-06T22:29:11Z","published":"2024-05-24T17:47:20Z","title":"Filtered Corpus Training (FiCT) Shows that Language Models can\n  Generalize from Indirect Evidence","summary":"  This paper introduces Filtered Corpus Training, a method that trains language\nmodels (LMs) on corpora with certain linguistic constructions filtered out from\nthe training data, and uses it to measure the ability of LMs to perform\nlinguistic generalization on the basis of indirect evidence. We apply the\nmethod to both LSTM and Transformer LMs (of roughly comparable size),\ndeveloping filtered corpora that target a wide range of linguistic phenomena.\nOur results show that while transformers are better qua LMs (as measured by\nperplexity), both models perform equally and surprisingly well on linguistic\ngeneralization measures, suggesting that they are capable of generalizing from\nindirect evidence.\n","authors":["Abhinav Patil","Jaap Jumelet","Yu Ying Chiu","Andy Lapastora","Peter Shen","Lexie Wang","Clevis Willrich","Shane Steinert-Threlkeld"],"pdf_url":"https://arxiv.org/pdf/2405.15750v2.pdf","comment":"Forthcoming in Transactions of the Association for Computational\n  Linguistics (TACL). This is a pre-MIT Press publication version. For code and\n  trained models, see http://github.com/CLMBRs/corpus-filtering"},{"id":"http://arxiv.org/abs/2404.09753v2","updated":"2024-08-06T21:54:20Z","published":"2024-04-15T12:54:31Z","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models","summary":"  We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.\n","authors":["Nicolas Wagner","Dongyang Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2404.09753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.06654v3","updated":"2024-08-06T21:48:58Z","published":"2024-04-09T23:41:27Z","title":"RULER: What's the Real Context Size of Your Long-Context Language\n  Models?","summary":"  The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve\na piece of information (the \"needle\") from long distractor texts (the\n\"haystack\"), has been widely adopted to evaluate long-context language models\n(LMs). However, this simple retrieval-based test is indicative of only a\nsuperficial form of long-context understanding. To provide a more comprehensive\nevaluation of long-context LMs, we create a new synthetic benchmark RULER with\nflexible configurations for customized sequence length and task complexity.\nRULER expands upon the vanilla NIAH test to encompass variations with diverse\ntypes and quantities of needles. Moreover, RULER introduces new task categories\nmulti-hop tracing and aggregation to test behaviors beyond searching from\ncontext. We evaluate 17 long-context LMs with 13 representative tasks in RULER.\nDespite achieving nearly perfect accuracy in the vanilla NIAH test, almost all\nmodels exhibit large performance drops as the context length increases. While\nthese models all claim context sizes of 32K tokens or greater, only half of\nthem can maintain satisfactory performance at the length of 32K. Our analysis\nof Yi-34B, which supports context length of 200K, reveals large room for\nimprovement as we increase input length and task complexity. We open source\nRULER to spur comprehensive evaluation of long-context LMs.\n","authors":["Cheng-Ping Hsieh","Simeng Sun","Samuel Kriman","Shantanu Acharya","Dima Rekesh","Fei Jia","Yang Zhang","Boris Ginsburg"],"pdf_url":"https://arxiv.org/pdf/2404.06654v3.pdf","comment":"COLM 2024; Code is available at https://github.com/hsiehjackson/RULER"},{"id":"http://arxiv.org/abs/2403.10795v2","updated":"2024-08-06T21:14:23Z","published":"2024-03-16T03:54:38Z","title":"Can Large Language Models Solve Robot Routing?","summary":"  Routing problems are common in mobile robotics, encompassing tasks such as\ninspection, surveillance, and coverage. Depending on the objective and\nconstraints, these problems often reduce to variants of the Traveling Salesman\nProblem (TSP), with solutions traditionally derived by translating high-level\nobjectives into an optimization formulation and using modern solvers to arrive\nat a solution. Here, we explore the potential of Large Language Models (LLMs)\nto replace the entire pipeline from tasks described in natural language to the\ngeneration of robot routes. We systematically investigate the performance of\nLLMs in robot routing by constructing a dataset with 80 unique robot routing\nproblems across 8 variants in both single and multi-robot settings. We evaluate\nLLMs through three frameworks: single attempt, self-debugging, and\nself-debugging with self-verification and various contexts, including\nmathematical formulations, pseudo-code, and related research papers. Our\nfindings reveal that both self-debugging and self-verification enhance success\nrates without significantly lowering the optimality gap. We observe\ncontext-sensitive behavior - providing mathematical formulations as context\ndecreases the optimality gap but significantly decreases success rates and\nproviding pseudo-code and related research papers as context does not\nconsistently improve success rates or decrease the optimality gap. We identify\nkey challenges and propose future directions to enhance LLM performance in\nsolving robot routing problems. Our source code is available on the project\nwebsite: https://sites.google.com/view/words-to-routes/.\n","authors":["Zhehui Huang","Guangyao Shi","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2403.10795v2.pdf","comment":"Submitted to International Symposium of Robotics Research (ISRR 2024)"},{"id":"http://arxiv.org/abs/2310.05707v4","updated":"2024-08-06T19:53:17Z","published":"2023-10-09T13:29:37Z","title":"Guiding Language Model Reasoning with Planning Tokens","summary":"  Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\n(CoT) reasoning. However, most of the existing approaches to enhance this\nability rely heavily on data-driven methods, while neglecting the structural\naspects of the model's reasoning capacity. To encourage a more structural\ngeneration of CoT steps, we propose a hierarchical generation scheme: we let\nthe LM generate a planning token at the start of each reasoning step,\nintuitively serving as a high-level plan of the current step, and add their\nembeddings to the model parameters. Our approach requires a negligible increase\nin trainable parameters (0.001%) and can be applied through either full\nfine-tuning or a more parameter-efficient scheme. We demonstrate our method's\neffectiveness by applying it to three different LLMs, showing notable accuracy\nimprovements across three math word problem datasets and one multihop QA\ndataset with respect to standard fine-tuning baselines.\n","authors":["Xinyi Wang","Lucas Caccia","Oleksiy Ostapenko","Xingdi Yuan","William Yang Wang","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2310.05707v4.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2401.05300v2","updated":"2024-08-06T19:24:42Z","published":"2024-01-10T18:06:27Z","title":"I am a Strange Dataset: Metalinguistic Tests for Language Models","summary":"  Statements involving metalinguistic self-reference (\"This paper has six\nsections.\") are prevalent in many domains. Can current large language models\n(LLMs) handle such language? In this paper, we present \"I am a Strange\nDataset\", a new dataset for addressing this question. There are two subtasks:\ngeneration and verification. In generation, models continue statements like\n\"The penultimate word in this sentence is\" (where a correct continuation is\n\"is\"). In verification, models judge the truth of statements like \"The\npenultimate word in this sentence is sentence.\" (false). We also provide\nminimally different metalinguistic non-self-reference examples to complement\nthe main dataset by probing for whether models can handle metalinguistic\nlanguage at all. The dataset is hand-crafted by experts and validated by\nnon-expert annotators. We test a variety of open-source LLMs (7B to 70B\nparameters) as well as closed-source LLMs through APIs. All models perform\nclose to chance across both subtasks and even on the non-self-referential\nmetalinguistic control data, though we find some steady improvement with model\nscale. GPT 4 is the only model to consistently do significantly better than\nchance, and it is still only in the 60% range, while our untrained human\nannotators score well in the 89-93% range. The dataset and evaluation toolkit\nare available at https://github.com/TristanThrush/i-am-a-strange-dataset.\n","authors":["Tristan Thrush","Jared Moore","Miguel Monares","Christopher Potts","Douwe Kiela"],"pdf_url":"https://arxiv.org/pdf/2401.05300v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2408.03414v1","updated":"2024-08-06T19:23:42Z","published":"2024-08-06T19:23:42Z","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","summary":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","authors":["Marcus Buckmann","Edward Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03414v1.pdf","comment":"41 pages, 24 figures"},{"id":"http://arxiv.org/abs/2408.03402v1","updated":"2024-08-06T18:53:54Z","published":"2024-08-06T18:53:54Z","title":"ULLME: A Unified Framework for Large Language Model Embeddings with\n  Generation-Augmented Learning","summary":"  Large Language Models (LLMs) excel in various natural language processing\ntasks, but leveraging them for dense passage embedding remains challenging.\nThis is due to their causal attention mechanism and the misalignment between\ntheir pre-training objectives and the text ranking tasks. Despite some recent\nefforts to address these issues, existing frameworks for LLM-based text\nembeddings have been limited by their support for only a limited range of LLM\narchitectures and fine-tuning strategies, limiting their practical application\nand versatility. In this work, we introduce the Unified framework for Large\nLanguage Model Embedding (ULLME), a flexible, plug-and-play implementation that\nenables bidirectional attention across various LLMs and supports a range of\nfine-tuning strategies. We also propose Generation-augmented Representation\nLearning (GRL), a novel fine-tuning method to boost LLMs for text embedding\ntasks. GRL enforces consistency between representation-based and\ngeneration-based relevance scores, leveraging LLMs' powerful generative\nabilities for learning passage embeddings. To showcase our framework's\nflexibility and effectiveness, we release three pre-trained models from ULLME\nwith different backbone architectures, ranging from 1.5B to 8B parameters, all\nof which demonstrate strong performance on the Massive Text Embedding\nBenchmark. Our framework is publicly available at:\nhttps://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found\nat https://rb.gy/ws1ile.\n","authors":["Hieu Man","Nghia Trung Ngo","Franck Dernoncourt","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.03402v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.17717v2","updated":"2024-08-06T18:42:22Z","published":"2024-02-27T17:52:33Z","title":"AmbigNLG: Addressing Task Ambiguity in Instruction for NLG","summary":"  In this study, we introduce AmbigNLG, a new task designed to tackle the\nchallenge of task ambiguity in instructions for Natural Language Generation\n(NLG) tasks. Despite the impressive capabilities of Large Language Models\n(LLMs) in understanding and executing a wide range of tasks through natural\nlanguage interaction, their performance is significantly hindered by the\nambiguity present in real-world instructions. To address this, AmbigNLG seeks\nto identify and mitigate such ambiguities, aiming to refine instructions to\nmatch user expectations better. We introduce a dataset, AmbigSNI-NLG,\nconsisting of 2,500 instances, and develop an ambiguity taxonomy for\ncategorizing and annotating instruction ambiguities. Our approach demonstrates\nsubstantial improvements in text generation quality, highlighting the critical\nrole of clear and specific instructions in enhancing LLM performance in NLG\ntasks.\n","authors":["Ayana Niwa","Hayate Iso"],"pdf_url":"https://arxiv.org/pdf/2402.17717v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2404.01399v4","updated":"2024-08-06T18:18:16Z","published":"2024-04-01T18:10:05Z","title":"Developing Safe and Responsible Large Language Model : Can We Balance\n  Bias Reduction and Language Understanding in Large Language Models?","summary":"  Large Language Models (LLMs) have advanced various Natural Language\nProcessing (NLP) tasks, such as text generation and translation, among others.\nHowever, these models often generate text that can perpetuate biases. Existing\napproaches to mitigate these biases usually compromise knowledge retention.\nThis study explores whether LLMs can produce safe, unbiased outputs without\nsacrificing knowledge or comprehension. We introduce the Safe and Responsible\nLarge Language Model (\\textbf{SR}$_{\\text{LLM}}$), which has been instruction\nfine-tuned atop an inherently safe fine-tuned LLM to reduce biases in generated\ntexts. We developed a specialized dataset with examples of unsafe and\ncorresponding safe variations to train \\textbf{SR}$_{\\text{LLM}}$ to identify\nand correct biased text. Experiments on our specialized dataset and\nout-of-distribution test sets reveal that \\textbf{SR}$_{\\text{LLM}}$\neffectively reduces biases while preserving knowledge integrity. This\nperformance surpasses that of traditional fine-tuning of smaller language\nmodels and base LLMs that merely reply on prompting techniques. Our findings\nindicate that instruction fine-tuning is an effective strategy for minimizing\nbias in LLMs while retaining knowledge. The code and dataset are accessible at\n\\href{https://github.com/shainarazavi/Safe-Responsible-LLM}{SR-LLM}.\n","authors":["Shaina Raza","Oluwanifemi Bamgbose","Shardul Ghuge","Fatemeh Tavakol","Deepak John Reji","Syed Raza Bashir"],"pdf_url":"https://arxiv.org/pdf/2404.01399v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03359v1","updated":"2024-08-06T15:55:05Z","published":"2024-08-06T15:55:05Z","title":"LAMPO: Large Language Models as Preference Machines for Few-shot Ordinal\n  Classification","summary":"  We introduce LAMPO, a novel paradigm that leverages Large Language Models\n(LLMs) for solving few-shot multi-class ordinal classification tasks. Unlike\nconventional methods, which concatenate all demonstration examples with the\ntest instance and prompt LLMs to produce the pointwise prediction, our\nframework uses the LLM as a preference machine that makes a relative\ncomparative decision between the test instance and each demonstration. A\nself-supervised method is then introduced to aggregate these binary comparisons\ninto the final ordinal decision. LAMPO addresses several limitations inherent\nin previous methods, including context length constraints, ordering biases, and\nchallenges associated with absolute point-wise estimation. Extensive\nexperiments on seven public datasets demonstrate LAMPO's remarkably competitive\nperformance across a diverse spectrum of applications (e.g., movie review\nanalysis and hate speech detection). Notably, in certain applications, the\nimprovement can be substantial, exceeding 20% in an absolute term. Moreover, we\nbelieve LAMPO represents an interesting addition to the non-parametric\napplication layered on top of LLMs, as it supports black-box LLMs without\nnecessitating the outputting of LLM's internal states (e.g., embeddings), as\nseen in previous approaches.\n","authors":["Zhen Qin","Junru Wu","Jiaming Shen","Tianqi Liu","Xuanhui Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03359v1.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2408.03354v1","updated":"2024-08-06T09:15:25Z","published":"2024-08-06T09:15:25Z","title":"The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums","summary":"  Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the accuracy of an LLM system\nbuilt on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do\nso, a random sample of 500 daily conversations from three cybercrime forums,\nXSS, Exploit.in, and RAMP, was extracted, and the LLM system was instructed to\nsummarize the conversations and code 10 key CTI variables, such as whether a\nlarge organization and/or a critical infrastructure is being targeted. Then,\ntwo coders reviewed each conversation and evaluated whether the information\nextracted by the LLM was accurate. The LLM system performed strikingly well,\nwith an average accuracy score of 98%. Various ways to enhance the model were\nuncovered, such as the need to help the LLM distinguish between stories and\npast events, as well as being careful with verb tenses in prompts.\nNevertheless, the results of this study highlight the efficiency and relevance\nof using LLMs for cyber threat intelligence.\n","authors":["Vanessa Clairoux-Trepanier","Isa-May Beauchamp","Estelle Ruellan","Masarah Paquet-Clouston","Serge-Olivier Paquette","Eric Clay"],"pdf_url":"https://arxiv.org/pdf/2408.03354v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.03326v1","updated":"2024-08-06T17:59:44Z","published":"2024-08-06T17:59:44Z","title":"LLaVA-OneVision: Easy Visual Task Transfer","summary":"  We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.\n","authors":["Bo Li","Yuanhan Zhang","Dong Guo","Renrui Zhang","Feng Li","Hao Zhang","Kaichen Zhang","Yanwei Li","Ziwei Liu","Chunyuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03326v1.pdf","comment":"Project Homepage:\n  https://llava-vl.github.io/blog/2024-08-05-llava-onevision/"},{"id":"http://arxiv.org/abs/2408.03322v1","updated":"2024-08-06T17:58:18Z","published":"2024-08-06T17:58:18Z","title":"Segment Anything in Medical Images and Videos: Benchmark and Deployment","summary":"  Recent advances in segmentation foundation models have enabled accurate and\nefficient segmentation across a wide range of natural images and videos, but\ntheir utility to medical data remains unclear. In this work, we first present a\ncomprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11\nmedical image modalities and videos and point out its strengths and weaknesses\nby comparing it to SAM1 and MedSAM. Then, we develop a transfer learning\npipeline and demonstrate SAM2 can be quickly adapted to medical domain by\nfine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio\nAPI for efficient 3D image and video segmentation. The code has been made\npublicly available at \\url{https://github.com/bowang-lab/MedSAM}.\n","authors":["Jun Ma","Sumin Kim","Feifei Li","Mohammed Baharoon","Reza Asakereh","Hongwei Lyu","Bo Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03322v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01197v2","updated":"2024-08-06T17:58:00Z","published":"2024-04-01T15:55:25Z","title":"Getting it Right: Improving Spatial Consistency in Text-to-Image Models","summary":"  One of the key shortcomings in current text-to-image (T2I) models is their\ninability to consistently generate images which faithfully follow the spatial\nrelationships specified in the text prompt. In this paper, we offer a\ncomprehensive investigation of this limitation, while also developing datasets\nand methods that support algorithmic solutions to improve spatial reasoning in\nT2I models. We find that spatial relationships are under-represented in the\nimage descriptions found in current vision-language datasets. To alleviate this\ndata bottleneck, we create SPRIGHT, the first spatially focused, large-scale\ndataset, by re-captioning 6 million images from 4 widely used vision datasets\nand through a 3-fold evaluation and analysis pipeline, show that SPRIGHT\nimproves the proportion of spatial relationships in existing datasets. We show\nthe efficacy of SPRIGHT data by showing that using only $\\sim$0.25% of SPRIGHT\nresults in a 22% improvement in generating spatially accurate images while also\nimproving FID and CMMD scores. We also find that training on images containing\na larger number of objects leads to substantial improvements in spatial\nconsistency, including state-of-the-art results on T2I-CompBench with a spatial\nscore of 0.2133, by fine-tuning on <500 images. Through a set of controlled\nexperiments and ablations, we document additional findings that could support\nfuture work that seeks to understand factors that affect spatial consistency in\ntext-to-image models.\n","authors":["Agneet Chatterjee","Gabriela Ben Melech Stan","Estelle Aflalo","Sayak Paul","Dhruba Ghosh","Tejas Gokhale","Ludwig Schmidt","Hannaneh Hajishirzi","Vasudev Lal","Chitta Baral","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2404.01197v2.pdf","comment":"Accepted to ECCV 2024. Project Page : https://spright-t2i.github.io/"},{"id":"http://arxiv.org/abs/2404.01282v2","updated":"2024-08-06T17:56:53Z","published":"2024-04-01T17:54:34Z","title":"LoSA: Long-Short-range Adapter for Scaling End-to-End Temporal Action\n  Localization","summary":"  Temporal Action Localization (TAL) involves localizing and classifying action\nsnippets in an untrimmed video. The emergence of large video foundation models\nhas led RGB-only video backbones to outperform previous methods needing both\nRGB and optical flow modalities. Leveraging these large models is often limited\nto training only the TAL head due to the prohibitively large GPU memory\nrequired to adapt the video backbone for TAL. To overcome this limitation, we\nintroduce LoSA, the first memory-and-parameter-efficient backbone adapter\ndesigned specifically for TAL to handle untrimmed videos. LoSA specializes for\nTAL by introducing Long-Short-range Adapters that adapt the intermediate layers\nof the video backbone over different temporal ranges. These adapters run\nparallel to the video backbone to significantly reduce memory footprint. LoSA\nalso includes Long-Short-range Gated Fusion that strategically combines the\noutput of these adapters from the video backbone layers to enhance the video\nfeatures provided to the TAL head. Experiments show that LoSA significantly\noutperforms all existing methods on standard TAL benchmarks, THUMOS-14 and\nActivityNet-v1.3, by scaling end-to-end backbone adaptation to\nbillion-parameter-plus models like VideoMAEv2~(ViT-g) and leveraging them\nbeyond head-only transfer learning.\n","authors":["Akshita Gupta","Gaurav Mittal","Ahmed Magooda","Ye Yu","Graham W. Taylor","Mei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.01282v2.pdf","comment":"Under submission"},{"id":"http://arxiv.org/abs/2408.00756v2","updated":"2024-08-06T17:40:07Z","published":"2024-08-01T17:57:25Z","title":"Segment anything model 2: an application to 2D and 3D medical images","summary":"  Segment Anything Model (SAM) has gained significant attention because of its\nability to segment varous objects in images given a prompt. The recently\ndeveloped SAM 2 has extended this ability to video inputs. This opens an\nopportunity to apply SAM to 3D images, one of the fundamental tasks in the\nmedical imaging field. In this paper, we extensively evaluate SAM 2's ability\nto segment both 2D and 3D medical images by first collecting 18 medical imaging\ndatasets, including common 3D modalities such as computed tomography (CT),\nmagnetic resonance imaging (MRI), and positron emission tomography (PET) as\nwell as 2D modalities such as X-ray and ultrasound. Two evaluation pipelines of\nSAM 2 are considered: (1) multi-frame 3D segmentation, where prompts are\nprovided to one or multiple slice(s) selected from the volume, and (2)\nsingle-frame 2D segmentation, where prompts are provided to each slice. The\nformer is only applicable to 3D modalities, while the latter applies to both 2D\nand 3D modalities. Our results show that SAM 2 exhibits similar performance as\nSAM under single-frame 2D segmentation, and has variable performance under\nmulti-frame 3D segmentation depending on the choices of slices to annotate, the\ndirection of the propagation, the predictions utilized during the propagation,\netc.\n","authors":["Haoyu Dong","Hanxue Gu","Yaqian Chen","Jichen Yang","Maciej A. Mazurowski"],"pdf_url":"https://arxiv.org/pdf/2408.00756v2.pdf","comment":"12 pages, 9 figures. An updated version with new results and\n  corrections"},{"id":"http://arxiv.org/abs/2402.00035v4","updated":"2024-08-06T17:36:06Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mlanie Ducoffe","Audrey Galametz","Guillaume Povda","Ryma Boumazouza","Nomie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v4.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2402.04492v2","updated":"2024-08-06T17:31:33Z","published":"2024-02-07T00:31:49Z","title":"ColorSwap: A Color and Word Order Dataset for Multimodal Evaluation","summary":"  This paper introduces the ColorSwap dataset, designed to assess and improve\nthe proficiency of multimodal models in matching objects with their colors. The\ndataset is comprised of 2,000 unique image-caption pairs, grouped into 1,000\nexamples. Each example includes a caption-image pair, along with a\n``color-swapped'' pair. We follow the Winoground schema: the two captions in an\nexample have the same words, but the color words have been rearranged to modify\ndifferent objects. The dataset was created through a novel blend of automated\ncaption and image generation with humans in the loop. We evaluate image-text\nmatching (ITM) and visual language models (VLMs) and find that even the latest\nones are still not robust at this task. GPT-4V and LLaVA score 72% and 42% on\nour main VLM metric, although they may improve with more advanced prompting\ntechniques. On the main ITM metric, contrastive models such as CLIP and SigLIP\nperform close to chance (at 12% and 30%, respectively), although the\nnon-contrastive BLIP ITM model is stronger (87%). We also find that finetuning\non fewer than 2,000 examples yields significant performance gains on this\nout-of-distribution word-order understanding task. The dataset is here:\nhttps://github.com/Top34051/colorswap and here:\nhttps://huggingface.co/datasets/stanfordnlp/colorswap.\n","authors":["Jirayu Burapacheep","Ishan Gaur","Agam Bhatia","Tristan Thrush"],"pdf_url":"https://arxiv.org/pdf/2402.04492v2.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2408.03312v1","updated":"2024-08-06T17:29:01Z","published":"2024-08-06T17:29:01Z","title":"MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture\n  Generation","summary":"  Recent advancements in the field of Diffusion Transformers have substantially\nimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.\nHowever, the effectiveness of the Transformer architecture in the domain of\nco-speech gesture generation remains relatively unexplored, as prior\nmethodologies have predominantly employed the Convolutional Neural Network\n(CNNs) or simple a few transformer layers. In an attempt to bridge this\nresearch gap, we introduce a novel Masked Diffusion Transformer for co-speech\ngesture generation, referred to as MDT-A2G, which directly implements the\ndenoising process on gesture sequences. To enhance the contextual reasoning\ncapability of temporally aligned speech-driven gestures, we incorporate a novel\nMasked Diffusion Transformer. This model employs a mask modeling scheme\nspecifically designed to strengthen temporal relation learning among sequence\ngestures, thereby expediting the learning process and leading to coherent and\nrealistic motions. Apart from audio, Our MDT-A2G model also integrates\nmulti-modal information, encompassing text, emotion, and identity. Furthermore,\nwe propose an efficient inference strategy that diminishes the denoising\ncomputation by leveraging previously calculated results, thereby achieving a\nspeedup with negligible performance degradation. Experimental results\ndemonstrate that MDT-A2G excels in gesture generation, boasting a learning\nspeed that is over 6$\\times$ faster than traditional diffusion transformers and\nan inference speed that is 5.7$\\times$ than the standard diffusion model.\n","authors":["Xiaofeng Mao","Zhengkai Jiang","Qilin Wang","Chencan Fu","Jiangning Zhang","Jiafu Wu","Yabiao Wang","Chengjie Wang","Wei Li","Mingmin Chi"],"pdf_url":"https://arxiv.org/pdf/2408.03312v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19308v2","updated":"2024-08-06T17:22:17Z","published":"2024-07-27T17:45:20Z","title":"Comprehensive Attribution: Inherently Explainable Vision Model with\n  Feature Detector","summary":"  As deep vision models' popularity rapidly increases, there is a growing\nemphasis on explanations for model predictions. The inherently explainable\nattribution method aims to enhance the understanding of model behavior by\nidentifying the important regions in images that significantly contribute to\npredictions. It is achieved by cooperatively training a selector (generating an\nattribution map to identify important features) and a predictor (making\npredictions using the identified features). Despite many advancements, existing\nmethods suffer from the incompleteness problem, where discriminative features\nare masked out, and the interlocking problem, where the non-optimized selector\ninitially selects noise, causing the predictor to fit on this noise and\nperpetuate the cycle. To address these problems, we introduce a new objective\nthat discourages the presence of discriminative features in the masked-out\nregions thus enhancing the comprehensiveness of feature selection. A\npre-trained detector is introduced to detect discriminative features in the\nmasked-out region. If the selector selects noise instead of discriminative\nfeatures, the detector can observe and break the interlocking situation by\npenalizing the selector. Extensive experiments show that our model makes\naccurate predictions with higher accuracy than the regular black-box model, and\nproduces attribution maps with high feature coverage, localization ability,\nfidelity and robustness. Our code will be available at\n\\href{https://github.com/Zood123/COMET}{https://github.com/Zood123/COMET}.\n","authors":["Xianren Zhang","Dongwon Lee","Suhang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.19308v2.pdf","comment":"Accepted as a conference paper by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2304.05339v2","updated":"2024-08-06T17:09:59Z","published":"2023-04-11T16:58:59Z","title":"Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images","summary":"  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n","authors":["Suprim Nakarmi","Sanam Pudasaini","Safal Thapaliya","Pratima Upretee","Retina Shrestha","Basant Giri","Bhanu Bhakta Neupane","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2304.05339v2.pdf","comment":"21 pages (including supplementary information), 5 figures, 7 tables,\n  Accepted for publication at the Journal of Machine Learning for Biomedical\n  Imaging (MELBA) https://melba-journal.org/2024:014"},{"id":"http://arxiv.org/abs/2408.03302v1","updated":"2024-08-06T17:08:05Z","published":"2024-08-06T17:08:05Z","title":"TextIM: Part-aware Interactive Motion Synthesis from Text","summary":"  In this work, we propose TextIM, a novel framework for synthesizing\nTEXT-driven human Interactive Motions, with a focus on the precise alignment of\npart-level semantics. Existing methods often overlook the critical roles of\ninteractive body parts and fail to adequately capture and align part-level\nsemantics, resulting in inaccuracies and even erroneous movement outcomes. To\naddress these issues, TextIM utilizes a decoupled conditional diffusion\nframework to enhance the detailed alignment between interactive movements and\ncorresponding semantic intents from textual descriptions. Our approach\nleverages large language models, functioning as a human brain, to identify\ninteracting human body parts and to comprehend interaction semantics to\ngenerate complicated and subtle interactive motion. Guided by the refined\nmovements of the interacting parts, TextIM further extends these movements into\na coherent whole-body motion. We design a spatial coherence module to\ncomplement the entire body movements while maintaining consistency and harmony\nacross body parts using a part graph convolutional network. For training and\nevaluation, we carefully selected and re-labeled interactive motions from\nHUMANML3D to develop a specialized dataset. Experimental results demonstrate\nthat TextIM produces semantically accurate human interactive motions,\nsignificantly enhancing the realism and applicability of synthesized\ninteractive motions in diverse scenarios, even including interactions with\ndeformable and dynamically changing objects.\n","authors":["Siyuan Fan","Bo Du","Xiantao Cai","Bo Peng","Longling Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11914v2","updated":"2024-08-06T17:00:49Z","published":"2024-05-20T09:49:13Z","title":"PT43D: A Probabilistic Transformer for Generating 3D Shapes from Single\n  Highly-Ambiguous RGB Images","summary":"  Generating 3D shapes from single RGB images is essential in various\napplications such as robotics. Current approaches typically target images\ncontaining clear and complete visual descriptions of the object, without\nconsidering common realistic cases where observations of objects that are\nlargely occluded or truncated. We thus propose a transformer-based\nautoregressive model to generate the probabilistic distribution of 3D shapes\nconditioned on an RGB image containing potentially highly ambiguous\nobservations of the object. To handle realistic scenarios such as occlusion or\nfield-of-view truncation, we create simulated image-to-shape training pairs\nthat enable improved fine-tuning for real-world scenarios. We then adopt\ncross-attention to effectively identify the most relevant region of interest\nfrom the input image for shape generation. This enables inference of sampled\nshapes with reasonable diversity and strong alignment with the input image. We\ntrain and test our model on our synthetic data then fine-tune and test it on\nreal-world data. Experiments demonstrate that our model outperforms state of\nthe art in both scenarios.\n","authors":["Yiheng Xiong","Angela Dai"],"pdf_url":"https://arxiv.org/pdf/2405.11914v2.pdf","comment":"10 pages, 6 figures. Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2408.03291v1","updated":"2024-08-06T16:40:04Z","published":"2024-08-06T16:40:04Z","title":"DopQ-ViT: Towards Distribution-Friendly and Outlier-Aware Post-Training\n  Quantization for Vision Transformers","summary":"  Vision transformers (ViTs) have garnered significant attention for their\nperformance in vision tasks; however, the high computational cost and\nsignificant latency issues have hinder widespread adoption. Post-training\nquantization (PTQ), a promising method for model compression, still faces\naccuracy degradation challenges with ViTs. There are two reasons for this: the\nexisting quantization paradigm does not fit the power-law distribution of\npost-Softmax activations well, and accuracy inevitably decreases after\nreparameterizing post-LayerNorm activations. We propose a Distribution-Friendly\nand Outlier-Aware Post-training Quantization method for Vision Transformers,\nnamed DopQ-ViT. DopQ-ViT analyzes the inefficiencies of current quantizers and\nintroduces a distribution-friendly Tan Quantizer called TanQ. TanQ focuses more\non values near 1, more accurately preserving the power-law distribution of\npost-Softmax activations, and achieves favorable results. Moreover, when\nreparameterizing post-LayerNorm activations from channel-wise to layer-wise\nquantization, the accuracy degradation is mainly due to the significant impact\nof outliers in the scaling factors. Therefore, DopQ-ViT proposes a method to\nSearch for the Optimal Scaling Factor, denoted as SOSF, which compensates for\nthe influence of outliers and preserves the performance of the quantization\nmodel. DopQ-ViT has undergone extensive validation and demonstrates significant\nperformance improvements in quantization models, particularly in low-bit\nsettings.\n","authors":["Lianwei Yang","Haisong Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03291v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.14242v2","updated":"2024-08-06T16:36:11Z","published":"2023-11-24T01:15:57Z","title":"RSB-Pose: Robust Short-Baseline Binocular 3D Human Pose Estimation with\n  Occlusion Handling","summary":"  In the domain of 3D Human Pose Estimation, which finds widespread daily\napplications, the requirement for convenient acquisition equipment continues to\ngrow. To satisfy this demand, we set our sights on a short-baseline binocular\nsetting that offers both portability and a geometric measurement property that\nradically mitigates depth ambiguity. However, as the binocular baseline\nshortens, two serious challenges emerge: first, the robustness of 3D\nreconstruction against 2D errors deteriorates; and second, occlusion reoccurs\ndue to the limited visual differences between two views. To address the first\nchallenge, we propose the Stereo Co-Keypoints Estimation module to improve the\nview consistency of 2D keypoints and enhance the 3D robustness. In this module,\nthe disparity is utilized to represent the correspondence of binocular 2D\npoints and the Stereo Volume Feature is introduced to contain binocular\nfeatures across different disparities. Through the regression of SVF, two-view\n2D keypoints are simultaneously estimated in a collaborative way which\nrestricts their view consistency. Furthermore, to deal with occlusions, a\nPre-trained Pose Transformer module is introduced. Through this module, 3D\nposes are refined by perceiving pose coherence, a representation of joint\ncorrelations. This perception is injected by the Pose Transformer network and\nlearned through a pre-training task that recovers iterative masked joints.\nComprehensive experiments carried out on H36M and MHAD datasets, complemented\nby visualizations, validate the effectiveness of our approach in the\nshort-baseline binocular 3D Human Pose Estimation and occlusion handling.\n","authors":["Xiaoyue Wan","Zhuo Chen","Yiming Bao","Xu Zhao"],"pdf_url":"https://arxiv.org/pdf/2311.14242v2.pdf","comment":"13 pages, 8 figures, currently under review at IEEE Transactions on\n  Image Processing journal"},{"id":"http://arxiv.org/abs/2406.04485v3","updated":"2024-08-06T16:35:50Z","published":"2024-06-06T20:15:42Z","title":"GenAI Arena: An Open Evaluation Platform for Generative Models","summary":"  Generative AI has made remarkable strides to revolutionize fields such as\nimage and video generation. These advancements are driven by innovative\nalgorithms, architecture, and data. However, the rapid proliferation of\ngenerative models has highlighted a critical gap: the absence of trustworthy\nevaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc\noften fail to capture the nuanced quality and user satisfaction associated with\ngenerative outputs. This paper proposes an open platform GenAI-Arena to\nevaluate different image and video generative models, where users can actively\nparticipate in evaluating these models. By leveraging collective user feedback\nand votes, GenAI-Arena aims to provide a more democratic and accurate measure\nof model performance. It covers three arenas for text-to-image generation,\ntext-to-video generation, and image editing respectively. Currently, we cover a\ntotal of 27 open-source generative models. GenAI-Arena has been operating for\nfour months, amassing over 6000 votes from the community. We describe our\nplatform, analyze the data, and explain the statistical methods for ranking the\nmodels. To further promote the research in building model-based evaluation\nmetrics, we release a cleaned version of our preference data for the three\ntasks, namely GenAI-Bench. We prompt the existing multi-modal models like\nGemini, GPT-4o to mimic human voting. We compute the correlation between model\nvoting with human voting to understand their judging abilities. Our results\nshow existing multimodal models are still lagging in assessing the generated\nvisual content, even the best model GPT-4o only achieves a Pearson correlation\nof 0.22 in the quality subscore, and behaves like random guessing in others.\n","authors":["Dongfu Jiang","Max Ku","Tianle Li","Yuansheng Ni","Shizhuo Sun","Rongqi Fan","Wenhu Chen"],"pdf_url":"https://arxiv.org/pdf/2406.04485v3.pdf","comment":"9 pages,7 figures"},{"id":"http://arxiv.org/abs/2408.03286v1","updated":"2024-08-06T16:34:04Z","published":"2024-08-06T16:34:04Z","title":"Biomedical SAM 2: Segment Anything in Biomedical Images and Videos","summary":"  Medical image segmentation and video object segmentation are essential for\ndiagnosing and analyzing diseases by identifying and measuring biological\nstructures. Recent advances in natural domain have been driven by foundation\nmodels like the Segment Anything Model 2 (SAM 2). To explore the performance of\nSAM 2 in biomedical applications, we designed two evaluation pipelines for\nsingle-frame image segmentation and multi-frame video segmentation with varied\nprompt designs, revealing SAM 2's limitations in medical contexts.\nConsequently, we developed BioSAM 2, an enhanced foundation model optimized for\nbiomedical data based on SAM 2. Our experiments show that BioSAM 2 not only\nsurpasses the performance of existing state-of-the-art foundation models but\nalso matches or even exceeds specialist models, demonstrating its efficacy and\npotential in the medical domain.\n","authors":["Zhiling Yan","Weixiang Sun","Rong Zhou","Zhengqing Yuan","Kai Zhang","Yiwei Li","Tianming Liu","Quanzheng Li","Xiang Li","Lifang He","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03286v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03284v1","updated":"2024-08-06T16:31:45Z","published":"2024-08-06T16:31:45Z","title":"ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually\n  Synced Facial Performer","summary":"  Lip-syncing videos with given audio is the foundation for various\napplications including the creation of virtual presenters or performers. While\nrecent studies explore high-fidelity lip-sync with different techniques, their\ntask-orientated models either require long-term videos for clip-specific\ntraining or retain visible artifacts. In this paper, we propose a unified and\neffective framework ReSyncer, that synchronizes generalized audio-visual facial\ninformation. The key design is revisiting and rewiring the Style-based\ngenerator to efficiently adopt 3D facial dynamics predicted by a principled\nstyle-injected Transformer. By simply re-configuring the information insertion\nmechanisms within the noise and style space, our framework fuses motion and\nappearance with unified training. Extensive experiments demonstrate that\nReSyncer not only produces high-fidelity lip-synced videos according to audio,\nbut also supports multiple appealing properties that are suitable for creating\nvirtual presenters and performers, including fast personalized fine-tuning,\nvideo-driven lip-syncing, the transfer of speaking styles, and even face\nswapping. Resources can be found at\nhttps://guanjz20.github.io/projects/ReSyncer.\n","authors":["Jiazhi Guan","Zhiliang Xu","Hang Zhou","Kaisiyuan Wang","Shengyi He","Zhanwang Zhang","Borong Liang","Haocheng Feng","Errui Ding","Jingtuo Liu","Jingdong Wang","Youjian Zhao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03284v1.pdf","comment":"Accepted to European Conference on Computer Vision (ECCV), 2024.\n  Project page: https://guanjz20.github.io/projects/ReSyncer"},{"id":"http://arxiv.org/abs/2408.03282v1","updated":"2024-08-06T16:29:51Z","published":"2024-08-06T16:29:51Z","title":"AMES: Asymmetric and Memory-Efficient Similarity Estimation for\n  Instance-level Retrieval","summary":"  This work investigates the problem of instance-level image retrieval\nre-ranking with the constraint of memory efficiency, ultimately aiming to limit\nmemory usage to 1KB per image. Departing from the prevalent focus on\nperformance enhancements, this work prioritizes the crucial trade-off between\nperformance and memory requirements. The proposed model uses a\ntransformer-based architecture designed to estimate image-to-image similarity\nby capturing interactions within and across images based on their local\ndescriptors. A distinctive property of the model is the capability for\nasymmetric similarity estimation. Database images are represented with a\nsmaller number of descriptors compared to query images, enabling performance\nimprovements without increasing memory consumption. To ensure adaptability\nacross different applications, a universal model is introduced that adjusts to\na varying number of local descriptors during the testing phase. Results on\nstandard benchmarks demonstrate the superiority of our approach over both\nhand-crafted and learned models. In particular, compared with current\nstate-of-the-art methods that overlook their memory footprint, our approach not\nonly attains superior performance but does so with a significantly reduced\nmemory footprint. The code and pretrained models are publicly available at:\nhttps://github.com/pavelsuma/ames\n","authors":["Pavel Suma","Giorgos Kordopatis-Zilos","Ahmet Iscen","Giorgos Tolias"],"pdf_url":"https://arxiv.org/pdf/2408.03282v1.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2403.15313v2","updated":"2024-08-06T15:58:35Z","published":"2024-03-22T16:06:05Z","title":"CR3DT: Camera-RADAR Fusion for 3D Detection and Tracking","summary":"  To enable self-driving vehicles accurate detection and tracking of\nsurrounding objects is essential. While Light Detection and Ranging (LiDAR)\nsensors have set the benchmark for high-performance systems, the appeal of\ncamera-only solutions lies in their cost-effectiveness. Notably, despite the\nprevalent use of Radio Detection and Ranging (RADAR) sensors in automotive\nsystems, their potential in 3D detection and tracking has been largely\ndisregarded due to data sparsity and measurement noise. As a recent\ndevelopment, the combination of RADARs and cameras is emerging as a promising\nsolution. This paper presents Camera-RADAR 3D Detection and Tracking (CR3DT), a\ncamera-RADAR fusion model for 3D object detection, and Multi-Object Tracking\n(MOT). Building upon the foundations of the State-of-the-Art (SotA) camera-only\nBEVDet architecture, CR3DT demonstrates substantial improvements in both\ndetection and tracking capabilities, by incorporating the spatial and velocity\ninformation of the RADAR sensor. Experimental results demonstrate an absolute\nimprovement in detection performance of 5.3% in mean Average Precision (mAP)\nand a 14.9% increase in Average Multi-Object Tracking Accuracy (AMOTA) on the\nnuScenes dataset when leveraging both modalities. CR3DT bridges the gap between\nhigh-performance and cost-effective perception systems in autonomous driving,\nby capitalizing on the ubiquitous presence of RADAR in automotive applications.\nThe code is available at: https://github.com/ETH-PBL/CR3DT.\n","authors":["Nicolas Baumann","Michael Baumgartner","Edoardo Ghignone","Jonas Khne","Tobias Fischer","Yung-Hsu Yang","Marc Pollefeys","Michele Magno"],"pdf_url":"https://arxiv.org/pdf/2403.15313v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10344v3","updated":"2024-08-06T15:39:03Z","published":"2024-02-15T22:17:17Z","title":"Evaluating Neural Radiance Fields (NeRFs) for 3D Plant Geometry\n  Reconstruction in Field Conditions","summary":"  We evaluate different Neural Radiance Fields (NeRFs) techniques for the 3D\nreconstruction of plants in varied environments, from indoor settings to\noutdoor fields. Traditional methods usually fail to capture the complex\ngeometric details of plants, which is crucial for phenotyping and breeding\nstudies. We evaluate the reconstruction fidelity of NeRFs in three scenarios\nwith increasing complexity and compare the results with the point cloud\nobtained using LiDAR as ground truth. In the most realistic field scenario, the\nNeRF models achieve a 74.6% F1 score after 30 minutes of training on the GPU,\nhighlighting the efficacy of NeRFs for 3D reconstruction in challenging\nenvironments. Additionally, we propose an early stopping technique for NeRF\ntraining that almost halves the training time while achieving only a reduction\nof 7.4% in the average F1 score. This optimization process significantly\nenhances the speed and efficiency of 3D reconstruction using NeRFs. Our\nfindings demonstrate the potential of NeRFs in detailed and realistic 3D plant\nreconstruction and suggest practical approaches for enhancing the speed and\nefficiency of NeRFs in the 3D reconstruction process.\n","authors":["Muhammad Arbab Arshad","Talukder Jubery","James Afful","Anushrut Jignasu","Aditya Balu","Baskar Ganapathysubramanian","Soumik Sarkar","Adarsh Krishnamurthy"],"pdf_url":"https://arxiv.org/pdf/2402.10344v3.pdf","comment":"Published in 'Plant Phenomics'"},{"id":"http://arxiv.org/abs/2402.15745v2","updated":"2024-08-06T15:28:30Z","published":"2024-02-24T06:57:15Z","title":"GAOKAO-MM: A Chinese Human-Level Benchmark for Multimodal Models\n  Evaluation","summary":"  The Large Vision-Language Models (LVLMs) have demonstrated great abilities in\nimage perception and language understanding. However, existing multimodal\nbenchmarks focus on primary perception abilities and commonsense knowledge\nwhich are insufficient to reflect the comprehensive capabilities of LVLMs. We\npropose GAOKAO-MM, a multimodal benchmark based on the Chinese College Entrance\nExamination (GAOKAO), comprising of 8 subjects and 12 types of images, such as\ndiagrams, function graphs, maps and photos. GAOKAO-MM derives from native\nChinese context and sets human-level requirements for the model's abilities,\nincluding perception, understanding, knowledge and reasoning. We evaluate 10\nLVLMs and find that the accuracies of all of them are lower than 50%, with\nGPT-4-Vison (48.1%), Qwen-VL-Plus (41.2%) and Gemini-Pro-Vision (35.1%) ranking\nin the top three positions. The results of our multi-dimension analysis\nindicate that LVLMs have moderate distance towards Artificial General\nIntelligence (AGI) and provide insights facilitating the development of\nmultilingual LVLMs.\n","authors":["Yi Zong","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2402.15745v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03291v2","updated":"2024-08-06T15:14:01Z","published":"2024-07-03T17:24:36Z","title":"VCHAR:Variance-Driven Complex Human Activity Recognition framework with\n  Generative Representation","summary":"  Complex human activity recognition (CHAR) remains a pivotal challenge within\nubiquitous computing, especially in the context of smart environments. Existing\nstudies typically require meticulous labeling of both atomic and complex\nactivities, a task that is labor-intensive and prone to errors due to the\nscarcity and inaccuracies of available datasets. Most prior research has\nfocused on datasets that either precisely label atomic activities or, at\nminimum, their sequence approaches that are often impractical in real world\nsettings.In response, we introduce VCHAR (Variance-Driven Complex Human\nActivity Recognition), a novel framework that treats the outputs of atomic\nactivities as a distribution over specified intervals. Leveraging generative\nmethodologies, VCHAR elucidates the reasoning behind complex activity\nclassifications through video-based explanations, accessible to users without\nprior machine learning expertise. Our evaluation across three publicly\navailable datasets demonstrates that VCHAR enhances the accuracy of complex\nactivity recognition without necessitating precise temporal or sequential\nlabeling of atomic activities. Furthermore, user studies confirm that VCHAR's\nexplanations are more intelligible compared to existing methods, facilitating a\nbroader understanding of complex activity recognition among non-experts.\n","authors":["Yuan Sun","Navid Salami Pargoo","Taqiya Ehsan","Zhao Zhang","Jorge Ortiz"],"pdf_url":"https://arxiv.org/pdf/2407.03291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03238v1","updated":"2024-08-06T14:50:48Z","published":"2024-08-06T14:50:48Z","title":"LAC-Net: Linear-Fusion Attention-Guided Convolutional Network for\n  Accurate Robotic Grasping Under the Occlusion","summary":"  This paper addresses the challenge of perceiving complete object shapes\nthrough visual perception. While prior studies have demonstrated encouraging\noutcomes in segmenting the visible parts of objects within a scene, amodal\nsegmentation, in particular, has the potential to allow robots to infer the\noccluded parts of objects. To this end, this paper introduces a new framework\nthat explores amodal segmentation for robotic grasping in cluttered scenes,\nthus greatly enhancing robotic grasping abilities. Initially, we use a\nconventional segmentation algorithm to detect the visible segments of the\ntarget object, which provides shape priors for completing the full object mask.\nParticularly, to explore how to utilize semantic features from RGB images and\ngeometric information from depth images, we propose a Linear-fusion\nAttention-guided Convolutional Network (LAC-Net). LAC-Net utilizes the\nlinear-fusion strategy to effectively fuse this cross-modal data, and then uses\nthe prior visible mask as attention map to guide the network to focus on target\nfeature locations for further complete mask recovery. Using the amodal mask of\nthe target object provides advantages in selecting more accurate and robust\ngrasp points compared to relying solely on the visible segments. The results on\ndifferent datasets show that our method achieves state-of-the-art performance.\nFurthermore, the robot experiments validate the feasibility and robustness of\nthis method in the real world. Our code and demonstrations are available on the\nproject page: https://jrryzh.github.io/LAC-Net.\n","authors":["Jinyu Zhang","Yongchong Gu","Jianxiong Gao","Haitao Lin","Qiang Sun","Xinwei Sun","Xiangyang Xue","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2408.03238v1.pdf","comment":"accepted by IROS2024"},{"id":"http://arxiv.org/abs/2408.03230v1","updated":"2024-08-06T14:44:55Z","published":"2024-08-06T14:44:55Z","title":"Contrastive Learning for Image Complexity Representation","summary":"  Quantifying and evaluating image complexity can be instrumental in enhancing\nthe performance of various computer vision tasks. Supervised learning can\neffectively learn image complexity features from well-annotated datasets.\nHowever, creating such datasets requires expensive manual annotation costs. The\nmodels may learn human subjective biases from it. In this work, we introduce\nthe MoCo v2 framework. We utilize contrastive learning to represent image\ncomplexity, named CLIC (Contrastive Learning for Image Complexity). We find\nthat there are complexity differences between different local regions of an\nimage, and propose Random Crop and Mix (RCM), which can produce positive\nsamples consisting of multi-scale local crops. RCM can also expand the train\nset and increase data diversity without introducing additional data. We conduct\nextensive experiments with CLIC, comparing it with both unsupervised and\nsupervised methods. The results demonstrate that the performance of CLIC is\ncomparable to that of state-of-the-art supervised methods. In addition, we\nestablish the pipelines that can apply CLIC to computer vision tasks to\neffectively improve their performance.\n","authors":["Shipeng Liu","Liang Zhao","Dengfeng Chen","Zhanping Song"],"pdf_url":"https://arxiv.org/pdf/2408.03230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03225v1","updated":"2024-08-06T14:36:43Z","published":"2024-08-06T14:36:43Z","title":"Line-based 6-DoF Object Pose Estimation and Tracking With an Event\n  Camera","summary":"  Pose estimation and tracking of objects is a fundamental application in 3D\nvision. Event cameras possess remarkable attributes such as high dynamic range,\nlow latency, and resilience against motion blur, which enables them to address\nchallenging high dynamic range scenes or high-speed motion. These features make\nevent cameras an ideal complement over standard cameras for object pose\nestimation. In this work, we propose a line-based robust pose estimation and\ntracking method for planar or non-planar objects using an event camera.\nFirstly, we extract object lines directly from events, then provide an initial\npose using a globally-optimal Branch-and-Bound approach, where 2D-3D line\ncorrespondences are not known in advance. Subsequently, we utilize event-line\nmatching to establish correspondences between 2D events and 3D models.\nFurthermore, object poses are refined and continuously tracked by minimizing\nevent-line distances. Events are assigned different weights based on these\ndistances, employing robust estimation algorithms. To evaluate the precision of\nthe proposed methods in object pose estimation and tracking, we have devised\nand established an event-based moving object dataset. Compared against\nstate-of-the-art methods, the robustness and accuracy of our methods have been\nvalidated both on synthetic experiments and the proposed dataset. The source\ncode is available at https://github.com/Zibin6/LOPET.\n","authors":["Zibin Liu","Banglei Guan","Yang Shang","Qifeng Yu","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2408.03225v1.pdf","comment":"Accepted by IEEE Transactions on Image Processing,2024"},{"id":"http://arxiv.org/abs/2408.03219v1","updated":"2024-08-06T14:25:23Z","published":"2024-08-06T14:25:23Z","title":"Learning to Learn without Forgetting using Attention","summary":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","authors":["Anna Vettoruzzo","Joaquin Vanschoren","Mohamed-Rafik Bouguelia","Thorsteinn Rgnvaldsson"],"pdf_url":"https://arxiv.org/pdf/2408.03219v1.pdf","comment":"Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2408.03209v1","updated":"2024-08-06T14:08:22Z","published":"2024-08-06T14:08:22Z","title":"IPAdapter-Instruct: Resolving Ambiguity in Image-based Conditioning\n  using Instruct Prompts","summary":"  Diffusion models continuously push the boundary of state-of-the-art image\ngeneration, but the process is hard to control with any nuance: practice proves\nthat textual prompts are inadequate for accurately describing image style or\nfine structural details (such as faces). ControlNet and IPAdapter address this\nshortcoming by conditioning the generative process on imagery instead, but each\nindividual instance is limited to modeling a single conditional posterior: for\npractical use-cases, where multiple different posteriors are desired within the\nsame workflow, training and using multiple adapters is cumbersome. We propose\nIPAdapter-Instruct, which combines natural-image conditioning with ``Instruct''\nprompts to swap between interpretations for the same conditioning image: style\ntransfer, object extraction, both, or something else still? IPAdapterInstruct\nefficiently learns multiple tasks with minimal loss in quality compared to\ndedicated per-task models.\n","authors":["Ciara Rowles","Shimon Vainer","Dante De Nigris","Slava Elizarov","Konstantin Kutsy","Simon Donn"],"pdf_url":"https://arxiv.org/pdf/2408.03209v1.pdf","comment":"17 pages, 10 figures, Project page:\n  https://unity-research.github.io/IP-Adapter-Instruct.github.io/"},{"id":"http://arxiv.org/abs/2408.03208v1","updated":"2024-08-06T14:06:53Z","published":"2024-08-06T14:06:53Z","title":"Personalizing Federated Instrument Segmentation with Visual Trait Priors\n  in Robotic Surgery","summary":"  Personalized federated learning (PFL) for surgical instrument segmentation\n(SIS) is a promising approach. It enables multiple clinical sites to\ncollaboratively train a series of models in privacy, with each model tailored\nto the individual distribution of each site. Existing PFL methods rarely\nconsider the personalization of multi-headed self-attention, and do not account\nfor appearance diversity and instrument shape similarity, both inherent in\nsurgical scenes. We thus propose PFedSIS, a novel PFL method with visual trait\npriors for SIS, incorporating global-personalized disentanglement (GPD),\nappearance-regulation personalized enhancement (APE), and shape-similarity\nglobal enhancement (SGE), to boost SIS performance in each site. GPD represents\nthe first attempt at head-wise assignment for multi-headed self-attention\npersonalization. To preserve the unique appearance representation of each site\nand gradually leverage the inter-site difference, APE introduces appearance\nregulation and provides customized layer-wise aggregation solutions via\nhypernetworks for each site's personalized parameters. The mutual shape\ninformation of instruments is maintained and shared via SGE, which enhances the\ncross-style shape consistency on the image level and computes the\nshape-similarity contribution of each site on the prediction level for updating\nthe global parameters. PFedSIS outperforms state-of-the-art methods with +1.51%\nDice, +2.11% IoU, -2.79 ASSD, -15.55 HD95 performance gains. The corresponding\ncode and models will be released at https://github.com/wzjialang/PFedSIS.\n","authors":["Jialang Xu","Jiacheng Wang","Lequan Yu","Danail Stoyanov","Yueming Jin","Evangelos B. Mazomenos"],"pdf_url":"https://arxiv.org/pdf/2408.03208v1.pdf","comment":"9 pages, 3 figures, under review"},{"id":"http://arxiv.org/abs/2408.03194v1","updated":"2024-08-06T13:53:45Z","published":"2024-08-06T13:53:45Z","title":"SGSR: Structure-Guided Multi-Contrast MRI Super-Resolution via\n  Spatio-Frequency Co-Query Attention","summary":"  Magnetic Resonance Imaging (MRI) is a leading diagnostic modality for a wide\nrange of exams, where multiple contrast images are often acquired for\ncharacterizing different tissues. However, acquiring high-resolution MRI\ntypically extends scan time, which can introduce motion artifacts.\nSuper-resolution of MRI therefore emerges as a promising approach to mitigate\nthese challenges. Earlier studies have investigated the use of multiple\ncontrasts for MRI super-resolution (MCSR), whereas majority of them did not\nfully exploit the rich contrast-invariant structural information. To fully\nutilize such crucial prior knowledge of multi-contrast MRI, in this work, we\npropose a novel structure-guided MCSR (SGSR) framework based on a new\nspatio-frequency co-query attention (CQA) mechanism. Specifically, CQA performs\nattention on features of multiple contrasts with a shared structural query,\nwhich is particularly designed to extract, fuse, and refine the common\nstructures from different contrasts. We further propose a novel\nfrequency-domain CQA module in addition to the spatial domain, to enable more\nfine-grained structural refinement. Extensive experiments on fastMRI knee data\nand low-field brain MRI show that SGSR outperforms state-of-the-art MCSR\nmethods with statistical significance.\n","authors":["Shaoming Zheng","Yinsong Wang","Siyi Du","Chen Qin"],"pdf_url":"https://arxiv.org/pdf/2408.03194v1.pdf","comment":"The 15th International Workshop on Machine Learning in Medical\n  Imaging (MLMI 2024)"},{"id":"http://arxiv.org/abs/2408.03193v1","updated":"2024-08-06T13:49:01Z","published":"2024-08-06T13:49:01Z","title":"Efficient NeRF Optimization -- Not All Samples Remain Equally Hard","summary":"  We propose an application of online hard sample mining for efficient training\nof Neural Radiance Fields (NeRF). NeRF models produce state-of-the-art quality\nfor many 3D reconstruction and rendering tasks but require substantial\ncomputational resources. The encoding of the scene information within the NeRF\nnetwork parameters necessitates stochastic sampling. We observe that during the\ntraining, a major part of the compute time and memory usage is spent on\nprocessing already learnt samples, which no longer affect the model update\nsignificantly. We identify the backward pass on the stochastic samples as the\ncomputational bottleneck during the optimization. We thus perform the first\nforward pass in inference mode as a relatively low-cost search for hard\nsamples. This is followed by building the computational graph and updating the\nNeRF network parameters using only the hard samples. To demonstrate the\neffectiveness of the proposed approach, we apply our method to Instant-NGP,\nresulting in significant improvements of the view-synthesis quality over the\nbaseline (1 dB improvement on average per training time, or 2x speedup to reach\nthe same PSNR level) along with approx. 40% memory savings coming from using\nonly the hard samples to build the computational graph. As our method only\ninterfaces with the network module, we expect it to be widely applicable.\n","authors":["Juuso Korhonen","Goutham Rangu","Hamed R. Tavakoli","Juho Kannala"],"pdf_url":"https://arxiv.org/pdf/2408.03193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.15890v3","updated":"2024-08-06T13:47:24Z","published":"2023-11-27T14:56:47Z","title":"Stability-Informed Initialization of Neural Ordinary Differential\n  Equations","summary":"  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n","authors":["Theodor Westny","Arman Mohammadi","Daniel Jung","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2311.15890v3.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning"},{"id":"http://arxiv.org/abs/2308.10015v2","updated":"2024-08-06T13:25:29Z","published":"2023-08-19T13:46:49Z","title":"DyFFPAD: Dynamic Fusion of Convolutional and Handcrafted Features for\n  Fingerprint Presentation Attack Detection","summary":"  Automatic fingerprint recognition systems suffer from the threat of\npresentation attacks due to their wide range of deployment in areas including\nnational borders and commercial applications. A presentation attack can be\nperformed by creating a spoof of a user's fingerprint with or without their\nconsent. This paper presents a dynamic ensemble of deep CNN and handcrafted\nfeatures to detect presentation attacks in known-material and unknown-material\nprotocols of the livness detection competition. The proposed presentation\nattack detection model, in this way, utilizes the capabilities of both deep CNN\nand handcrafted features techniques and exhibits better performance than their\nindividual performances. The proposed method is validated using benchmark\ndatabases from the Liveness Detection Competition in 2015, 2017, and 2019,\nyielding overall accuracy of 96.10\\%, 96.49\\%, and 94.99\\% on them,\nrespectively. The proposed method outperforms state-of-the-art methods in terms\nof classification accuracy.\n","authors":["Anuj Rai","Parsheel Kumar Tiwari","Jyotishna Baishya","Ram Prakash Sharma","Somnath Dey"],"pdf_url":"https://arxiv.org/pdf/2308.10015v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2305.09397"},{"id":"http://arxiv.org/abs/2408.03178v1","updated":"2024-08-06T13:22:51Z","published":"2024-08-06T13:22:51Z","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion","summary":"  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n","authors":["Xingguang Yan","Han-Hung Lee","Ziyu Wan","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2408.03178v1.pdf","comment":"Project Page: https://omages.github.io/"},{"id":"http://arxiv.org/abs/2407.15706v5","updated":"2024-08-06T13:20:16Z","published":"2024-07-22T15:16:47Z","title":"Multi-Modality Co-Learning for Efficient Skeleton-based Action\n  Recognition","summary":"  Skeleton-based action recognition has garnered significant attention due to\nthe utilization of concise and resilient skeletons. Nevertheless, the absence\nof detailed body information in skeletons restricts performance, while other\nmultimodal methods require substantial inference resources and are inefficient\nwhen using multimodal data during both training and inference stages. To\naddress this and fully harness the complementary multimodal features, we\npropose a novel multi-modality co-learning (MMCL) framework by leveraging the\nmultimodal large language models (LLMs) as auxiliary networks for efficient\nskeleton-based action recognition, which engages in multi-modality co-learning\nduring the training stage and keeps efficiency by employing only concise\nskeletons in inference. Our MMCL framework primarily consists of two modules.\nFirst, the Feature Alignment Module (FAM) extracts rich RGB features from video\nframes and aligns them with global skeleton features via contrastive learning.\nSecond, the Feature Refinement Module (FRM) uses RGB images with temporal\ninformation and text instruction to generate instructive features based on the\npowerful generalization of multimodal LLMs. These instructive text features\nwill further refine the classification scores and the refined scores will\nenhance the model's robustness and generalization in a manner similar to soft\nlabels. Extensive experiments on NTU RGB+D, NTU RGB+D 120 and Northwestern-UCLA\nbenchmarks consistently verify the effectiveness of our MMCL, which outperforms\nthe existing skeleton-based action recognition methods. Meanwhile, experiments\non UTD-MHAD and SYSU-Action datasets demonstrate the commendable generalization\nof our MMCL in zero-shot and domain-adaptive action recognition. Our code is\npublicly available at: https://github.com/liujf69/MMCL-Action.\n","authors":["Jinfu Liu","Chen Chen","Mengyuan Liu"],"pdf_url":"https://arxiv.org/pdf/2407.15706v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.00956v3","updated":"2024-08-06T13:07:37Z","published":"2024-05-02T02:34:19Z","title":"SimEndoGS: Efficient Data-driven Scene Simulation using Robotic Surgery\n  Videos via Physics-embedded 3D Gaussians","summary":"  Surgical scene simulation plays a crucial role in surgical education and\nsimulator-based robot learning. Traditional approaches for creating these\nenvironments with surgical scene involve a labor-intensive process where\ndesigners hand-craft tissues models with textures and geometries for soft body\nsimulations. This manual approach is not only time-consuming but also limited\nin the scalability and realism. In contrast, data-driven simulation offers a\ncompelling alternative. It has the potential to automatically reconstruct 3D\nsurgical scenes from real-world surgical video data, followed by the\napplication of soft body physics. This area, however, is relatively uncharted.\nIn our research, we introduce 3D Gaussian as a learnable representation for\nsurgical scene, which is learned from stereo endoscopic video. To prevent\nover-fitting and ensure the geometrical correctness of these scenes, we\nincorporate depth supervision and anisotropy regularization into the Gaussian\nlearning process. Furthermore, we apply the Material Point Method, which is\nintegrated with physical properties, to the 3D Gaussians to achieve realistic\nscene deformations. Our method was evaluated on our collected in-house and\npublic surgical videos datasets. Results show that it can reconstruct and\nsimulate surgical scenes from endoscopic videos efficiently-taking only a few\nminutes to reconstruct the surgical scene-and produce both visually and\nphysically plausible deformations at a speed approaching real-time. The results\ndemonstrate great potential of our proposed method to enhance the efficiency\nand variety of simulations available for surgical education and robot learning.\n","authors":["Zhenya Yang","Kai Chen","Yonghao Long","Qi Dou"],"pdf_url":"https://arxiv.org/pdf/2405.00956v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12705v2","updated":"2024-08-06T13:06:26Z","published":"2024-07-17T16:26:30Z","title":"IMAGDressing-v1: Customizable Virtual Dressing","summary":"  Latest advances have achieved realistic virtual try-on (VTON) through\nlocalized garment inpainting using latent diffusion models, significantly\nenhancing consumers' online shopping experience. However, existing VTON\ntechnologies neglect the need for merchants to showcase garments\ncomprehensively, including flexible control over garments, optional faces,\nposes, and scenes. To address this issue, we define a virtual dressing (VD)\ntask focused on generating freely editable human images with fixed garments and\noptional conditions. Meanwhile, we design a comprehensive affinity metric index\n(CAMI) to evaluate the consistency between generated images and reference\ngarments. Then, we propose IMAGDressing-v1, which incorporates a garment UNet\nthat captures semantic features from CLIP and texture features from VAE. We\npresent a hybrid attention module, including a frozen self-attention and a\ntrainable cross-attention, to integrate garment features from the garment UNet\ninto a frozen denoising UNet, ensuring users can control different scenes\nthrough text. IMAGDressing-v1 can be combined with other extension plugins,\nsuch as ControlNet and IP-Adapter, to enhance the diversity and controllability\nof generated images. Furthermore, to address the lack of data, we release the\ninteractive garment pairing (IGPair) dataset, containing over 300,000 pairs of\nclothing and dressed images, and establish a standard pipeline for data\nassembly. Extensive experiments demonstrate that our IMAGDressing-v1 achieves\nstate-of-the-art human image synthesis performance under various controlled\nconditions. The code and model will be available at\nhttps://github.com/muzishen/IMAGDressing.\n","authors":["Fei Shen","Xin Jiang","Xin He","Hu Ye","Cong Wang","Xiaoyu Du","Zechao Li","Jinhui Tang"],"pdf_url":"https://arxiv.org/pdf/2407.12705v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03164v1","updated":"2024-08-06T13:05:32Z","published":"2024-08-06T13:05:32Z","title":"Dilated Convolution with Learnable Spacings makes visual models more\n  aligned with humans: a Grad-CAM study","summary":"  Dilated Convolution with Learnable Spacing (DCLS) is a recent advanced\nconvolution method that allows enlarging the receptive fields (RF) without\nincreasing the number of parameters, like the dilated convolution, yet without\nimposing a regular grid. DCLS has been shown to outperform the standard and\ndilated convolutions on several computer vision benchmarks. Here, we show that,\nin addition, DCLS increases the models' interpretability, defined as the\nalignment with human visual strategies. To quantify it, we use the Spearman\ncorrelation between the models' GradCAM heatmaps and the ClickMe dataset\nheatmaps, which reflect human visual attention. We took eight reference models\n- ResNet50, ConvNeXt (T, S and B), CAFormer, ConvFormer, and FastViT (sa 24 and\n36) - and drop-in replaced the standard convolution layers with DCLS ones. This\nimproved the interpretability score in seven of them. Moreover, we observed\nthat Grad-CAM generated random heatmaps for two models in our study: CAFormer\nand ConvFormer models, leading to low interpretability scores. We addressed\nthis issue by introducing Threshold-Grad-CAM, a modification built on top of\nGrad-CAM that enhanced interpretability across nearly all models. The code and\ncheckpoints to reproduce this study are available at:\nhttps://github.com/rabihchamas/DCLS-GradCAM-Eval.\n","authors":["Rabih Chamas","Ismail Khalfaoui-Hassani","Timothee Masquelier"],"pdf_url":"https://arxiv.org/pdf/2408.03164v1.pdf","comment":"Accepted at The Trustworthy AI Workshop, IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.03156v1","updated":"2024-08-06T12:55:17Z","published":"2024-08-06T12:55:17Z","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models","summary":"  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n","authors":["Sho Ozaki","Shizuo Kaji","Toshikazu Imae","Kanabu Nawa","Hideomi Yamashita","Keiichi Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2408.03156v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2407.15488v3","updated":"2024-08-06T12:54:41Z","published":"2024-07-22T09:05:16Z","title":"DiffX: Guide Your Layout to Cross-Modal Generative Modeling","summary":"  Diffusion models have made significant strides in language-driven and\nlayout-driven image generation. However, most diffusion models are limited to\nvisible RGB image generation. In fact, human perception of the world is\nenriched by diverse viewpoints, such as chromatic contrast, thermal\nillumination, and depth information. In this paper, we introduce a novel\ndiffusion model for general layout-guided cross-modal generation, called DiffX.\nNotably, DiffX presents a simple yet effective cross-modal generative modeling\npipeline, which conducts diffusion and denoising processes in the\nmodality-shared latent space. Moreover, we introduce the Joint-Modality\nEmbedder (JME) to enhance interaction between layout and text conditions by\nincorporating a gated attention mechanism. Meanwhile, the advanced Long-CLIP is\nemployed for long caption embedding for user instruction. To facilitate the\nuser-instructed generative training, we construct the cross-modal image\ndatasets with detailed text captions assisted by the Large-Multimodal Model\n(LMM). Through extensive experiments, DiffX demonstrates robustness in\ncross-modal generation across three ``RGB+X'' datasets: FLIR, MFNet, and\nCOME15K, guided by various layout conditions. It also shows the potential for\nthe adaptive generation of ``RGB+X+Y+Z'' images or more diverse modalities on\nCOME15K and MCXFace datasets. Our code and constructed cross-modal image\ndatasets are available at https://github.com/zeyuwang-zju/DiffX.\n","authors":["Zeyu Wang","Jingyu Lin","Yifei Qian","Yi Huang","Shicen Tian","Bosong Chai","Juncan Deng","Lan Du","Cunjian Chen","Yufei Guo","Kejie Huang"],"pdf_url":"https://arxiv.org/pdf/2407.15488v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01661v3","updated":"2024-08-06T12:54:26Z","published":"2024-05-02T18:31:47Z","title":"When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX","summary":"  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n","authors":["Bettina Finzel","Patrick Hilme","Johannes Rabold","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2405.01661v3.pdf","comment":"preliminary version, submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2408.03149v1","updated":"2024-08-06T12:45:56Z","published":"2024-08-06T12:45:56Z","title":"Leveraging Entity Information for Cross-Modality Correlation Learning:\n  The Entity-Guided Multimodal Summarization","summary":"  The rapid increase in multimedia data has spurred advancements in Multimodal\nSummarization with Multimodal Output (MSMO), which aims to produce a multimodal\nsummary that integrates both text and relevant images. The inherent\nheterogeneity of content within multimodal inputs and outputs presents a\nsignificant challenge to the execution of MSMO. Traditional approaches\ntypically adopt a holistic perspective on coarse image-text data or individual\nvisual objects, overlooking the essential connections between objects and the\nentities they represent. To integrate the fine-grained entity knowledge, we\npropose an Entity-Guided Multimodal Summarization model (EGMS). Our model,\nbuilding on BART, utilizes dual multimodal encoders with shared weights to\nprocess text-image and entity-image information concurrently. A gating\nmechanism then combines visual data for enhanced textual summary generation,\nwhile image selection is refined through knowledge distillation from a\npre-trained vision-language model. Extensive experiments on public MSMO dataset\nvalidate the superiority of the EGMS method, which also prove the necessity to\nincorporate entity information into MSMO problem.\n","authors":["Yanghai Zhang","Ye Liu","Shiwei Wu","Kai Zhang","Xukai Liu","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03149v1.pdf","comment":"In ACL-Findings 2024"},{"id":"http://arxiv.org/abs/2408.03143v1","updated":"2024-08-06T12:37:47Z","published":"2024-08-06T12:37:47Z","title":"SuperSimpleNet: Unifying Unsupervised and Supervised Learning for Fast\n  and Reliable Surface Defect Detection","summary":"  The aim of surface defect detection is to identify and localise abnormal\nregions on the surfaces of captured objects, a task that's increasingly\ndemanded across various industries. Current approaches frequently fail to\nfulfil the extensive demands of these industries, which encompass high\nperformance, consistency, and fast operation, along with the capacity to\nleverage the entirety of the available training data. Addressing these gaps, we\nintroduce SuperSimpleNet, an innovative discriminative model that evolved from\nSimpleNet. This advanced model significantly enhances its predecessor's\ntraining consistency, inference time, as well as detection performance.\nSuperSimpleNet operates in an unsupervised manner using only normal training\nimages but also benefits from labelled abnormal training images when they are\navailable. SuperSimpleNet achieves state-of-the-art results in both the\nsupervised and the unsupervised settings, as demonstrated by experiments across\nfour challenging benchmark datasets. Code:\nhttps://github.com/blaz-r/SuperSimpleNet .\n","authors":["Bla Rolih","Matic Fuka","Danijel Skoaj"],"pdf_url":"https://arxiv.org/pdf/2408.03143v1.pdf","comment":"Accepted to ICPR 2024"},{"id":"http://arxiv.org/abs/2402.17485v2","updated":"2024-08-06T12:33:30Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v2","updated":"2024-08-06T12:25:48Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v2.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2408.00998v2","updated":"2024-08-06T12:01:17Z","published":"2024-08-02T04:13:38Z","title":"FBSDiff: Plug-and-Play Frequency Band Substitution of Diffusion Features\n  for Highly Controllable Text-Driven Image Translation","summary":"  Large-scale text-to-image diffusion models have been a revolutionary\nmilestone in the evolution of generative AI and multimodal technology, allowing\nwonderful image generation with natural-language text prompt. However, the\nissue of lacking controllability of such models restricts their practical\napplicability for real-life content creation. Thus, attention has been focused\non leveraging a reference image to control text-to-image synthesis, which is\nalso regarded as manipulating (or editing) a reference image as per a text\nprompt, namely, text-driven image-to-image translation. This paper contributes\na novel, concise, and efficient approach that adapts pre-trained large-scale\ntext-to-image (T2I) diffusion model to the image-to-image (I2I) paradigm in a\nplug-and-play manner, realizing high-quality and versatile text-driven I2I\ntranslation without any model training, model fine-tuning, or online\noptimization process. To guide T2I generation with a reference image, we\npropose to decompose diverse guiding factors with different frequency bands of\ndiffusion features in the DCT spectral space, and accordingly devise a novel\nfrequency band substitution layer which realizes dynamic control of the\nreference image to the T2I generation result in a plug-and-play manner. We\ndemonstrate that our method allows flexible control over both guiding factor\nand guiding intensity of the reference image simply by tuning the type and\nbandwidth of the substituted frequency band, respectively. Extensive\nqualitative and quantitative experiments verify superiority of our approach\nover related methods in I2I translation visual quality, versatility, and\ncontrollability. The code is publicly available at:\nhttps://github.com/XiangGao1102/FBSDiff.\n","authors":["Xiang Gao","Jiaying Liu"],"pdf_url":"https://arxiv.org/pdf/2408.00998v2.pdf","comment":"Accepted conference paper of ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.03120v1","updated":"2024-08-06T11:49:13Z","published":"2024-08-06T11:49:13Z","title":"Benchmarking In-the-wild Multimodal Disease Recognition and A Versatile\n  Baseline","summary":"  Existing plant disease classification models have achieved remarkable\nperformance in recognizing in-laboratory diseased images. However, their\nperformance often significantly degrades in classifying in-the-wild images.\nFurthermore, we observed that in-the-wild plant images may exhibit similar\nappearances across various diseases (i.e., small inter-class discrepancy) while\nthe same diseases may look quite different (i.e., large intra-class variance).\nMotivated by this observation, we propose an in-the-wild multimodal plant\ndisease recognition dataset that contains the largest number of disease classes\nbut also text-based descriptions for each disease. Particularly, the newly\nprovided text descriptions are introduced to provide rich information in\ntextual modality and facilitate in-the-wild disease classification with small\ninter-class discrepancy and large intra-class variance issues. Therefore, our\nproposed dataset can be regarded as an ideal testbed for evaluating disease\nrecognition methods in the real world. In addition, we further present a strong\nyet versatile baseline that models text descriptions and visual data through\nmultiple prototypes for a given class. By fusing the contributions of\nmultimodal prototypes in classification, our baseline can effectively address\nthe small inter-class discrepancy and large intra-class variance issues.\nRemarkably, our baseline model can not only classify diseases but also\nrecognize diseases in few-shot or training-free scenarios. Extensive\nbenchmarking results demonstrate that our proposed in-the-wild multimodal\ndataset sets many new challenges to the plant disease recognition task and\nthere is a large space to improve for future works.\n","authors":["Tianqi Wei","Zhi Chen","Zi Huang","Xin Yu"],"pdf_url":"https://arxiv.org/pdf/2408.03120v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02209v2","updated":"2024-08-06T11:46:39Z","published":"2024-08-05T03:18:58Z","title":"Source-Free Domain-Invariant Performance Prediction","summary":"  Accurately estimating model performance poses a significant challenge,\nparticularly in scenarios where the source and target domains follow different\ndata distributions. Most existing performance prediction methods heavily rely\non the source data in their estimation process, limiting their applicability in\na more realistic setting where only the trained model is accessible. The few\nmethods that do not require source data exhibit considerably inferior\nperformance. In this work, we propose a source-free approach centred on\nuncertainty-based estimation, using a generative model for calibration in the\nabsence of source data. We establish connections between our approach for\nunsupervised calibration and temperature scaling. We then employ a\ngradient-based strategy to evaluate the correctness of the calibrated\npredictions. Our experiments on benchmark object recognition datasets reveal\nthat existing source-based methods fall short with limited source sample\navailability. Furthermore, our approach significantly outperforms the current\nstate-of-the-art source-free and source-based methods, affirming its\neffectiveness in domain-invariant performance estimation.\n","authors":["Ekaterina Khramtsova","Mahsa Baktashmotlagh","Guido Zuccon","Xi Wang","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.02209v2.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2311.02558v3","updated":"2024-08-06T11:35:04Z","published":"2023-11-05T03:53:42Z","title":"Multi-Agent 3D Map Reconstruction and Change Detection in Microgravity\n  with Free-Flying Robots","summary":"  Assistive free-flyer robots autonomously caring for future crewed outposts --\nsuch as NASA's Astrobee robots on the International Space Station (ISS) -- must\nbe able to detect day-to-day interior changes to track inventory, detect and\ndiagnose faults, and monitor the outpost status. This work presents a framework\nfor multi-agent cooperative mapping and change detection to enable robotic\nmaintenance of space outposts. One agent is used to reconstruct a 3D model of\nthe environment from sequences of images and corresponding depth information.\nAnother agent is used to periodically scan the environment for inconsistencies\nagainst the 3D model. Change detection is validated after completing the\nsurveys using real image and pose data collected by Astrobee robots in a ground\ntesting environment and from microgravity aboard the ISS. This work outlines\nthe objectives, requirements, and algorithmic modules for the multi-agent\nreconstruction system, including recommendations for its use by assistive\nfree-flyers aboard future microgravity outposts.\n  *Denotes Equal Contribution\n","authors":["Holly Dinkel","Julia Di","Jamie Santos","Keenan Albee","Paulo Borges","Marina Moreira","Oleg Alexandrov","Brian Coltin","Trey Smith"],"pdf_url":"https://arxiv.org/pdf/2311.02558v3.pdf","comment":"11 pages, 8 figures, Manuscript presented at the 74th International\n  Astronautical Congress, IAC 2023, Baku, Azerbaijan, 2 - 6 October 2023. Video\n  presentation: [https://www.youtube.com/watch?v=VfjV-zwFEtU]. Code:\n  [https://github.com/hollydinkel/astrobeecd]"},{"id":"http://arxiv.org/abs/2408.03097v1","updated":"2024-08-06T10:56:53Z","published":"2024-08-06T10:56:53Z","title":"Prototype Learning for Micro-gesture Classification","summary":"  In this paper, we briefly introduce the solution developed by our team,\nHFUT-VUT, for the track of Micro-gesture Classification in the MiGA challenge\nat IJCAI 2024. The task of micro-gesture classification task involves\nrecognizing the category of a given video clip, which focuses on more\nfine-grained and subtle body movements compared to typical action recognition\ntasks. Given the inherent complexity of micro-gesture recognition, which\nincludes large intra-class variability and minimal inter-class differences, we\nutilize two innovative modules, i.e., the cross-modal fusion module and\nprototypical refinement module, to improve the discriminative ability of MG\nfeatures, thereby improving the classification accuracy. Our solution achieved\nsignificant success, ranking 1st in the track of Micro-gesture Classification.\nWe surpassed the performance of last year's leading team by a substantial\nmargin, improving Top-1 accuracy by 6.13%.\n","authors":["Guoliang Chen","Fei Wang","Kun Li","Zhiliang Wu","Hehe Fan","Yi Yang","Meng Wang","Dan Guo"],"pdf_url":"https://arxiv.org/pdf/2408.03097v1.pdf","comment":"1st Place in Micro-gesture Classification in MiGA at IJCAI-2024"},{"id":"http://arxiv.org/abs/2408.03078v1","updated":"2024-08-06T10:13:57Z","published":"2024-08-06T10:13:57Z","title":"BodySLAM: A Generalized Monocular Visual SLAM Framework for Surgical\n  Applications","summary":"  Endoscopic surgery relies on two-dimensional views, posing challenges for\nsurgeons in depth perception and instrument manipulation. While Simultaneous\nLocalization and Mapping (SLAM) has emerged as a promising solution to address\nthese limitations, its implementation in endoscopic procedures presents\nsignificant challenges due to hardware limitations, such as the use of a\nmonocular camera and the absence of odometry sensors. This study presents a\nrobust deep learning-based SLAM approach that combines state-of-the-art and\nnewly developed models. It consists of three main parts: the Monocular Pose\nEstimation Module that introduces a novel unsupervised method based on the\nCycleGAN architecture, the Monocular Depth Estimation Module that leverages the\nnovel Zoe architecture, and the 3D Reconstruction Module which uses information\nfrom the previous models to create a coherent surgical map. The performance of\nthe procedure was rigorously evaluated using three publicly available datasets\n(Hamlyn, EndoSLAM, and SCARED) and benchmarked against two state-of-the-art\nmethods, EndoSFMLearner and EndoDepth. The integration of Zoe in the MDEM\ndemonstrated superior performance compared to state-of-the-art depth estimation\nalgorithms in endoscopy, whereas the novel approach in the MPEM exhibited\ncompetitive performance and the lowest inference time. The results showcase the\nrobustness of our approach in laparoscopy, gastroscopy, and colonoscopy, three\ndifferent scenarios in endoscopic surgery. The proposed SLAM approach has the\npotential to improve the accuracy and efficiency of endoscopic procedures by\nproviding surgeons with enhanced depth perception and 3D reconstruction\ncapabilities.\n","authors":["G. Manni","C. Lauretti","F. Prata","R. Papalia","L. Zollo","P. Soda"],"pdf_url":"https://arxiv.org/pdf/2408.03078v1.pdf","comment":"15 pages, 7 figures"},{"id":"http://arxiv.org/abs/2406.02265v3","updated":"2024-08-06T10:10:58Z","published":"2024-06-04T12:41:54Z","title":"Understanding Retrieval Robustness for Retrieval-Augmented Image\n  Captioning","summary":"  Recent advances in retrieval-augmented models for image captioning highlight\nthe benefit of retrieving related captions for efficient, lightweight models\nwith strong domain-transfer capabilities. While these models demonstrate the\nsuccess of retrieval augmentation, retrieval models are still far from perfect\nin practice: the retrieved information can sometimes mislead the model,\nresulting in incorrect generation and worse performance. In this paper, we\nanalyze the robustness of a retrieval-augmented captioning model SmallCap. Our\nanalysis shows that the model is sensitive to tokens that appear in the\nmajority of the retrieved captions, and the input attribution shows that those\ntokens are likely copied into the generated output. Given these findings, we\npropose to train the model by sampling retrieved captions from more diverse\nsets. This decreases the chance that the model learns to copy majority tokens,\nand improves both in-domain and cross-domain performance.\n","authors":["Wenyan Li","Jiaang Li","Rita Ramos","Raphael Tang","Desmond Elliott"],"pdf_url":"https://arxiv.org/pdf/2406.02265v3.pdf","comment":"9 pages, long paper at ACL 2024"},{"id":"http://arxiv.org/abs/2407.14086v2","updated":"2024-08-06T09:56:36Z","published":"2024-07-19T07:48:45Z","title":"Temporal Correlation Meets Embedding: Towards a 2nd Generation of\n  JDE-based Real-Time Multi-Object Tracking","summary":"  Joint Detection and Embedding (JDE) trackers have demonstrated excellent\nperformance in Multi-Object Tracking (MOT) tasks by incorporating the\nextraction of appearance features as auxiliary tasks through embedding\nRe-Identification task (ReID) into the detector, achieving a balance between\ninference speed and tracking performance. However, solving the competition\nbetween the detector and the feature extractor has always been a challenge.\nMeanwhile, the issue of directly embedding the ReID task into MOT has remained\nunresolved. The lack of high discriminability in appearance features results in\ntheir limited utility. In this paper, a new learning approach using\ncross-correlation to capture temporal information of objects is proposed. The\nfeature extraction network is no longer trained solely on appearance features\nfrom each frame but learns richer motion features by utilizing feature heatmaps\nfrom consecutive frames, which addresses the challenge of inter-class feature\nsimilarity. Furthermore, our learning approach is applied to a more lightweight\nfeature extraction network, and treat the feature matching scores as strong\ncues rather than auxiliary cues, with an appropriate weight calculation to\nreflect the compatibility between our obtained features and the MOT task. Our\ntracker, named TCBTrack, achieves state-of-the-art performance on multiple\npublic benchmarks, i.e., MOT17, MOT20, and DanceTrack datasets. Specifically,\non the DanceTrack test set, we achieve 56.8 HOTA, 58.1 IDF1 and 92.5 MOTA,\nmaking it the best online tracker capable of achieving real-time performance.\nComparative evaluations with other trackers prove that our tracker achieves the\nbest balance between speed, robustness and accuracy. Code is available at\nhttps://github.com/yfzhang1214/TCBTrack.\n","authors":["Yunfei Zhang","Chao Liang","Jin Gao","Zhipeng Zhang","Weiming Hu","Stephen Maybank","Xue Zhou","Liang Li"],"pdf_url":"https://arxiv.org/pdf/2407.14086v2.pdf","comment":"A submission to IJCV"},{"id":"http://arxiv.org/abs/2408.03065v1","updated":"2024-08-06T09:35:50Z","published":"2024-08-06T09:35:50Z","title":"SCOPE: A Synthetic Multi-Modal Dataset for Collective Perception\n  Including Physical-Correct Weather Conditions","summary":"  Collective perception has received considerable attention as a promising\napproach to overcome occlusions and limited sensing ranges of vehicle-local\nperception in autonomous driving. In order to develop and test novel collective\nperception technologies, appropriate datasets are required. These datasets must\ninclude not only different environmental conditions, as they strongly influence\nthe perception capabilities, but also a wide range of scenarios with different\nroad users as well as realistic sensor models. Therefore, we propose the\nSynthetic COllective PErception (SCOPE) dataset. SCOPE is the first synthetic\nmulti-modal dataset that incorporates realistic camera and LiDAR models as well\nas parameterized and physically accurate weather simulations for both sensor\ntypes. The dataset contains 17,600 frames from over 40 diverse scenarios with\nup to 24 collaborative agents, infrastructure sensors, and passive traffic,\nincluding cyclists and pedestrians. In addition, recordings from two novel\ndigital-twin maps from Karlsruhe and T\\\"ubingen are included. The dataset is\navailable at https://ekut-es.github.io/scope\n","authors":["Jrg Gamerdinger","Sven Teufel","Patrick Schulz","Stephan Amann","Jan-Patrick Kirchner","Oliver Bringmann"],"pdf_url":"https://arxiv.org/pdf/2408.03065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03060v1","updated":"2024-08-06T09:23:24Z","published":"2024-08-06T09:23:24Z","title":"MGFs: Masked Gaussian Fields for Meshing Building based on Multi-View\n  Images","summary":"  Over the last few decades, image-based building surface reconstruction has\ngarnered substantial research interest and has been applied across various\nfields, such as heritage preservation, architectural planning, etc. Compared to\nthe traditional photogrammetric and NeRF-based solutions, recently, Gaussian\nfields-based methods have exhibited significant potential in generating surface\nmeshes due to their time-efficient training and detailed 3D information\npreservation. However, most gaussian fields-based methods are trained with all\nimage pixels, encompassing building and nonbuilding areas, which results in a\nsignificant noise for building meshes and degeneration in time efficiency. This\npaper proposes a novel framework, Masked Gaussian Fields (MGFs), designed to\ngenerate accurate surface reconstruction for building in a time-efficient way.\nThe framework first applies EfficientSAM and COLMAP to generate multi-level\nmasks of building and the corresponding masked point clouds. Subsequently, the\nmasked gaussian fields are trained by integrating two innovative losses: a\nmulti-level perceptual masked loss focused on constructing building regions and\na boundary loss aimed at enhancing the details of the boundaries between\ndifferent masks. Finally, we improve the tetrahedral surface mesh extraction\nmethod based on the masked gaussian spheres. Comprehensive experiments on UAV\nimages demonstrate that, compared to the traditional method and several\nNeRF-based and Gaussian-based SOTA solutions, our approach significantly\nimproves both the accuracy and efficiency of building surface reconstruction.\nNotably, as a byproduct, there is an additional gain in the novel view\nsynthesis of building.\n","authors":["Tengfei Wang","Zongqian Zhan","Rui Xia","Linxia Ji","Xin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03060v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03046v1","updated":"2024-08-06T09:02:31Z","published":"2024-08-06T09:02:31Z","title":"Comb, Prune, Distill: Towards Unified Pruning for Vision Model\n  Compression","summary":"  Lightweight and effective models are essential for devices with limited\nresources, such as intelligent vehicles. Structured pruning offers a promising\napproach to model compression and efficiency enhancement. However, existing\nmethods often tie pruning techniques to specific model architectures or vision\ntasks. To address this limitation, we propose a novel unified pruning framework\nComb, Prune, Distill (CPD), which addresses both model-agnostic and\ntask-agnostic concerns simultaneously. Our framework employs a combing step to\nresolve hierarchical layer-wise dependency issues, enabling architecture\nindependence. Additionally, the pruning pipeline adaptively remove parameters\nbased on the importance scoring metrics regardless of vision tasks. To support\nthe model in retaining its learned information, we introduce knowledge\ndistillation during the pruning step. Extensive experiments demonstrate the\ngeneralizability of our framework, encompassing both convolutional neural\nnetwork (CNN) and transformer models, as well as image classification and\nsegmentation tasks. In image classification we achieve a speedup of up to x4.3\nwith a accuracy loss of 1.8% and in semantic segmentation up to x1.89 with a\n5.1% loss in mIoU.\n","authors":["Jonas Schmitt","Ruiping Liu","Junwei Zheng","Jiaming Zhang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2408.03046v1.pdf","comment":"Accepted by ITSC 2024. Code is publicly available at:\n  https://github.com/Cranken/CPD"},{"id":"http://arxiv.org/abs/2408.03043v1","updated":"2024-08-06T08:58:20Z","published":"2024-08-06T08:58:20Z","title":"Targeted Visual Prompting for Medical Visual Question Answering","summary":"  With growing interest in recent years, medical visual question answering\n(Med-VQA) has rapidly evolved, with multimodal large language models (MLLMs)\nemerging as an alternative to classical model architectures. Specifically,\ntheir ability to add visual information to the input of pre-trained LLMs brings\nnew capabilities for image interpretation. However, simple visual errors cast\ndoubt on the actual visual understanding abilities of these models. To address\nthis, region-based questions have been proposed as a means to assess and\nenhance actual visual understanding through compositional evaluation. To\ncombine these two perspectives, this paper introduces targeted visual prompting\nto equip MLLMs with region-based questioning capabilities. By presenting the\nmodel with both the isolated region and the region in its context in a\ncustomized visual prompt, we show the effectiveness of our method across\nmultiple datasets while comparing it to several baseline models. Our code and\ndata are available at https://github.com/sergiotasconmorales/locvqallm.\n","authors":["Sergio Tascon-Morales","Pablo Mrquez-Neila","Raphael Sznitman"],"pdf_url":"https://arxiv.org/pdf/2408.03043v1.pdf","comment":"Accepted at the MICCAI AMAI Workshop 2024"},{"id":"http://arxiv.org/abs/2407.20171v2","updated":"2024-08-06T08:42:47Z","published":"2024-07-29T17:00:09Z","title":"Diffusion Feedback Helps CLIP See Better","summary":"  Contrastive Language-Image Pre-training (CLIP), which excels at abstracting\nopen-world representations across domains and modalities, has become a\nfoundation for a variety of vision and multimodal tasks. However, recent\nstudies reveal that CLIP has severe visual shortcomings, such as which can\nhardly distinguish orientation, quantity, color, structure, etc. These visual\nshortcomings also limit the perception capabilities of multimodal large\nlanguage models (MLLMs) built on CLIP. The main reason could be that the\nimage-text pairs used to train CLIP are inherently biased, due to the lack of\nthe distinctiveness of the text and the diversity of images. In this work, we\npresent a simple post-training approach for CLIP models, which largely\novercomes its visual shortcomings via a self-supervised diffusion process. We\nintroduce DIVA, which uses the DIffusion model as a Visual Assistant for CLIP.\nSpecifically, DIVA leverages generative feedback from text-to-image diffusion\nmodels to optimize CLIP representations, with only images (without\ncorresponding text). We demonstrate that DIVA improves CLIP's performance on\nthe challenging MMVP-VLM benchmark which assesses fine-grained visual abilities\nto a large extent (e.g., 3-7%), and enhances the performance of MLLMs and\nvision models on multimodal understanding and segmentation tasks. Extensive\nevaluation on 29 image classification and retrieval benchmarks confirms that\nour framework preserves CLIP's strong zero-shot capabilities. The code is\navailable at https://github.com/baaivision/DIVA.\n","authors":["Wenxuan Wang","Quan Sun","Fan Zhang","Yepeng Tang","Jing Liu","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2407.20171v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03035v1","updated":"2024-08-06T08:31:34Z","published":"2024-08-06T08:31:34Z","title":"Training-Free Condition Video Diffusion Models for single frame\n  Spatial-Semantic Echocardiogram Synthesis","summary":"  Conditional video diffusion models (CDM) have shown promising results for\nvideo synthesis, potentially enabling the generation of realistic\nechocardiograms to address the problem of data scarcity. However, current CDMs\nrequire a paired segmentation map and echocardiogram dataset. We present a new\nmethod called Free-Echo for generating realistic echocardiograms from a single\nend-diastolic segmentation map without additional training data. Our method is\nbased on the 3D-Unet with Temporal Attention Layers model and is conditioned on\nthe segmentation map using a training-free conditioning method based on SDEdit.\nWe evaluate our model on two public echocardiogram datasets, CAMUS and\nEchoNet-Dynamic. We show that our model can generate plausible echocardiograms\nthat are spatially aligned with the input segmentation map, achieving\nperformance comparable to training-based CDMs. Our work opens up new\npossibilities for generating echocardiograms from a single segmentation map,\nwhich can be used for data augmentation, domain adaptation, and other\napplications in medical imaging. Our code is available at\n\\url{https://github.com/gungui98/echo-free}\n","authors":["Van Phi Nguyen","Tri Nhan Luong Ha","Huy Hieu Pham","Quoc Long Tran"],"pdf_url":"https://arxiv.org/pdf/2408.03035v1.pdf","comment":"Accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2406.09913v3","updated":"2024-08-06T08:27:55Z","published":"2024-06-14T10:47:52Z","title":"OpenECAD: An Efficient Visual Language Model for Editable 3D-CAD Design","summary":"  Computer-aided design (CAD) tools are utilized in the manufacturing industry\nfor modeling everything from cups to spacecraft. These programs are complex to\nuse and typically require years of training and experience to master.\nStructured and well-constrained 2D sketches and 3D constructions are crucial\ncomponents of CAD modeling. A well-executed CAD model can be seamlessly\nintegrated into the manufacturing process, thereby enhancing production\nefficiency. Deep generative models of 3D shapes and 3D object reconstruction\nmodels have garnered significant research interest. However, most of these\nmodels produce discrete forms of 3D objects that are not editable. Moreover,\nthe few models based on CAD operations often have substantial input\nrestrictions. In this work, we fine-tuned pre-trained models to create OpenECAD\nmodels (0.55B, 0.89B, 2.4B and 3.1B), leveraging the visual, logical, coding,\nand general capabilities of visual language models. OpenECAD models can process\nimages of 3D designs as input and generate highly structured 2D sketches and 3D\nconstruction commands, ensuring that the designs are editable. These outputs\ncan be directly used with existing CAD tools' APIs to generate project files.\nTo train our network, we created a series of OpenECAD datasets. These datasets\nare derived from existing public CAD datasets, adjusted and augmented to meet\nthe specific requirements of vision language model (VLM) training.\nAdditionally, we have introduced an approach that utilizes dependency\nrelationships to define and generate sketches, further enriching the content\nand functionality of the datasets.\n","authors":["Zhe Yuan","Jianqi Shi","Yanhong Huang"],"pdf_url":"https://arxiv.org/pdf/2406.09913v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03030v1","updated":"2024-08-06T08:24:47Z","published":"2024-08-06T08:24:47Z","title":"Nighttime Pedestrian Detection Based on Fore-Background Contrast\n  Learning","summary":"  The significance of background information is frequently overlooked in\ncontemporary research concerning channel attention mechanisms. This study\naddresses the issue of suboptimal single-spectral nighttime pedestrian\ndetection performance under low-light conditions by incorporating background\ninformation into the channel attention mechanism. Despite numerous studies\nfocusing on the development of efficient channel attention mechanisms, the\nrelevance of background information has been largely disregarded. By adopting a\ncontrast learning approach, we reexamine channel attention with regard to\npedestrian objects and background information for nighttime pedestrian\ndetection, resulting in the proposed Fore-Background Contrast Attention (FBCA).\nFBCA possesses two primary attributes: (1) channel descriptors form remote\ndependencies with global spatial feature information; (2) the integration of\nbackground information enhances the distinction between channels concentrating\non low-light pedestrian features and those focusing on background information.\nConsequently, the acquired channel descriptors exhibit a higher semantic level\nand spatial accuracy. Experimental outcomes demonstrate that FBCA significantly\noutperforms existing methods in single-spectral nighttime pedestrian detection,\nachieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian\ndatasets. Furthermore, this methodology also yields performance improvements\nfor the multispectral LLVIP dataset. These findings indicate that integrating\nbackground information into the channel attention mechanism effectively\nmitigates detector performance degradation caused by illumination factors in\nnighttime scenarios.\n","authors":["He Yao","Yongjun Zhang","Huachun Jian","Li Zhang","Ruzhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.03030v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03014v1","updated":"2024-08-06T07:51:20Z","published":"2024-08-06T07:51:20Z","title":"CKNN: Cleansed k-Nearest Neighbor for Unsupervised Video Anomaly\n  Detection","summary":"  In this paper, we address the problem of unsupervised video anomaly detection\n(UVAD). The task aims to detect abnormal events in test video using unlabeled\nvideos as training data. The presence of anomalies in the training data poses a\nsignificant challenge in this task, particularly because they form clusters in\nthe feature space. We refer to this property as the \"Anomaly Cluster\" issue.\nThe condensed nature of these anomalies makes it difficult to distinguish\nbetween normal and abnormal data in the training set. Consequently, training\nconventional anomaly detection techniques using an unlabeled dataset often\nleads to sub-optimal results. To tackle this difficulty, we propose a new\nmethod called Cleansed k-Nearest Neighbor (CKNN), which explicitly filters out\nthe Anomaly Clusters by cleansing the training dataset. Following the k-nearest\nneighbor algorithm in the feature space provides powerful anomaly detection\ncapability. Although the identified Anomaly Cluster issue presents a\nsignificant challenge to applying k-nearest neighbor in UVAD, our proposed\ncleansing scheme effectively addresses this problem. We evaluate the proposed\nmethod on various benchmark datasets and demonstrate that CKNN outperforms the\nprevious state-of-the-art UVAD method by up to 8.5% (from 82.0 to 89.0) in\nterms of AUROC. Moreover, we emphasize that the performance of the proposed\nmethod is comparable to that of the state-of-the-art method trained using\nanomaly-free data.\n","authors":["Jihun Yi","Sungroh Yoon"],"pdf_url":"https://arxiv.org/pdf/2408.03014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17572v3","updated":"2024-08-06T07:36:21Z","published":"2024-07-24T18:05:13Z","title":"CityX: Controllable Procedural Content Generation for Unbounded 3D\n  Cities","summary":"  Generating a realistic, large-scale 3D virtual city remains a complex\nchallenge due to the involvement of numerous 3D assets, various city styles,\nand strict layout constraints. Existing approaches provide promising attempts\nat procedural content generation to create large-scale scenes using Blender\nagents. However, they face crucial issues such as difficulties in scaling up\ngeneration capability and achieving fine-grained control at the semantic layout\nlevel. To address these problems, we propose a novel multi-modal controllable\nprocedural content generation method, named CityX, which enhances realistic,\nunbounded 3D city generation guided by multiple layout conditions, including\nOSM, semantic maps, and satellite images. Specifically, the proposed method\ncontains a general protocol for integrating various PCG plugins and a\nmulti-agent framework for transforming instructions into executable Blender\nactions. Through this effective framework, CityX shows the potential to build\nan innovative ecosystem for 3D scene generation by bridging the gap between the\nquality of generated assets and industrial requirements. Extensive experiments\nhave demonstrated the effectiveness of our method in creating high-quality,\ndiverse, and unbounded cities guided by multi-modal conditions. Our project\npage: https://cityx-lab.github.io.\n","authors":["Shougao Zhang","Mengqi Zhou","Yuxi Wang","Chuanchen Luo","Rongyu Wang","Yiwei Li","Xucheng Yin","Zhaoxiang Zhang","Junran Peng"],"pdf_url":"https://arxiv.org/pdf/2407.17572v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14362v3","updated":"2024-08-06T07:32:46Z","published":"2024-03-21T12:45:01Z","title":"Less but Better: Enabling Generalized Zero-shot Learning Towards Unseen\n  Domains by Intrinsic Learning from Redundant LLM Semantics","summary":"  Generalized zero-shot learning (GZSL) focuses on recognizing seen and unseen\nclasses against domain shift problem (DSP) where data of unseen classes may be\nmisclassified as seen classes. However, existing GZSL is still limited to seen\ndomains. In the current work, we pioneer cross-domain GZSL (CDGZSL) which\naddresses GZSL towards unseen domains. Different from existing GZSL methods\nwhich alleviate DSP by generating features of unseen classes with semantics,\nCDGZSL needs to construct a common feature space across domains and acquire the\ncorresponding intrinsic semantics shared among domains to transfer from seen to\nunseen domains. Considering the information asymmetry problem caused by\nredundant class semantics annotated with large language models (LLMs), we\npresent Meta Domain Alignment Semantic Refinement (MDASR). Technically, MDASR\nconsists of two parts: Inter-class Similarity Alignment (ISA), which eliminates\nthe non-intrinsic semantics not shared across all domains under the guidance of\ninter-class feature relationships, and Unseen-class Meta Generation (UMG),\nwhich preserves intrinsic semantics to maintain connectivity between seen and\nunseen classes by simulating feature generation. MDASR effectively aligns the\nredundant semantic space with the common feature space, mitigating the\ninformation asymmetry in CDGZSL. The effectiveness of MDASR is demonstrated on\nthe Office-Home and Mini-DomainNet, and we have shared the LLM-based semantics\nfor these datasets as the benchmark.\n","authors":["Jiaqi Yue","Jiancheng Zhao","Chunhui Zhao"],"pdf_url":"https://arxiv.org/pdf/2403.14362v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03006v1","updated":"2024-08-06T07:30:53Z","published":"2024-08-06T07:30:53Z","title":"Dual-path Collaborative Generation Network for Emotional Video\n  Captioning","summary":"  Emotional Video Captioning is an emerging task that aims to describe factual\ncontent with the intrinsic emotions expressed in videos. The essential of the\nEVC task is to effectively perceive subtle and ambiguous visual emotional cues\nduring the caption generation, which is neglected by the traditional video\ncaptioning. Existing emotional video captioning methods perceive global visual\nemotional cues at first, and then combine them with the video features to guide\nthe emotional caption generation, which neglects two characteristics of the EVC\ntask. Firstly, their methods neglect the dynamic subtle changes in the\nintrinsic emotions of the video, which makes it difficult to meet the needs of\ncommon scenes with diverse and changeable emotions. Secondly, as their methods\nincorporate emotional cues into each step, the guidance role of emotion is\noveremphasized, which makes factual content more or less ignored during\ngeneration. To this end, we propose a dual-path collaborative generation\nnetwork, which dynamically perceives visual emotional cues evolutions while\ngenerating emotional captions by collaborative learning. Specifically, in the\ndynamic emotion perception path, we propose a dynamic emotion evolution module,\nwhich first aggregates visual features and historical caption features to\nsummarize the global visual emotional cues, and then dynamically selects\nemotional cues required to be re-composed at each stage. Besides, in the\nadaptive caption generation path, to balance the description of factual content\nand emotional cues, we propose an emotion adaptive decoder. Thus, our methods\ncan generate emotion-related words at the necessary time step, and our caption\ngeneration balances the guidance of factual content and emotional cues well.\nExtensive experiments on three challenging datasets demonstrate the superiority\nof our approach and each proposed module.\n","authors":["Cheng Ye","Weidong Chen","Jingyu Li","Lei Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2408.03006v1.pdf","comment":"Acccepted by ACM Multimedia 2024, oral"},{"id":"http://arxiv.org/abs/2402.16907v2","updated":"2024-08-06T07:24:35Z","published":"2024-02-25T04:24:28Z","title":"Diffusion Posterior Proximal Sampling for Image Restoration","summary":"  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n","authors":["Hongjie Wu","Linchao He","Mingqin Zhang","Dongdong Chen","Kunming Luo","Mengting Luo","Ji-Zhe Zhou","Hu Chen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.16907v2.pdf","comment":"ACM Multimedia 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03001v1","updated":"2024-08-06T07:19:51Z","published":"2024-08-06T07:19:51Z","title":"Multitask and Multimodal Neural Tuning for Large Models","summary":"  In recent years, large-scale multimodal models have demonstrated impressive\ncapabilities across various domains. However, enabling these models to\neffectively perform multiple multimodal tasks simultaneously remains a\nsignificant challenge. To address this, we introduce a novel tuning method\ncalled neural tuning, designed to handle diverse multimodal tasks concurrently,\nincluding reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. Neural tuning emulates sparse distributed\nrepresentation in human brain, where only specific subsets of neurons are\nactivated for each task. Additionally, we present a new benchmark, MMUD, where\neach sample is annotated with multiple task labels. By applying neural tuning\nto pretrained large models on the MMUD benchmark, we achieve simultaneous task\nhandling in a streamlined and efficient manner. All models, code, and datasets\nwill be publicly available after publication, facilitating further research and\ndevelopment in this field.\n","authors":["Hao Sun","Yu Song","Jihong Hu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.03001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02993v1","updated":"2024-08-06T06:59:15Z","published":"2024-08-06T06:59:15Z","title":"DreamLCM: Towards High-Quality Text-to-3D Generation via Latent\n  Consistency Model","summary":"  Recently, the text-to-3D task has developed rapidly due to the appearance of\nthe SDS method. However, the SDS method always generates 3D objects with poor\nquality due to the over-smooth issue. This issue is attributed to two factors:\n1) the DDPM single-step inference produces poor guidance gradients; 2) the\nrandomness from the input noises and timesteps averages the details of the 3D\ncontents.In this paper, to address the issue, we propose DreamLCM which\nincorporates the Latent Consistency Model (LCM). DreamLCM leverages the\npowerful image generation capabilities inherent in LCM, enabling generating\nconsistent and high-quality guidance, i.e., predicted noises or images. Powered\nby the improved guidance, the proposed method can provide accurate and detailed\ngradients to optimize the target 3D models.In addition, we propose two\nstrategies to enhance the generation quality further. Firstly, we propose a\nguidance calibration strategy, utilizing Euler Solver to calibrate the guidance\ndistribution to accelerate 3D models to converge. Secondly, we propose a dual\ntimestep strategy, increasing the consistency of guidance and optimizing 3D\nmodels from geometry to appearance in DreamLCM. Experiments show that DreamLCM\nachieves state-of-the-art results in both generation quality and training\nefficiency. The code is available at https://github.com/1YimingZhong/DreamLCM.\n","authors":["Yiming Zhong","Xiaolin Zhang","Yao Zhao","Yunchao Wei"],"pdf_url":"https://arxiv.org/pdf/2408.02993v1.pdf","comment":"15 pages, 9 figures, ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.02983v1","updated":"2024-08-06T06:33:24Z","published":"2024-08-06T06:33:24Z","title":"Diffusion Model Meets Non-Exemplar Class-Incremental Learning and Beyond","summary":"  Non-exemplar class-incremental learning (NECIL) is to resist catastrophic\nforgetting without saving old class samples. Prior methodologies generally\nemploy simple rules to generate features for replaying, suffering from large\ndistribution gap between replayed features and real ones. To address the\naforementioned issue, we propose a simple, yet effective\n\\textbf{Diff}usion-based \\textbf{F}eature \\textbf{R}eplay (\\textbf{DiffFR})\nmethod for NECIL. First, to alleviate the limited representational capacity\ncaused by fixing the feature extractor, we employ Siamese-based self-supervised\nlearning for initial generalizable features. Second, we devise diffusion models\nto generate class-representative features highly similar to real features,\nwhich provides an effective way for exemplar-free knowledge memorization.\nThird, we introduce prototype calibration to direct the diffusion model's focus\ntowards learning the distribution shapes of features, rather than the entire\ndistribution. Extensive experiments on public datasets demonstrate significant\nperformance gains of our DiffFR, outperforming the state-of-the-art NECIL\nmethods by 3.0\\% in average. The code will be made publicly available soon.\n","authors":["Jichuan Zhang","Yali Li","Xin Liu","Shengjin Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16499v2","updated":"2024-08-06T06:31:34Z","published":"2023-11-27T15:49:41Z","title":"InceptionHuman: Controllable Prompt-to-NeRF for Photorealistic 3D Human\n  Generation","summary":"  This paper presents InceptionHuman, a prompt-to-NeRF framework that allows\neasy control via a combination of prompts in different modalities (e.g., text,\nposes, edge, segmentation map, etc) as inputs to generate photorealistic 3D\nhumans. While many works have focused on generating 3D human models, they\nsuffer one or more of the following: lack of distinctive features, unnatural\nshading/shadows, unnatural poses/clothes, limited views, etc. InceptionHuman\nachieves consistent 3D human generation within a progressively refined NeRF\nspace with two novel modules, Iterative Pose-Aware Refinement (IPAR) and\nProgressive-Augmented Reconstruction (PAR). IPAR iteratively refines the\ndiffusion-generated images and synthesizes high-quality 3D-aware views\nconsidering the close-pose RGB values. PAR employs a pretrained diffusion prior\nto augment the generated synthetic views and adds regularization for\nview-independent appearance. Overall, the synthesis of photorealistic novel\nviews empowers the resulting 3D human NeRF from 360-degree perspectives.\nExtensive qualitative and quantitative experimental comparison show that our\nInceptionHuman models achieve state-of-the-art application quality.\n","authors":["Shiu-hong Kao","Xinhang Liu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2311.16499v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02980v1","updated":"2024-08-06T06:25:39Z","published":"2024-08-06T06:25:39Z","title":"Sample-agnostic Adversarial Perturbation for Vision-Language\n  Pre-training Models","summary":"  Recent studies on AI security have highlighted the vulnerability of\nVision-Language Pre-training (VLP) models to subtle yet intentionally designed\nperturbations in images and texts. Investigating multimodal systems' robustness\nvia adversarial attacks is crucial in this field. Most multimodal attacks are\nsample-specific, generating a unique perturbation for each sample to construct\nadversarial samples. To the best of our knowledge, it is the first work through\nmultimodal decision boundaries to explore the creation of a universal,\nsample-agnostic perturbation that applies to any image. Initially, we explore\nstrategies to move sample points beyond the decision boundaries of linear\nclassifiers, refining the algorithm to ensure successful attacks under the top\n$k$ accuracy metric. Based on this foundation, in visual-language tasks, we\ntreat visual and textual modalities as reciprocal sample points and decision\nhyperplanes, guiding image embeddings to traverse text-constructed decision\nboundaries, and vice versa. This iterative process consistently refines a\nuniversal perturbation, ultimately identifying a singular direction within the\ninput space which is exploitable to impair the retrieval performance of VLP\nmodels. The proposed algorithms support the creation of global perturbations or\nadversarial patches. Comprehensive experiments validate the effectiveness of\nour method, showcasing its data, task, and model transferability across various\nVLP models and datasets. Code: https://github.com/LibertazZ/MUAP\n","authors":["Haonan Zheng","Wen Jiang","Xinyang Deng","Wenrui Li"],"pdf_url":"https://arxiv.org/pdf/2408.02980v1.pdf","comment":"13 pages, 8 figures, published in ACMMM2024"},{"id":"http://arxiv.org/abs/2408.02978v1","updated":"2024-08-06T06:24:10Z","published":"2024-08-06T06:24:10Z","title":"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval","summary":"  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n","authors":["Ruixiang Zhao","Jian Jia","Yan Li","Xuehan Bai","Quan Chen","Han Li","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02978v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.18136v2","updated":"2024-08-06T06:04:41Z","published":"2024-04-28T10:16:35Z","title":"SafePaint: Anti-forensic Image Inpainting with Domain Adaptation","summary":"  Existing image inpainting methods have achieved remarkable accomplishments in\ngenerating visually appealing results, often accompanied by a trend toward\ncreating more intricate structural textures. However, while these models excel\nat creating more realistic image content, they often leave noticeable traces of\ntampering, posing a significant threat to security. In this work, we take the\nanti-forensic capabilities into consideration, firstly proposing an end-to-end\ntraining framework for anti-forensic image inpainting named SafePaint.\nSpecifically, we innovatively formulated image inpainting as two major tasks:\nsemantically plausible content completion and region-wise optimization. The\nformer is similar to current inpainting methods that aim to restore the missing\nregions of corrupted images. The latter, through domain adaptation, endeavors\nto reconcile the discrepancies between the inpainted region and the unaltered\narea to achieve anti-forensic goals. Through comprehensive theoretical\nanalysis, we validate the effectiveness of domain adaptation for anti-forensic\nperformance. Furthermore, we meticulously crafted a region-wise separated\nattention (RWSA) module, which not only aligns with our objective of\nanti-forensics but also enhances the performance of the model. Extensive\nqualitative and quantitative evaluations show our approach achieves comparable\nresults to existing image inpainting methods while offering anti-forensic\ncapabilities not available in other methods.\n","authors":["Dunyun Chen","Xin Liao","Xiaoshuai Wu","Shiwei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.18136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21317v2","updated":"2024-08-06T05:42:42Z","published":"2024-07-31T03:58:48Z","title":"Pathology Foundation Models","summary":"  Pathology has played a crucial role in the diagnosis and evaluation of\npatient tissue samples obtained from surgeries and biopsies for many years. The\nadvent of Whole Slide Scanners and the development of deep learning\ntechnologies have significantly advanced the field, leading to extensive\nresearch and development in pathology AI (Artificial Intelligence). These\nadvancements have contributed to reducing the workload of pathologists and\nsupporting decision-making in treatment plans. Recently, large-scale AI models\nknown as Foundation Models (FMs), which are more accurate and applicable to a\nwide range of tasks compared to traditional AI, have emerged, and expanded\ntheir application scope in the healthcare field. Numerous FMs have been\ndeveloped in pathology, and there are reported cases of their application in\nvarious tasks, such as disease diagnosis, rare cancer diagnosis, patient\nsurvival prognosis prediction, biomarker expression prediction, and the scoring\nof immunohistochemical expression intensity. However, several challenges remain\nfor the clinical application of FMs, which healthcare professionals, as users,\nmust be aware of. Research is ongoing to address these challenges. In the\nfuture, it is expected that the development of Generalist Medical AI, which\nintegrates pathology FMs with FMs from other medical domains, will progress,\nleading to the effective utilization of AI in real clinical settings to promote\nprecision and personalized medicine.\n","authors":["Mieko Ochi","Daisuke Komura","Shumpei Ishikawa"],"pdf_url":"https://arxiv.org/pdf/2407.21317v2.pdf","comment":"19 pages, 1 figure, 3 tables"},{"id":"http://arxiv.org/abs/2408.02966v1","updated":"2024-08-06T05:24:06Z","published":"2024-08-06T05:24:06Z","title":"Fast Point Cloud Geometry Compression with Context-based Residual Coding\n  and INR-based Refinement","summary":"  Compressing a set of unordered points is far more challenging than\ncompressing images/videos of regular sample grids, because of the difficulties\nin characterizing neighboring relations in an irregular layout of points. Many\nresearchers resort to voxelization to introduce regularity, but this approach\nsuffers from quantization loss. In this research, we use the KNN method to\ndetermine the neighborhoods of raw surface points. This gives us a means to\ndetermine the spatial context in which the latent features of 3D points are\ncompressed by arithmetic coding. As such, the conditional probability model is\nadaptive to local geometry, leading to significant rate reduction.\nAdditionally, we propose a dual-layer architecture where a non-learning base\nlayer reconstructs the main structures of the point cloud at low complexity,\nwhile a learned refinement layer focuses on preserving fine details. This\ndesign leads to reductions in model complexity and coding latency by two orders\nof magnitude compared to SOTA methods. Moreover, we incorporate an implicit\nneural representation (INR) into the refinement layer, allowing the decoder to\nsample points on the underlying surface at arbitrary densities. This work is\nthe first to effectively exploit content-aware local contexts for compressing\nirregular raw point clouds, achieving high rate-distortion performance, low\ncomplexity, and the ability to function as an arbitrary-scale upsampling\nnetwork simultaneously.\n","authors":["Hao Xu","Xi Zhang","Xiaolin Wu"],"pdf_url":"https://arxiv.org/pdf/2408.02966v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2405.05953v4","updated":"2024-08-06T05:19:06Z","published":"2024-05-09T17:46:22Z","title":"Frame Interpolation with Consecutive Brownian Bridge Diffusion","summary":"  Recent work in Video Frame Interpolation (VFI) tries to formulate VFI as a\ndiffusion-based conditional image generation problem, synthesizing the\nintermediate frame given a random noise and neighboring frames. Due to the\nrelatively high resolution of videos, Latent Diffusion Models (LDMs) are\nemployed as the conditional generation model, where the autoencoder compresses\nimages into latent representations for diffusion and then reconstructs images\nfrom these latent representations. Such a formulation poses a crucial\nchallenge: VFI expects that the output is deterministically equal to the ground\ntruth intermediate frame, but LDMs randomly generate a diverse set of different\nimages when the model runs multiple times. The reason for the diverse\ngeneration is that the cumulative variance (variance accumulated at each step\nof generation) of generated latent representations in LDMs is large. This makes\nthe sampling trajectory random, resulting in diverse rather than deterministic\ngenerations. To address this problem, we propose our unique solution: Frame\nInterpolation with Consecutive Brownian Bridge Diffusion. Specifically, we\npropose consecutive Brownian Bridge diffusion that takes a deterministic\ninitial value as input, resulting in a much smaller cumulative variance of\ngenerated latent representations. Our experiments suggest that our method can\nimprove together with the improvement of the autoencoder and achieve\nstate-of-the-art performance in VFI, leaving strong potential for further\nenhancement.\n","authors":["Zonglin Lyu","Ming Li","Jianbo Jiao","Chen Chen"],"pdf_url":"https://arxiv.org/pdf/2405.05953v4.pdf","comment":"corrected typo"},{"id":"http://arxiv.org/abs/2407.11652v4","updated":"2024-08-06T05:10:56Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v4.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.02957v1","updated":"2024-08-06T04:55:33Z","published":"2024-08-06T04:55:33Z","title":"Online Temporal Action Localization with Memory-Augmented Transformer","summary":"  Online temporal action localization (On-TAL) is the task of identifying\nmultiple action instances given a streaming video. Since existing methods take\nas input only a video segment of fixed size per iteration, they are limited in\nconsidering long-term context and require tuning the segment size carefully. To\novercome these limitations, we propose memory-augmented transformer (MATR).\nMATR utilizes the memory queue that selectively preserves the past segment\nfeatures, allowing to leverage long-term context for inference. We also propose\na novel action localization method that observes the current input segment to\npredict the end time of the ongoing action and accesses the memory queue to\nestimate the start time of the action. Our method outperformed existing methods\non two datasets, THUMOS14 and MUSES, surpassing not only TAL methods in the\nonline setting but also some offline TAL methods.\n","authors":["Youngkil Song","Dongkeun Kim","Minsu Cho","Suha Kwak"],"pdf_url":"https://arxiv.org/pdf/2408.02957v1.pdf","comment":"Accepted to ECCV 2024, Project page:\n  https://cvlab.postech.ac.kr/research/MATR/"},{"id":"http://arxiv.org/abs/2408.02954v1","updated":"2024-08-06T04:44:10Z","published":"2024-08-06T04:44:10Z","title":"WWW: Where, Which and Whatever Enhancing Interpretability in Multimodal\n  Deepfake Detection","summary":"  All current benchmarks for multimodal deepfake detection manipulate entire\nframes using various generation techniques, resulting in oversaturated\ndetection accuracies exceeding 94% at the video-level classification. However,\nthese benchmarks struggle to detect dynamic deepfake attacks with challenging\nframe-by-frame alterations presented in real-world scenarios. To address this\nlimitation, we introduce FakeMix, a novel clip-level evaluation benchmark aimed\nat identifying manipulated segments within both video and audio, providing\ninsight into the origins of deepfakes. Furthermore, we propose novel evaluation\nmetrics, Temporal Accuracy (TA) and Frame-wise Discrimination Metric (FDM), to\nassess the robustness of deepfake detection models. Evaluating state-of-the-art\nmodels against diverse deepfake benchmarks, particularly FakeMix, demonstrates\nthe effectiveness of our approach comprehensively. Specifically, while\nachieving an Average Precision (AP) of 94.2% at the video-level, the evaluation\nof the existing models at the clip-level using the proposed metrics, TA and\nFDM, yielded sharp declines in accuracy to 53.1%, and 52.1%, respectively.\n","authors":["Juho Jung","Sangyoun Lee","Jooeon Kang","Yunjin Na"],"pdf_url":"https://arxiv.org/pdf/2408.02954v1.pdf","comment":"4 pages, 2 figures, 2 tables, Accepted as Oral Presentation at The\n  Trustworthy AI Workshop @ IJCAI 2024"},{"id":"http://arxiv.org/abs/2408.02336v2","updated":"2024-08-06T04:04:23Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2403.16428v2","updated":"2024-08-06T03:44:00Z","published":"2024-03-25T05:12:21Z","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand\n  Interactions with Objects","summary":"  We interact with the world with our hands and see it through our own\n(egocentric) perspective. A holistic 3Dunderstanding of such interactions from\negocentric views is important for tasks in robotics, AR/VR, action recognition\nand motion generation. Accurately reconstructing such interactions in 3D is\nchallenging due to heavy occlusion, viewpoint bias, camera distortion, and\nmotion blur from the head movement. To this end, we designed the HANDS23\nchallenge based on the AssemblyHands and ARCTIC datasets with carefully\ndesigned training and testing splits. Based on the results of the top submitted\nmethods and more recent baselines on the leaderboards, we perform a thorough\nanalysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates\nthe effectiveness of addressing distortion specific to egocentric cameras,\nadopting high-capacity transformers to learn complex hand-object interactions,\nand fusing predictions from different views. Our study further reveals\nchallenging scenarios intractable with state-of-the-art methods, such as fast\nhand motion, object reconstruction from narrow egocentric views, and close\ncontact between two hands and objects. Our efforts will enrich the community's\nknowledge foundation and facilitate future hand studies on egocentric\nhand-object interactions.\n","authors":["Zicong Fan","Takehiko Ohkawa","Linlin Yang","Nie Lin","Zhishan Zhou","Shihao Zhou","Jiajun Liang","Zhong Gao","Xuanyang Zhang","Xue Zhang","Fei Li","Zheng Liu","Feng Lu","Karim Abou Zeid","Bastian Leibe","Jeongwan On","Seungryul Baek","Aditya Prakash","Saurabh Gupta","Kun He","Yoichi Sato","Otmar Hilliges","Hyung Jin Chang","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2403.16428v2.pdf","comment":"Accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2404.18203v2","updated":"2024-08-06T03:37:31Z","published":"2024-04-28T14:47:09Z","title":"LMM-PCQA: Assisting Point Cloud Quality Assessment with LMM","summary":"  Although large multi-modality models (LMMs) have seen extensive exploration\nand application in various quality assessment studies, their integration into\nPoint Cloud Quality Assessment (PCQA) remains unexplored. Given LMMs'\nexceptional performance and robustness in low-level vision and quality\nassessment tasks, this study aims to investigate the feasibility of imparting\nPCQA knowledge to LMMs through text supervision. To achieve this, we transform\nquality labels into textual descriptions during the fine-tuning phase, enabling\nLMMs to derive quality rating logits from 2D projections of point clouds. To\ncompensate for the loss of perception in the 3D domain, structural features are\nextracted as well. These quality logits and structural features are then\ncombined and regressed into quality scores. Our experimental results affirm the\neffectiveness of our approach, showcasing a novel integration of LMMs into PCQA\nthat enhances model understanding and assessment accuracy. We hope our\ncontributions can inspire subsequent investigations into the fusion of LMMs\nwith PCQA, fostering advancements in 3D visual quality analysis and beyond. The\ncode is available at https://github.com/zzc-1998/LMM-PCQA.\n","authors":["Zicheng Zhang","Haoning Wu","Yingjie Zhou","Chunyi Li","Wei Sun","Chaofeng Chen","Xiongkuo Min","Xiaohong Liu","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2404.18203v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12794v2","updated":"2024-08-06T03:28:12Z","published":"2024-04-19T11:17:35Z","title":"MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware\n  State Space Model","summary":"  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment\nmoving objects in point clouds of the current scan using motion information\nfrom previous scans. Despite the promising results achieved by previous MOS\nmethods, several key issues, such as the weak coupling of temporal and spatial\ninformation, still need further study. In this paper, we propose a novel\nLiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,\ntermed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue\nBootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial\ninformation in point clouds and alleviate the issue of overlooked temporal\nclues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to\nendow the model with the capacity to understand the temporal correlations of\nthe same object across different time steps. Specifically, MSSM emphasizes the\nmotion states of the same object at different time steps through two distinct\ntemporal modeling and correlation steps. We utilize an improved state space\nmodel to represent these motion differences, significantly modeling the motion\nstates. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road\nbenchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art\nperformance. The source code is publicly available at\nhttps://github.com/Terminal-K/MambaMOS.\n","authors":["Kang Zeng","Hao Shi","Jiacheng Lin","Siyu Li","Jintao Cheng","Kaiwei Wang","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12794v2.pdf","comment":"Accepted to ACM MM 2024. The source code is publicly available at\n  https://github.com/Terminal-K/MambaMOS"},{"id":"http://arxiv.org/abs/2408.02929v1","updated":"2024-08-06T03:23:42Z","published":"2024-08-06T03:23:42Z","title":"Segmenting Small Stroke Lesions with Novel Labeling Strategies","summary":"  Deep neural networks have demonstrated exceptional efficacy in stroke lesion\nsegmentation. However, the delineation of small lesions, critical for stroke\ndiagnosis, remains a challenge. In this study, we propose two straightforward\nyet powerful approaches that can be seamlessly integrated into a variety of\nnetworks: Multi-Size Labeling (MSL) and Distance-Based Labeling (DBL), with the\naim of enhancing the segmentation accuracy of small lesions. MSL divides lesion\nmasks into various categories based on lesion volume while DBL emphasizes the\nlesion boundaries. Experimental evaluations on the Anatomical Tracings of\nLesions After Stroke (ATLAS) v2.0 dataset showcase that an ensemble of MSL and\nDBL achieves consistently better or equal performance on recall (3.6% and\n3.7%), F1 (2.4% and 1.5%), and Dice scores (1.3% and 0.0%) compared to the\ntop-1 winner of the 2022 MICCAI ATLAS Challenge on both the subset only\ncontaining small lesions and the entire dataset, respectively. Notably, on the\nmini-lesion subset, a single MSL model surpasses the previous best ensemble\nstrategy, with enhancements of 1.0% and 0.3% on F1 and Dice scores,\nrespectively. Our code is available at:\nhttps://github.com/nadluru/StrokeLesSeg.\n","authors":["Liang Shang","Zhengyang Lou","Andrew L. Alexander","Vivek Prabhakaran","William A. Sethares","Veena A. Nair","Nagesh Adluru"],"pdf_url":"https://arxiv.org/pdf/2408.02929v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02924v1","updated":"2024-08-06T03:20:10Z","published":"2024-08-06T03:20:10Z","title":"Evaluation of Segment Anything Model 2: The Role of SAM2 in the\n  Underwater Environment","summary":"  With breakthroughs in large-scale modeling, the Segment Anything Model (SAM)\nand its extensions have been attempted for applications in various underwater\nvisualization tasks in marine sciences, and have had a significant impact on\nthe academic community. Recently, Meta has further developed the Segment\nAnything Model 2 (SAM2), which significantly improves running speed and\nsegmentation accuracy compared to its predecessor. This report aims to explore\nthe potential of SAM2 in marine science by evaluating it on the underwater\ninstance segmentation benchmark datasets UIIS and USIS10K. The experiments show\nthat the performance of SAM2 is extremely dependent on the type of\nuser-provided prompts. When using the ground truth bounding box as prompt, SAM2\nperformed excellently in the underwater instance segmentation domain. However,\nwhen running in automatic mode, SAM2's ability with point prompts to sense and\nsegment underwater instances is significantly degraded. It is hoped that this\npaper will inspire researchers to further explore the SAM model family in the\nunderwater domain. The results and evaluation codes in this paper are available\nat https://github.com/LiamLian0727/UnderwaterSAM2Eval.\n","authors":["Shijie Lian","Hua Li"],"pdf_url":"https://arxiv.org/pdf/2408.02924v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v2","updated":"2024-08-06T03:19:25Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v2.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.02922v1","updated":"2024-08-06T03:15:18Z","published":"2024-08-06T03:15:18Z","title":"Pose Magic: Efficient and Temporally Consistent Human Pose Estimation\n  with a Hybrid Mamba-GCN Network","summary":"  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are\nprimarily based on Transformers. However, existing Transformer-based 3D HPE\nbackbones often encounter a trade-off between accuracy and computational\nefficiency. To resolve the above dilemma, in this work, leveraging recent\nadvances in state space models, we utilize Mamba for high-quality and efficient\nlong-range modeling. Nonetheless, Mamba still faces challenges in precisely\nexploiting the local dependencies between joints. To address these issues, we\npropose a new attention-free hybrid spatiotemporal architecture named Hybrid\nMamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN\nby capturing relationships between neighboring joints, thus producing new\nrepresentations to complement Mamba's outputs. By adaptively fusing\nrepresentations from Mamba and GCN, Pose Magic demonstrates superior capability\nin learning the underlying 3D structure. To meet the requirements of real-time\ninference, we also provide a fully causal version. Extensive experiments show\nthat Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving\n$74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and\nthe ability to generalize to unseen sequence lengths.\n","authors":["Xinyi Zhang","Qiqi Bao","Qinpeng Cui","Wenming Yang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.01351v2","updated":"2024-08-06T02:58:13Z","published":"2023-09-04T04:29:01Z","title":"Adv3D: Generating 3D Adversarial Examples for 3D Object Detection in\n  Driving Scenarios with NeRF","summary":"  Deep neural networks (DNNs) have been proven extremely susceptible to\nadversarial examples, which raises special safety-critical concerns for\nDNN-based autonomous driving stacks (i.e., 3D object detection). Although there\nare extensive works on image-level attacks, most are restricted to 2D pixel\nspaces, and such attacks are not always physically realistic in our 3D world.\nHere we present Adv3D, the first exploration of modeling adversarial examples\nas Neural Radiance Fields (NeRFs). Advances in NeRF provide photorealistic\nappearances and 3D accurate generation, yielding a more realistic and\nrealizable adversarial example. We train our adversarial NeRF by minimizing the\nsurrounding objects' confidence predicted by 3D detectors on the training set.\nThen we evaluate Adv3D on the unseen validation set and show that it can cause\na large performance reduction when rendering NeRF in any sampled pose. To\ngenerate physically realizable adversarial examples, we propose primitive-aware\nsampling and semantic-guided regularization that enable 3D patch attacks with\ncamouflage adversarial texture. Experimental results demonstrate that the\ntrained adversarial NeRF generalizes well to different poses, scenes, and 3D\ndetectors. Finally, we provide a defense method to our attacks that involves\nadversarial training through data augmentation. Project page:\nhttps://len-li.github.io/adv3d-web\n","authors":["Leheng Li","Qing Lian","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2309.01351v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17379v2","updated":"2024-08-06T02:44:44Z","published":"2024-07-24T15:59:01Z","title":"MMRA: A Benchmark for Evaluating Multi-Granularity and Multi-Image\n  Relational Association Capabilities in Large Visual Language Models","summary":"  Given the remarkable success that large visual language models (LVLMs) have\nachieved in image perception tasks, the endeavor to make LVLMs perceive the\nworld like humans is drawing increasing attention. Current multi-modal\nbenchmarks primarily focus on facts or specific topic-related knowledge\ncontained within individual images. However, they often overlook the\nassociative relations between multiple images, which require the identification\nand analysis of similarities among entities or content present in different\nimages. Therefore, we propose the multi-image relation association task and a\nmeticulously curated Multi-granularity Multi-image Relational Association\n(MMRA) benchmark, comprising 1,024 samples. In order to systematically and\ncomprehensively evaluate current LVLMs, we establish an associational relation\nsystem among images that contain 11 subtasks (e.g, UsageSimilarity, SubEvent)\nat two granularity levels (i.e., image and entity) according to the relations\nin ConceptNet. Our experiments reveal that on the MMRA benchmark, current\nmulti-image LVLMs exhibit distinct advantages and disadvantages across various\nsubtasks. Notably, fine-grained, entity-level multi-image perception tasks pose\na greater challenge for LVLMs compared to image-level tasks. Moreover, LVLMs\nperform poorly on spatial-related tasks, indicating that LVLMs still have\nlimited spatial awareness. Additionally, our findings indicate that while LVLMs\ndemonstrate a strong capability to perceive image details, enhancing their\nability to associate information across multiple images hinges on improving the\nreasoning capabilities of their language model component. Moreover, we explored\nthe ability of LVLMs to perceive image sequences within the context of our\nmulti-image association task. Our experiments show that the majority of current\nLVLMs do not adequately model image sequences during the pre-training process.\n","authors":["Siwei Wu","Kang Zhu","Yu Bai","Yiming Liang","Yizhi Li","Haoning Wu","J. H. Liu","Ruibo Liu","Xingwei Qu","Xuxin Cheng","Ge Zhang","Wenhao Huang","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2407.17379v2.pdf","comment":"VLMs, Multi-Image Association"},{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2408.01934v2","updated":"2024-08-06T02:39:46Z","published":"2024-08-04T05:22:08Z","title":"A Survey and Evaluation of Adversarial Attacks for Object Detection","summary":"  Deep learning models excel in various computer vision tasks but are\nsusceptible to adversarial examples-subtle perturbations in input data that\nlead to incorrect predictions. This vulnerability poses significant risks in\nsafety-critical applications such as autonomous vehicles, security\nsurveillance, and aircraft health monitoring. While numerous surveys focus on\nadversarial attacks in image classification, the literature on such attacks in\nobject detection is limited. This paper offers a comprehensive taxonomy of\nadversarial attacks specific to object detection, reviews existing adversarial\nrobustness evaluation metrics, and systematically assesses open-source attack\nmethods and model robustness. Key observations are provided to enhance the\nunderstanding of attack effectiveness and corresponding countermeasures.\nAdditionally, we identify crucial research challenges to guide future efforts\nin securing automated object detection systems.\n","authors":["Khoi Nguyen Tiet Nguyen","Wenyu Zhang","Kangkang Lu","Yuhuan Wu","Xingjian Zheng","Hui Li Tan","Liangli Zhen"],"pdf_url":"https://arxiv.org/pdf/2408.01934v2.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2303.00952v5","updated":"2024-08-06T02:39:05Z","published":"2023-03-02T04:12:53Z","title":"Towards Activated Muscle Group Estimation in the Wild","summary":"  In this paper, we tackle the new task of video-based Activated Muscle Group\nEstimation (AMGE) aiming at identifying active muscle regions during physical\nactivity in the wild. To this intent, we provide the MuscleMap dataset\nfeaturing >15K video clips with 135 different activities and 20 labeled muscle\ngroups. This dataset opens the vistas to multiple video-based applications in\nsports and rehabilitation medicine under flexible environment constraints. The\nproposed MuscleMap dataset is constructed with YouTube videos, specifically\ntargeting High-Intensity Interval Training (HIIT) physical exercise in the\nwild. To make the AMGE model applicable in real-life situations, it is crucial\nto ensure that the model can generalize well to numerous types of physical\nactivities not present during training and involving new combinations of\nactivated muscles. To achieve this, our benchmark also covers an evaluation\nsetting where the model is exposed to activity types excluded from the training\nset. Our experiments reveal that the generalizability of existing architectures\nadapted for the AMGE task remains a challenge. Therefore, we also propose a new\napproach, TransM3E, which employs a multi-modality feature fusion mechanism\nbetween both the video transformer model and the skeleton-based graph\nconvolution model with novel cross-modal knowledge distillation executed on\nmulti-classification tokens. The proposed method surpasses all popular video\nclassification models when dealing with both, previously seen and new types of\nphysical activities. The database and code can be found at\nhttps://github.com/KPeng9510/MuscleMap.\n","authors":["Kunyu Peng","David Schneider","Alina Roitberg","Kailun Yang","Jiaming Zhang","Chen Deng","Kaiyu Zhang","M. Saquib Sarfraz","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2303.00952v5.pdf","comment":"Accepted to ACM MM 2024. The database and code can be found at\n  https://github.com/KPeng9510/MuscleMap"},{"id":"http://arxiv.org/abs/2408.02906v1","updated":"2024-08-06T02:38:22Z","published":"2024-08-06T02:38:22Z","title":"Dual-View Pyramid Pooling in Deep Neural Networks for Improved Medical\n  Image Classification and Confidence Calibration","summary":"  Spatial pooling (SP) and cross-channel pooling (CCP) operators have been\napplied to aggregate spatial features and pixel-wise features from feature maps\nin deep neural networks (DNNs), respectively. Their main goal is to reduce\ncomputation and memory overhead without visibly weakening the performance of\nDNNs. However, SP often faces the problem of losing the subtle feature\nrepresentations, while CCP has a high possibility of ignoring salient feature\nrepresentations, which may lead to both miscalibration of confidence issues and\nsuboptimal medical classification results. To address these problems, we\npropose a novel dual-view framework, the first to systematically investigate\nthe relative roles of SP and CCP by analyzing the difference between spatial\nfeatures and pixel-wise features. Based on this framework, we propose a new\npooling method, termed dual-view pyramid pooling (DVPP), to aggregate\nmulti-scale dual-view features. DVPP aims to boost both medical image\nclassification and confidence calibration performance by fully leveraging the\nmerits of SP and CCP operators from a dual-axis perspective. Additionally, we\ndiscuss how to fulfill DVPP with five parameter-free implementations. Extensive\nexperiments on six 2D/3D medical image classification tasks show that our DVPP\nsurpasses state-of-the-art pooling methods in terms of medical image\nclassification results and confidence calibration across different DNNs.\n","authors":["Xiaoqing Zhang","Qiushi Nie","Zunjie Xiao","Jilu Zhao","Xiao Wu","Pengxin Guo","Runzhi Li","Jin Liu","Yanjie Wei","Yi Pan"],"pdf_url":"https://arxiv.org/pdf/2408.02906v1.pdf","comment":"27"},{"id":"http://arxiv.org/abs/2402.13699v4","updated":"2024-08-06T02:32:36Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. Many of the measurements acquired during the tuning process come in\nthe form of images that need to be properly analyzed to guide the subsequent\ntuning steps. By design, features present in such images capture certain\nbehaviors or states of the measured QD devices. When considered carefully, such\nfeatures can aid the control and calibration of QD devices. An important\nexample of such images are so-called \\textit{triangle plots}, which visually\nrepresent current flow and reveal characteristics important for QD device\ncalibration. While image-based classification tools, such as convolutional\nneural networks (CNNs), can be used to verify whether a given measurement is\n\\textit{good} and thus warrants the initiation of the next phase of tuning,\nthey do not provide any insights into how the device should be adjusted in the\ncase of \\textit{bad} images. This is because CNNs sacrifice prediction and\nmodel intelligibility for high accuracy. To ameliorate this trade-off, a recent\nstudy introduced an image vectorization approach that relies on the Gabor\nwavelet transform [1]. Here we propose an alternative vectorization method that\ninvolves mathematical modeling of synthetic triangles to mimic the experimental\ndata. Using explainable boosting machines, we show that this new method offers\nsuperior explainability of model prediction without sacrificing accuracy. This\nwork demonstrates the feasibility and advantages of applying explainable\nmachine learning techniques to the analysis of quantum dot measurements, paving\nthe way for further advances in automated and transparent QD device tuning.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v4.pdf","comment":"17 pages, 4 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2408.02904v1","updated":"2024-08-06T02:27:54Z","published":"2024-08-06T02:27:54Z","title":"Enabling Intelligent Traffic Systems: A Deep Learning Method for\n  Accurate Arabic License Plate Recognition","summary":"  This paper introduces a novel two-stage framework for accurate Egyptian\nVehicle License Plate Recognition (EVLPR). The first stage employs image\nprocessing techniques to reliably localize license plates, while the second\nstage utilizes a custom-designed deep learning model for robust Arabic\ncharacter recognition. The proposed system achieves a remarkable 99.3% accuracy\non a diverse dataset, surpassing existing approaches. Its potential\napplications extend to intelligent traffic management, including traffic\nviolation detection and parking optimization. Future research will focus on\nenhancing the system's capabilities through architectural refinements, expanded\ndatasets, and addressing system dependencies.\n","authors":["M. A. Sayedelahl"],"pdf_url":"https://arxiv.org/pdf/2408.02904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02901v1","updated":"2024-08-06T02:15:12Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v1.pdf","comment":"6 pages; library tech report"},{"id":"http://arxiv.org/abs/2408.02900v1","updated":"2024-08-06T02:09:35Z","published":"2024-08-06T02:09:35Z","title":"MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular\n  Annotations for Medicine","summary":"  This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal\ndataset for medicine, covering over 25 million images across 10 modalities,\nwith multigranular annotations for more than 65 diseases. These enriched\nannotations encompass both global textual information, such as disease/lesion\ntype, modality, region-specific descriptions, and inter-regional relationships,\nas well as detailed local annotations for regions of interest (ROIs), including\nbounding boxes, segmentation masks. Unlike existing approach which is limited\nby the availability of image-text pairs, we have developed the first automated\npipeline that scales up multimodal data by generating multigranular visual and\ntexual annotations (in the form of image-ROI-description triplets) without the\nneed for any paired text descriptions. Specifically, data from over 90\ndifferent sources have been collected, preprocessed, and grounded using\ndomain-specific expert models to identify ROIs related to abnormal regions. We\nthen build a comprehensive knowledge base and prompt multimodal large language\nmodels to perform retrieval-augmented generation with the identified ROIs as\nguidance, resulting in multigranular texual descriptions. Compared to existing\ndatasets, MedTrinity-25M provides the most enriched annotations, supporting a\ncomprehensive range of multimodal tasks such as captioning and report\ngeneration, as well as vision-centric tasks like classification and\nsegmentation. Pretraining on MedTrinity-25M, our model achieves\nstate-of-the-art performance on VQA-RAD and PathVQA, surpassing both multimodal\nlarge language models and other representative SoTA approaches. This dataset\ncan also be utilized to support large-scale pre-training of multimodal medical\nAI models, contributing to the development of future foundation models in the\nmedical domain.\n","authors":["Yunfei Xie","Ce Zhou","Lang Gao","Juncheng Wu","Xianhang Li","Hong-Yu Zhou","Sheng Liu","Lei Xing","James Zou","Cihang Xie","Yuyin Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.02900v1.pdf","comment":"The project page is at https://yunfeixie233.github.io/MedTrinity-25M"},{"id":"http://arxiv.org/abs/2406.14643v2","updated":"2024-08-06T02:04:35Z","published":"2024-06-20T18:07:19Z","title":"Holistic Evaluation for Interleaved Text-and-Image Generation","summary":"  Interleaved text-and-image generation has been an intriguing research\ndirection, where the models are required to generate both images and text\npieces in an arbitrary order. Despite the emerging advancements in interleaved\ngeneration, the progress in its evaluation still significantly lags behind.\nExisting evaluation benchmarks do not support arbitrarily interleaved images\nand text for both inputs and outputs, and they only cover a limited number of\ndomains and use cases. Also, current works predominantly use similarity-based\nmetrics which fall short in assessing the quality in open-ended scenarios. To\nthis end, we introduce InterleavedBench, the first benchmark carefully curated\nfor the evaluation of interleaved text-and-image generation. InterleavedBench\nfeatures a rich array of tasks to cover diverse real-world use cases. In\naddition, we present InterleavedEval, a strong reference-free metric powered by\nGPT-4o to deliver accurate and explainable evaluation. We carefully define five\nessential evaluation aspects for InterleavedEval, including text quality,\nperceptual quality, image coherence, text-image coherence, and helpfulness, to\nensure a comprehensive and fine-grained assessment. Through extensive\nexperiments and rigorous human evaluation, we show that our benchmark and\nmetric can effectively evaluate the existing models with a strong correlation\nwith human judgments surpassing previous reference-based metrics. We also\nprovide substantial findings and insights to foster future research in\ninterleaved generation and its evaluation.\n","authors":["Minqian Liu","Zhiyang Xu","Zihao Lin","Trevor Ashby","Joy Rimchala","Jiaxin Zhang","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2406.14643v2.pdf","comment":"13 pages, 6 figures, 6 tables. Website:\n  https://vt-nlp.github.io/InterleavedEval/. Dataset:\n  https://huggingface.co/mqliu/InterleavedBench"},{"id":"http://arxiv.org/abs/2408.02891v1","updated":"2024-08-06T01:41:40Z","published":"2024-08-06T01:41:40Z","title":"Diverse Generation while Maintaining Semantic Coordination: A\n  Diffusion-Based Data Augmentation Method for Object Detection","summary":"  Recent studies emphasize the crucial role of data augmentation in enhancing\nthe performance of object detection models. However,existing methodologies\noften struggle to effectively harmonize dataset diversity with semantic\ncoordination.To bridge this gap, we introduce an innovative augmentation\ntechnique leveraging pre-trained conditional diffusion models to mediate this\nbalance. Our approach encompasses the development of a Category Affinity\nMatrix, meticulously designed to enhance dataset diversity, and a Surrounding\nRegion Alignment strategy, which ensures the preservation of semantic\ncoordination in the augmented images. Extensive experimental evaluations\nconfirm the efficacy of our method in enriching dataset diversity while\nseamlessly maintaining semantic coordination. Our method yields substantial\naverage improvements of +1.4AP, +0.9AP, and +3.4AP over existing alternatives\non three distinct object detection models, respectively.\n","authors":["Sen Nie","Zhuo Wang","Xinxin Wang","Kun He"],"pdf_url":"https://arxiv.org/pdf/2408.02891v1.pdf","comment":"15 pages, 7 figures, ICPR2024"},{"id":"http://arxiv.org/abs/2408.02888v1","updated":"2024-08-06T01:34:43Z","published":"2024-08-06T01:34:43Z","title":"VizECGNet: Visual ECG Image Network for Cardiovascular Diseases\n  Classification with Multi-Modal Training and Knowledge Distillation","summary":"  An electrocardiogram (ECG) captures the heart's electrical signal to assess\nvarious heart conditions. In practice, ECG data is stored as either digitized\nsignals or printed images. Despite the emergence of numerous deep learning\nmodels for digitized signals, many hospitals prefer image storage due to cost\nconsiderations. Recognizing the unavailability of raw ECG signals in many\nclinical settings, we propose VizECGNet, which uses only printed ECG graphics\nto determine the prognosis of multiple cardiovascular diseases. During\ntraining, cross-modal attention modules (CMAM) are used to integrate\ninformation from two modalities - image and signal, while self-modality\nattention modules (SMAM) capture inherent long-range dependencies in ECG data\nof each modality. Additionally, we utilize knowledge distillation to improve\nthe similarity between two distinct predictions from each modality stream. This\ninnovative multi-modal deep learning architecture enables the utilization of\nonly ECG images during inference. VizECGNet with image input achieves higher\nperformance in precision, recall, and F1-Score compared to signal-based ECG\nclassification models, with improvements of 3.50%, 8.21%, and 7.38%,\nrespectively.\n","authors":["Ju-Hyeon Nam","Seo-Hyung Park","Su Jung Kim","Sang-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2408.02888v1.pdf","comment":"Accepted in International Conference on Image Processing (ICIP) 2024"},{"id":"http://arxiv.org/abs/2304.07444v4","updated":"2024-08-06T01:31:28Z","published":"2023-04-15T01:33:14Z","title":"The Art of Camouflage: Few-Shot Learning for Animal Detection and\n  Segmentation","summary":"  Camouflaged object detection and segmentation is a new and challenging\nresearch topic in computer vision. There is a serious issue of lacking data on\nconcealed objects such as camouflaged animals in natural scenes. In this paper,\nwe address the problem of few-shot learning for camouflaged object detection\nand segmentation. To this end, we first collect a new dataset, CAMO-FS, for the\nbenchmark. As camouflaged instances are challenging to recognize due to their\nsimilarity compared to the surroundings, we guide our models to obtain\ncamouflaged features that highly distinguish the instances from the background.\nIn this work, we propose FS-CDIS, a framework to efficiently detect and segment\ncamouflaged instances via two loss functions contributing to the training\nprocess. Firstly, the instance triplet loss with the characteristic of\ndifferentiating the anchor, which is the mean of all camouflaged foreground\npoints, and the background points are employed to work at the instance level.\nSecondly, to consolidate the generalization at the class level, we present\ninstance memory storage with the scope of storing camouflaged features of the\nsame category, allowing the model to capture further class-level information\nduring the learning process. The extensive experiments demonstrated that our\nproposed method achieves state-of-the-art performance on the newly collected\ndataset. Code is available at https://github.com/danhntd/FS-CDIS.\n","authors":["Thanh-Danh Nguyen","Anh-Khoa Nguyen Vu","Nhat-Duy Nguyen","Vinh-Tiep Nguyen","Thanh Duc Ngo","Thanh-Toan Do","Minh-Triet Tran","Tam V. Nguyen"],"pdf_url":"https://arxiv.org/pdf/2304.07444v4.pdf","comment":"IEEE Access 2024"},{"id":"http://arxiv.org/abs/2408.02879v1","updated":"2024-08-06T01:13:09Z","published":"2024-08-06T01:13:09Z","title":"Body of Her: A Preliminary Study on End-to-End Humanoid Agent","summary":"  Interactive virtual humanoid agent is a crucial interface with the physical\nworld. A relatively complete humanoid agent first needs to have face and body,\nthen possess both verbal and non-verbal (such as eye contact, facial\nexpression, lip motion, gesture, and manipulation) abilities, and finally, it\nis capable of real-time duplex communication, e.g., the ability to actively\ninterrupt conversations. Most prior systems typically only consider a subset of\nthese elements, leaving a gap from realistic humanoid agent. In this work, we\npropose a real-time, duplex, interactive end-to-end network capable of modeling\nrealistic agent behaviors, including speech, full-body movements for talking,\nresponding, idling, and manipulation. This system is a multimodal model\nintegrating audio and visual inputs, extended from a pre-trained large language\nmodel (LLM). We collect approximately 200,000 hours of audio, around 130,000\nhours of video data, and about 20,000 alignment samples to build the model. The\nfinal model demonstrates capabilities that are difficult to achieve in previous\nsystems, such as generalized object manipulation. This work performs a\npreliminary exploration of the end-to-end approach in this field, aiming to\ninspire further research towards scaling up.\n","authors":["Tenglong Ao"],"pdf_url":"https://arxiv.org/pdf/2408.02879v1.pdf","comment":"Technical Report v1; Project Page:\n  https://aubrey-ao.github.io/BodyOfHer"},{"id":"http://arxiv.org/abs/2403.09975v2","updated":"2024-08-06T00:28:44Z","published":"2024-03-15T02:42:28Z","title":"Skeleton-Based Human Action Recognition with Noisy Labels","summary":"  Understanding human actions from body poses is critical for assistive robots\nsharing space with humans in order to make informed and safe decisions about\nthe next interaction. However, precise temporal localization and annotation of\nactivity sequences is time-consuming and the resulting labels are often noisy.\nIf not effectively addressed, label noise negatively affects the model's\ntraining, resulting in lower recognition quality. Despite its importance,\naddressing label noise for skeleton-based action recognition has been\noverlooked so far. In this study, we bridge this gap by implementing a\nframework that augments well-established skeleton-based human action\nrecognition methods with label-denoising strategies from various research areas\nto serve as the initial benchmark. Observations reveal that these baselines\nyield only marginal performance when dealing with sparse skeleton data.\nConsequently, we introduce a novel methodology, NoiseEraSAR, which integrates\nglobal sample selection, co-teaching, and Cross-Modal Mixture-of-Experts\n(CM-MOE) strategies, aimed at mitigating the adverse impacts of label noise.\nOur proposed approach demonstrates better performance on the established\nbenchmark, setting new state-of-the-art standards. The source code for this\nstudy is accessible at https://github.com/xuyizdby/NoiseEraSAR.\n","authors":["Yi Xu","Kunyu Peng","Di Wen","Ruiping Liu","Junwei Zheng","Yufan Chen","Jiaming Zhang","Alina Roitberg","Kailun Yang","Rainer Stiefelhagen"],"pdf_url":"https://arxiv.org/pdf/2403.09975v2.pdf","comment":"Accepted to IROS 2024. The source code for this study is accessible\n  at https://github.com/xuyizdby/NoiseEraSAR"},{"id":"http://arxiv.org/abs/2310.02650v3","updated":"2024-08-06T00:08:40Z","published":"2023-10-04T08:18:30Z","title":"Active Visual Localization for Multi-Agent Collaboration: A Data-Driven\n  Approach","summary":"  Rather than having each newly deployed robot create its own map of its\nsurroundings, the growing availability of SLAM-enabled devices provides the\noption of simply localizing in a map of another robot or device. In cases such\nas multi-robot or human-robot collaboration, localizing all agents in the same\nmap is even necessary. However, localizing e.g. a ground robot in the map of a\ndrone or head-mounted MR headset presents unique challenges due to viewpoint\nchanges. This work investigates how active visual localization can be used to\novercome such challenges of viewpoint changes. Specifically, we focus on the\nproblem of selecting the optimal viewpoint at a given location. We compare\nexisting approaches in the literature with additional proposed baselines and\npropose a novel data-driven approach. The result demonstrates the superior\nperformance of the data-driven approach when compared to existing methods, both\nin controlled simulation experiments and real-world deployment.\n","authors":["Matthew Hanlon","Boyang Sun","Marc Pollefeys","Hermann Blum"],"pdf_url":"https://arxiv.org/pdf/2310.02650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00391v3","updated":"2024-08-06T23:58:07Z","published":"2023-12-31T04:14:43Z","title":"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with\n  Diffusion-Controllable Adversaries","summary":"  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail safety-critical traffic scenarios. However,\ntraditional methods for generating such scenarios often fall short in terms of\ncontrollability and realism; they also neglect the dynamics of agent\ninteractions. To address these limitations, we introduce SAFE-SIM, a novel\ndiffusion-based controllable closed-loop safety-critical simulation framework.\nOur approach yields two distinct advantages: 1) generating realistic long-tail\nsafety-critical scenarios that closely reflect real-world conditions, and 2)\nproviding controllable adversarial behavior for more comprehensive and\ninteractive evaluations. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process\nof diffusion models, which allows an adversarial agent to challenge a planner\nwith plausible maneuvers while all agents in the scene exhibit reactive and\nrealistic behaviors. Furthermore, we propose novel guidance objectives and a\npartial diffusion process that enables users to control key aspects of the\nscenarios, such as the collision type and aggressiveness of the adversarial\nagent, while maintaining the realism of the behavior. We validate our framework\nempirically using the nuScenes and nuPlan datasets across multiple planners,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that diffusion models provide a robust and versatile foundation for\nsafety-critical, interactive traffic simulation, extending their utility across\nthe broader autonomous driving landscape. Project website:\nhttps://safe-sim.github.io/.\n","authors":["Wei-Jer Chang","Francesco Pittaluga","Masayoshi Tomizuka","Wei Zhan","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2401.00391v3.pdf","comment":"Accepted by ECCV2024; Project website: https://safe-sim.github.io/"},{"id":"http://arxiv.org/abs/2408.03464v1","updated":"2024-08-06T22:39:34Z","published":"2024-08-06T22:39:34Z","title":"AI Foundation Models in Remote Sensing: A Survey","summary":"  Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing has been significantly enhanced by the advent of\nfoundation models--large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain, covering models released between June 2021 and June 2024. We categorize\nthese models based on their applications in computer vision and domain-specific\ntasks, offering insights into their architectures, pre-training datasets, and\nmethodologies. Through detailed performance comparisons, we highlight emerging\ntrends and the significant advancements achieved by these foundation models.\nAdditionally, we discuss the technical challenges, practical implications, and\nfuture research directions, addressing the need for high-quality data,\ncomputational resources, and improved model generalization. Our research also\nfinds that pre-training methods, particularly self-supervised learning\ntechniques like contrastive learning and masked autoencoders, significantly\nenhance the performance and robustness of foundation models in remote sensing\ntasks such as scene classification, object detection, and other applications.\nThis survey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.\n","authors":["Siqi Lu","Junlin Guo","James R Zimmer-Dauphinee","Jordan M Nieusma","Xiao Wang","Parker VanValkenburgh","Steven A Wernke","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2408.03464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19184v2","updated":"2024-08-06T22:36:04Z","published":"2024-07-27T05:52:31Z","title":"Enhancing Tree Type Detection in Forest Fire Risk Assessment:\n  Multi-Stage Approach and Color Encoding with Forest Fire Risk Evaluation\n  Framework for UAV Imagery","summary":"  Forest fires pose a significant threat to ecosystems, economies, and human\nhealth worldwide. Early detection and assessment of forest fires are crucial\nfor effective management and conservation efforts. Unmanned Aerial Vehicles\n(UAVs) equipped with advanced computer vision algorithms offer a promising\nsolution for forest fire detection and assessment. In this paper, we optimize\nan integrated forest fire risk assessment framework using UAVs and multi-stage\nobject detection algorithms. We introduce improvements to our previous\nframework, including the adoption of Faster R-CNN, Grid R-CNN, Sparse R-CNN,\nCascade R-CNN, Dynamic R-CNN, and Libra R-CNN detectors, and explore\noptimizations such as CBAM for attention enhancement, random erasing for\npreprocessing, and different color space representations. We evaluate these\nenhancements through extensive experimentation using aerial image footage from\nvarious regions in British Columbia, Canada. Our findings demonstrate the\neffectiveness of multi-stage detectors and optimizations in improving the\naccuracy of forest fire risk assessment. This research contributes to the\nadvancement of UAV-based forest fire detection and assessment systems,\nenhancing their efficiency and effectiveness in supporting sustainable forest\nmanagement and conservation efforts.\n","authors":["Jinda Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.19184v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12972v2","updated":"2024-08-06T22:28:25Z","published":"2024-01-23T18:58:35Z","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","summary":"  Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.\n","authors":["Apoorva Beedu","Karan Samel","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2401.12972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01532v2","updated":"2024-08-06T21:19:20Z","published":"2024-08-02T18:45:01Z","title":"Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and\n  Localization","summary":"  In the digital age, the emergence of deepfakes and synthetic media presents a\nsignificant threat to societal and political integrity. Deepfakes based on\nmulti-modal manipulation, such as audio-visual, are more realistic and pose a\ngreater threat. Current multi-modal deepfake detectors are often based on the\nattention-based fusion of heterogeneous data streams from multiple modalities.\nHowever, the heterogeneous nature of the data (such as audio and visual\nsignals) creates a distributional modality gap and poses a significant\nchallenge in effective fusion and hence multi-modal deepfake detection. In this\npaper, we propose a novel multi-modal attention framework based on recurrent\nneural networks (RNNs) that leverages contextual information for audio-visual\ndeepfake detection. The proposed approach applies attention to multi-modal\nmulti-sequence representations and learns the contributing features among them\nfor deepfake detection and localization. Thorough experimental validations on\naudio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and\nLAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison\nwith the published studies demonstrates superior performance of our approach\nwith an improved accuracy and precision by 3.47% and 2.05% in deepfake\ndetection and localization, respectively. Thus, obtaining state-of-the-art\nperformance. To facilitate reproducibility, the code and the datasets\ninformation is available at https://github.com/vcbsl/audiovisual-deepfake/.\n","authors":["Vinaya Sree Katamneni","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2408.01532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02348v3","updated":"2024-08-06T21:16:43Z","published":"2024-04-02T22:49:25Z","title":"COVID-19 Detection Based on Blood Test Parameters using Various\n  Artificial Intelligence Methods","summary":"  In 2019, the world faced a new challenge: a COVID-19 disease caused by the\nnovel coronavirus, SARS-CoV-2. The virus rapidly spread across the globe,\nleading to a high rate of mortality, which prompted health organizations to\ntake measures to control its transmission. Early disease detection is crucial\nin the treatment process, and computer-based automatic detection systems have\nbeen developed to aid in this effort. These systems often rely on artificial\nintelligence (AI) approaches such as machine learning, neural networks, fuzzy\nsystems, and deep learning to classify diseases. This study aimed to\ndifferentiate COVID-19 patients from others using self-categorizing classifiers\nand employing various AI methods. This study used two datasets: the blood test\nsamples and radiography images. The best results for the blood test samples\nobtained from San Raphael Hospital, which include two classes of individuals,\nthose with COVID-19 and those with non-COVID diseases, were achieved through\nthe use of the Ensemble method (a combination of a neural network and two\nmachines learning methods). The results showed that this approach for COVID-19\ndiagnosis is cost-effective and provides results in a shorter amount of time\nthan other methods. The proposed model achieved an accuracy of 94.09% on the\ndataset used. Secondly, the radiographic images were divided into four classes:\nnormal, viral pneumonia, ground glass opacity, and COVID-19 infection. These\nwere used for segmentation and classification. The lung lobes were extracted\nfrom the images and then categorized into specific classes. We achieved an\naccuracy of 91.1% on the image dataset. Generally, this study highlights the\npotential of AI in detecting and managing COVID-19 and underscores the\nimportance of continued research and development in this field.\n","authors":["Kavian Khanjani","Seyed Rasoul Hosseini","Hamid Taheri","Shahrzad Shashaani","Mohammad Teshnehlab"],"pdf_url":"https://arxiv.org/pdf/2404.02348v3.pdf","comment":"This paper is under review by Int. J. of Computational Science and\n  Engineering"},{"id":"http://arxiv.org/abs/2403.19782v2","updated":"2024-08-06T21:14:43Z","published":"2024-03-28T19:07:26Z","title":"ENet-21: An Optimized light CNN Structure for Lane Detection","summary":"  Lane detection for autonomous vehicles is an important concept, yet it is a\nchallenging issue of driver assistance systems in modern vehicles. The\nemergence of deep learning leads to significant progress in self-driving cars.\nConventional deep learning-based methods handle lane detection problems as a\nbinary segmentation task and determine whether a pixel belongs to a line. These\nmethods rely on the assumption of a fixed number of lanes, which does not\nalways work. This study aims to develop an optimal structure for the lane\ndetection problem, offering a promising solution for driver assistance features\nin modern vehicles by utilizing a machine learning method consisting of binary\nsegmentation and Affinity Fields that can manage varying numbers of lanes and\nlane change scenarios. In this approach, the Convolutional Neural Network\n(CNN), is selected as a feature extractor, and the final output is obtained\nthrough clustering of the semantic segmentation and Affinity Field outputs. Our\nmethod uses less complex CNN architecture than existing ones. Experiments on\nthe TuSimple dataset support the effectiveness of the proposed method.\n","authors":["Seyed Rasoul Hosseini","Hamid Taheri","Mohammad Teshnehlab"],"pdf_url":"https://arxiv.org/pdf/2403.19782v2.pdf","comment":"The paper is under review by Int. J. of Mechatronics and Automation"},{"id":"http://arxiv.org/abs/2407.19305v2","updated":"2024-08-06T21:07:17Z","published":"2024-07-27T17:27:05Z","title":"GP-VLS: A general-purpose vision language model for surgery","summary":"  Surgery requires comprehensive medical knowledge, visual assessment skills,\nand procedural expertise. While recent surgical AI models have focused on\nsolving task-specific problems, there is a need for general-purpose systems\nthat can understand surgical scenes and interact through natural language. This\npaper introduces GP-VLS, a general-purpose vision language model for surgery\nthat integrates medical and surgical knowledge with visual scene understanding.\nFor comprehensively evaluating general-purpose surgical models, we propose\nSurgiQual, which evaluates across medical and surgical knowledge benchmarks as\nwell as surgical vision-language questions. To train GP-VLS, we develop six new\ndatasets spanning medical knowledge, surgical textbooks, and vision-language\npairs for tasks like phase recognition and tool identification. We show that\nGP-VLS significantly outperforms existing open- and closed-source models on\nsurgical vision-language tasks, with 8-21% improvements in accuracy across\nSurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical\nand surgical knowledge tests compared to open-source alternatives. Overall,\nGP-VLS provides an open-source foundation for developing AI assistants to\nsupport surgeons across a wide range of tasks and scenarios. The code and data\nfor this work is publicly available at gpvls-surgery-vlm.github.io.\n","authors":["Samuel Schmidgall","Joseph Cho","Cyril Zakka","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2407.19305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03448v1","updated":"2024-08-06T21:00:02Z","published":"2024-08-06T21:00:02Z","title":"Post-Mortem Human Iris Segmentation Analysis with Deep Learning","summary":"  Iris recognition is widely used in several fields such as mobile phones,\nfinancial transactions, identification cards, airport security, international\nborder control, voter registration for living persons. However, the possibility\nof identifying deceased individuals based on their iris patterns has emerged\nrecently as a supplementary or alternative method valuable in forensic\nanalysis. Simultaneously, it poses numerous new technological challenges and\none of the most challenging among them is the image segmentation stage as\nconventional iris recognition approaches have struggled to reliably execute it.\nThis paper presents and compares Deep Learning (DL) models designed for\nsegmenting iris images collected from the deceased subjects, by training SegNet\nand DeepLabV3+ semantic segmentation methods where using VGG19, ResNet18,\nResNet50, MobileNetv2, Xception, or InceptionResNetv2 as backbones. In this\nstudy, our experiments demonstrate that our proposed method effectively learns\nand identifies specific deformations inherent in post-mortem samples and\nproviding a significant improvement in accuracy. By employing our novel method\nMobileNetv2 as the backbone of DeepLabV3+ and replacing the final layer with a\nhybrid loss function combining Boundary and Dice loss, we achieve Mean\nIntersection over Union of 95.54% on the Warsaw-BioBase-PostMortem-Iris-v1\ndataset. To the best of our knowledge, this study provides the most extensive\nevaluation of DL models for post-mortem iris segmentation.\n","authors":["Afzal Hossain","Tipu Sultan","Stephanie Schuckers"],"pdf_url":"https://arxiv.org/pdf/2408.03448v1.pdf","comment":"submitted to ijcb 2024 special session"},{"id":"http://arxiv.org/abs/2408.03433v1","updated":"2024-08-06T20:19:06Z","published":"2024-08-06T20:19:06Z","title":"Hybrid diffusion models: combining supervised and generative pretraining\n  for label-efficient fine-tuning of segmentation models","summary":"  We are considering in this paper the task of label-efficient fine-tuning of\nsegmentation models: We assume that a large labeled dataset is available and\nallows to train an accurate segmentation model in one domain, and that we have\nto adapt this model on a related domain where only a few samples are available.\nWe observe that this adaptation can be done using two distinct methods: The\nfirst method, supervised pretraining, is simply to take the model trained on\nthe first domain using classical supervised learning, and fine-tune it on the\nsecond domain with the available labeled samples. The second method is to\nperform self-supervised pretraining on the first domain using a generic pretext\ntask in order to get high-quality representations which can then be used to\ntrain a model on the second domain in a label-efficient way. We propose in this\npaper to fuse these two approaches by introducing a new pretext task, which is\nto perform simultaneously image denoising and mask prediction on the first\ndomain. We motivate this choice by showing that in the same way that an image\ndenoiser conditioned on the noise level can be considered as a generative model\nfor the unlabeled image distribution using the theory of diffusion models, a\nmodel trained using this new pretext task can be considered as a generative\nmodel for the joint distribution of images and segmentation masks under the\nassumption that the mapping from images to segmentation masks is deterministic.\nWe then empirically show on several datasets that fine-tuning a model\npretrained using this approach leads to better results than fine-tuning a\nsimilar model trained using either supervised or unsupervised pretraining only.\n","authors":["Bruno Sauvalle","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.03433v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2405.14386v2","updated":"2024-08-06T20:14:27Z","published":"2024-05-23T10:04:23Z","title":"Capsule Network Projectors are Equivariant and Invariant Learners","summary":"  Learning invariant representations has been the longstanding approach to\nself-supervised learning. However, recently progress has been made in\npreserving equivariant properties in representations, yet do so with highly\nprescribed architectures. In this work, we propose an invariant-equivariant\nself-supervised architecture that employs Capsule Networks (CapsNets) which\nhave been shown to capture equivariance with respect to novel viewpoints. We\ndemonstrate that the use of CapsNets in equivariant self-supervised\narchitectures achieves improved downstream performance on equivariant tasks\nwith higher efficiency and fewer network parameters. To accommodate the\narchitectural changes of CapsNets, we introduce a new objective function based\non entropy minimisation. This approach which we name CapsIE (Capsule Invariant\nEquivariant Network) achieves state-of-the-art performance across invariant and\nequivariant tasks on the 3DIEBench dataset compared to prior equivariant SSL\nmethods, while outperforming supervised baselines. Our results demonstrate the\nability of CapsNets to learn complex and generalised representations for\nlarge-scale, multi-task datasets compared to previous CapsNet benchmarks. Code\nis available at https://github.com/AberdeenML/CapsIE.\n","authors":["Miles Everett","Aiden Durrant","Mingjun Zhong","Georgios Leontidis"],"pdf_url":"https://arxiv.org/pdf/2405.14386v2.pdf","comment":"17 pages, 7 figures, 10 Tables; code to be released at:\n  https://github.com/AberdeenML/CapsIE V2: corrected typos, added a new Table 3\n  and additional results in Table 1 and Table 2"},{"id":"http://arxiv.org/abs/2309.15329v2","updated":"2024-08-06T19:51:49Z","published":"2023-09-27T00:20:36Z","title":"BASED: Bundle-Adjusting Surgical Endoscopic Dynamic Video Reconstruction\n  using Neural Radiance Fields","summary":"  Reconstruction of deformable scenes from endoscopic videos is important for\nmany applications such as intraoperative navigation, surgical visual\nperception, and robotic surgery. It is a foundational requirement for realizing\nautonomous robotic interventions for minimally invasive surgery. However,\nprevious approaches in this domain have been limited by their modular nature\nand are confined to specific camera and scene settings. Our work adopts the\nNeural Radiance Fields (NeRF) approach to learning 3D implicit representations\nof scenes that are both dynamic and deformable over time, and furthermore with\nunknown camera poses. We demonstrate this approach on endoscopic surgical\nscenes from robotic surgery. This work removes the constraints of known camera\nposes and overcomes the drawbacks of the state-of-the-art unstructured dynamic\nscene reconstruction technique, which relies on the static part of the scene\nfor accurate reconstruction. Through several experimental datasets, we\ndemonstrate the versatility of our proposed model to adapt to diverse camera\nand scene settings, and show its promise for both current and future robotic\nsurgical systems.\n","authors":["Shreya Saha","Zekai Liang","Shan Lin","Jingpei Lu","Michael Yip","Sainan Liu"],"pdf_url":"https://arxiv.org/pdf/2309.15329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02138v2","updated":"2024-08-06T19:27:12Z","published":"2024-08-04T20:35:33Z","title":"RICA2: Rubric-Informed, Calibrated Assessment of Actions","summary":"  The ability to quantify how well an action is carried out, also known as\naction quality assessment (AQA), has attracted recent interest in the vision\ncommunity. Unfortunately, prior methods often ignore the score rubric used by\nhuman experts and fall short of quantifying the uncertainty of the model\nprediction. To bridge the gap, we present RICA^2 - a deep probabilistic model\nthat integrates score rubric and accounts for prediction uncertainty for AQA.\nCentral to our method lies in stochastic embeddings of action steps, defined on\na graph structure that encodes the score rubric. The embeddings spread\nprobabilistic density in the latent space and allow our method to represent\nmodel uncertainty. The graph encodes the scoring criteria, based on which the\nquality scores can be decoded. We demonstrate that our method establishes new\nstate of the art on public benchmarks, including FineDiving, MTL-AQA, and\nJIGSAWS, with superior performance in score prediction and uncertainty\ncalibration. Our code is available at https://abrarmajeedi.github.io/rica2_aqa/\n","authors":["Abrar Majeedi","Viswanatha Reddy Gajjala","Satya Sai Srinath Namburi GNVV","Yin Li"],"pdf_url":"https://arxiv.org/pdf/2408.02138v2.pdf","comment":"Accepted at European Conference on Computer Vision (ECCV) 2024"},{"id":"http://arxiv.org/abs/2408.02226v2","updated":"2024-08-06T19:12:35Z","published":"2024-08-05T04:10:52Z","title":"ProCreate, Don't Reproduce! Propulsive Energy Diffusion for Creative\n  Generation","summary":"  In this paper, we propose ProCreate, a simple and easy-to-implement method to\nimprove sample diversity and creativity of diffusion-based image generative\nmodels and to prevent training data reproduction. ProCreate operates on a set\nof reference images and actively propels the generated image embedding away\nfrom the reference embeddings during the generation process. We propose FSCG-8\n(Few-Shot Creative Generation 8), a few-shot creative generation dataset on\neight different categories -- encompassing different concepts, styles, and\nsettings -- in which ProCreate achieves the highest sample diversity and\nfidelity. Furthermore, we show that ProCreate is effective at preventing\nreplicating training data in a large-scale evaluation using training text\nprompts. Code and FSCG-8 are available at\nhttps://github.com/Agentic-Learning-AI-Lab/procreate-diffusion-public. The\nproject page is available at https://procreate-diffusion.github.io.\n","authors":["Jack Lu","Ryan Teehan","Mengye Ren"],"pdf_url":"https://arxiv.org/pdf/2408.02226v2.pdf","comment":"Accepted to ECCV 2024. Project page:\n  https://procreate-diffusion.github.io"},{"id":"http://arxiv.org/abs/2408.03404v1","updated":"2024-08-06T18:55:31Z","published":"2024-08-06T18:55:31Z","title":"Set2Seq Transformer: Learning Permutation Aware Set Representations of\n  Artistic Sequences","summary":"  We propose Set2Seq Transformer, a novel sequential multiple instance\narchitecture, that learns to rank permutation aware set representations of\nsequences. First, we illustrate that learning temporal position-aware\nrepresentations of discrete timesteps can greatly improve static visual\nmultiple instance learning methods that do not regard temporality and\nconcentrate almost exclusively on visual content analysis. We further\ndemonstrate the significant advantages of end-to-end sequential multiple\ninstance learning, integrating visual content and temporal information in a\nmultimodal manner. As application we focus on fine art analysis related tasks.\nTo that end, we show that our Set2Seq Transformer can leverage visual set and\ntemporal position-aware representations for modelling visual artists' oeuvres\nfor predicting artistic success. Finally, through extensive quantitative and\nqualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual\nlearning-to-rank downstream task, we show that our Set2Seq Transformer captures\nessential temporal information improving the performance of strong static and\nsequential multiple instance learning methods for predicting artistic success.\n","authors":["Athanasios Efthymiou","Stevan Rudinac","Monika Kackovic","Nachoem Wijnberg","Marcel Worring"],"pdf_url":"https://arxiv.org/pdf/2408.03404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00448v2","updated":"2024-08-06T18:52:25Z","published":"2024-06-01T14:10:45Z","title":"Bilateral Guided Radiance Field Processing","summary":"  Neural Radiance Fields (NeRF) achieves unprecedented performance in\nsynthesizing novel view synthesis, utilizing multi-view consistency. When\ncapturing multiple inputs, image signal processing (ISP) in modern cameras will\nindependently enhance them, including exposure adjustment, color correction,\nlocal tone mapping, etc. While these processings greatly improve image quality,\nthey often break the multi-view consistency assumption, leading to \"floaters\"\nin the reconstructed radiance fields. To address this concern without\ncompromising visual aesthetics, we aim to first disentangle the enhancement by\nISP at the NeRF training stage and re-apply user-desired enhancements to the\nreconstructed radiance fields at the finishing stage. Furthermore, to make the\nre-applied enhancements consistent between novel views, we need to perform\nimaging signal processing in 3D space (i.e. \"3D ISP\"). For this goal, we adopt\nthe bilateral grid, a locally-affine model, as a generalized representation of\nISP processing. Specifically, we optimize per-view 3D bilateral grids with\nradiance fields to approximate the effects of camera pipelines for each input\nview. To achieve user-adjustable 3D finishing, we propose to learn a low-rank\n4D bilateral grid from a given single view edit, lifting photo enhancements to\nthe whole 3D scene. We demonstrate our approach can boost the visual quality of\nnovel view synthesis by effectively removing floaters and performing\nenhancements from user retouching. The source code and our data are available\nat: https://bilarfpro.github.io.\n","authors":["Yuehao Wang","Chaoyi Wang","Bingchen Gong","Tianfan Xue"],"pdf_url":"https://arxiv.org/pdf/2406.00448v2.pdf","comment":"SIGGRAPH (ACM TOG), 2024. Project page: https://bilarfpro.github.io"},{"id":"http://arxiv.org/abs/2407.19166v2","updated":"2024-08-06T18:52:04Z","published":"2024-07-27T04:37:16Z","title":"Revisit Self-supervised Depth Estimation with Local\n  Structure-from-Motion","summary":"  Both self-supervised depth estimation and Structure-from-Motion (SfM) recover\nscene depth from RGB videos. Despite sharing a similar objective, the two\napproaches are disconnected. Prior works of self-supervision backpropagate\nlosses defined within immediate neighboring frames. Instead of\nlearning-through-loss, this work proposes an alternative scheme by performing\nlocal SfM. First, with calibrated RGB or RGB-D images, we employ a depth and\ncorrespondence estimator to infer depthmaps and pair-wise correspondence maps.\nThen, a novel bundle-RANSAC-adjustment algorithm jointly optimizes camera poses\nand one depth adjustment for each depthmap. Finally, we fix camera poses and\nemploy a NeRF, however, without a neural network, for dense triangulation and\ngeometric verification. Poses, depth adjustments, and triangulated sparse\ndepths are our outputs. For the first time, we show self-supervision within $5$\nframes already benefits SoTA supervised depth and correspondence models. The\nproject page is held in the link (https://shngjz.github.io/SSfM.github.io/).\n","authors":["Shengjie Zhu","Xiaoming Liu"],"pdf_url":"https://arxiv.org/pdf/2407.19166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03393v1","updated":"2024-08-06T18:38:55Z","published":"2024-08-06T18:38:55Z","title":"Biomedical Image Segmentation: A Systematic Literature Review of Deep\n  Learning Based Object Detection Methods","summary":"  Biomedical image segmentation plays a vital role in diagnosis of diseases\nacross various organs. Deep learning-based object detection methods are\ncommonly used for such segmentation. There exists an extensive research in this\ntopic. However, there is no standard review on this topic. Existing surveys\noften lack a standardized approach or focus on broader segmentation techniques.\nIn this paper, we conducted a systematic literature review (SLR), collected and\nanalysed 148 articles that explore deep learning object detection methods for\nbiomedical image segmentation. We critically analyzed these methods, identified\nthe key challenges, and discussed the future directions. From the selected\narticles we extracted the results including the deep learning models, targeted\nimaging modalities, targeted diseases, and the metrics for the analysis of the\nmethods. The results have been presented in tabular and/or charted forms. The\nresults are presented in three major categories including two stage detection\nmodels, one stage detection models and point-based detection models. Each\narticle is individually analyzed along with its pros and cons. Finally, we\ndiscuss open challenges, potential benefits, and future research directions.\nThis SLR aims to provide the research community with a quick yet deeper\nunderstanding of these segmentation models, ultimately facilitating the\ndevelopment of more powerful solutions for biomedical image analysis.\n","authors":["Fazli Wahid","Yingliang Ma","Dawar Khan","Muhammad Aamir","Syed U. K. Bukhari"],"pdf_url":"https://arxiv.org/pdf/2408.03393v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11645v2","updated":"2024-08-06T18:31:05Z","published":"2024-06-17T15:28:35Z","title":"SeamPose: Repurposing Seams as Capacitive Sensors in a Shirt for\n  Upper-Body Pose Tracking","summary":"  Seams are areas of overlapping fabric formed by stitching two or more pieces\nof fabric together in the cut-and-sew apparel manufacturing process. In\nSeamPose, we repurposed seams as capacitive sensors in a shirt for continuous\nupper-body pose estimation. Compared to previous all-textile motion-capturing\ngarments that place the electrodes on the clothing surface, our solution\nleverages existing seams inside of a shirt by machine-sewing insulated\nconductive threads over the seams. The unique invisibilities and placements of\nthe seams afford the sensing shirt to look and wear similarly as a conventional\nshirt while providing exciting pose-tracking capabilities. To validate this\napproach, we implemented a proof-of-concept untethered shirt with 8 capacitive\nsensing seams. With a 12-participant user study, our customized deep-learning\npipeline accurately estimates the relative (to the pelvis) upper-body 3D joint\npositions with a mean per joint position error (MPJPE) of 6.0 cm. SeamPose\nrepresents a step towards unobtrusive integration of smart clothing for\neveryday pose estimation.\n","authors":["Tianhong Catherine Yu","Manru Mary Zhang","Peter He","Chi-Jung Lee","Cassidy Cheesman","Saif Mahmud","Ruidong Zhang","Franois Guimbretire","Cheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.11645v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03388v1","updated":"2024-08-06T18:18:37Z","published":"2024-08-06T18:18:37Z","title":"A Non-negative VAE:the Generalized Gamma Belief Network","summary":"  The gamma belief network (GBN), often regarded as a deep topic model, has\ndemonstrated its potential for uncovering multi-layer interpretable latent\nrepresentations in text data. Its notable capability to acquire interpretable\nlatent factors is partially attributed to sparse and non-negative\ngamma-distributed latent variables. However, the existing GBN and its\nvariations are constrained by the linear generative model, thereby limiting\ntheir expressiveness and applicability. To address this limitation, we\nintroduce the generalized gamma belief network (Generalized GBN) in this paper,\nwhich extends the original linear generative model to a more expressive\nnon-linear generative model. Since the parameters of the Generalized GBN no\nlonger possess an analytic conditional posterior, we further propose an\nupward-downward Weibull inference network to approximate the posterior\ndistribution of the latent variables. The parameters of both the generative\nmodel and the inference network are jointly trained within the variational\ninference framework. Finally, we conduct comprehensive experiments on both\nexpressivity and disentangled representation learning tasks to evaluate the\nperformance of the Generalized GBN against state-of-the-art Gaussian\nvariational autoencoders serving as baselines.\n","authors":["Zhibin Duan","Tiansheng Wen","Muyao Wang","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03361v1","updated":"2024-08-06T17:59:21Z","published":"2024-08-06T17:59:21Z","title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI","summary":"  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 285 datasets\nacross 39 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 52\\%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n","authors":["Pengcheng Chen","Jin Ye","Guoan Wang","Yanjun Li","Zhongying Deng","Wei Li","Tianbin Li","Haodong Duan","Ziyan Huang","Yanzhou Su","Benyou Wang","Shaoting Zhang","Bin Fu","Jianfei Cai","Bohan Zhuang","Eric J Seibel","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.03361v1.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.08565v2","updated":"2024-08-06T15:40:04Z","published":"2024-02-13T16:05:51Z","title":"Artificial Intelligence for Literature Reviews: Opportunities and\n  Challenges","summary":"  This manuscript presents a comprehensive review of the use of Artificial\nIntelligence (AI) in Systematic Literature Reviews (SLRs). A SLR is a rigorous\nand organised methodology that assesses and integrates previous research on a\ngiven topic. Numerous tools have been developed to assist and partially\nautomate the SLR process. The increasing role of AI in this field shows great\npotential in providing more effective support for researchers, moving towards\nthe semi-automatic creation of literature reviews. Our study focuses on how AI\ntechniques are applied in the semi-automation of SLRs, specifically in the\nscreening and extraction phases. We examine 21 leading SLR tools using a\nframework that combines 23 traditional features with 11 AI features. We also\nanalyse 11 recent tools that leverage large language models for searching the\nliterature and assisting academic writing. Finally, the paper discusses current\ntrends in the field, outlines key research challenges, and suggests directions\nfor future research.\n","authors":["Francisco Bolanos","Angelo Salatino","Francesco Osborne","Enrico Motta"],"pdf_url":"https://arxiv.org/pdf/2402.08565v2.pdf","comment":"Updated with the reviewers comments. This version is now accepted at\n  the Artificial Intelligence Review journal"},{"id":"http://arxiv.org/abs/2407.13349v5","updated":"2024-08-06T14:10:16Z","published":"2024-07-18T09:49:13Z","title":"DCNv3: Towards Next Generation Deep Cross Network for CTR Prediction","summary":"  Deep \\& Cross Network and its derivative models have become an important\nparadigm for click-through rate (CTR) prediction due to their effective balance\nbetween computational cost and performance. However, these models face four\nmajor limitations: (1) the performance of existing explicit feature interaction\nmethods is often weaker than that of implicit DNN, undermining their necessity;\n(2) many models fail to adaptively filter noise while enhancing the order of\nfeature interactions; (3) the fusion methods of most models cannot provide\nsuitable supervision signals for their different interaction procedures; (4)\nwhile most models claim to capture high-order feature interactions, they often\ndo so implicitly and non-interpretably through deep neural networks (DNN),\nwhich limits the trustworthiness of the model's predictions.\n  To address the identified limitations, this paper proposes the next\ngeneration Deep Cross Network (DCNv3) and Shallow & Deep Cross Network (SDCNv3)\nfor CTR prediction. These models ensure interpretability in feature interaction\nmodeling while exponentially increasing the order of feature interactions to\nachieve genuine Deep Crossing rather than just Deep & Cross. Additionally, we\nemploy a Self-Mask operation to filter noise and reduce the number of\nparameters in the cross network by half. In the fusion layer, we use a simple\nyet effective loss weight calculation method called Tri-BCE to provide\nappropriate supervision signals. Comprehensive experiments on six datasets\ndemonstrate the effectiveness, efficiency, and interpretability of DCNv3 and\nSDCNv3. The code, running logs, and detailed hyperparameter configurations are\navailable at: https://github.com/salmon1802/DCNv3.\n","authors":["Honghao Li","Yiwen Zhang","Yi Zhang","Hanwei Li","Lei Sang","Jieming Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.13349v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03166v1","updated":"2024-08-06T13:07:08Z","published":"2024-08-06T13:07:08Z","title":"CADRL: Category-aware Dual-agent Reinforcement Learning for Explainable\n  Recommendations over Knowledge Graphs","summary":"  Knowledge graphs (KGs) have been widely adopted to mitigate data sparsity and\naddress cold-start issues in recommender systems. While existing KGs-based\nrecommendation methods can predict user preferences and demands, they fall\nshort in generating explicit recommendation paths and lack explainability. As a\nstep beyond the above methods, recent advancements utilize reinforcement\nlearning (RL) to find suitable items for a given user via explainable\nrecommendation paths. However, the performance of these solutions is still\nlimited by the following two points. (1) Lack of ability to capture contextual\ndependencies from neighboring information. (2) The excessive reliance on short\nrecommendation paths due to efficiency concerns. To surmount these challenges,\nwe propose a category-aware dual-agent reinforcement learning (CADRL) model for\nexplainable recommendations over KGs. Specifically, our model comprises two\ncomponents: (1) a category-aware gated graph neural network that jointly\ncaptures context-aware item representations from neighboring entities and\ncategories, and (2) a dual-agent RL framework where two agents efficiently\ntraverse long paths to search for suitable items. Finally, experimental results\nshow that CADRL outperforms state-of-the-art models in terms of both\neffectiveness and efficiency on large-scale datasets.\n","authors":["Shangfei Zheng","Hongzhi Yin","Tong Chen","Xiangjie Kong","Jian Hou","Pengpeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.03166v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.09064v2","updated":"2024-08-06T11:43:53Z","published":"2024-07-12T07:34:10Z","title":"Multi-Modal Dataset Creation for Federated Learning with DICOM\n  Structured Reports","summary":"  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n","authors":["Malte Tlle","Lukas Burger","Halvar Kelm","Florian Andr","Peter Bannas","Gerhard Diller","Norbert Frey","Philipp Garthe","Stefan Gro","Anja Hennemuth","Lars Kaderali","Nina Krger","Andreas Leha","Simon Martin","Alexander Meyer","Eike Nagel","Stefan Orwat","Clemens Scherer","Moritz Seiffert","Jan Moritz Seliger","Stefan Simm","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.09064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03091v1","updated":"2024-08-06T10:45:14Z","published":"2024-08-06T10:45:14Z","title":"Modeling User Intent Beyond Trigger: Incorporating Uncertainty for\n  Trigger-Induced Recommendation","summary":"  To cater to users' desire for an immersive browsing experience, numerous\ne-commerce platforms provide various recommendation scenarios, with a focus on\nTrigger-Induced Recommendation (TIR) tasks. However, the majority of current\nTIR methods heavily rely on the trigger item to understand user intent, lacking\na higher-level exploration and exploitation of user intent (e.g., popular items\nand complementary items), which may result in an overly convergent\nunderstanding of users' short-term intent and can be detrimental to users'\nlong-term purchasing experiences. Moreover, users' short-term intent shows\nuncertainty and is affected by various factors such as browsing context and\nhistorical behaviors, which poses challenges to user intent modeling. To\naddress these challenges, we propose a novel model called Deep Uncertainty\nIntent Network (DUIN), comprising three essential modules: i) Explicit Intent\nExploit Module extracting explicit user intent using the contrastive learning\nparadigm; ii) Latent Intent Explore Module exploring latent user intent by\nleveraging the multi-view relationships between items; iii) Intent Uncertainty\nMeasurement Module offering a distributional estimation and capturing the\nuncertainty associated with user intent. Experiments on three real-world\ndatasets demonstrate the superior performance of DUIN compared to existing\nbaselines. Notably, DUIN has been deployed across all TIR scenarios in our\ne-commerce platform, with online A/B testing results conclusively validating\nits superiority.\n","authors":["Jianxing Ma","Zhibo Xiao","Luwei Yang","Hansheng Xue","Xuanzhou Liu","Wen Jiang","Wei Ning","Guannan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03091v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02354v2","updated":"2024-08-06T10:11:28Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v2.pdf","comment":"5 pages, accepted for CIKM'24"},{"id":"http://arxiv.org/abs/2408.03025v1","updated":"2024-08-06T08:16:26Z","published":"2024-08-06T08:16:26Z","title":"The Crowd in MOOCs: A Study of Learning Patterns at Scale","summary":"  The increasing availability of learning activity data in Massive Open Online\nCourses (MOOCs) enables us to conduct a large-scale analysis of learners'\nlearning behavior. In this paper, we analyze a dataset of 351 million learning\nactivities from 0.8 million unique learners enrolled in over 1.6 thousand\ncourses within two years. Specifically, we mine and identify the learning\npatterns of the crowd from both temporal and course enrollment perspectives\nleveraging mutual information theory and sequential pattern mining methods.\nFrom the temporal perspective, we find that the time intervals between\nconsecutive learning activities of learners exhibit a mix of power-law and\nperiodic cosine function distribution. By qualifying the relationship between\ncourse pairs, we observe that the most frequently co-enrolled courses usually\nfall in the same category or the same university. We demonstrate these findings\ncan facilitate manifold applications including recommendation tasks on courses.\nA simple recommendation model utilizing the course enrollment patterns is\ncompetitive to the baselines with 200$\\times$ faster training time.\n","authors":["Xin Zhou","Aixin Sun","Jie Zhang","Donghui Lin"],"pdf_url":"https://arxiv.org/pdf/2408.03025v1.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.03010v1","updated":"2024-08-06T07:45:05Z","published":"2024-08-06T07:45:05Z","title":"Fact Finder -- Enhancing Domain Expertise of Large Language Models by\n  Incorporating Knowledge Graphs","summary":"  Recent advancements in Large Language Models (LLMs) have showcased their\nproficiency in answering natural language queries. However, their effectiveness\nis hindered by limited domain-specific knowledge, raising concerns about the\nreliability of their responses. We introduce a hybrid system that augments LLMs\nwith domain-specific knowledge graphs (KGs), thereby aiming to enhance factual\ncorrectness using a KG-based retrieval approach. We focus on a medical KG to\ndemonstrate our methodology, which includes (1) pre-processing, (2) Cypher\nquery generation, (3) Cypher query processing, (4) KG retrieval, and (5)\nLLM-enhanced response generation. We evaluate our system on a curated dataset\nof 69 samples, achieving a precision of 78\\% in retrieving correct KG nodes.\nOur findings indicate that the hybrid system surpasses a standalone LLM in\naccuracy and completeness, as verified by an LLM-as-a-Judge evaluation method.\nThis positions the system as a promising tool for applications that demand\nfactual correctness and completeness, such as target identification -- a\ncritical process in pinpointing biological entities for disease treatment or\ncrop enhancement. Moreover, its intuitive search interface and ability to\nprovide accurate responses within seconds make it well-suited for\ntime-sensitive, precision-focused research contexts. We publish the source code\ntogether with the dataset and the prompt templates used.\n","authors":["Daniel Steinigen","Roman Teucher","Timm Heine Ruland","Max Rudat","Nicolas Flores-Herr","Peter Fischer","Nikola Milosevic","Christopher Schymura","Angelo Ziletti"],"pdf_url":"https://arxiv.org/pdf/2408.03010v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.01931v2","updated":"2024-08-06T05:45:06Z","published":"2024-08-04T05:07:58Z","title":"Sharpness-Aware Cross-Domain Recommendation to Cold-Start Users","summary":"  Cross-Domain Recommendation (CDR) is a promising paradigm inspired by\ntransfer learning to solve the cold-start problem in recommender systems.\nExisting state-of-the-art CDR methods train an explicit mapping function to\ntransfer the cold-start users from a data-rich source domain to a target\ndomain. However, a limitation of these methods is that the mapping function is\ntrained on overlapping users across domains, while only a small number of\noverlapping users are available for training. By visualizing the loss landscape\nof the existing CDR model, we find that training on a small number of\noverlapping users causes the model to converge to sharp minima, leading to poor\ngeneralization. Based on this observation, we leverage loss-geometry-based\nmachine learning approach and propose a novel CDR method called Sharpness-Aware\nCDR (SCDR). Our proposed method simultaneously optimizes recommendation loss\nand loss sharpness, leading to better generalization with theoretical\nguarantees. Empirical studies on real-world datasets demonstrate that SCDR\nsignificantly outperforms the other CDR models for cold-start recommendation\ntasks, while concurrently enhancing the model's robustness to adversarial\nattacks.\n","authors":["Guohang Zeng","Qian Zhang","Guangquan Zhang","Jie Lu"],"pdf_url":"https://arxiv.org/pdf/2408.01931v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02937v1","updated":"2024-08-06T03:44:06Z","published":"2024-08-06T03:44:06Z","title":"A Real-Time Adaptive Multi-Stream GPU System for Online Approximate\n  Nearest Neighborhood Search","summary":"  In recent years, Approximate Nearest Neighbor Search (ANNS) has played a\npivotal role in modern search and recommendation systems, especially in\nemerging LLM applications like Retrieval-Augmented Generation. There is a\ngrowing exploration into harnessing the parallel computing capabilities of GPUs\nto meet the substantial demands of ANNS. However, existing systems primarily\nfocus on offline scenarios, overlooking the distinct requirements of online\napplications that necessitate real-time insertion of new vectors. This\nlimitation renders such systems inefficient for real-world scenarios. Moreover,\nprevious architectures struggled to effectively support real-time insertion due\nto their reliance on serial execution streams. In this paper, we introduce a\nnovel Real-Time Adaptive Multi-Stream GPU ANNS System (RTAMS-GANNS). Our\narchitecture achieves its objectives through three key advancements: 1) We\ninitially examined the real-time insertion mechanisms in existing GPU ANNS\nsystems and discovered their reliance on repetitive copying and memory\nallocation, which significantly hinders real-time effectiveness on GPUs. As a\nsolution, we introduce a dynamic vector insertion algorithm based on memory\nblocks, which includes in-place rearrangement. 2) To enable real-time vector\ninsertion in parallel, we introduce a multi-stream parallel execution mode,\nwhich differs from existing systems that operate serially within a single\nstream. Our system utilizes a dynamic resource pool, allowing multiple streams\nto execute concurrently without additional execution blocking. 3) Through\nextensive experiments and comparisons, our approach effectively handles varying\nQPS levels across different datasets, reducing latency by up to 40%-80%. The\nproposed system has also been deployed in real-world industrial search and\nrecommendation systems, serving hundreds of millions of users daily, and has\nachieved good results.\n","authors":["Yiping Sun","Yang Shi","Jiaolong Du"],"pdf_url":"https://arxiv.org/pdf/2408.02937v1.pdf","comment":"Accepted by CIKM'24"},{"id":"http://arxiv.org/abs/2408.00247v3","updated":"2024-08-06T03:27:02Z","published":"2024-08-01T02:54:23Z","title":"Simple but Efficient: A Multi-Scenario Nearline Retrieval Framework for\n  Recommendation on Taobao","summary":"  In recommendation systems, the matching stage is becoming increasingly\ncritical, serving as the upper limit for the entire recommendation process.\nRecently, some studies have started to explore the use of multi-scenario\ninformation for recommendations, such as model-based and data-based approaches.\nHowever, the matching stage faces significant challenges due to the need for\nultra-large-scale retrieval and meeting low latency requirements. As a result,\nthe methods applied at this stage (collaborative filtering and two-tower\nmodels) are often designed to be lightweight, hindering the full utilization of\nextensive information. On the other hand, the ranking stage features the most\nsophisticated models with the strongest scoring capabilities, but due to the\nlimited screen size of mobile devices, most of the ranked results may not gain\nexposure or be displayed. In this paper, we introduce an innovative\nmulti-scenario nearline retrieval framework. It operates by harnessing ranking\nlogs from various scenarios through Flink, allowing us to incorporate finely\nranked results from other scenarios into our matching stage in near real-time.\nBesides, we propose a streaming scoring module, which selects a crucial subset\nfrom the candidate pool. Implemented on the \"Guess You Like\" (homepage of the\nTaobao APP), China's premier e-commerce platform, our method has shown\nsubstantial improvements-most notably, a 5% uptick in product transactions.\nFurthermore, the proposed approach is not only model-free but also highly\nefficient, suggesting it can be quickly implemented in diverse scenarios and\ndemonstrate promising performance.\n","authors":["Yingcai Ma","Ziyang Wang","Yuliang Yan","Jian Wu","Yuning Jiang","Longbin Li","Wen Chen","Jianhang Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00247v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00799v2","updated":"2024-08-06T03:03:16Z","published":"2024-07-22T02:27:57Z","title":"Deep Uncertainty-Based Explore for Index Construction and Retrieval in\n  Recommendation System","summary":"  In recommendation systems, the relevance and novelty of the final results are\nselected through a cascade system of Matching -> Ranking -> Strategy. The\nmatching model serves as the starting point of the pipeline and determines the\nupper bound of the subsequent stages. Balancing the relevance and novelty of\nmatching results is a crucial step in the design and optimization of\nrecommendation systems, contributing significantly to improving recommendation\nquality. However, the typical matching algorithms have not simultaneously\naddressed the relevance and novelty perfectly. One main reason is that deep\nmatching algorithms exhibit significant uncertainty when estimating items in\nthe long tail (e.g., due to insufficient training samples) items.The\nuncertainty not only affects the training of the models but also influences the\nconfidence in the index construction and beam search retrieval process of these\nmodels. This paper proposes the UICR (Uncertainty-based explore for Index\nConstruction and Retrieval) algorithm, which introduces the concept of\nuncertainty modeling in the matching stage and achieves multi-task modeling of\nmodel uncertainty and index uncertainty. The final matching results are\nobtained by combining the relevance score and uncertainty score infered by the\nmodel. Experimental results demonstrate that the UICR improves novelty without\nsacrificing relevance on realworld industrial productive environments and\nmultiple open-source datasets. Remarkably, online A/B test results of display\nadvertising in Shopee demonstrates the effectiveness of the proposed algorithm.\n","authors":["Xin Jiang","Kaiqiang Wang","Yinlong Wang","Fengchang Lv","Taiyang Peng","Shuai Yang","Xianteng Wu","Pengye Zhang","Shuo Yuan","Yifan Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.00799v2.pdf","comment":"accepted by cikm2024"},{"id":"http://arxiv.org/abs/2407.19943v2","updated":"2024-08-06T21:16:16Z","published":"2024-07-29T12:23:59Z","title":"Practical and Robust Safety Guarantees for Advanced Counterfactual\n  Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. Our\ncontributions are two-fold. First, we generalize the existing safe CLTR\napproach to make it applicable to state-of-the-art doubly robust CLTR and trust\nbias. Second, we propose a novel approach, proximal ranking policy optimization\n(PRPO), that provides safety in deployment without assumptions about user\nbehavior. PRPO removes incentives for learning ranking behavior that is too\ndissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much\nlearned models can degrade performance metrics, without relying on any specific\nuser assumptions. Our experiments show that both our novel safe doubly robust\nmethod and PRPO provide higher performance than the existing safe inverse\npropensity scoring approach. However, in unexpected circumstances, the safe\ndoubly robust approach can become unsafe and bring detrimental performance. In\ncontrast, PRPO always maintains safety, even in maximally adversarial\nsituations. By avoiding assumptions, PRPO is the first method with\nunconditional safety in deployment that translates to robust safety for\nreal-world applications.\n","authors":["Shashank Gupta","Harrie Oosterhuis","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2407.19943v2.pdf","comment":"Accepted as full paper at CIKM 2024"},{"id":"http://arxiv.org/abs/2405.12480v2","updated":"2024-08-06T20:42:53Z","published":"2024-05-21T03:50:32Z","title":"Towards Detecting and Mitigating Cognitive Bias in Spoken Conversational\n  Search","summary":"  Instruments such as eye-tracking devices have contributed to understanding\nhow users interact with screen-based search engines. However, user-system\ninteractions in audio-only channels -- as is the case for Spoken Conversational\nSearch (SCS) -- are harder to characterize, given the lack of instruments to\neffectively and precisely capture interactions. Furthermore, in this era of\ninformation overload, cognitive bias can significantly impact how we seek and\nconsume information -- especially in the context of controversial topics or\nmultiple viewpoints. This paper draws upon insights from multiple disciplines\n(including information seeking, psychology, cognitive science, and wearable\nsensors) to provoke novel conversations in the community. To this end, we\ndiscuss future opportunities and propose a framework including multimodal\ninstruments and methods for experimental designs and settings. We demonstrate\npreliminary results as an example. We also outline the challenges and offer\nsuggestions for adopting this multimodal approach, including ethical\nconsiderations, to assist future researchers and practitioners in exploring\ncognitive biases in SCS.\n","authors":["Kaixin Ji","Sachin Pathiyan Cherumanal","Johanne R. Trippas","Danula Hettiachchi","Flora D. Salim","Falk Scholer","Damiano Spina"],"pdf_url":"https://arxiv.org/pdf/2405.12480v2.pdf","comment":"Extended version of MobileHCI'24 LBW paper"},{"id":"http://arxiv.org/abs/2408.03402v1","updated":"2024-08-06T18:53:54Z","published":"2024-08-06T18:53:54Z","title":"ULLME: A Unified Framework for Large Language Model Embeddings with\n  Generation-Augmented Learning","summary":"  Large Language Models (LLMs) excel in various natural language processing\ntasks, but leveraging them for dense passage embedding remains challenging.\nThis is due to their causal attention mechanism and the misalignment between\ntheir pre-training objectives and the text ranking tasks. Despite some recent\nefforts to address these issues, existing frameworks for LLM-based text\nembeddings have been limited by their support for only a limited range of LLM\narchitectures and fine-tuning strategies, limiting their practical application\nand versatility. In this work, we introduce the Unified framework for Large\nLanguage Model Embedding (ULLME), a flexible, plug-and-play implementation that\nenables bidirectional attention across various LLMs and supports a range of\nfine-tuning strategies. We also propose Generation-augmented Representation\nLearning (GRL), a novel fine-tuning method to boost LLMs for text embedding\ntasks. GRL enforces consistency between representation-based and\ngeneration-based relevance scores, leveraging LLMs' powerful generative\nabilities for learning passage embeddings. To showcase our framework's\nflexibility and effectiveness, we release three pre-trained models from ULLME\nwith different backbone architectures, ranging from 1.5B to 8B parameters, all\nof which demonstrate strong performance on the Massive Text Embedding\nBenchmark. Our framework is publicly available at:\nhttps://github.com/nlp-uoregon/ullme. A demo video for ULLME can also be found\nat https://rb.gy/ws1ile.\n","authors":["Hieu Man","Nghia Trung Ngo","Franck Dernoncourt","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2408.03402v1.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.03323v1","updated":"2024-08-06T17:58:29Z","published":"2024-08-06T17:58:29Z","title":"ClassiFIM: An Unsupervised Method To Detect Phase Transitions","summary":"  Estimation of the Fisher Information Metric (FIM-estimation) is an important\ntask that arises in unsupervised learning of phase transitions, a problem\nproposed by physicists. This work completes the definition of the task by\ndefining rigorous evaluation metrics distMSE, distMSEPS, and distRE and\nintroduces ClassiFIM, a novel machine learning method designed to solve the\nFIM-estimation task. Unlike existing methods for unsupervised learning of phase\ntransitions, ClassiFIM directly estimates a well-defined quantity (the FIM),\nallowing it to be rigorously compared to any present and future other methods\nthat estimate the same. ClassiFIM transforms a dataset for the FIM-estimation\ntask into a dataset for an auxiliary binary classification task and involves\nselecting and training a model for the latter. We prove that the output of\nClassiFIM approaches the exact FIM in the limit of infinite dataset size and\nunder certain regularity conditions. We implement ClassiFIM on multiple\ndatasets, including datasets describing classical and quantum phase\ntransitions, and find that it achieves a good ground truth approximation with\nmodest computational resources. Furthermore, we independently implement two\nalternative state-of-the-art methods for unsupervised estimation of phase\ntransition locations on the same datasets and find that ClassiFIM predicts such\nlocations at least as well as these other methods. To emphasize the generality\nof our method, we also propose and generate the MNIST-CNN dataset, which\nconsists of the output of CNNs trained on MNIST for different hyperparameter\nchoices. Using ClassiFIM on this dataset suggests there is a phase transition\nin the distribution of image-prediction pairs for CNNs trained on MNIST,\ndemonstrating the broad scope of FIM-estimation beyond physics.\n","authors":["Victor Kasatkin","Evgeny Mozgunov","Nicholas Ezzell","Utkarsh Mishra","Itay Hen","Daniel Lidar"],"pdf_url":"https://arxiv.org/pdf/2408.03323v1.pdf","comment":"23 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.21770v2","updated":"2024-08-06T17:57:41Z","published":"2024-07-31T17:46:51Z","title":"MoMa: Efficient Early-Fusion Pre-training with Mixture of Modality-Aware\n  Experts","summary":"  We introduce MoMa, a novel modality-aware mixture-of-experts (MoE)\narchitecture designed for pre-training mixed-modal, early-fusion language\nmodels. MoMa processes images and text in arbitrary sequences by dividing\nexpert modules into modality-specific groups. These groups exclusively process\ndesignated tokens while employing learned routing within each group to maintain\nsemantically informed adaptivity. Our empirical results reveal substantial\npre-training efficiency gains through this modality-specific parameter\nallocation. Under a 1-trillion-token training budget, the MoMa 1.4B model,\nfeaturing 4 text experts and 4 image experts, achieves impressive FLOPs\nsavings: 3.7x overall, with 2.6x for text and 5.2x for image processing\ncompared to a compute-equivalent dense baseline, measured by pre-training loss.\nThis outperforms the standard expert-choice MoE with 8 mixed-modal experts,\nwhich achieves 3x overall FLOPs savings (3x for text, 2.8x for image).\nCombining MoMa with mixture-of-depths (MoD) further improves pre-training FLOPs\nsavings to 4.2x overall (text: 3.4x, image: 5.3x), although this combination\nhurts performance in causal inference due to increased sensitivity to router\naccuracy. These results demonstrate MoMa's potential to significantly advance\nthe efficiency of mixed-modal, early-fusion language model pre-training, paving\nthe way for more resource-efficient and capable multimodal AI systems.\n","authors":["Xi Victoria Lin","Akshat Shrivastava","Liang Luo","Srinivasan Iyer","Mike Lewis","Gargi Gosh","Luke Zettlemoyer","Armen Aghajanyan"],"pdf_url":"https://arxiv.org/pdf/2407.21770v2.pdf","comment":"v2 -> update related work section"},{"id":"http://arxiv.org/abs/2408.03320v1","updated":"2024-08-06T17:55:58Z","published":"2024-08-06T17:55:58Z","title":"Hedge Fund Portfolio Construction Using PolyModel Theory and\n  iTransformer","summary":"  When constructing portfolios, a key problem is that a lot of financial time\nseries data are sparse, making it challenging to apply machine learning\nmethods. Polymodel theory can solve this issue and demonstrate superiority in\nportfolio construction from various aspects. To implement the PolyModel theory\nfor constructing a hedge fund portfolio, we begin by identifying an asset pool,\nutilizing over 10,000 hedge funds for the past 29 years' data. PolyModel theory\nalso involves choosing a wide-ranging set of risk factors, which includes\nvarious financial indices, currencies, and commodity prices. This comprehensive\nselection mirrors the complexities of the real-world environment. Leveraging on\nthe PolyModel theory, we create quantitative measures such as Long-term Alpha,\nLong-term Ratio, and SVaR. We also use more classical measures like the Sharpe\nratio or Morningstar's MRAR. To enhance the performance of the constructed\nportfolio, we also employ the latest deep learning techniques (iTransformer) to\ncapture the upward trend, while efficiently controlling the downside, using all\nthe features. The iTransformer model is specifically designed to address the\nchallenges in high-dimensional time series forecasting and could largely\nimprove our strategies. More precisely, our strategies achieve better Sharpe\nratio and annualized return. The above process enables us to create multiple\nportfolio strategies aiming for high returns and low risks when compared to\nvarious benchmarks.\n","authors":["Siqiao Zhao","Zhikang Dong","Zeyu Cao","Raphael Douady"],"pdf_url":"https://arxiv.org/pdf/2408.03320v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.17403v2","updated":"2024-08-06T17:41:52Z","published":"2023-09-29T17:04:06Z","title":"Maximal Volume Matrix Cross Approximation for Image Compression and\n  Least Squares Solution","summary":"  We study the classic matrix cross approximation based on the maximal volume\nsubmatrices. Our main results consist of an improvement of the classic estimate\nfor matrix cross approximation and a greedy approach for finding the maximal\nvolume submatrices. More precisely, we present a new proof of the classic\nestimate of the inequality with an improved constant. Also, we present a family\nof greedy maximal volume algorithms to improve the computational efficiency of\nmatrix cross approximation. The proposed algorithms are shown to have\ntheoretical guarantees of convergence. Finally, we present two applications:\nimage compression and the least squares approximation of continuous functions.\nOur numerical results at the end of the paper demonstrate the effective\nperformance of our approach.\n","authors":["Kenneth Allen","Ming-Jun Lai","Zhaiming Shen"],"pdf_url":"https://arxiv.org/pdf/2309.17403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.00035v4","updated":"2024-08-06T17:36:06Z","published":"2024-01-08T12:19:46Z","title":"Robustness Assessment of a Runway Object Classifier for Safe Aircraft\n  Taxiing","summary":"  As deep neural networks (DNNs) are becoming the prominent solution for many\ncomputational problems, the aviation industry seeks to explore their potential\nin alleviating pilot workload and in improving operational safety. However, the\nuse of DNNs in this type of safety-critical applications requires a thorough\ncertification process. This need can be addressed through formal verification,\nwhich provides rigorous assurances -- e.g.,~by proving the absence of certain\nmispredictions. In this case-study paper, we demonstrate this process using an\nimage-classifier DNN currently under development at Airbus and intended for use\nduring the aircraft taxiing phase. We use formal methods to assess this DNN's\nrobustness to three common image perturbation types: noise, brightness and\ncontrast, and some of their combinations. This process entails multiple\ninvocations of the underlying verifier, which might be computationally\nexpensive; and we therefore propose a method that leverages the monotonicity of\nthese robustness properties, as well as the results of past verification\nqueries, in order to reduce the overall number of verification queries required\nby nearly 60%. Our results provide an indication of the level of robustness\nachieved by the DNN classifier under study, and indicate that it is\nconsiderably more vulnerable to noise than to brightness or contrast\nperturbations.\n","authors":["Yizhak Elboher","Raya Elsaleh","Omri Isac","Mlanie Ducoffe","Audrey Galametz","Guillaume Povda","Ryma Boumazouza","Nomie Cohen","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2402.00035v4.pdf","comment":"This is a preprint version of the paper in the proceedings of 43rd\n  Digital Avionics Systems Conference (DASC)"},{"id":"http://arxiv.org/abs/2408.03314v1","updated":"2024-08-06T17:35:05Z","published":"2024-08-06T17:35:05Z","title":"Scaling LLM Test-Time Compute Optimally can be More Effective than\n  Scaling Model Parameters","summary":"  Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.\n","authors":["Charlie Snell","Jaehoon Lee","Kelvin Xu","Aviral Kumar"],"pdf_url":"https://arxiv.org/pdf/2408.03314v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.03720v3","updated":"2024-08-06T17:26:55Z","published":"2023-10-05T17:40:09Z","title":"SteP: Stacked LLM Policies for Web Actions","summary":"  Performing tasks on the web presents fundamental challenges to large language\nmodels (LLMs), including combinatorially large open-world tasks and variations\nacross web interfaces. Simply specifying a large prompt to handle all possible\nbehaviors and states is extremely complex, and results in behavior leaks\nbetween unrelated behaviors. Decomposition to distinct policies can address\nthis challenge, but requires carefully handing off control between policies. We\npropose Stacked LLM Policies for Web Actions (SteP), an approach to dynamically\ncompose policies to solve a diverse set of web tasks. SteP defines a Markov\nDecision Process where the state is a stack of policies representing the\ncontrol state, i.e., the chain of policy calls. Unlike traditional methods that\nare restricted to static hierarchies, SteP enables dynamic control that adapts\nto the complexity of the task. We evaluate SteP against multiple baselines and\nweb environments including WebArena, MiniWoB++, and a CRM. On WebArena, SteP\nimproves (14.9\\% to 33.5\\%) over SOTA that use GPT-4 policies, while on\nMiniWob++, SteP is competitive with prior works while using significantly less\ndata. Our code and data are available at\nhttps://asappresearch.github.io/webagents-step.\n","authors":["Paloma Sodhi","S. R. K. Branavan","Yoav Artzi","Ryan McDonald"],"pdf_url":"https://arxiv.org/pdf/2310.03720v3.pdf","comment":"Accepted at Conference on Language Modeling (COLM) 2024. 30 pages, 15\n  figures"},{"id":"http://arxiv.org/abs/2408.03307v1","updated":"2024-08-06T17:16:10Z","published":"2024-08-06T17:16:10Z","title":"Pre-training and in-context learning IS Bayesian inference a la De\n  Finetti","summary":"  Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.\n","authors":["Naimeng Ye","Hanming Yang","Andrew Siah","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2408.03307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03304v1","updated":"2024-08-06T17:11:40Z","published":"2024-08-06T17:11:40Z","title":"Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks","summary":"  Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.\n","authors":["Rafael Sterzinger","Christian Stippel","Robert Sablatnig"],"pdf_url":"https://arxiv.org/pdf/2408.03304v1.pdf","comment":"16 pages, accepted at ICPR2024"},{"id":"http://arxiv.org/abs/2304.05339v2","updated":"2024-08-06T17:09:59Z","published":"2023-04-11T16:58:59Z","title":"Deep-learning Assisted Detection and Quantification of (oo)cysts of\n  Giardia and Cryptosporidium on Smartphone Microscopy Images","summary":"  The consumption of microbial-contaminated food and water is responsible for\nthe deaths of millions of people annually. Smartphone-based microscopy systems\nare portable, low-cost, and more accessible alternatives for the detection of\nGiardia and Cryptosporidium than traditional brightfield microscopes. However,\nthe images from smartphone microscopes are noisier and require manual cyst\nidentification by trained technicians, usually unavailable in resource-limited\nsettings. Automatic detection of (oo)cysts using deep-learning-based object\ndetection could offer a solution for this limitation. We evaluate the\nperformance of four state-of-the-art object detectors to detect (oo)cysts of\nGiardia and Cryptosporidium on a custom dataset that includes both smartphone\nand brightfield microscopic images from vegetable samples. Faster RCNN,\nRetinaNet, You Only Look Once (YOLOv8s), and Deformable Detection Transformer\n(Deformable DETR) deep-learning models were employed to explore their efficacy\nand limitations. Our results show that while the deep-learning models perform\nbetter with the brightfield microscopy image dataset than the smartphone\nmicroscopy image dataset, the smartphone microscopy predictions are still\ncomparable to the prediction performance of non-experts. Also, we publicly\nrelease brightfield and smartphone microscopy datasets with the benchmark\nresults for the detection of Giardia and Cryptosporidium, independently\ncaptured on reference (or standard lab setting) and vegetable samples. Our code\nand dataset are available at\nhttps://github.com/naamiinepal/smartphone_microscopy and\nhttps://doi.org/10.5281/zenodo.7813183, respectively.\n","authors":["Suprim Nakarmi","Sanam Pudasaini","Safal Thapaliya","Pratima Upretee","Retina Shrestha","Basant Giri","Bhanu Bhakta Neupane","Bishesh Khanal"],"pdf_url":"https://arxiv.org/pdf/2304.05339v2.pdf","comment":"21 pages (including supplementary information), 5 figures, 7 tables,\n  Accepted for publication at the Journal of Machine Learning for Biomedical\n  Imaging (MELBA) https://melba-journal.org/2024:014"},{"id":"http://arxiv.org/abs/2408.03290v1","updated":"2024-08-06T16:39:42Z","published":"2024-08-06T16:39:42Z","title":"SARA: Singular-Value Based Adaptive Low-Rank Adaption","summary":"  With the increasing number of parameters in large pre-trained models, LoRA as\na parameter-efficient fine-tuning(PEFT) method is widely used for not adding\ninference overhead. The LoRA method assumes that weight changes during\nfine-tuning can be approximated by low-rank matrices. However, the rank values\nneed to be manually verified to match different downstream tasks, and they\ncannot accommodate the varying importance of different layers in the model. In\nthis work, we first analyze the relationship between the performance of\ndifferent layers and their ranks using SVD. Based on this, we design the\nSingular-Value Based Adaptive Low-Rank Adaption(SARA), which adaptively finds\nthe rank during initialization by performing SVD on the pre-trained weights.\nAdditionally, we explore the Mixture-of-SARA(Mo-SARA), which significantly\nreduces the number of parameters by fine-tuning only multiple parallel sets of\nsingular values controlled by a router. Extensive experiments on various\ncomplex tasks demonstrate the simplicity and parameter efficiency of our\nmethods. They can effectively and adaptively find the most suitable rank for\neach layer of each model.\n","authors":["Jihao Gu","Shuai Chen","Zelin Wang","Yibo Zhang","Ping Gong"],"pdf_url":"https://arxiv.org/pdf/2408.03290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08871v3","updated":"2024-08-06T16:38:41Z","published":"2024-02-14T00:35:10Z","title":"Position: Topological Deep Learning is the New Frontier for Relational\n  Learning","summary":"  Topological deep learning (TDL) is a rapidly evolving field that uses\ntopological features to understand and design deep learning models. This paper\nposits that TDL is the new frontier for relational learning. TDL may complement\ngraph representation learning and geometric deep learning by incorporating\ntopological concepts, and can thus provide a natural choice for various machine\nlearning settings. To this end, this paper discusses open problems in TDL,\nranging from practical benefits to theoretical foundations. For each problem,\nit outlines potential solutions and future research opportunities. At the same\ntime, this paper serves as an invitation to the scientific community to\nactively participate in TDL research to unlock the potential of this emerging\nfield.\n","authors":["Theodore Papamarkou","Tolga Birdal","Michael Bronstein","Gunnar Carlsson","Justin Curry","Yue Gao","Mustafa Hajij","Roland Kwitt","Pietro Li","Paolo Di Lorenzo","Vasileios Maroulas","Nina Miolane","Farzana Nasrin","Karthikeyan Natesan Ramamurthy","Bastian Rieck","Simone Scardapane","Michael T. Schaub","Petar Velikovi","Bei Wang","Yusu Wang","Guo-Wei Wei","Ghada Zamzmi"],"pdf_url":"https://arxiv.org/pdf/2402.08871v3.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2408.03287v1","updated":"2024-08-06T16:35:25Z","published":"2024-08-06T16:35:25Z","title":"Malicious Internet Entity Detection Using Local Graph Inference","summary":"  Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.\n","authors":["Simon Mandlik","Tomas Pevny","Vaclav Smidl","Lukas Bajer"],"pdf_url":"https://arxiv.org/pdf/2408.03287v1.pdf","comment":"A preprint. Full publication:\n  https://ieeexplore.ieee.org/document/10418120"},{"id":"http://arxiv.org/abs/2402.00809v5","updated":"2024-08-06T16:32:38Z","published":"2024-02-01T17:45:26Z","title":"Position: Bayesian Deep Learning is Needed in the Age of Large-Scale AI","summary":"  In the current landscape of deep learning research, there is a predominant\nemphasis on achieving high predictive accuracy in supervised tasks involving\nlarge image and language datasets. However, a broader perspective reveals a\nmultitude of overlooked metrics, tasks, and data types, such as uncertainty,\nactive and continual learning, and scientific data, that demand attention.\nBayesian deep learning (BDL) constitutes a promising avenue, offering\nadvantages across these diverse settings. This paper posits that BDL can\nelevate the capabilities of deep learning. It revisits the strengths of BDL,\nacknowledges existing challenges, and highlights some exciting research avenues\naimed at addressing these obstacles. Looking ahead, the discussion focuses on\npossible ways to combine large-scale foundation models with BDL to unlock their\nfull potential.\n","authors":["Theodore Papamarkou","Maria Skoularidou","Konstantina Palla","Laurence Aitchison","Julyan Arbel","David Dunson","Maurizio Filippone","Vincent Fortuin","Philipp Hennig","Jos Miguel Hernndez-Lobato","Aliaksandr Hubin","Alexander Immer","Theofanis Karaletsos","Mohammad Emtiyaz Khan","Agustinus Kristiadi","Yingzhen Li","Stephan Mandt","Christopher Nemeth","Michael A. Osborne","Tim G. J. Rudner","David Rgamer","Yee Whye Teh","Max Welling","Andrew Gordon Wilson","Ruqi Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.00809v5.pdf","comment":"Proceedings of the 41st International Conference on Machine Learning,\n  Vienna, Austria. PMLR 235, 2024"},{"id":"http://arxiv.org/abs/2408.03281v1","updated":"2024-08-06T16:28:30Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v1.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval;Leaderboard\n  at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.03274v1","updated":"2024-08-06T16:17:51Z","published":"2024-08-06T16:17:51Z","title":"Compress and Compare: Interactively Evaluating Efficiency and Behavior\n  Across ML Model Compression Experiments","summary":"  To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.\n","authors":["Angie Boggust","Venkatesh Sivaraman","Yannick Assogba","Donghao Ren","Dominik Moritz","Fred Hohman"],"pdf_url":"https://arxiv.org/pdf/2408.03274v1.pdf","comment":"Accepted to VIS 2024"},{"id":"http://arxiv.org/abs/2312.03179v4","updated":"2024-08-06T16:11:29Z","published":"2023-12-05T23:05:36Z","title":"CaloQVAE : Simulating high-energy particle-calorimeter interactions\n  using hybrid quantum-classical generative models","summary":"  The Large Hadron Collider's high luminosity era presents major computational\nchallenges in the analysis of collision events. Large amounts of Monte Carlo\n(MC) simulation will be required to constrain the statistical uncertainties of\nthe simulated datasets below these of the experimental data. Modelling of\nhigh-energy particles propagating through the calorimeter section of the\ndetector is the most computationally intensive MC simulation task. We introduce\na technique combining recent advancements in generative models and quantum\nannealing for fast and efficient simulation of high-energy particle-calorimeter\ninteractions.\n","authors":["Sehmimul Hoque","Hao Jia","Abhishek Abhishek","Mojde Fadaie","J. Quetzalcoatl Toledo-Marn","Tiago Vale","Roger G. Melko","Maximilian Swiatlowski","Wojciech T. Fedorko"],"pdf_url":"https://arxiv.org/pdf/2312.03179v4.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2212.03559v3","updated":"2024-08-06T15:56:31Z","published":"2022-12-07T10:19:39Z","title":"GraphLearner: Graph Node Clustering with Fully Learnable Augmentation","summary":"  Contrastive deep graph clustering (CDGC) leverages the power of contrastive\nlearning to group nodes into different clusters. The quality of contrastive\nsamples is crucial for achieving better performance, making augmentation\ntechniques a key factor in the process. However, the augmentation samples in\nexisting methods are always predefined by human experiences, and agnostic from\nthe downstream task clustering, thus leading to high human resource costs and\npoor performance. To overcome these limitations, we propose a Graph Node\nClustering with Fully Learnable Augmentation, termed GraphLearner. It\nintroduces learnable augmentors to generate high-quality and task-specific\naugmented samples for CDGC. GraphLearner incorporates two learnable augmentors\nspecifically designed for capturing attribute and structural information.\nMoreover, we introduce two refinement matrices, including the high-confidence\npseudo-label matrix and the cross-view sample similarity matrix, to enhance the\nreliability of the learned affinity matrix. During the training procedure, we\nnotice the distinct optimization goals for training learnable augmentors and\ncontrastive learning networks. In other words, we should both guarantee the\nconsistency of the embeddings as well as the diversity of the augmented\nsamples. To address this challenge, we propose an adversarial learning\nmechanism within our method. Besides, we leverage a two-stage training strategy\nto refine the high-confidence matrices. Extensive experimental results on six\nbenchmark datasets validate the effectiveness of GraphLearner.The code and\nappendix of GraphLearner are available at\nhttps://github.com/xihongyang1999/GraphLearner on Github.\n","authors":["Xihong Yang","Erxue Min","Ke Liang","Yue Liu","Siwei Wang","Sihang Zhou","Huijun Wu","Xinwang Liu","En Zhu"],"pdf_url":"https://arxiv.org/pdf/2212.03559v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.10799v2","updated":"2024-08-06T15:33:20Z","published":"2024-05-17T14:10:24Z","title":"Training Compute Thresholds: Features and Functions in AI Regulation","summary":"  Regulators in the US and EU are using thresholds based on training\ncompute--the number of computational operations used in training--to identify\ngeneral-purpose artificial intelligence (GPAI) models that may pose risks of\nlarge-scale societal harm. We argue that training compute currently is the most\nsuitable metric to identify GPAI models that deserve regulatory oversight and\nfurther scrutiny. Training compute correlates with model capabilities and\nrisks, is quantifiable, can be measured early in the AI lifecycle, and can be\nverified by external actors, among other advantageous features. These features\nmake compute thresholds considerably more suitable than other proposed metrics\nto serve as an initial filter to trigger additional regulatory requirements and\nscrutiny. However, training compute is an imperfect proxy for risk. As such,\ncompute thresholds should not be used in isolation to determine appropriate\nmitigation measures. Instead, they should be used to detect potentially risky\nGPAI models that warrant regulatory oversight, such as through notification\nrequirements, and further scrutiny, such as via model evaluations and risk\nassessments, the results of which may inform which mitigation measures are\nappropriate. In fact, this appears largely consistent with how compute\nthresholds are used today. As GPAI technology and market structures evolve,\nregulators should update compute thresholds and complement them with other\nmetrics into regulatory review processes.\n","authors":["Lennart Heim","Leonie Koessler"],"pdf_url":"https://arxiv.org/pdf/2405.10799v2.pdf","comment":"v2: Major revision of earlier working paper"},{"id":"http://arxiv.org/abs/2405.06605v3","updated":"2024-08-06T15:20:00Z","published":"2024-05-10T17:12:48Z","title":"Calo-VQ: Vector-Quantized Two-Stage Generative Model in Calorimeter\n  Simulation","summary":"  We introduce a novel machine learning method developed for the fast\nsimulation of calorimeter detector response, adapting vector-quantized\nvariational autoencoder (VQ-VAE). Our model adopts a two-stage generation\nstrategy: initially compressing geometry-aware calorimeter data into a discrete\nlatent space, followed by the application of a sequence model to learn and\ngenerate the latent tokens. Extensive experimentation on the Calo-challenge\ndataset underscores the efficiency of our approach, showcasing a remarkable\nimprovement in the generation speed compared with conventional method by a\nfactor of 2000. Remarkably, our model achieves the generation of calorimeter\nshowers within milliseconds. Furthermore, comprehensive quantitative\nevaluations across various metrics are performed to validate physics\nperformance of generation.\n","authors":["Qibin Liu","Chase Shimmin","Xiulong Liu","Eli Shlizerman","Shu Li","Shih-Chieh Hsu"],"pdf_url":"https://arxiv.org/pdf/2405.06605v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.08354v3","updated":"2024-08-06T15:14:36Z","published":"2023-04-17T15:16:10Z","title":"Tool Learning with Foundation Models","summary":"  Humans possess an extraordinary ability to create and utilize tools, allowing\nthem to overcome physical limitations and explore new frontiers. With the\nadvent of foundation models, AI systems have the potential to be equally adept\nin tool use as humans. This paradigm, i.e., tool learning with foundation\nmodels, combines the strengths of specialized tools and foundation models to\nachieve enhanced accuracy, efficiency, and automation in problem-solving.\nDespite its immense potential, there is still a lack of a comprehensive\nunderstanding of key challenges, opportunities, and future endeavors in this\nfield. To this end, we present a systematic investigation of tool learning in\nthis paper. We first introduce the background of tool learning, including its\ncognitive origins, the paradigm shift of foundation models, and the\ncomplementary roles of tools and models. Then we recapitulate existing tool\nlearning research into tool-augmented and tool-oriented learning. We formulate\na general tool learning framework: starting from understanding the user\ninstruction, models should learn to decompose a complex task into several\nsubtasks, dynamically adjust their plan through reasoning, and effectively\nconquer each sub-task by selecting appropriate tools. We also discuss how to\ntrain models for improved tool-use capabilities and facilitate the\ngeneralization in tool learning. Considering the lack of a systematic tool\nlearning evaluation in prior works, we experiment with 18 representative tools\nand show the potential of current foundation models in skillfully utilizing\ntools. Finally, we discuss several open problems that require further\ninvestigation for tool learning. In general, we hope this paper could inspire\nfuture research in integrating tools with foundation models.\n","authors":["Yujia Qin","Shengding Hu","Yankai Lin","Weize Chen","Ning Ding","Ganqu Cui","Zheni Zeng","Yufei Huang","Chaojun Xiao","Chi Han","Yi Ren Fung","Yusheng Su","Huadong Wang","Cheng Qian","Runchu Tian","Kunlun Zhu","Shihao Liang","Xingyu Shen","Bokai Xu","Zhen Zhang","Yining Ye","Bowen Li","Ziwei Tang","Jing Yi","Yuzhang Zhu","Zhenning Dai","Lan Yan","Xin Cong","Yaxi Lu","Weilin Zhao","Yuxiang Huang","Junxi Yan","Xu Han","Xian Sun","Dahai Li","Jason Phang","Cheng Yang","Tongshuang Wu","Heng Ji","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2304.08354v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17216v2","updated":"2024-08-06T15:08:44Z","published":"2024-07-24T12:15:59Z","title":"An Adaptive Second-order Method for a Class of Nonconvex Nonsmooth\n  Composite Optimization","summary":"  This paper explores a specific type of nonconvex sparsity-promoting\nregularization problems, namely those involving $\\ell_p$-norm regularization,\nin conjunction with a twice continuously differentiable loss function. We\npropose a novel second-order algorithm designed to effectively address this\nclass of challenging nonconvex and nonsmooth problems, showcasing several\ninnovative features: (i) The use of an alternating strategy to solve a\nreweighted $\\ell_1$ regularized subproblem and the subspace approximate Newton\nstep. (ii) The reweighted $\\ell_1$ regularized subproblem relies on a convex\napproximation to the nonconvex regularization term, enabling a closed-form\nsolution characterized by the soft-thresholding operator. This feature allows\nour method to be applied to various nonconvex regularization problems. (iii)\nOur algorithm ensures that the iterates maintain their sign values and that\nnonzero components are kept away from 0 for a sufficient number of iterations,\neventually transitioning to a perturbed Newton method. (iv) We provide\ntheoretical guarantees of global convergence, local superlinear convergence in\nthe presence of the Kurdyka-\\L ojasiewicz (KL) property, and local quadratic\nconvergence when employing the exact Newton step in our algorithm. We also\nshowcase the effectiveness of our approach through experiments on a diverse set\nof model prediction problems.\n","authors":["Hao Wang","Xiangyu Yang","Yichen Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.17216v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16495v2","updated":"2024-08-06T15:03:50Z","published":"2024-04-25T10:40:49Z","title":"T-Explainer: A Model-Agnostic Explainability Framework Based on\n  Gradients","summary":"  The development of machine learning applications has increased significantly\nin recent years, motivated by the remarkable ability of learning-powered\nsystems to discover and generalize intricate patterns hidden in massive\ndatasets. Modern learning models, while powerful, often have a level of\ncomplexity that renders them opaque black boxes, resulting in a notable lack of\ntransparency that hinders our ability to decipher their reasoning. Opacity\nchallenges the interpretability and practical application of machine learning,\nespecially in critical domains where understanding the underlying reasons is\nessential for informed decision-making. Explainable Artificial Intelligence\n(XAI) rises to address that challenge, unraveling the complexity of black boxes\nby providing elucidating explanations. Among the various XAI approaches,\nfeature attribution/importance stands out for its capacity to delineate the\nsignificance of input features in the prediction process. However, most\nexisting attribution methods have limitations, such as instability, when\ndivergent explanations may result from similar or even the same instance. This\nwork introduces T-Explainer, a novel local additive attribution explainer based\non Taylor expansion. It has desirable properties, such as local accuracy and\nconsistency, making T-Explainer stable over multiple runs. We demonstrate\nT-Explainer's effectiveness in quantitative benchmark experiments against\nwell-known attribution methods. Additionally, we provide several tools to\nevaluate and visualize explanations, turning T-Explainer into a comprehensive\nXAI framework.\n","authors":["Evandro S. Ortigossa","Fbio F. Dias","Brian Barr","Claudio T. Silva","Luis Gustavo Nonato"],"pdf_url":"https://arxiv.org/pdf/2404.16495v2.pdf","comment":"16 pages -- 2 figures and 20 tables -- Under review. This work has\n  been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2407.13605v2","updated":"2024-08-06T14:55:04Z","published":"2024-07-18T15:44:23Z","title":"Physics-guided Active Sample Reweighting for Urban Flow Prediction","summary":"  Urban flow prediction is a spatio-temporal modeling task that estimates the\nthroughput of transportation services like buses, taxis, and ride-sharing,\nwhere data-driven models have become the most popular solution in the past\ndecade. Meanwhile, the implicitly learned mapping between historical\nobservations to the prediction targets tend to over-simplify the dynamics of\nreal-world urban flows, leading to suboptimal predictions. Some recent\nspatio-temporal prediction solutions bring remedies with the notion of\nphysics-guided machine learning (PGML), which describes spatio-temporal data\nwith nuanced and principled physics laws, thus enhancing both the prediction\naccuracy and interpretability. However, these spatio-temporal PGML methods are\nbuilt upon a strong assumption that the observed data fully conforms to the\ndifferential equations that define the physical system, which can quickly\nbecome ill-posed in urban flow prediction tasks. The observed urban flow data,\nespecially when sliced into time-dependent snapshots to facilitate predictions,\nis typically incomplete and sparse, and prone to inherent noise incurred in the\ncollection process. As a result, such physical inconsistency between the data\nand PGML model significantly limits the predictive power and robustness of the\nsolution. Moreover, due to the interval-based predictions and intermittent\nnature of data filing in many transportation services, the instantaneous\ndynamics of urban flows can hardly be captured, rendering differential\nequation-based continuous modeling a loose fit for this setting. To overcome\nthe challenges, we develop a discretized physics-guided network (PN), and\npropose a data-aware framework Physics-guided Active Sample Reweighting\n(P-GASR) to enhance PN. Experimental results in four real-world datasets\ndemonstrate that our method achieves state-of-the-art performance with a\ndemonstrable improvement in robustness.\n","authors":["Wei Jiang","Tong Chen","Guanhua Ye","Wentao Zhang","Lizhen Cui","Zi Huang","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2407.13605v2.pdf","comment":"This paper is accepted by Proceedings of the 33nd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2408.03236v1","updated":"2024-08-06T14:48:34Z","published":"2024-08-06T14:48:34Z","title":"Analysis of Partially-Calibrated Sparse Subarrays for Direction Finding\n  with Extended Degrees of Freedom","summary":"  This paper investigates the problem of direction-of-arrival (DOA) estimation\nusing multiple partially-calibrated sparse subarrays. In particular, we present\nthe Generalized Coarray Multiple Signal Classification (GCA-MUSIC) DOA\nestimation algorithm to scenarios with partially-calibrated sparse subarrays.\nThe proposed GCA-MUSIC algorithm exploits the difference coarray for each\nsubarray, followed by a specific pseudo-spectrum merging rule that is based on\nthe intersection of the signal subspaces associated to each subarray. This rule\nassumes that there is no a priori knowledge about the cross-covariance between\nsubarrays. In that way, only the second-order statistics of each subarray are\nused to estimate the directions with increased degrees of freedom, i.e., the\nestimation procedure preserves the coarray Multiple Signal Classification and\nsparse arrays properties to estimate more sources than the number of physical\nsensors in each subarray. Numerical simulations show that the proposed\nGCA-MUSIC has better performance than other similar strategies.\n","authors":["W. S. Leite","R. C. de Lamare"],"pdf_url":"https://arxiv.org/pdf/2408.03236v1.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03223v1","updated":"2024-08-06T14:36:29Z","published":"2024-08-06T14:36:29Z","title":"Don't Think It Twice: Exploit Shift Invariance for Efficient Online\n  Streaming Inference of CNNs","summary":"  Deep learning time-series processing often relies on convolutional neural\nnetworks with overlapping windows. This overlap allows the network to produce\nan output faster than the window length. However, it introduces additional\ncomputations. This work explores the potential to optimize computational\nefficiency during inference by exploiting convolution's shift-invariance\nproperties to skip the calculation of layer activations between successive\noverlapping windows. Although convolutions are shift-invariant, zero-padding\nand pooling operations, widely used in such networks, are not efficient and\ncomplicate efficient streaming inference. We introduce StreamiNNC, a strategy\nto deploy Convolutional Neural Networks for online streaming inference. We\nexplore the adverse effects of zero padding and pooling on the accuracy of\nstreaming inference, deriving theoretical error upper bounds for pooling during\nstreaming. We address these limitations by proposing signal padding and pooling\nalignment and provide guidelines for designing and deploying models for\nStreamiNNC. We validate our method in simulated data and on three real-world\nbiomedical signal processing applications. StreamiNNC achieves a low deviation\nbetween streaming output and normal inference for all three networks (2.03 -\n3.55% NRMSE). This work demonstrates that it is possible to linearly speed up\nthe inference of streaming CNNs processing overlapping windows, negating the\nadditional computation typically incurred by overlapping windows.\n","authors":["Christodoulos Kechris","Jonathan Dan","Jose Miranda","David Atienza"],"pdf_url":"https://arxiv.org/pdf/2408.03223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09146v4","updated":"2024-08-06T14:30:52Z","published":"2024-02-14T12:55:28Z","title":"ResQuNNs:Towards Enabling Deep Learning in Quantum Convolution Neural\n  Networks","summary":"  In this paper, we present a novel framework for enhancing the performance of\nQuanvolutional Neural Networks (QuNNs) by introducing trainable quanvolutional\nlayers and addressing the critical challenges associated with them. Traditional\nquanvolutional layers, although beneficial for feature extraction, have largely\nbeen static, offering limited adaptability. Unlike state-of-the-art, our\nresearch overcomes this limitation by enabling training within these layers,\nsignificantly increasing the flexibility and potential of QuNNs. However, the\nintroduction of multiple trainable quanvolutional layers induces complexities\nin gradient-based optimization, primarily due to the difficulty in accessing\ngradients across these layers. To resolve this, we propose a novel\narchitecture, Residual Quanvolutional Neural Networks (ResQuNNs), leveraging\nthe concept of residual learning, which facilitates the flow of gradients by\nadding skip connections between layers. By inserting residual blocks between\nquanvolutional layers, we ensure enhanced gradient access throughout the\nnetwork, leading to improved training performance. Moreover, we provide\nempirical evidence on the strategic placement of these residual blocks within\nQuNNs. Through extensive experimentation, we identify an efficient\nconfiguration of residual blocks, which enables gradients across all the layers\nin the network that eventually results in efficient training. Our findings\nsuggest that the precise location of residual blocks plays a crucial role in\nmaximizing the performance gains in QuNNs. Our results mark a substantial step\nforward in the evolution of quantum deep learning, offering new avenues for\nboth theoretical development and practical quantum computing applications.\n","authors":["Muhammad Kashif","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2402.09146v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05530v2","updated":"2024-08-06T14:30:31Z","published":"2024-04-08T13:59:02Z","title":"Best-of-Venom: Attacking RLHF by Injecting Poisoned Preference Data","summary":"  Reinforcement Learning from Human Feedback (RLHF) is a popular method for\naligning Language Models (LM) with human values and preferences. RLHF requires\na large number of preference pairs as training data, which are often used in\nboth the Supervised Fine-Tuning and Reward Model training and therefore\npublicly available datasets are commonly used. In this work, we study to what\nextent a malicious actor can manipulate the LMs generations by poisoning the\npreferences, i.e., injecting poisonous preference pairs into these datasets and\nthe RLHF training process. We propose strategies to build poisonous preference\npairs and test their performance by poisoning two widely used preference\ndatasets. Our results show that preference poisoning is highly effective:\ninjecting a small amount of poisonous data (1-5\\% of the original dataset), we\ncan effectively manipulate the LM to generate a target entity in a target\nsentiment (positive or negative). The findings from our experiments also shed\nlight on strategies to defend against the preference poisoning attack.\n","authors":["Tim Baumgrtner","Yang Gao","Dana Alon","Donald Metzler"],"pdf_url":"https://arxiv.org/pdf/2404.05530v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03220v1","updated":"2024-08-06T14:26:09Z","published":"2024-08-06T14:26:09Z","title":"Masked Random Noise for Communication Efficient Federaetd Learning","summary":"  Federated learning is a promising distributed training paradigm that\neffectively safeguards data privacy. However, it may involve significant\ncommunication costs, which hinders training efficiency. In this paper, we aim\nto enhance communication efficiency from a new perspective. Specifically, we\nrequest the distributed clients to find optimal model updates relative to\nglobal model parameters within predefined random noise. For this purpose, we\npropose Federated Masked Random Noise (FedMRN), a novel framework that enables\nclients to learn a 1-bit mask for each model parameter and apply masked random\nnoise (i.e., the Hadamard product of random noise and masks) to represent model\nupdates. To make FedMRN feasible, we propose an advanced mask training\nstrategy, called progressive stochastic masking (PSM). After local training,\neach client only need to transmit local masks and a random seed to the server.\nAdditionally, we provide theoretical guarantees for the convergence of FedMRN\nunder both strongly convex and non-convex assumptions. Extensive experiments\nare conducted on four popular datasets. The results show that FedMRN exhibits\nsuperior convergence speed and test accuracy compared to relevant baselines,\nwhile attaining a similar level of accuracy as FedAvg.\n","authors":["Shiwei Li","Yingyi Cheng","Haozhao Wang","Xing Tang","Shijie Xu","Weihong Luo","Yuhua Li","Dugang Liu","Xiuqiang He","and Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03220v1.pdf","comment":"Accepted by MM 2024"},{"id":"http://arxiv.org/abs/2408.03219v1","updated":"2024-08-06T14:25:23Z","published":"2024-08-06T14:25:23Z","title":"Learning to Learn without Forgetting using Attention","summary":"  Continual learning (CL) refers to the ability to continually learn over time\nby accommodating new knowledge while retaining previously learned experience.\nWhile this concept is inherent in human learning, current machine learning\nmethods are highly prone to overwrite previously learned patterns and thus\nforget past experience. Instead, model parameters should be updated selectively\nand carefully, avoiding unnecessary forgetting while optimally leveraging\npreviously learned patterns to accelerate future learning. Since hand-crafting\neffective update mechanisms is difficult, we propose meta-learning a\ntransformer-based optimizer to enhance CL. This meta-learned optimizer uses\nattention to learn the complex relationships between model parameters across a\nstream of tasks, and is designed to generate effective weight updates for the\ncurrent task while preventing catastrophic forgetting on previously encountered\ntasks. Evaluations on benchmark datasets like SplitMNIST, RotatedMNIST, and\nSplitCIFAR-100 affirm the efficacy of the proposed approach in terms of both\nforward and backward transfer, even on small sets of labeled data, highlighting\nthe advantages of integrating a meta-learned optimizer within the continual\nlearning framework.\n","authors":["Anna Vettoruzzo","Joaquin Vanschoren","Mohamed-Rafik Bouguelia","Thorsteinn Rgnvaldsson"],"pdf_url":"https://arxiv.org/pdf/2408.03219v1.pdf","comment":"Published at 3rd Conference on Lifelong Learning Agents (CoLLAs),\n  2024"},{"id":"http://arxiv.org/abs/2408.03215v1","updated":"2024-08-06T14:19:06Z","published":"2024-08-06T14:19:06Z","title":"FedBAT: Communication-Efficient Federated Learning via Learnable\n  Binarization","summary":"  Federated learning is a promising distributed machine learning paradigm that\ncan effectively exploit large-scale data without exposing users' privacy.\nHowever, it may incur significant communication overhead, thereby potentially\nimpairing the training efficiency. To address this challenge, numerous studies\nsuggest binarizing the model updates. Nonetheless, traditional methods usually\nbinarize model updates in a post-training manner, resulting in significant\napproximation errors and consequent degradation in model accuracy. To this end,\nwe propose Federated Binarization-Aware Training (FedBAT), a novel framework\nthat directly learns binary model updates during the local training process,\nthus inherently reducing the approximation errors. FedBAT incorporates an\ninnovative binarization operator, along with meticulously designed derivatives\nto facilitate efficient learning. In addition, we establish theoretical\nguarantees regarding the convergence of FedBAT. Extensive experiments are\nconducted on four popular datasets. The results show that FedBAT significantly\naccelerates the convergence and exceeds the accuracy of baselines by up to 9\\%,\neven surpassing that of FedAvg in some cases.\n","authors":["Shiwei Li","Wenchao Xu","Haozhao Wang","Xing Tang","Yining Qi","Shijie Xu","Weihong Luo","Yuhua Li","Xiuqiang He","Ruixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03215v1.pdf","comment":"Accepted by ICML 2024"},{"id":"http://arxiv.org/abs/2405.11647v3","updated":"2024-08-06T14:12:26Z","published":"2024-05-19T18:57:25Z","title":"Hummer: Towards Limited Competitive Preference Dataset","summary":"  Preference datasets are essential for incorporating human preferences into\npre-trained language models, playing a key role in the success of Reinforcement\nLearning from Human Feedback. However, these datasets often demonstrate\nconflicting alignment objectives, leading to increased vulnerability to\njailbreak attacks and challenges in adapting downstream tasks to prioritize\nspecific alignment objectives without negatively impacting others. In this\nwork, we introduce a novel statistical metric, Alignment Dimension Conflict, to\nquantify the degree of conflict within preference datasets. We then present\n\\texttt{Hummer} and its fine-grained variant, \\texttt{Hummer-F}, as innovative\npairwise preference datasets with reduced-conflict alignment objectives.\n\\texttt{Hummer} is built based on UltraFeedback and is enhanced by AI feedback\nfrom GPT-4, marking as the first preference dataset aimed at reducing the\ncompetition between alignment objectives. Furthermore, we develop reward\nmodels, HummerRM and HummerRM-F, which employ a hybrid sampling approach to\nbalance diverse alignment objectives effectively. This sampling method\npositions HummerRM as an ideal model for domain-specific further fine-tuning\nand reducing vulnerabilities to attacks.\n","authors":["Li Jiang","Yusen Wu","Junwu Xiong","Jingqing Ruan","Yichuan Ding","Qingpei Guo","Zujie Wen","Jun Zhou","Xiaotie Deng"],"pdf_url":"https://arxiv.org/pdf/2405.11647v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03199v1","updated":"2024-08-06T13:58:37Z","published":"2024-08-06T13:58:37Z","title":"Convergence Conditions for Stochastic Line Search Based Optimization of\n  Over-parametrized Models","summary":"  In this paper, we deal with algorithms to solve the finite-sum problems\nrelated to fitting over-parametrized models, that typically satisfy the\ninterpolation condition. In particular, we focus on approaches based on\nstochastic line searches and employing general search directions. We define\nconditions on the sequence of search directions that guarantee finite\ntermination and bounds for the backtracking procedure. Moreover, we shed light\non the additional property of directions needed to prove fast (linear)\nconvergence of the general class of algorithms when applied to PL functions in\nthe interpolation regime. From the point of view of algorithms design, the\nproposed analysis identifies safeguarding conditions that could be employed in\nrelevant algorithmic framework. In particular, it could be of interest to\nintegrate stochastic line searches within momentum, conjugate gradient or\nadaptive preconditioning methods.\n","authors":["Matteo Lapucci","Davide Pucci"],"pdf_url":"https://arxiv.org/pdf/2408.03199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03195v1","updated":"2024-08-06T13:55:51Z","published":"2024-08-06T13:55:51Z","title":"RELIEF: Reinforcement Learning Empowered Graph Feature Prompt Tuning","summary":"  The advent of the \"pre-train, prompt\" paradigm has recently extended its\ngeneralization ability and data efficiency to graph representation learning,\nfollowing its achievements in Natural Language Processing (NLP). Initial graph\nprompt tuning approaches tailored specialized prompting functions for Graph\nNeural Network (GNN) models pre-trained with specific strategies, such as edge\nprediction, thus limiting their applicability. In contrast, another pioneering\nline of research has explored universal prompting via adding prompts to the\ninput graph's feature space, thereby removing the reliance on specific\npre-training strategies. However, the necessity to add feature prompts to all\nnodes remains an open question. Motivated by findings from prompt tuning\nresearch in the NLP domain, which suggest that highly capable pre-trained\nmodels need less conditioning signal to achieve desired behaviors, we advocate\nfor strategically incorporating necessary and lightweight feature prompts to\ncertain graph nodes to enhance downstream task performance. This introduces a\ncombinatorial optimization problem, requiring a policy to decide 1) which nodes\nto prompt and 2) what specific feature prompts to attach. We then address the\nproblem by framing the prompt incorporation process as a sequential\ndecision-making problem and propose our method, RELIEF, which employs\nReinforcement Learning (RL) to optimize it. At each step, the RL agent selects\na node (discrete action) and determines the prompt content (continuous action),\naiming to maximize cumulative performance gain. Extensive experiments on graph\nand node-level tasks with various pre-training strategies in few-shot scenarios\ndemonstrate that our RELIEF outperforms fine-tuning and other prompt-based\napproaches in classification performance and data efficiency.\n","authors":["Jiapeng Zhu","Zichen Ding","Jianxiang Yu","Jiaqi Tan","Xiang Li","Weining Qian"],"pdf_url":"https://arxiv.org/pdf/2408.03195v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07186v5","updated":"2024-08-06T13:47:50Z","published":"2023-12-12T11:48:56Z","title":"Towards Optimal Sobolev Norm Rates for the Vector-Valued Regularized\n  Least-Squares Algorithm","summary":"  We present the first optimal rates for infinite-dimensional vector-valued\nridge regression on a continuous scale of norms that interpolate between $L_2$\nand the hypothesis space, which we consider as a vector-valued reproducing\nkernel Hilbert space. These rates allow to treat the misspecified case in which\nthe true regression function is not contained in the hypothesis space. We\ncombine standard assumptions on the capacity of the hypothesis space with a\nnovel tensor product construction of vector-valued interpolation spaces in\norder to characterize the smoothness of the regression function. Our upper\nbound not only attains the same rate as real-valued kernel ridge regression,\nbut also removes the assumption that the target regression function is bounded.\nFor the lower bound, we reduce the problem to the scalar setting using a\nprojection argument. We show that these rates are optimal in most cases and\nindependent of the dimension of the output space. We illustrate our results for\nthe special case of vector-valued Sobolev spaces.\n","authors":["Zhu Li","Dimitri Meunier","Mattes Mollenhauer","Arthur Gretton"],"pdf_url":"https://arxiv.org/pdf/2312.07186v5.pdf","comment":"Published JMLR version. arXiv admin note: text overlap with\n  arXiv:2208.01711"},{"id":"http://arxiv.org/abs/2311.15890v3","updated":"2024-08-06T13:47:24Z","published":"2023-11-27T14:56:47Z","title":"Stability-Informed Initialization of Neural Ordinary Differential\n  Equations","summary":"  This paper addresses the training of Neural Ordinary Differential Equations\n(neural ODEs), and in particular explores the interplay between numerical\nintegration techniques, stability regions, step size, and initialization\ntechniques. It is shown how the choice of integration technique implicitly\nregularizes the learned model, and how the solver's corresponding stability\nregion affects training and prediction performance. From this analysis, a\nstability-informed parameter initialization technique is introduced. The\neffectiveness of the initialization method is displayed across several learning\nbenchmarks and industrial applications.\n","authors":["Theodor Westny","Arman Mohammadi","Daniel Jung","Erik Frisk"],"pdf_url":"https://arxiv.org/pdf/2311.15890v3.pdf","comment":"In Proceedings of the 41 st International Conference on Machine\n  Learning"},{"id":"http://arxiv.org/abs/2408.03178v1","updated":"2024-08-06T13:22:51Z","published":"2024-08-06T13:22:51Z","title":"An Object is Worth 64x64 Pixels: Generating 3D Object via Image\n  Diffusion","summary":"  We introduce a new approach for generating realistic 3D models with UV maps\nthrough a representation termed \"Object Images.\" This approach encapsulates\nsurface geometry, appearance, and patch structures within a 64x64 pixel image,\neffectively converting complex 3D shapes into a more manageable 2D format. By\ndoing so, we address the challenges of both geometric and semantic irregularity\ninherent in polygonal meshes. This method allows us to use image generation\nmodels, such as Diffusion Transformers, directly for 3D shape generation.\nEvaluated on the ABO dataset, our generated shapes with patch structures\nachieve point cloud FID comparable to recent 3D generative models, while\nnaturally supporting PBR material generation.\n","authors":["Xingguang Yan","Han-Hung Lee","Ziyu Wan","Angel X. Chang"],"pdf_url":"https://arxiv.org/pdf/2408.03178v1.pdf","comment":"Project Page: https://omages.github.io/"},{"id":"http://arxiv.org/abs/2408.03172v1","updated":"2024-08-06T13:16:16Z","published":"2024-08-06T13:16:16Z","title":"Leveraging Parameter Efficient Training Methods for Low Resource Text\n  Classification: A Case Study in Marathi","summary":"  With the surge in digital content in low-resource languages, there is an\nescalating demand for advanced Natural Language Processing (NLP) techniques\ntailored to these languages. BERT (Bidirectional Encoder Representations from\nTransformers), serving as the foundational framework for numerous NLP\narchitectures and language models, is increasingly employed for the development\nof low-resource NLP models. Parameter Efficient Fine-Tuning (PEFT) is a method\nfor fine-tuning Large Language Models (LLMs) and reducing the training\nparameters to some extent to decrease the computational costs needed for\ntraining the model and achieve results comparable to a fully fine-tuned model.\nIn this work, we present a study of PEFT methods for the Indic low-resource\nlanguage Marathi. We conduct a comprehensive analysis of PEFT methods applied\nto various monolingual and multilingual Marathi BERT models. These approaches\nare evaluated on prominent text classification datasets like MahaSent,\nMahaHate, and MahaNews. The incorporation of PEFT techniques is demonstrated to\nsignificantly expedite the training speed of the models, addressing a critical\naspect of model development and deployment. In this study, we explore Low-Rank\nAdaptation of Large Language Models (LoRA) and adapter methods for low-resource\ntext classification. We show that these methods are competitive with full\nfine-tuning and can be used without loss in accuracy. This study contributes\nvaluable insights into the effectiveness of Marathi BERT models, offering a\nfoundation for the continued advancement of NLP capabilities in Marathi and\nsimilar Indic languages.\n","authors":["Pranita Deshmukh","Nikita Kulkarni","Sanhita Kulkarni","Kareena Manghani","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2408.03172v1.pdf","comment":"Accepted at I2CT 2024"},{"id":"http://arxiv.org/abs/2408.03156v1","updated":"2024-08-06T12:55:17Z","published":"2024-08-06T12:55:17Z","title":"Iterative CT Reconstruction via Latent Variable Optimization of Shallow\n  Diffusion Models","summary":"  Image generative AI has garnered significant attention in recent years. In\nparticular, the diffusion model, a core component of recent generative AI,\nproduces high-quality images with rich diversity. In this study, we propose a\nnovel CT reconstruction method by combining the denoising diffusion\nprobabilistic model with iterative CT reconstruction. In sharp contrast to\nprevious studies, we optimize the fidelity loss of CT reconstruction with\nrespect to the latent variable of the diffusion model, instead of the image and\nmodel parameters. To suppress anatomical structure changes produced by the\ndiffusion model, we shallow the diffusion and reverse processes, and fix a set\nof added noises in the reverse process to make it deterministic during\ninference. We demonstrate the effectiveness of the proposed method through\nsparse view CT reconstruction of 1/10 view projection data. Despite the\nsimplicity of the implementation, the proposed method shows the capability of\nreconstructing high-quality images while preserving the patient's anatomical\nstructure, and outperforms existing methods including iterative reconstruction,\niterative reconstruction with total variation, and the diffusion model alone in\nterms of quantitative indices such as SSIM and PSNR. We also explore further\nsparse view CT using 1/20 view projection data with the same trained diffusion\nmodel. As the number of iterations increases, image quality improvement\ncomparable to that of 1/10 sparse view CT reconstruction is achieved. In\nprinciple, the proposed method can be widely applied not only to CT but also to\nother imaging modalities such as MRI, PET, and SPECT.\n","authors":["Sho Ozaki","Shizuo Kaji","Toshikazu Imae","Kanabu Nawa","Hideomi Yamashita","Keiichi Nakagawa"],"pdf_url":"https://arxiv.org/pdf/2408.03156v1.pdf","comment":"19 pages, 9 figures"},{"id":"http://arxiv.org/abs/2405.01661v3","updated":"2024-08-06T12:54:26Z","published":"2024-05-02T18:31:47Z","title":"When a Relation Tells More Than a Concept: Exploring and Evaluating\n  Classifier Decisions with CoReX","summary":"  Explanations for Convolutional Neural Networks (CNNs) based on relevance of\ninput pixels might be too unspecific to evaluate which and how input features\nimpact model decisions. Especially in complex real-world domains like biology,\nthe presence of specific concepts and of relations between concepts might be\ndiscriminating between classes. Pixel relevance is not expressive enough to\nconvey this type of information. In consequence, model evaluation is limited\nand relevant aspects present in the data and influencing the model decisions\nmight be overlooked. This work presents a novel method to explain and evaluate\nCNN models, which uses a concept- and relation-based explainer (CoReX). It\nexplains the predictive behavior of a model on a set of images by masking\n(ir-)relevant concepts from the decision-making process and by constraining\nrelations in a learned interpretable surrogate model. We test our approach with\nseveral image data sets and CNN architectures. Results show that CoReX\nexplanations are faithful to the CNN model in terms of predictive outcomes. We\nfurther demonstrate through a human evaluation that CoReX is a suitable tool\nfor generating combined explanations that help assessing the classification\nquality of CNNs. We further show that CoReX supports the identification and\nre-classification of incorrect or ambiguous classifications.\n","authors":["Bettina Finzel","Patrick Hilme","Johannes Rabold","Ute Schmid"],"pdf_url":"https://arxiv.org/pdf/2405.01661v3.pdf","comment":"preliminary version, submitted to Machine Learning"},{"id":"http://arxiv.org/abs/2408.03152v1","updated":"2024-08-06T12:52:03Z","published":"2024-08-06T12:52:03Z","title":"TSC: A Simple Two-Sided Constraint against Over-Smoothing","summary":"  Graph Convolutional Neural Network (GCN), a widely adopted method for\nanalyzing relational data, enhances node discriminability through the\naggregation of neighboring information. Usually, stacking multiple layers can\nimprove the performance of GCN by leveraging information from high-order\nneighbors. However, the increase of the network depth will induce the\nover-smoothing problem, which can be attributed to the quality and quantity of\nneighbors changing: (a) neighbor quality, node's neighbors become overlapping\nin high order, leading to aggregated information becoming indistinguishable,\n(b) neighbor quantity, the exponentially growing aggregated neighbors submerges\nthe node's initial feature by recursively aggregating operations. Current\nsolutions mainly focus on one of the above causes and seldom consider both at\nonce.\n  Aiming at tackling both causes of over-smoothing in one shot, we introduce a\nsimple Two-Sided Constraint (TSC) for GCNs, comprising two straightforward yet\npotent techniques: random masking and contrastive constraint. The random\nmasking acts on the representation matrix's columns to regulate the degree of\ninformation aggregation from neighbors, thus preventing the convergence of node\nrepresentations. Meanwhile, the contrastive constraint, applied to the\nrepresentation matrix's rows, enhances the discriminability of the nodes.\nDesigned as a plug-in module, TSC can be easily coupled with GCN or SGC\narchitectures. Experimental analyses on diverse real-world graph datasets\nverify that our approach markedly reduces the convergence of node's\nrepresentation and the performance degradation in deeper GCN.\n","authors":["Furong Peng","Kang Liu","Xuan Lu","Yuhua Qian","Hongren Yan","Chao Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03152v1.pdf","comment":"accept by KDD2024"},{"id":"http://arxiv.org/abs/2408.03150v1","updated":"2024-08-06T12:49:33Z","published":"2024-08-06T12:49:33Z","title":"Conditioning LLMs with Emotion in Neural Machine Translation","summary":"  Large Language Models (LLMs) have shown remarkable performance in Natural\nLanguage Processing tasks, including Machine Translation (MT). In this work, we\npropose a novel MT pipeline that integrates emotion information extracted from\na Speech Emotion Recognition (SER) model into LLMs to enhance translation\nquality. We first fine-tune five existing LLMs on the Libri-trans dataset and\nselect the most performant model. Subsequently, we augment LLM prompts with\ndifferent dimensional emotions and train the selected LLM under these different\nconfigurations. Our experiments reveal that integrating emotion information,\nespecially arousal, into LLM prompts leads to notable improvements in\ntranslation quality.\n","authors":["Charles Brazier","Jean-Luc Rouas"],"pdf_url":"https://arxiv.org/pdf/2408.03150v1.pdf","comment":"6 pages, In Proceedings of the 21st International Conference on\n  Spoken Language Translation (IWSLT), Bangkok, Thailand, 2024"},{"id":"http://arxiv.org/abs/2408.03144v1","updated":"2024-08-06T12:39:12Z","published":"2024-08-06T12:39:12Z","title":"Active Learning for Level Set Estimation Using Randomized Straddle\n  Algorithms","summary":"  Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.\n","authors":["Yu Inatsu","Shion Takeno","Kentaro Kutsukake","Ichiro Takeuchi"],"pdf_url":"https://arxiv.org/pdf/2408.03144v1.pdf","comment":"21 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.00573v2","updated":"2024-08-06T12:36:57Z","published":"2024-08-01T14:06:34Z","title":"Convergence Analysis of Natural Gradient Descent for Over-parameterized\n  Physics-Informed Neural Networks","summary":"  First-order methods, such as gradient descent (GD) and stochastic gradient\ndescent (SGD), have been proven effective in training neural networks. In the\ncontext of over-parameterization, there is a line of work demonstrating that\nrandomly initialized (stochastic) gradient descent converges to a globally\noptimal solution at a linear convergence rate for the quadratic loss function.\nHowever, the learning rate of GD for training two-layer neural networks\nexhibits poor dependence on the sample size and the Gram matrix, leading to a\nslow training process. In this paper, we show that for the $L^2$ regression\nproblems, the learning rate can be improved from $\\mathcal{O}(\\lambda_0/n^2)$\nto $\\mathcal{O}(1/\\|\\bm{H}^{\\infty}\\|_2)$, which implies that GD actually\nenjoys a faster convergence rate. Furthermore, we generalize the method to GD\nin training two-layer Physics-Informed Neural Networks (PINNs), showing a\nsimilar improvement for the learning rate. Although the improved learning rate\nhas a mild dependence on the Gram matrix, we still need to set it small enough\nin practice due to the unknown eigenvalues of the Gram matrix. More\nimportantly, the convergence rate is tied to the least eigenvalue of the Gram\nmatrix, which can lead to slow convergence. In this work, we provide the\nconvergence analysis of natural gradient descent (NGD) in training two-layer\nPINNs, demonstrating that the learning rate can be $\\mathcal{O}(1)$, and at\nthis rate, the convergence rate is independent of the Gram matrix.\n","authors":["Xianliang Xu","Ting Du","Wang Kong","Ye Li","Zhongyi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.00573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01294v2","updated":"2024-08-06T12:24:07Z","published":"2024-08-02T14:31:37Z","title":"Feature Clock: High-Dimensional Effects in Two-Dimensional Plots","summary":"  Humans struggle to perceive and interpret high-dimensional data. Therefore,\nhigh-dimensional data are often projected into two dimensions for\nvisualization. Many applications benefit from complex nonlinear dimensionality\nreduction techniques, but the effects of individual high-dimensional features\nare hard to explain in the two-dimensional space. Most visualization solutions\nuse multiple two-dimensional plots, each showing the effect of one\nhigh-dimensional feature in two dimensions; this approach creates a need for a\nvisual inspection of k plots for a k-dimensional input space. Our solution,\nFeature Clock, provides a novel approach that eliminates the need to inspect\nthese k plots to grasp the influence of original features on the data structure\ndepicted in two dimensions. Feature Clock enhances the explainability and\ncompactness of visualizations of embedded data and is available in an\nopen-source Python library.\n","authors":["Olga Ovcharenko","Rita Sevastjanova","Valentina Boeva"],"pdf_url":"https://arxiv.org/pdf/2408.01294v2.pdf","comment":"To be published in IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2407.09064v2","updated":"2024-08-06T11:43:53Z","published":"2024-07-12T07:34:10Z","title":"Multi-Modal Dataset Creation for Federated Learning with DICOM\n  Structured Reports","summary":"  Purpose: Federated training is often hindered by heterogeneous datasets due\nto divergent data storage options, inconsistent naming schemes, varied\nannotation procedures, and disparities in label quality. This is particularly\nevident in the emerging multi-modal learning paradigms, where dataset\nharmonization including a uniform data representation and filtering options are\nof paramount importance.\n  Methods: DICOM structured reports enable the standardized linkage of\narbitrary information beyond the imaging domain and can be used within Python\ndeep learning pipelines with highdicom. Building on this, we developed an open\nplatform for data integration and interactive filtering capabilities that\nsimplifies the process of assembling multi-modal datasets.\n  Results: In this study, we extend our prior work by showing its applicability\nto more and divergent data types, as well as streamlining datasets for\nfederated training within an established consortium of eight university\nhospitals in Germany. We prove its concurrent filtering ability by creating\nharmonized multi-modal datasets across all locations for predicting the outcome\nafter minimally invasive heart valve replacement. The data includes DICOM data\n(i.e. computed tomography images, electrocardiography scans) as well as\nannotations (i.e. calcification segmentations, pointsets and pacemaker\ndependency), and metadata (i.e. prosthesis and diagnoses).\n  Conclusion: Structured reports bridge the traditional gap between imaging\nsystems and information systems. Utilizing the inherent DICOM reference system\narbitrary data types can be queried concurrently to create meaningful cohorts\nfor clinical studies. The graphical interface as well as example structured\nreport templates will be made publicly available.\n","authors":["Malte Tlle","Lukas Burger","Halvar Kelm","Florian Andr","Peter Bannas","Gerhard Diller","Norbert Frey","Philipp Garthe","Stefan Gro","Anja Hennemuth","Lars Kaderali","Nina Krger","Andreas Leha","Simon Martin","Alexander Meyer","Eike Nagel","Stefan Orwat","Clemens Scherer","Moritz Seiffert","Jan Moritz Seliger","Stefan Simm","Tim Friede","Tim Seidler","Sandy Engelhardt"],"pdf_url":"https://arxiv.org/pdf/2407.09064v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.11044v2","updated":"2024-08-06T11:29:06Z","published":"2024-01-19T22:11:54Z","title":"Preservation of Feature Stability in Machine Learning Under Data\n  Uncertainty for Decision Support in Critical Domains","summary":"  In a world where Machine Learning (ML) is increasingly deployed to support\ndecision-making in critical domains, providing decision-makers with\nexplainable, stable, and relevant inputs becomes fundamental. Understanding how\nmachine learning works under missing data and how this affects feature\nvariability is paramount. This is even more relevant as machine learning\napproaches focus on standardising decision-making approaches that rely on an\nidealised set of features. However, decision-making in human activities often\nrelies on incomplete data, even in critical domains. This paper addresses this\ngap by conducting a set of experiments using traditional machine learning\nmethods that look for optimal decisions in comparison to a recently deployed\nmachine learning method focused on a classification that is more descriptive\nand mimics human decision making, allowing for the natural integration of\nexplainability. We found that the ML descriptive approach maintains higher\nclassification accuracy while ensuring the stability of feature selection as\ndata incompleteness increases. This suggests that descriptive classification\nmethods can be helpful in uncertain decision-making scenarios.\n","authors":["Karol Capaa","Paulina Tworek","Jose Sousa"],"pdf_url":"https://arxiv.org/pdf/2401.11044v2.pdf","comment":"30 pages, 6 figures, supplementary materials"},{"id":"http://arxiv.org/abs/2407.03234v3","updated":"2024-08-06T11:15:00Z","published":"2024-07-03T16:03:42Z","title":"Self-Evaluation as a Defense Against Adversarial Attacks on LLMs","summary":"  We introduce a defense against adversarial attacks on LLMs utilizing\nself-evaluation. Our method requires no model fine-tuning, instead using\npre-trained models to evaluate the inputs and outputs of a generator model,\nsignificantly reducing the cost of implementation in comparison to other,\nfinetuning-based methods. Our method can significantly reduce the attack\nsuccess rate of attacks on both open and closed-source LLMs, beyond the\nreductions demonstrated by Llama-Guard2 and commonly used content moderation\nAPIs. We present an analysis of the effectiveness of our method, including\nattempts to attack the evaluator in various settings, demonstrating that it is\nalso more resilient to attacks than existing methods. Code and data will be\nmade available at https://github.com/Linlt-leon/self-eval.\n","authors":["Hannah Brown","Leon Lin","Kenji Kawaguchi","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2407.03234v3.pdf","comment":"8 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03099v1","updated":"2024-08-06T11:04:07Z","published":"2024-08-06T11:04:07Z","title":"Topic Modeling with Fine-tuning LLMs and Bag of Sentences","summary":"  Large language models (LLM)'s are increasingly used for topic modeling\noutperforming classical topic models such as LDA. Commonly, pre-trained LLM\nencoders such as BERT are used out-of-the-box despite the fact that fine-tuning\nis known to improve LLMs considerably. The challenge lies in obtaining a\nsuitable (labeled) dataset for fine-tuning. In this paper, we use the recent\nidea to use bag of sentences as the elementary unit in computing topics. In\nturn, we derive an approach FT-Topic to perform unsupervised fine-tuning\nrelying primarily on two steps for constructing a training dataset in an\nautomatic fashion. First, a heuristic method to identifies pairs of sentence\ngroups that are either assumed to be of the same or different topics. Second,\nwe remove sentence pairs that are likely labeled incorrectly. The dataset is\nthen used to fine-tune an encoder LLM, which can be leveraged by any topic\nmodeling approach using embeddings. However, in this work, we demonstrate its\neffectiveness by deriving a novel state-of-the-art topic modeling method called\nSenClu, which achieves fast inference through an expectation-maximization\nalgorithm and hard assignments of sentence groups to a single topic, while\ngiving users the possibility to encode prior knowledge on the topic-document\ndistribution. Code is at \\url{https://github.com/JohnTailor/FT-Topic}\n","authors":["Johannes Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.03099v1.pdf","comment":"This is the submitted journal version of enhanced with the novel\n  fine-tuning part of \"Efficient and Flexible Topic Modeling using Pretrained\n  Embeddings and Bag of Sentences'' which appeared at the International\n  Conference on Agents and Artificial Intelligence(ICAART) in 2024"},{"id":"http://arxiv.org/abs/2211.16237v4","updated":"2024-08-06T10:51:40Z","published":"2022-11-29T14:21:34Z","title":"Closing the gap between SVRG and TD-SVRG with Gradient Splitting","summary":"  Temporal difference (TD) learning is a policy evaluation in reinforcement\nlearning whose performance can be enhanced by variance reduction methods.\nRecently, multiple works have sought to fuse TD learning with Stochastic\nVariance Reduced Gradient (SVRG) method to achieve a geometric rate of\nconvergence. However, the resulting convergence rate is significantly weaker\nthan what is achieved by SVRG in the setting of convex optimization. In this\nwork we utilize a recent interpretation of TD-learning as the splitting of the\ngradient of an appropriately chosen function, thus simplifying the algorithm\nand fusing TD with SVRG. Our main result is a geometric convergence bound with\npredetermined learning rate of $1/8$, which is identical to the convergence\nbound available for SVRG in the convex setting. Our theoretical findings are\nsupported by a set of experiments.\n","authors":["Arsenii Mustafin","Alex Olshevsky","Ioannis Ch. Paschalidis"],"pdf_url":"https://arxiv.org/pdf/2211.16237v4.pdf","comment":"42 pages, 8 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.03093v1","updated":"2024-08-06T10:48:15Z","published":"2024-08-06T10:48:15Z","title":"Learning Provably Robust Policies in Uncertain Parametric Environments","summary":"  We present a data-driven approach for learning MDP policies that are robust\nacross stochastic environments whose transition probabilities are defined by\nparameters with an unknown distribution. We produce probably approximately\ncorrect (PAC) guarantees for the performance of these learned policies in a\nnew, unseen environment over the unknown distribution. Our approach is based on\nfinite samples of the MDP environments, for each of which we build an\napproximation of the model as an interval MDP, by exploring a set of generated\ntrajectories. We use the built approximations to synthesise a single policy\nthat performs well (meets given requirements) across the sampled environments,\nand furthermore bound its risk (of not meeting the given requirements) when\ndeployed in an unseen environment. Our procedure offers a trade-off between the\nguaranteed performance of the learned policy and the risk of not meeting the\nguarantee in an unseen environment. Our approach exploits knowledge of the\nenvironment's state space and graph structure, and we show how additional\nknowledge of its parametric structure can be leveraged to optimize learning and\nto obtain tighter guarantees from less samples. We evaluate our approach on a\ndiverse range of established benchmarks, demonstrating that we can generate\nhighly performing and robust policies, along with guarantees that tightly\nquantify their performance and the associated risk.\n","authors":["Yannik Schnitzer","Alessandro Abate","David Parker"],"pdf_url":"https://arxiv.org/pdf/2408.03093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20800v2","updated":"2024-08-06T10:45:42Z","published":"2024-05-31T14:01:12Z","title":"Shape Constraints in Symbolic Regression using Penalized Least Squares","summary":"  We study the addition of shape constraints (SC) and their consideration\nduring the parameter identification step of symbolic regression (SR). SC serve\nas a means to introduce prior knowledge about the shape of the otherwise\nunknown model function into SR. Unlike previous works that have explored SC in\nSR, we propose minimizing SC violations during parameter identification using\ngradient-based numerical optimization. We test three algorithm variants to\nevaluate their performance in identifying three symbolic expressions from\nsynthetically generated data sets. This paper examines two benchmark scenarios:\none with varying noise levels and another with reduced amounts of training\ndata. The results indicate that incorporating SC into the expression search is\nparticularly beneficial when data is scarce. Compared to using SC only in the\nselection process, our approach of minimizing violations during parameter\nidentification shows a statistically significant benefit in some of our test\ncases, without being significantly worse in any instance.\n","authors":["Viktor Martinek","Julia Reuter","Ophelia Frotscher","Sanaz Mostaghim","Markus Richter","Roland Herzog"],"pdf_url":"https://arxiv.org/pdf/2405.20800v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03088v1","updated":"2024-08-06T10:41:46Z","published":"2024-08-06T10:41:46Z","title":"QADQN: Quantum Attention Deep Q-Network for Financial Market Prediction","summary":"  Financial market prediction and optimal trading strategy development remain\nchallenging due to market complexity and volatility. Our research in quantum\nfinance and reinforcement learning for decision-making demonstrates the\napproach of quantum-classical hybrid algorithms to tackling real-world\nfinancial challenges. In this respect, we corroborate the concept with rigorous\nbacktesting and validate the framework's performance under realistic market\nconditions, by including fixed transaction cost per trade. This paper\nintroduces a Quantum Attention Deep Q-Network (QADQN) approach to address these\nchallenges through quantum-enhanced reinforcement learning. Our QADQN\narchitecture uses a variational quantum circuit inside a traditional deep\nQ-learning framework to take advantage of possible quantum advantages in\ndecision-making. We gauge the QADQN agent's performance on historical data from\nmajor market indices, including the S&P 500. We evaluate the agent's learning\nprocess by examining its reward accumulation and the effectiveness of its\nexperience replay mechanism. Our empirical results demonstrate the QADQN's\nsuperior performance, achieving better risk-adjusted returns with Sortino\nratios of 1.28 and 1.19 for non-overlapping and overlapping test periods\nrespectively, indicating effective downside risk management.\n","authors":["Siddhant Dutta","Nouhaila Innan","Alberto Marchisio","Sadok Ben Yahia","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2408.03088v1.pdf","comment":"Accepted at the 2024 IEEE International Conference on Quantum\n  Computing and Engineering (QCE24), QCRL, September 2024"},{"id":"http://arxiv.org/abs/2308.09310v3","updated":"2024-08-06T10:38:22Z","published":"2023-08-18T05:11:50Z","title":"Variance reduction techniques for stochastic proximal point algorithms","summary":"  In the context of finite sums minimization, variance reduction techniques are\nwidely used to improve the performance of state-of-the-art stochastic gradient\nmethods. Their practical impact is clear, as well as their theoretical\nproperties. Stochastic proximal point algorithms have been studied as an\nalternative to stochastic gradient algorithms since they are more stable with\nrespect to the choice of the step size. However, their variance-reduced\nversions are not as well studied as the gradient ones. In this work, we propose\nthe first unified study of variance reduction techniques for stochastic\nproximal point algorithms. We introduce a generic stochastic proximal-based\nalgorithm that can be specified to give the proximal version of SVRG, SAGA, and\nsome of their variants. For this algorithm, in the smooth setting, we provide\nseveral convergence rates for the iterates and the objective function values,\nwhich are faster than those of the vanilla stochastic proximal point algorithm.\nMore specifically, for convex functions, we prove a sublinear convergence rate\nof $O(1/k)$. In addition, under the Polyak-{\\L}ojasiewicz (PL) condition, we\nobtain linear convergence rates. Finally, our numerical experiments demonstrate\nthe advantages of the proximal variance reduction methods over their gradient\ncounterparts in terms of the stability with respect to the choice of the step\nsize in most cases, especially for difficult problems.\n","authors":["Cheik Traor","Vassilis Apidopoulos","Saverio Salzo","Silvia Villa"],"pdf_url":"https://arxiv.org/pdf/2308.09310v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03085v1","updated":"2024-08-06T10:25:02Z","published":"2024-08-06T10:25:02Z","title":"Matrix Multiplication on Quantum Computer","summary":"  This paper introduces an innovative and practical approach to universal\nquantum matrix multiplication. We designed optimized quantum adders and\nmultipliers based on Quantum Fourier Transform (QFT), which significantly\nreduced the number of gates used compared to classical adders and multipliers.\nSubsequently, we construct a basic universal quantum matrix multiplication and\nextend it to the Strassen algorithm. We conduct comparative experiments to\nanalyze the performance of the quantum matrix multiplication and evaluate the\nacceleration provided by the optimized quantum adder and multiplier.\nFurthermore, we investigate the advantages and disadvantages of the quantum\nStrassen algorithm compared to basic quantum matrix multiplication.\n","authors":["Jiaqi Yao","Ding Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03085v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03084v1","updated":"2024-08-06T10:24:54Z","published":"2024-08-06T10:24:54Z","title":"Research on Autonomous Driving Decision-making Strategies based Deep\n  Reinforcement Learning","summary":"  The behavior decision-making subsystem is a key component of the autonomous\ndriving system, which reflects the decision-making ability of the vehicle and\nthe driver, and is an important symbol of the high-level intelligence of the\nvehicle. However, the existing rule-based decision-making schemes are limited\nby the prior knowledge of designers, and it is difficult to cope with complex\nand changeable traffic scenarios. In this work, an advanced deep reinforcement\nlearning model is adopted, which can autonomously learn and optimize driving\nstrategies in a complex and changeable traffic environment by modeling the\ndriving decision-making process as a reinforcement learning problem.\nSpecifically, we used Deep Q-Network (DQN) and Proximal Policy Optimization\n(PPO) for comparative experiments. DQN guides the agent to choose the best\naction by approximating the state-action value function, while PPO improves the\ndecision-making quality by optimizing the policy function. We also introduce\nimprovements in the design of the reward function to promote the robustness and\nadaptability of the model in real-world driving situations. Experimental\nresults show that the decision-making strategy based on deep reinforcement\nlearning has better performance than the traditional rule-based method in a\nvariety of driving tasks.\n","authors":["Zixiang Wang","Hao Yan","Changsong Wei","Junyu Wang","Shi Bo","Minheng Xiao"],"pdf_url":"https://arxiv.org/pdf/2408.03084v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02354v2","updated":"2024-08-06T10:11:28Z","published":"2024-08-05T10:02:29Z","title":"RECE: Reduced Cross-Entropy Loss for Large-Catalogue Sequential\n  Recommenders","summary":"  Scalability is a major challenge in modern recommender systems. In sequential\nrecommendations, full Cross-Entropy (CE) loss achieves state-of-the-art\nrecommendation quality but consumes excessive GPU memory with large item\ncatalogs, limiting its practicality. Using a GPU-efficient locality-sensitive\nhashing-like algorithm for approximating large tensor of logits, this paper\nintroduces a novel RECE (REduced Cross-Entropy) loss. RECE significantly\nreduces memory consumption while allowing one to enjoy the state-of-the-art\nperformance of full CE loss. Experimental results on various datasets show that\nRECE cuts training peak memory usage by up to 12 times compared to existing\nmethods while retaining or exceeding performance metrics of CE loss. The\napproach also opens up new possibilities for large-scale applications in other\ndomains.\n","authors":["Danil Gusak","Gleb Mezentsev","Ivan Oseledets","Evgeny Frolov"],"pdf_url":"https://arxiv.org/pdf/2408.02354v2.pdf","comment":"5 pages, accepted for CIKM'24"},{"id":"http://arxiv.org/abs/2307.15325v2","updated":"2024-08-06T10:09:47Z","published":"2023-07-28T06:03:19Z","title":"Equivariance and partial observations in Koopman operator theory for\n  partial differential equations","summary":"  The Koopman operator has become an essential tool for data-driven analysis,\nprediction and control of complex systems. The main reason is the enormous\npotential of identifying linear function space representations of nonlinear\ndynamics from measurements. This equally applies to ordinary, stochastic, and\npartial differential equations (PDEs). Until now, with a few exceptions only,\nthe PDE case is mostly treated rather superficially, and the specific structure\nof the underlying dynamics is largely ignored. In this paper, we show that\nsymmetries in the system dynamics can be carried over to the Koopman operator,\nwhich allows us to massively increase the model efficacy. Moreover, the\nsituation where we only have access to partial observations (i.e.,\nmeasurements, as is very common for experimental data) has not been treated to\nits full extent, either. Moreover, we address the highly-relevant case where we\ncannot measure the full state, such that alternative approaches such as delay\ncoordinates have to be considered. We derive rigorous statements on the\nrequired number of observables in this situation, based on embedding theory. We\npresent numerical evidence using various numerical examples including the wave\nequation and the Kuramoto-Sivashinsky equation.\n","authors":["Sebastian Peitz","Hans Harder","Feliks Nske","Friedrich Philipp","Manuel Schaller","Karl Worthmann"],"pdf_url":"https://arxiv.org/pdf/2307.15325v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12465v3","updated":"2024-08-06T09:57:36Z","published":"2024-05-21T02:41:40Z","title":"A finite element-based physics-informed operator learning framework for\n  spatiotemporal partial differential equations on arbitrary domains","summary":"  We propose a novel finite element-based physics-informed operator learning\nframework that allows for predicting spatiotemporal dynamics governed by\npartial differential equations (PDEs). The proposed framework employs a loss\nfunction inspired by the finite element method (FEM) with the implicit Euler\ntime integration scheme. A transient thermal conduction problem is considered\nto benchmark the performance. The proposed operator learning framework takes a\ntemperature field at the current time step as input and predicts a temperature\nfield at the next time step. The Galerkin discretized weak formulation of the\nheat equation is employed to incorporate physics into the loss function, which\nis coined finite operator learning (FOL). Upon training, the networks\nsuccessfully predict the temperature evolution over time for any initial\ntemperature field at high accuracy compared to the FEM solution. The framework\nis also confirmed to be applicable to a heterogeneous thermal conductivity and\narbitrary geometry. The advantages of FOL can be summarized as follows: First,\nthe training is performed in an unsupervised manner, avoiding the need for a\nlarge data set prepared from costly simulations or experiments. Instead, random\ntemperature patterns generated by the Gaussian random process and the Fourier\nseries, combined with constant temperature fields, are used as training data to\ncover possible temperature cases. Second, shape functions and backward\ndifference approximation are exploited for the domain discretization, resulting\nin a purely algebraic equation. This enhances training efficiency, as one\navoids time-consuming automatic differentiation when optimizing weights and\nbiases while accepting possible discretization errors. Finally, thanks to the\ninterpolation power of FEM, any arbitrary geometry can be handled with FOL,\nwhich is crucial to addressing various engineering application scenarios.\n","authors":["Yusuke Yamazaki","Ali Harandi","Mayu Muramatsu","Alexandre Viardin","Markus Apel","Tim Brepols","Stefanie Reese","Shahed Rezaei"],"pdf_url":"https://arxiv.org/pdf/2405.12465v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10107v4","updated":"2024-08-06T08:39:33Z","published":"2024-01-18T16:18:18Z","title":"Comparison analysis between standard polysomnographic data and\n  in-ear-EEG signals: A preliminary study","summary":"  Study Objectives: Polysomnography (PSG) currently serves as the benchmark for\nevaluating sleep disorders. Its discomfort makes long-term monitoring\nunfeasible, leading to bias in sleep quality assessment. Hence, less invasive,\ncost-effective, and portable alternatives need to be explored. One promising\ncontender is the in-ear-EEG sensor. This study aims to establish a methodology\nto assess the similarity between the single-channel in-ear-EEG and standard PSG\nderivations.\n  Methods: The study involves four-hour signals recorded from ten healthy\nsubjects aged 18 to 60 years. Recordings are analyzed following two\ncomplementary approaches: (i) a hypnogram-based analysis aimed at assessing the\nagreement between PSG and in-ear-EEG-derived hypnograms; and (ii) a\nfeature-based analysis based on time- and frequency- domain feature extraction,\nunsupervised feature selection, and definition of Feature-based Similarity\nIndex via Jensen-Shannon Divergence (JSD-FSI).\n  Results: We find large variability between PSG and in-ear-EEG hypnograms\nscored by the same sleep expert according to Cohen's kappa metric, with\nsignificantly greater agreements for PSG scorers than for in-ear-EEG scorers (p\n< 0.001) based on Fleiss' kappa metric. On average, we demonstrate a high\nsimilarity between PSG and in-ear-EEG signals in terms of JSD-FSI (0.79 +/-\n0.06 -awake, 0.77 +/- 0.07 -NREM, and 0.67 +/- 0.10 -REM) and in line with the\nsimilarity values computed independently on standard PSG-channel-combinations.\n  Conclusions: In-ear-EEG is a valuable solution for home-based sleep\nmonitoring, however further studies with a larger and more heterogeneous\ndataset are needed.\n","authors":["Gianpaolo Palo","Luigi Fiorillo","Giuliana Monachino","Michal Bechny","Michel Walti","Elias Meier","Francesca Pentimalli Biscaretti di Ruffia","Mark Melnykowycz","Athina Tzovara","Valentina Agostini","Francesca Dalia Faraci"],"pdf_url":"https://arxiv.org/pdf/2401.10107v4.pdf","comment":"20 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.03029v1","updated":"2024-08-06T08:22:16Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to several baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14244v2","updated":"2024-08-06T08:11:34Z","published":"2024-04-22T14:57:17Z","title":"AI-Generated Faces in the Real World: A Large-Scale Case Study of\n  Twitter Profile Images","summary":"  Recent advances in the field of generative artificial intelligence (AI) have\nblurred the lines between authentic and machine-generated content, making it\nalmost impossible for humans to distinguish between such media. One notable\nconsequence is the use of AI-generated images for fake profiles on social\nmedia. While several types of disinformation campaigns and similar incidents\nhave been reported in the past, a systematic analysis has been lacking. In this\nwork, we conduct the first large-scale investigation of the prevalence of\nAI-generated profile pictures on Twitter. We tackle the challenges of a\nreal-world measurement study by carefully integrating various data sources and\ndesigning a multi-stage detection pipeline. Our analysis of nearly 15 million\nTwitter profile pictures shows that 0.052% were artificially generated,\nconfirming their notable presence on the platform. We comprehensively examine\nthe characteristics of these accounts and their tweet content, and uncover\npatterns of coordinated inauthentic behavior. The results also reveal several\nmotives, including spamming and political amplification campaigns. Our research\nreaffirms the need for effective detection and mitigation strategies to cope\nwith the potential negative effects of generative AI in the future.\n","authors":["Jonas Ricker","Dennis Assenmacher","Thorsten Holz","Asja Fischer","Erwin Quiring"],"pdf_url":"https://arxiv.org/pdf/2404.14244v2.pdf","comment":"Accepted to RAID 2024"},{"id":"http://arxiv.org/abs/2404.03453v3","updated":"2024-08-06T07:53:09Z","published":"2024-04-04T13:57:44Z","title":"Conditioning of Banach Space Valued Gaussian Random Variables: An\n  Approximation Approach Based on Martingales","summary":"  In this paper we investigate the conditional distributions of two Banach\nspace valued, jointly Gaussian random variables. We show that these conditional\ndistributions are again Gaussian and that their means and covariances are\ndetermined by a general finite dimensional approximation scheme based upon a\nmartingale approach. In particular, it turns out that the covariance operators\noccurring in this scheme converge with respect to the nuclear norm and that the\nconditional probabilities converge weakly. Moreover, we discuss in detail, how\nour approximation scheme can be implemented in several classes of important\nBanach spaces such as (reproducing kernel) Hilbert spaces and spaces of\ncontinuous functions. As an example, we then apply our general results to the\ncase of Gaussian processes with continuous paths conditioned to partial but\ninfinite observations of their paths. Here we show that conditioning on\nsufficiently rich, increasing sets of finitely many observations leads to\nconsistent approximations, that is, both the mean and covariance functions\nconverge uniformly and the conditional probabilities converge weakly. Moreover,\nwe discuss how these results improve our understanding of the popular Gaussian\nprocesses for machine learning.\n","authors":["Ingo Steinwart"],"pdf_url":"https://arxiv.org/pdf/2404.03453v3.pdf","comment":"55 pages plus 22 pages of supplemental material"},{"id":"http://arxiv.org/abs/2408.03013v1","updated":"2024-08-06T07:48:51Z","published":"2024-08-06T07:48:51Z","title":"NeurDB: On the Design and Implementation of an AI-powered Autonomous\n  Database","summary":"  Databases are increasingly embracing AI to provide autonomous system\noptimization and intelligent in-database analytics, aiming to relieve end-user\nburdens across various industry sectors. Nonetheless, most existing approaches\nfail to account for the dynamic nature of databases, which renders them\nineffective for real-world applications characterized by evolving data and\nworkloads. This paper introduces NeurDB, an AI-powered autonomous database that\ndeepens the fusion of AI and databases with adaptability to data and workload\ndrift. NeurDB establishes a new in-database AI ecosystem that seamlessly\nintegrates AI workflows within the database. This integration enables efficient\nand effective in-database AI analytics and fast-adaptive learned system\ncomponents. Empirical evaluations demonstrate that NeurDB substantially\noutperforms existing solutions in managing AI analytics tasks, with the\nproposed learned components more effectively handling environmental dynamism\nthan state-of-the-art approaches.\n","authors":["Zhanhao Zhao","Shaofeng Cai","Haotian Gao","Hexiang Pan","Siqi Xiang","Naili Xing","Gang Chen","Beng Chin Ooi","Yanyan Shen","Yuncheng Wu","Meihui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16907v2","updated":"2024-08-06T07:24:35Z","published":"2024-02-25T04:24:28Z","title":"Diffusion Posterior Proximal Sampling for Image Restoration","summary":"  Diffusion models have demonstrated remarkable efficacy in generating\nhigh-quality samples. Existing diffusion-based image restoration algorithms\nexploit pre-trained diffusion models to leverage data priors, yet they still\npreserve elements inherited from the unconditional generation paradigm. These\nstrategies initiate the denoising process with pure white noise and incorporate\nrandom noise at each generative step, leading to over-smoothed results. In this\npaper, we present a refined paradigm for diffusion-based image restoration.\nSpecifically, we opt for a sample consistent with the measurement identity at\neach generative step, exploiting the sampling selection as an avenue for output\nstability and enhancement. The number of candidate samples used for selection\nis adaptively determined based on the signal-to-noise ratio of the timestep.\nAdditionally, we start the restoration process with an initialization combined\nwith the measurement signal, providing supplementary information to better\nalign the generative process. Extensive experimental results and analyses\nvalidate that our proposed method significantly enhances image restoration\nperformance while consuming negligible additional computational resources.\n","authors":["Hongjie Wu","Linchao He","Mingqin Zhang","Dongdong Chen","Kunming Luo","Mengting Luo","Ji-Zhe Zhou","Hu Chen","Jiancheng Lv"],"pdf_url":"https://arxiv.org/pdf/2402.16907v2.pdf","comment":"ACM Multimedia 2024 Oral"},{"id":"http://arxiv.org/abs/2408.02998v1","updated":"2024-08-06T07:05:56Z","published":"2024-08-06T07:05:56Z","title":"Federated Learning Architectures: A Performance Evaluation with Crop\n  Yield Prediction Application","summary":"  Federated learning has become an emerging technology for data analysis for\nIoT applications. This paper implements centralized and decentralized federated\nlearning frameworks for crop yield prediction based on Long Short-Term Memory\nNetwork. For centralized federated learning, multiple clients and one server is\nconsidered, where the clients exchange their model updates with the server that\nworks as the aggregator to build the global model. For the decentralized\nframework, a collaborative network is formed among the devices either using\nring topology or using mesh topology. In this network, each device receives\nmodel updates from the neighbour devices, and performs aggregation to build the\nupgraded model. The performance of the centralized and decentralized federated\nlearning frameworks are evaluated in terms of prediction accuracy, precision,\nrecall, F1-Score, and training time. The experimental results present that\n$\\geq$97% and $>$97.5% prediction accuracy are achieved using the centralized\nand decentralized federated learning-based frameworks respectively. The results\nalso show that the using centralized federated learning the response time can\nbe reduced by $\\sim$75% than the cloud-only framework. Finally, the future\nresearch directions of the use of federated learning in crop yield prediction\nare explored in this paper.\n","authors":["Anwesha Mukherjee","Rajkumar Buyya"],"pdf_url":"https://arxiv.org/pdf/2408.02998v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02987v1","updated":"2024-08-06T06:42:53Z","published":"2024-08-06T06:42:53Z","title":"A Differential Smoothness-based Compact-Dynamic Graph Convolutional\n  Network for Spatiotemporal Signal Recovery","summary":"  High quality spatiotemporal signal is vitally important for real application\nscenarios like energy management, traffic planning and cyber security. Due to\nthe uncontrollable factors like abrupt sensors breakdown or communication\nfault, the spatiotemporal signal collected by sensors is always incomplete. A\ndynamic graph convolutional network (DGCN) is effective for processing\nspatiotemporal signal recovery. However, it adopts a static GCN and a sequence\nneural network to explore the spatial and temporal patterns, separately. Such a\nseparated two-step processing is loose spatiotemporal, thereby failing to\ncapture the complex inner spatiotemporal correlation. To address this issue,\nthis paper proposes a Compact-Dynamic Graph Convolutional Network (CDGCN) for\nspatiotemporal signal recovery with the following two-fold ideas: a) leveraging\nthe tensor M-product to build a unified tensor graph convolution framework,\nwhich considers both spatial and temporal patterns simultaneously; and b)\nconstructing a differential smoothness-based objective function to reduce the\nnoise interference in spatiotemporal signal, thereby further improve the\nrecovery accuracy. Experiments on real-world spatiotemporal datasets\ndemonstrate that the proposed CDGCN significantly outperforms the\nstate-of-the-art models in terms of recovery accuracy.\n","authors":["Pengcheng Gao","Zicheng Gao","Ye Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.02987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01829v2","updated":"2024-08-06T06:30:08Z","published":"2024-08-03T17:43:10Z","title":"Neural Network Emulator for Atmospheric Chemical ODE","summary":"  Modeling atmospheric chemistry is complex and computationally intense. Given\nthe recent success of Deep neural networks in digital signal processing, we\npropose a Neural Network Emulator for fast chemical concentration modeling. We\nconsider atmospheric chemistry as a time-dependent Ordinary Differential\nEquation. To extract the hidden correlations between initial states and future\ntime evolution, we propose ChemNNE, an Attention based Neural Network Emulator\n(NNE) that can model the atmospheric chemistry as a neural ODE process. To\nefficiently simulate the chemical changes, we propose the sinusoidal time\nembedding to estimate the oscillating tendency over time. More importantly, we\nuse the Fourier neural operator to model the ODE process for efficient\ncomputation. We also propose three physical-informed losses to supervise the\ntraining optimization. To evaluate our model, we propose a large-scale chemical\ndataset that can be used for neural network training and evaluation. The\nextensive experiments show that our approach achieves state-of-the-art\nperformance in modeling accuracy and computational speed.\n","authors":["Zhi-Song Liu","Petri Clusius","Michael Boy"],"pdf_url":"https://arxiv.org/pdf/2408.01829v2.pdf","comment":"25 pages, 8 figures"},{"id":"http://arxiv.org/abs/2303.17963v4","updated":"2024-08-06T06:28:57Z","published":"2023-03-31T11:06:09Z","title":"Learning-Based Optimal Control with Performance Guarantees for Unknown\n  Systems with Latent States","summary":"  As control engineering methods are applied to increasingly complex systems,\ndata-driven approaches for system identification appear as a promising\nalternative to physics-based modeling. While the Bayesian approaches prevalent\nfor safety-critical applications usually rely on the availability of state\nmeasurements, the states of a complex system are often not directly measurable.\nIt may then be necessary to jointly estimate the dynamics and the latent state,\nmaking the quantification of uncertainties and the design of controllers with\nformal performance guarantees considerably more challenging. This paper\nproposes a novel method for the computation of an optimal input trajectory for\nunknown nonlinear systems with latent states based on a combination of particle\nMarkov chain Monte Carlo methods and scenario theory. Probabilistic performance\nguarantees are derived for the resulting input trajectory, and an approach to\nvalidate the performance of arbitrary control laws is presented. The\neffectiveness of the proposed method is demonstrated in a numerical simulation.\n","authors":["Robert Lefringhausen","Supitsana Srithasan","Armin Lederer","Sandra Hirche"],"pdf_url":"https://arxiv.org/pdf/2303.17963v4.pdf","comment":"Accepted version submitted to the 2024 European Control Conference\n  (ECC)"},{"id":"http://arxiv.org/abs/2109.03445v6","updated":"2024-08-06T06:19:46Z","published":"2021-09-08T06:06:28Z","title":"Convergence of Batch Asynchronous Stochastic Approximation With\n  Applications to Reinforcement Learning","summary":"  We begin by briefly surveying some results on the convergence of the\nStochastic Gradient Descent (SGD) Method, proved in a companion paper by the\npresent authors. These results are based on viewing SGD as a version of\nStochastic Approximation (SA). Ever since its introduction in the classic paper\nof Robbins and Monro in 1951, SA has become a standard tool for finding a\nsolution of an equation of the form $f(\\theta) = 0$, when only noisy\nmeasurements of $f(\\cdot)$ are available. In most situations, \\textit{every\ncomponent} of the putative solution $\\theta_t$ is updated at each step $t$. In\nsome applications in Reinforcement Learning (RL), \\textit{only one component}\nof $\\theta_t$ is updated at each $t$. This is known as \\textbf{asynchronous}\nSA. In this paper, we study \\textbf{Block Asynchronous SA (BASA)}, in which, at\neach step $t$, \\textit{some but not necessarily all} components of $\\theta_t$\nare updated. The theory presented here embraces both conventional (synchronous)\nSA as well as asynchronous SA, and all in-between possibilities. We provide\nsufficient conditions for the convergence of BASA, and also prove bounds on the\n\\textit{rate} of convergence of $\\theta_t$ to the solution. For the case of\nconventional SGD, these results reduce to those proved in our companion paper.\nThen we apply these results to the problem of finding a fixed point of a map\nwith only noisy measurements. This problem arises frequently in RL. We prove\nsufficient conditions for convergence as well as estimates for the rate of\nconvergence.\n","authors":["Rajeeva L. Karandikar","M. Vidyasagar"],"pdf_url":"https://arxiv.org/pdf/2109.03445v6.pdf","comment":"34 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.02971v1","updated":"2024-08-06T06:00:17Z","published":"2024-08-06T06:00:17Z","title":"Wave Interpolation Neural Operator: Interpolated Prediction of Electric\n  Fields Across Untrained Wavelengths","summary":"  Designing photonic structures requires electromagnetic simulations, which\noften require high computational costs. Researchers have developed surrogate\nsolvers for predicting electric fields to alleviate the computational issues.\nHowever, existing surrogate solvers are limited to performing inference at\nfixed simulation conditions and require retraining for different conditions. To\naddress this, we propose Wave Interpolation Neural Operator (WINO), a novel\nsurrogate solver enabling simulation condition interpolation across a\ncontinuous spectrum of broadband wavelengths. WINO introduces the Fourier Group\nConvolution Shuffling operator and a new conditioning method to efficiently\npredict electric fields from both trained and untrained wavelength data,\nachieving significant improvements in parameter efficiency and spectral\ninterpolation performance. Our model demonstrates approximately 100 times\nfaster performance than traditional finite-difference frequency-domain\nsimulations. Moreover, compared to the state-of-the-art model, we achieve a 74%\nreduction in parameters and 80.5% improvements in prediction accuracy for\nuntrained wavelengths, and 13.2% improvements for trained wavelengths.\n","authors":["Joonhyuk Seo","Chanik Kang","Dongjin Seo","Haejun Chung"],"pdf_url":"https://arxiv.org/pdf/2408.02971v1.pdf","comment":"9 pages, 5 figures, 4 tables / Appendix: 4 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2408.02965v1","updated":"2024-08-06T05:21:31Z","published":"2024-08-06T05:21:31Z","title":"Data-Driven Stochastic Closure Modeling via Conditional Diffusion Model\n  and Neural Operator","summary":"  Closure models are widely used in simulating complex multiscale dynamical\nsystems such as turbulence and the earth system, for which direct numerical\nsimulation that resolves all scales is often too expensive. For those systems\nwithout a clear scale separation, deterministic and local closure models often\nlack enough generalization capability, which limits their performance in many\nreal-world applications. In this work, we propose a data-driven modeling\nframework for constructing stochastic and non-local closure models via\nconditional diffusion model and neural operator. Specifically, the Fourier\nneural operator is incorporated into a score-based diffusion model, which\nserves as a data-driven stochastic closure model for complex dynamical systems\ngoverned by partial differential equations (PDEs). We also demonstrate how\naccelerated sampling methods can improve the efficiency of the data-driven\nstochastic closure model. The results show that the proposed methodology\nprovides a systematic approach via generative machine learning techniques to\nconstruct data-driven stochastic closure models for multiscale dynamical\nsystems with continuous spatiotemporal fields.\n","authors":["Xinghao Dong","Chuanqi Chen","Jin-Long Wu"],"pdf_url":"https://arxiv.org/pdf/2408.02965v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02961v1","updated":"2024-08-06T05:15:57Z","published":"2024-08-06T05:15:57Z","title":"Synaptic Modulation using Interspike Intervals Increases Energy\n  Efficiency of Spiking Neural Networks","summary":"  Despite basic differences between Spiking Neural Networks (SNN) and\nArtificial Neural Networks (ANN), most research on SNNs involve adapting\nANN-based methods for SNNs. Pruning (dropping connections) and quantization\n(reducing precision) are often used to improve energy efficiency of SNNs. These\nmethods are very effective for ANNs whose energy needs are determined by\nsignals transmitted on synapses. However, the event-driven paradigm in SNNs\nimplies that energy is consumed by spikes. In this paper, we propose a new\nsynapse model whose weights are modulated by Interspike Intervals (ISI) i.e.\ntime difference between two spikes. SNNs composed of this synapse model, termed\nISI Modulated SNNs (IMSNN), can use gradient descent to estimate how the ISI of\na neuron changes after updating its synaptic parameters. A higher ISI implies\nfewer spikes and vice-versa. The learning algorithm for IMSNNs exploits this\ninformation to selectively propagate gradients such that learning is achieved\nby increasing the ISIs resulting in a network that generates fewer spikes. The\nperformance of IMSNNs with dense and convolutional layers have been evaluated\nin terms of classification accuracy and the number of spikes using the MNIST\nand FashionMNIST datasets. The performance comparison with conventional SNNs\nshows that IMSNNs exhibit upto 90% reduction in the number of spikes while\nmaintaining similar classification accuracy.\n","authors":["Dylan Adams","Magda Zajaczkowska","Ashiq Anjum","Andrea Soltoggio","Shirin Dora"],"pdf_url":"https://arxiv.org/pdf/2408.02961v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.11652v4","updated":"2024-08-06T05:10:56Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v4.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.02950v1","updated":"2024-08-06T04:28:16Z","published":"2024-08-06T04:28:16Z","title":"Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields\n  on irregular geometries","summary":"  We present Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised\ndeep learning framework for the prediction of incompressible steady-state fluid\nflow fields in irregular domains, where the predicted fields are a function of\nthe geometry of the domains. In KA-PointNet, we implement shared\nKolmogorov-Arnold Networks (KANs) in the segmentation branch of the PointNet\narchitecture. We utilize Jacobi polynomials to construct shared KANs. As a\nbenchmark test case, we consider incompressible laminar steady-state flow over\na cylinder, where the geometry of its cross-section varies over the data set.\nWe investigate the performance of Jacobi polynomials with different degrees as\nwell as special cases of Jacobi polynomials such as Legendre polynomials,\nChebyshev polynomials of the first and second kinds, and Gegenbauer\npolynomials, in terms of the computational cost of training and accuracy of\nprediction of the test set. Additionally, we compare the performance of\nPointNet with shared KANs (i.e., KA-PointNet) and PointNet with shared\nMultilayer Perceptrons (MLPs). It is observed that when the number of trainable\nparameters is approximately equal, PointNet with shared KANs (i.e.,\nKA-PointNet) outperforms PointNet with shared MLPs.\n","authors":["Ali Kashefi"],"pdf_url":"https://arxiv.org/pdf/2408.02950v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.02338v3","updated":"2024-08-06T04:15:27Z","published":"2024-02-04T04:21:34Z","title":"NetLLM: Adapting Large Language Models for Networking","summary":"  Many networking tasks now employ deep learning (DL) to solve complex\nprediction and optimization problems. However, current design philosophy of\nDL-based algorithms entails intensive engineering overhead due to the manual\ndesign of deep neural networks (DNNs) for different networking tasks. Besides,\nDNNs tend to achieve poor generalization performance on unseen data\ndistributions/environments.\n  Motivated by the recent success of large language models (LLMs), this work\nstudies the LLM adaptation for networking to explore a more sustainable design\nphilosophy. With the powerful pre-trained knowledge, the LLM is promising to\nserve as the foundation model to achieve \"one model for all tasks\" with even\nbetter performance and stronger generalization. In pursuit of this vision, we\npresent NetLLM, the first framework that provides a coherent design to harness\nthe powerful capabilities of LLMs with low efforts to solve networking\nproblems. Specifically, NetLLM empowers the LLM to effectively process\nmultimodal data in networking and efficiently generate task-specific answers.\nBesides, NetLLM drastically reduces the costs of fine-tuning the LLM to acquire\ndomain knowledge for networking. Across three networking-related use cases -\nviewport prediction, adaptive bitrate streaming and cluster job scheduling, we\nshowcase that the NetLLM-adapted LLM significantly outperforms state-of-the-art\nalgorithms.\n","authors":["Duo Wu","Xianda Wang","Yaqi Qiao","Zhi Wang","Junchen Jiang","Shuguang Cui","Fangxin Wang"],"pdf_url":"https://arxiv.org/pdf/2402.02338v3.pdf","comment":"This paper has been accepted by ACM SIGCOMM 2024. DOI:\n  https://doi.org/10.1145/3651890.3672268"},{"id":"http://arxiv.org/abs/2408.02946v1","updated":"2024-08-06T04:14:29Z","published":"2024-08-06T04:14:29Z","title":"Scaling Laws for Data Poisoning in LLMs","summary":"  Recent work shows that LLMs are vulnerable to data poisoning, in which they\nare trained on partially corrupted or harmful data. Poisoned data is hard to\ndetect, breaks guardrails, and leads to undesirable and harmful behavior. Given\nthe intense efforts by leading labs to train and deploy increasingly larger and\nmore capable LLMs, it is critical to ask if the risk of data poisoning will be\nnaturally mitigated by scale, or if it is an increasing threat. We consider\nthree threat models by which data poisoning can occur: malicious fine-tuning,\nimperfect data curation, and intentional data contamination. Our experiments\nevaluate the effects of data poisoning on 23 frontier LLMs ranging from 1.5-72\nbillion parameters on three datasets which speak to each of our threat models.\nWe find that larger LLMs are increasingly vulnerable, learning harmful behavior\n-- including sleeper agent behavior -- significantly more quickly than smaller\nLLMs with even minimal data poisoning. These results underscore the need for\nrobust safeguards against data poisoning in larger LLMs.\n","authors":["Dillon Bowen","Brendan Murphy","Will Cai","David Khachaturov","Adam Gleave","Kellin Pelrine"],"pdf_url":"https://arxiv.org/pdf/2408.02946v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2408.02336v2","updated":"2024-08-06T04:04:23Z","published":"2024-08-05T09:19:52Z","title":"Infusing Environmental Captions for Long-Form Video Language Grounding","summary":"  In this work, we tackle the problem of long-form video-language grounding\n(VLG). Given a long-form video and a natural language query, a model should\ntemporally localize the precise moment that answers the query. Humans can\neasily solve VLG tasks, even with arbitrarily long videos, by discarding\nirrelevant moments using extensive and robust knowledge gained from experience.\nUnlike humans, existing VLG methods are prone to fall into superficial cues\nlearned from small-scale datasets, even when they are within irrelevant frames.\nTo overcome this challenge, we propose EI-VLG, a VLG method that leverages\nricher textual information provided by a Multi-modal Large Language Model\n(MLLM) as a proxy for human experiences, helping to effectively exclude\nirrelevant frames. We validate the effectiveness of the proposed method via\nextensive experiments on a challenging EgoNLQ benchmark.\n","authors":["Hyogun Lee","Soyeon Hong","Mujeen Sung","Jinwoo Choi"],"pdf_url":"https://arxiv.org/pdf/2408.02336v2.pdf","comment":"7 pages, 3 figures"},{"id":"http://arxiv.org/abs/2312.05356v4","updated":"2024-08-06T03:57:33Z","published":"2023-12-08T20:28:08Z","title":"Neuron Patching: Semantic-based Neuron-level Language Model Repair for\n  Code Generation","summary":"  Large Language Models (LLMs) have already gained widespread adoption in\nsoftware engineering, particularly in code generation tasks. However, updating\nthese models with new knowledge can be prohibitively expensive, yet it is\nessential to maximize their utility, such as implementing a hotfix technique to\naddress urgent or critical LLM errors. In this paper, we propose \\textsc{MENT},\na novel and effective model editing approach to repair LLMs in coding tasks.\n\\textsc{MENT} is effective, efficient, and reliable, capable of correcting a\nneural model by patching just one or two neurons. As pioneering work on\nneuron-level model editing of generative models, we formalize the editing\nprocess and introduce the involved concepts. We also introduce new measures to\nevaluate its generalization ability and establish a benchmark for further\nstudy. Our approach is evaluated on three coding tasks: line-level code\ngeneration, shellcode generation, and intent-to-bash translation. The\nexperimental results demonstrate that the proposed approach significantly\noutperforms the state-of-the-art in both effectiveness and efficiency measures.\nFurthermore, we showcase the applications of \\textsc{MENT} for LLM reasoning in\nsoftware engineering. By editing LLM knowledge, the directly or indirectly\ndependent behaviors of API invocation in the chain-of-thought change\naccordingly. This illustrates the significance of repairing LLMs in the context\nof software engineering.\n","authors":["Jian Gu","Aldeida Aleti","Chunyang Chen","Hongyu Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.05356v4.pdf","comment":"12 pages, 7 figures, 7 tables, under peer-review"},{"id":"http://arxiv.org/abs/2408.02936v1","updated":"2024-08-06T03:42:38Z","published":"2024-08-06T03:42:38Z","title":"Achieving More with Less: A Tensor-Optimization-Powered Ensemble Method","summary":"  Ensemble learning is a method that leverages weak learners to produce a\nstrong learner. However, obtaining a large number of base learners requires\nsubstantial time and computational resources. Therefore, it is meaningful to\nstudy how to achieve the performance typically obtained with many base learners\nusing only a few. We argue that to achieve this, it is essential to enhance\nboth classification performance and generalization ability during the ensemble\nprocess. To increase model accuracy, each weak base learner needs to be more\nefficiently integrated. It is observed that different base learners exhibit\nvarying levels of accuracy in predicting different classes. To capitalize on\nthis, we introduce confidence tensors $\\tilde{\\mathbf{\\Theta}}$ and\n$\\tilde{\\mathbf{\\Theta}}_{rst}$ signifies that the $t$-th base classifier\nassigns the sample to class $r$ while it actually belongs to class $s$. To the\nbest of our knowledge, this is the first time an evaluation of the performance\nof base classifiers across different classes has been proposed. The proposed\nconfidence tensor compensates for the strengths and weaknesses of each base\nclassifier in different classes, enabling the method to achieve superior\nresults with a smaller number of base learners. To enhance generalization\nperformance, we design a smooth and convex objective function that leverages\nthe concept of margin, making the strong learner more discriminative.\nFurthermore, it is proved that in gradient matrix of the loss function, the sum\nof each column's elements is zero, allowing us to solve a constrained\noptimization problem using gradient-based methods. We then compare our\nalgorithm with random forests of ten times the size and other classical methods\nacross numerous datasets, demonstrating the superiority of our approach.\n","authors":["Jinghui Yuan","Weijin Jiang","Zhe Cao","Fangyuan Xie","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02936v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02932v1","updated":"2024-08-06T03:34:43Z","published":"2024-08-06T03:34:43Z","title":"Doubly Stochastic Adaptive Neighbors Clustering via the Marcus Mapping","summary":"  Clustering is a fundamental task in machine learning and data science, and\nsimilarity graph-based clustering is an important approach within this domain.\nDoubly stochastic symmetric similarity graphs provide numerous benefits for\nclustering problems and downstream tasks, yet learning such graphs remains a\nsignificant challenge. Marcus theorem states that a strictly positive symmetric\nmatrix can be transformed into a doubly stochastic symmetric matrix by diagonal\nmatrices. However, in clustering, learning sparse matrices is crucial for\ncomputational efficiency. We extend Marcus theorem by proposing the Marcus\nmapping, which indicates that certain sparse matrices can also be transformed\ninto doubly stochastic symmetric matrices via diagonal matrices. Additionally,\nwe introduce rank constraints into the clustering problem and propose the\nDoubly Stochastic Adaptive Neighbors Clustering algorithm based on the Marcus\nMapping (ANCMM). This ensures that the learned graph naturally divides into the\ndesired number of clusters. We validate the effectiveness of our algorithm\nthrough extensive comparisons with state-of-the-art algorithms. Finally, we\nexplore the relationship between the Marcus mapping and optimal transport. We\nprove that the Marcus mapping solves a specific type of optimal transport\nproblem and demonstrate that solving this problem through Marcus mapping is\nmore efficient than directly applying optimal transport methods.\n","authors":["Jinghui Yuan","Chusheng Zeng","Fangyuan Xie","Zhe Cao","Rong Wang","Feiping Nie","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2201.10838v7","updated":"2024-08-06T03:34:27Z","published":"2022-01-26T09:44:13Z","title":"Privacy-Preserving Logistic Regression Training with A Faster Gradient\n  Variant","summary":"  Logistic regression training over encrypted data has been an attractive idea\nto security concerns for years. In this paper, we propose a faster gradient\nvariant called $\\texttt{quadratic gradient}$ for privacy-preserving logistic\nregression training. The core of $\\texttt{quadratic gradient}$ can be seen as\nan extension of the simplified fixed Hessian. We enhance Nesterov's accelerated\ngradient (NAG) and Adaptive Gradient Algorithm (Adagrad) respectively with\n$\\texttt{quadratic gradient}$ and evaluate the enhanced algorithms on several\ndatasets. Experiments show that the enhanced methods have a state-of-the-art\nperformance in convergence speed compared to the raw first-order gradient\nmethods. We then adopt the enhanced NAG method to implement homomorphic\nlogistic regression training, obtaining a comparable result by only $3$\niterations. There is a promising chance that $\\texttt{quadratic gradient}$\ncould be used to enhance other first-order gradient methods for general\nnumerical optimization problems.\n","authors":["John Chiang"],"pdf_url":"https://arxiv.org/pdf/2201.10838v7.pdf","comment":"The basic work of this paper, $\\texttt{quadratic gradient}$ and the\n  enhanced full batch NAG, was nearly finished in September 2019. The initial\n  version of this paper was written in April 2020, rejected by ICANN 2020. The\n  enhanced mini-batch NAG was introduced into this paper in September 2020 and\n  later rejected by a special issue on the journal FGCS 2020"},{"id":"http://arxiv.org/abs/2402.12022v2","updated":"2024-08-06T03:34:06Z","published":"2024-02-19T10:31:53Z","title":"Distilling Large Language Models for Text-Attributed Graph Learning","summary":"  Text-Attributed Graphs (TAGs) are graphs of connected textual documents.\nGraph models can efficiently learn TAGs, but their training heavily relies on\nhuman-annotated labels, which are scarce or even unavailable in many\napplications. Large language models (LLMs) have recently demonstrated\nremarkable capabilities in few-shot and zero-shot TAG learning, but they suffer\nfrom scalability, cost, and privacy issues. Therefore, in this work, we focus\non synergizing LLMs and graph models with their complementary strengths by\ndistilling the power of LLMs to a local graph model on TAG learning. To address\nthe inherent gaps between LLMs (generative models for texts) and graph models\n(discriminative models for graphs), we propose first to let LLMs teach an\ninterpreter with rich textual rationale and then let a student model mimic the\ninterpreter's reasoning without LLMs' textual rationale. Extensive experiments\nvalidate the efficacy of our proposed framework.\n","authors":["Bo Pan","Zheng Zhang","Yifei Zhang","Yuntong Hu","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.12022v2.pdf","comment":"CIKM 2024"},{"id":"http://arxiv.org/abs/2404.06675v2","updated":"2024-08-06T03:33:41Z","published":"2024-04-10T01:35:17Z","title":"Toward Cross-Layer Energy Optimizations in AI Systems","summary":"  The \"AI for Science, Energy, and Security\" report from DOE outlines a\nsignificant focus on developing and optimizing artificial intelligence\nworkflows for a foundational impact on a broad range of DOE missions. With the\npervasive usage of artificial intelligence (AI) and machine learning (ML) tools\nand techniques, their energy efficiency is likely to become the gating factor\ntoward adoption. This is because generative AI (GenAI) models are massive\nenergy hogs: for instance, training a 200-billion parameter large language\nmodel (LLM) at Amazon is estimated to have taken 11.9 GWh, which is enough to\npower more than a thousand average U.S. households for a year. Inference\nconsumes even more energy, because a model trained once serve millions. Given\nthis scale, high energy efficiency is key to addressing the power delivery\nproblem of constructing and operating new supercomputers and datacenters\nspecialized for AI workloads. In that regard, we outline software- and\narchitecture-level research challenges and opportunities, setting the stage for\ncreating cross-layer energy optimizations in AI systems.\n","authors":["Jae-Won Chung","Nishil Talati","Mosharaf Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2404.06675v2.pdf","comment":"2024 Energy-Efficient Computing for Science Workshop"},{"id":"http://arxiv.org/abs/2408.02930v1","updated":"2024-08-06T03:26:01Z","published":"2024-08-06T03:26:01Z","title":"The Need for a Big World Simulator: A Scientific Challenge for Continual\n  Learning","summary":"  The \"small agent, big world\" frame offers a conceptual view that motivates\nthe need for continual learning. The idea is that a small agent operating in a\nmuch bigger world cannot store all information that the world has to offer. To\nperform well, the agent must be carefully designed to ingest, retain, and eject\nthe right information. To enable the development of performant continual\nlearning agents, a number of synthetic environments have been proposed.\nHowever, these benchmarks suffer from limitations, including unnatural\ndistribution shifts and a lack of fidelity to the \"small agent, big world\"\nframing. This paper aims to formalize two desiderata for the design of future\nsimulated environments. These two criteria aim to reflect the objectives and\ncomplexity of continual learning in practical settings while enabling rapid\nprototyping of algorithms on a smaller scale.\n","authors":["Saurabh Kumar","Hong Jun Jeon","Alex Lewandowski","Benjamin Van Roy"],"pdf_url":"https://arxiv.org/pdf/2408.02930v1.pdf","comment":"Accepted to the Finding the Frame Workshop at RLC 2024"},{"id":"http://arxiv.org/abs/2408.02927v1","updated":"2024-08-06T03:21:13Z","published":"2024-08-06T03:21:13Z","title":"HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy\n  Protection","summary":"  Data serves as the fundamental foundation for advancing deep learning,\nparticularly tabular data presented in a structured format, which is highly\nconducive to modeling. However, even in the era of LLM, obtaining tabular data\nfrom sensitive domains remains a challenge due to privacy or copyright\nconcerns. Hence, exploring how to effectively use models like LLMs to generate\nrealistic and privacy-preserving synthetic tabular data is urgent. In this\npaper, we take a step forward to explore LLMs for tabular data synthesis and\nprivacy protection, by introducing a new framework HARMONIC for tabular data\ngeneration and evaluation. In the tabular data generation of our framework,\nunlike previous small-scale LLM-based methods that rely on continued\npre-training, we explore the larger-scale LLMs with fine-tuning to generate\ntabular data and enhance privacy. Based on idea of the k-nearest neighbors\nalgorithm, an instruction fine-tuning dataset is constructed to inspire LLMs to\ndiscover inter-row relationships. Then, with fine-tuning, LLMs are trained to\nremember the format and connections of the data rather than the data itself,\nwhich reduces the risk of privacy leakage. In the evaluation part of our\nframework, we develop specific privacy risk metrics DLT for LLM synthetic data\ngeneration, as well as performance evaluation metrics LLE for downstream LLM\ntasks. Our experiments find that this tabular data generation framework\nachieves equivalent performance to existing methods with better privacy, which\nalso demonstrates our evaluation framework for the effectiveness of synthetic\ndata and privacy risks in LLM scenarios.\n","authors":["Yuxin Wang","Duanyu Feng","Yongfu Dai","Zhengyu Chen","Jimin Huang","Sophia Ananiadou","Qianqian Xie","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2408.02927v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00799v2","updated":"2024-08-06T03:03:16Z","published":"2024-07-22T02:27:57Z","title":"Deep Uncertainty-Based Explore for Index Construction and Retrieval in\n  Recommendation System","summary":"  In recommendation systems, the relevance and novelty of the final results are\nselected through a cascade system of Matching -> Ranking -> Strategy. The\nmatching model serves as the starting point of the pipeline and determines the\nupper bound of the subsequent stages. Balancing the relevance and novelty of\nmatching results is a crucial step in the design and optimization of\nrecommendation systems, contributing significantly to improving recommendation\nquality. However, the typical matching algorithms have not simultaneously\naddressed the relevance and novelty perfectly. One main reason is that deep\nmatching algorithms exhibit significant uncertainty when estimating items in\nthe long tail (e.g., due to insufficient training samples) items.The\nuncertainty not only affects the training of the models but also influences the\nconfidence in the index construction and beam search retrieval process of these\nmodels. This paper proposes the UICR (Uncertainty-based explore for Index\nConstruction and Retrieval) algorithm, which introduces the concept of\nuncertainty modeling in the matching stage and achieves multi-task modeling of\nmodel uncertainty and index uncertainty. The final matching results are\nobtained by combining the relevance score and uncertainty score infered by the\nmodel. Experimental results demonstrate that the UICR improves novelty without\nsacrificing relevance on realworld industrial productive environments and\nmultiple open-source datasets. Remarkably, online A/B test results of display\nadvertising in Shopee demonstrates the effectiveness of the proposed algorithm.\n","authors":["Xin Jiang","Kaiqiang Wang","Yinlong Wang","Fengchang Lv","Taiyang Peng","Shuai Yang","Xianteng Wu","Pengye Zhang","Shuo Yuan","Yifan Zeng"],"pdf_url":"https://arxiv.org/pdf/2408.00799v2.pdf","comment":"accepted by cikm2024"},{"id":"http://arxiv.org/abs/2404.07950v3","updated":"2024-08-06T02:42:32Z","published":"2024-03-18T16:50:23Z","title":"Reinforcement Learning with Generalizable Gaussian Splatting","summary":"  An excellent representation is crucial for reinforcement learning (RL)\nperformance, especially in vision-based reinforcement learning tasks. The\nquality of the environment representation directly influences the achievement\nof the learning task. Previous vision-based RL typically uses explicit or\nimplicit ways to represent environments, such as images, points, voxels, and\nneural radiance fields. However, these representations contain several\ndrawbacks. They cannot either describe complex local geometries or generalize\nwell to unseen scenes, or require precise foreground masks. Moreover, these\nimplicit neural representations are akin to a ``black box\", significantly\nhindering interpretability. 3D Gaussian Splatting (3DGS), with its explicit\nscene representation and differentiable rendering nature, is considered a\nrevolutionary change for reconstruction and representation methods. In this\npaper, we propose a novel Generalizable Gaussian Splatting framework to be the\nrepresentation of RL tasks, called GSRL. Through validation in the RoboMimic\nenvironment, our method achieves better results than other baselines in\nmultiple tasks, improving the performance by 10%, 44%, and 15% compared with\nbaselines on the hardest task. This work is the first attempt to leverage\ngeneralizable 3DGS as a representation for RL.\n","authors":["Jiaxu Wang","Qiang Zhang","Jingkai Sun","Jiahang Cao","Gang Han","Wen Zhao","Weining Zhang","Yecheng Shao","Yijie Guo","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2404.07950v3.pdf","comment":"7 pages,2 figures"},{"id":"http://arxiv.org/abs/2402.13699v4","updated":"2024-08-06T02:32:36Z","published":"2024-02-21T11:00:23Z","title":"Automation of Quantum Dot Measurement Analysis via Explainable Machine\n  Learning","summary":"  The rapid development of quantum dot (QD) devices for quantum computing has\nnecessitated more efficient and automated methods for device characterization\nand tuning. Many of the measurements acquired during the tuning process come in\nthe form of images that need to be properly analyzed to guide the subsequent\ntuning steps. By design, features present in such images capture certain\nbehaviors or states of the measured QD devices. When considered carefully, such\nfeatures can aid the control and calibration of QD devices. An important\nexample of such images are so-called \\textit{triangle plots}, which visually\nrepresent current flow and reveal characteristics important for QD device\ncalibration. While image-based classification tools, such as convolutional\nneural networks (CNNs), can be used to verify whether a given measurement is\n\\textit{good} and thus warrants the initiation of the next phase of tuning,\nthey do not provide any insights into how the device should be adjusted in the\ncase of \\textit{bad} images. This is because CNNs sacrifice prediction and\nmodel intelligibility for high accuracy. To ameliorate this trade-off, a recent\nstudy introduced an image vectorization approach that relies on the Gabor\nwavelet transform [1]. Here we propose an alternative vectorization method that\ninvolves mathematical modeling of synthetic triangles to mimic the experimental\ndata. Using explainable boosting machines, we show that this new method offers\nsuperior explainability of model prediction without sacrificing accuracy. This\nwork demonstrates the feasibility and advantages of applying explainable\nmachine learning techniques to the analysis of quantum dot measurements, paving\nthe way for further advances in automated and transparent QD device tuning.\n","authors":["Daniel Schug","Tyler J. Kovach","M. A. Wolfe","Jared Benson","Sanghyeok Park","J. P. Dodson","J. Corrigan","M. A. Eriksson","Justyna P. Zwolak"],"pdf_url":"https://arxiv.org/pdf/2402.13699v4.pdf","comment":"17 pages, 4 figures, abbreviated version published in Proceedings of\n  the XAI4Sci: Explainable machine learning for sciences workshop at AAAI 2024,\n  (Vancouver, Canada)"},{"id":"http://arxiv.org/abs/2305.09958v3","updated":"2024-08-06T02:32:05Z","published":"2023-05-17T05:35:49Z","title":"SIGMA: Similarity-based Efficient Global Aggregation for Heterophilous\n  Graph Neural Networks","summary":"  Graph neural networks (GNNs) realize great success in graph learning but\nsuffer from performance loss when meeting heterophily, i.e. neighboring nodes\nare dissimilar, due to their local and uniform aggregation. Existing attempts\nof heterophilous GNNs incorporate long-range or global aggregations to\ndistinguish nodes in the graph. However, these aggregations usually require\niteratively maintaining and updating full-graph information, which limits their\nefficiency when applying to large-scale graphs. In this paper, we propose\nSIGMA, an efficient global heterophilous GNN aggregation integrating the\nstructural similarity measurement SimRank. Our theoretical analysis illustrates\nthat SIGMA inherently captures distant global similarity even under\nheterophily, that conventional approaches can only achieve after iterative\naggregations. Furthermore, it enjoys efficient one-time computation with a\ncomplexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive\nevaluation demonstrates that SIGMA achieves state-of-the-art performance with\nsuperior aggregation and overall efficiency. Notably, it obtains 5$\\times$\nacceleration on the large-scale heterophily dataset \\emph{pokec} with over 30\nmillion edges compared to the best baseline aggregation.\n","authors":["Haoyu Liu","Ningyi Liao","Siqiang Luo"],"pdf_url":"https://arxiv.org/pdf/2305.09958v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02897v1","updated":"2024-08-06T02:06:04Z","published":"2024-08-06T02:06:04Z","title":"A Metric Driven Approach to Mixed Precision Training","summary":"  As deep learning methodologies have developed, it has been generally agreed\nthat increasing neural network size improves model quality. However, this is at\nthe expense of memory and compute requirements, which also need to be\nincreased. Various efficiency techniques have been proposed to rein in hardware\ncosts, one being the use of low precision numerics. Recent accelerators have\nintroduced several different 8-bit data types to help accommodate DNNs in terms\nof numerics. In this paper, we identify a metric driven methodology to aid in\nthe choice of numerics. We demonstrate how such a methodology can help scale\ntraining of a language representation model. The technique can be generalized\nto other model architectures.\n","authors":["Mitchelle Rasquinha","Gil Tabak"],"pdf_url":"https://arxiv.org/pdf/2408.02897v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12428v3","updated":"2024-08-06T01:45:44Z","published":"2023-10-19T02:42:20Z","title":"Enhanced Local Explainability and Trust Scores with Random Forest\n  Proximities","summary":"  We initiate a novel approach to explain the predictions and out of sample\nperformance of random forest (RF) regression and classification models by\nexploiting the fact that any RF can be mathematically formulated as an adaptive\nweighted K nearest-neighbors model. Specifically, we employ a recent result\nthat, for both regression and classification tasks, any RF prediction can be\nrewritten exactly as a weighted sum of the training targets, where the weights\nare RF proximities between the corresponding pairs of data points. We show that\nthis linearity facilitates a local notion of explainability of RF predictions\nthat generates attributions for any model prediction across observations in the\ntraining set, and thereby complements established feature-based methods like\nSHAP, which generate attributions for a model prediction across input features.\nWe show how this proximity-based approach to explainability can be used in\nconjunction with SHAP to explain not just the model predictions, but also\nout-of-sample performance, in the sense that proximities furnish a novel means\nof assessing when a given model prediction is more or less likely to be\ncorrect. We demonstrate this approach in the modeling of US corporate bond\nprices and returns in both regression and classification cases.\n","authors":["Joshua Rosaler","Dhruv Desai","Bhaskarjit Sarmah","Dimitrios Vamvourellis","Deran Onay","Dhagash Mehta","Stefano Pasquali"],"pdf_url":"https://arxiv.org/pdf/2310.12428v3.pdf","comment":"5 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.19779v2","updated":"2024-08-06T01:25:33Z","published":"2024-05-30T07:44:31Z","title":"Automatic Graph Topology-Aware Transformer","summary":"  Existing efforts are dedicated to designing many topologies and graph-aware\nstrategies for the graph Transformer, which greatly improve the model's\nrepresentation capabilities. However, manually determining the suitable\nTransformer architecture for a specific graph dataset or task requires\nextensive expert knowledge and laborious trials. This paper proposes an\nevolutionary graph Transformer architecture search framework (EGTAS) to\nautomate the construction of strong graph Transformers. We build a\ncomprehensive graph Transformer search space with the micro-level and\nmacro-level designs. EGTAS evolves graph Transformer topologies at the macro\nlevel and graph-aware strategies at the micro level. Furthermore, a surrogate\nmodel based on generic architectural coding is proposed to directly predict the\nperformance of graph Transformers, substantially reducing the evaluation cost\nof evolutionary search. We demonstrate the efficacy of EGTAS across a range of\ngraph-level and node-level tasks, encompassing both small-scale and large-scale\ngraph datasets. Experimental results and ablation studies show that EGTAS can\nconstruct high-performance architectures that rival state-of-the-art manual and\nautomated baselines.\n","authors":["Chao Wang","Jiaxuan Zhao","Lingling Li","Licheng Jiao","Fang Liu","Shuyuan Yang"],"pdf_url":"https://arxiv.org/pdf/2405.19779v2.pdf","comment":"This work has been accepted by IEEE Transactions on Neural Networks\n  and Learning Systems. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.02882v1","updated":"2024-08-06T01:20:12Z","published":"2024-08-06T01:20:12Z","title":"Compromising Embodied Agents with Contextual Backdoor Attacks","summary":"  Large language models (LLMs) have transformed the development of embodied\nintelligence. By providing a few contextual demonstrations, developers can\nutilize the extensive internal knowledge of LLMs to effortlessly translate\ncomplex tasks described in abstract language into sequences of code snippets,\nwhich will serve as the execution logic for embodied agents. However, this\npaper uncovers a significant backdoor security threat within this process and\nintroduces a novel method called \\method{}. By poisoning just a few contextual\ndemonstrations, attackers can covertly compromise the contextual environment of\na black-box LLM, prompting it to generate programs with context-dependent\ndefects. These programs appear logically sound but contain defects that can\nactivate and induce unintended behaviors when the operational agent encounters\nspecific triggers in its interactive environment. To compromise the LLM's\ncontextual environment, we employ adversarial in-context generation to optimize\npoisoned demonstrations, where an LLM judge evaluates these poisoned prompts,\nreporting to an additional LLM that iteratively optimizes the demonstration in\na two-player adversarial game using chain-of-thought reasoning. To enable\ncontext-dependent behaviors in downstream agents, we implement a dual-modality\nactivation strategy that controls both the generation and execution of program\ndefects through textual and visual triggers. We expand the scope of our attack\nby developing five program defect modes that compromise key aspects of\nconfidentiality, integrity, and availability in embodied agents. To validate\nthe effectiveness of our approach, we conducted extensive experiments across\nvarious tasks, including robot planning, robot manipulation, and compositional\nvisual reasoning. Additionally, we demonstrate the potential impact of our\napproach by successfully attacking real-world autonomous driving systems.\n","authors":["Aishan Liu","Yuguang Zhou","Xianglong Liu","Tianyuan Zhang","Siyuan Liang","Jiakai Wang","Yanjun Pu","Tianlin Li","Junqi Zhang","Wenbo Zhou","Qing Guo","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2408.02882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01331v2","updated":"2024-08-06T01:10:34Z","published":"2024-08-02T15:29:39Z","title":"UnifiedNN: Efficient Neural Network Training on the Cloud","summary":"  Nowadays, cloud-based services are widely favored over the traditional\napproach of locally training a Neural Network (NN) model. Oftentimes, a cloud\nservice processes multiple requests from users--thus training multiple NN\nmodels concurrently. However, training NN models concurrently is a challenging\nprocess, which typically requires significant amounts of available computing\nresources and takes a long time to complete. In this paper, we present\nUnifiedNN to effectively train multiple NN models concurrently on the cloud.\nUnifiedNN effectively \"combines\" multiple NN models and features several memory\nand time conservation mechanisms to train multiple NN models simultaneously\nwithout impacting the accuracy of the training process. Specifically, UnifiedNN\nmerges multiple NN models and creates a large singular unified model in order\nto efficiently train all models at once. We have implemented a prototype of\nUnifiedNN in PyTorch and we have compared its performance with relevant\nstate-of-the-art frameworks. Our experimental results demonstrate that\nUnifiedNN can reduce memory consumption by up to 53% and training time by up to\n81% when compared with vanilla PyTorch without impacting the model training and\ntesting accuracy. Finally, our results indicate that UnifiedNN can reduce\nmemory consumption by up to 52% and training time by up to 41% when compared to\nstate-of-the-art frameworks when training multiple models concurrently.\n","authors":["Sifat Ut Taki","Arthi Padmanabhan","Spyridon Mastorakis"],"pdf_url":"https://arxiv.org/pdf/2408.01331v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.07143v2","updated":"2024-08-06T00:51:37Z","published":"2023-10-11T02:36:52Z","title":"Imitation Learning from Purified Demonstrations","summary":"  Imitation learning has emerged as a promising approach for addressing\nsequential decision-making problems, with the assumption that expert\ndemonstrations are optimal. However, in real-world scenarios, most\ndemonstrations are often imperfect, leading to challenges in the effectiveness\nof imitation learning. While existing research has focused on optimizing with\nimperfect demonstrations, the training typically requires a certain proportion\nof optimal demonstrations to guarantee performance. To tackle these problems,\nwe propose to purify the potential noises in imperfect demonstrations first,\nand subsequently conduct imitation learning from these purified demonstrations.\nMotivated by the success of diffusion model, we introduce a two-step\npurification via diffusion process. In the first step, we apply a forward\ndiffusion process to smooth potential noises in imperfect demonstrations by\nintroducing additional noise. Subsequently, a reverse generative process is\nutilized to recover the optimal demonstration from the diffused ones. We\nprovide theoretical evidence supporting our approach, demonstrating that the\ndistance between the purified and optimal demonstration can be bounded.\nEmpirical results on MuJoCo and RoboSuite demonstrate the effectiveness of our\nmethod from different aspects.\n","authors":["Yunke Wang","Minjing Dong","Yukun Zhao","Bo Du","Chang Xu"],"pdf_url":"https://arxiv.org/pdf/2310.07143v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/1912.03573v2","updated":"2024-08-06T00:50:41Z","published":"2019-12-07T23:02:02Z","title":"Deep Variable-Block Chain with Adaptive Variable Selection","summary":"  The architectures of deep neural networks (DNN) rely heavily on the\nunderlying grid structure of variables, for instance, the lattice of pixels in\nan image. For general high dimensional data with variables not associated with\na grid, the multi-layer perceptron and deep belief network are often used.\nHowever, it is frequently observed that those networks do not perform\ncompetitively and they are not helpful for identifying important variables. In\nthis paper, we propose a framework that imposes on blocks of variables a chain\nstructure obtained by step-wise greedy search so that the DNN architecture can\nleverage the constructed grid. We call this new neural network Deep\nVariable-Block Chain (DVC). Because the variable blocks are used for\nclassification in a sequential manner, we further develop the capacity of\nselecting variables adaptively according to a number of regions trained by a\ndecision tree. Our experiments show that DVC outperforms other generic DNNs and\nother strong classifiers. Moreover, DVC can achieve high accuracy at much\nreduced dimensionality and sometimes reveals drastically different sets of\nrelevant variables for different regions.\n","authors":["Lixiang Zhang","Lin Lin","Jia Li"],"pdf_url":"https://arxiv.org/pdf/1912.03573v2.pdf","comment":"24 pages, 5 figures"},{"id":"http://arxiv.org/abs/2310.08847v3","updated":"2024-08-06T00:01:01Z","published":"2023-10-13T04:14:51Z","title":"On the Over-Memorization During Natural, Robust and Catastrophic\n  Overfitting","summary":"  Overfitting negatively impacts the generalization ability of deep neural\nnetworks (DNNs) in both natural and adversarial training. Existing methods\nstruggle to consistently address different types of overfitting, typically\ndesigning strategies that focus separately on either natural or adversarial\npatterns. In this work, we adopt a unified perspective by solely focusing on\nnatural patterns to explore different types of overfitting. Specifically, we\nexamine the memorization effect in DNNs and reveal a shared behaviour termed\nover-memorization, which impairs their generalization capacity. This behaviour\nmanifests as DNNs suddenly becoming high-confidence in predicting certain\ntraining patterns and retaining a persistent memory for them. Furthermore, when\nDNNs over-memorize an adversarial pattern, they tend to simultaneously exhibit\nhigh-confidence prediction for the corresponding natural pattern. These\nfindings motivate us to holistically mitigate different types of overfitting by\nhindering the DNNs from over-memorization training patterns. To this end, we\npropose a general framework, Distraction Over-Memorization (DOM), which\nexplicitly prevents over-memorization by either removing or augmenting the\nhigh-confidence natural patterns. Extensive experiments demonstrate the\neffectiveness of our proposed method in mitigating overfitting across various\ntraining paradigms.\n","authors":["Runqi Lin","Chaojian Yu","Bo Han","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2310.08847v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00391v3","updated":"2024-08-06T23:58:07Z","published":"2023-12-31T04:14:43Z","title":"SAFE-SIM: Safety-Critical Closed-Loop Traffic Simulation with\n  Diffusion-Controllable Adversaries","summary":"  Evaluating the performance of autonomous vehicle planning algorithms\nnecessitates simulating long-tail safety-critical traffic scenarios. However,\ntraditional methods for generating such scenarios often fall short in terms of\ncontrollability and realism; they also neglect the dynamics of agent\ninteractions. To address these limitations, we introduce SAFE-SIM, a novel\ndiffusion-based controllable closed-loop safety-critical simulation framework.\nOur approach yields two distinct advantages: 1) generating realistic long-tail\nsafety-critical scenarios that closely reflect real-world conditions, and 2)\nproviding controllable adversarial behavior for more comprehensive and\ninteractive evaluations. We develop a novel approach to simulate\nsafety-critical scenarios through an adversarial term in the denoising process\nof diffusion models, which allows an adversarial agent to challenge a planner\nwith plausible maneuvers while all agents in the scene exhibit reactive and\nrealistic behaviors. Furthermore, we propose novel guidance objectives and a\npartial diffusion process that enables users to control key aspects of the\nscenarios, such as the collision type and aggressiveness of the adversarial\nagent, while maintaining the realism of the behavior. We validate our framework\nempirically using the nuScenes and nuPlan datasets across multiple planners,\ndemonstrating improvements in both realism and controllability. These findings\naffirm that diffusion models provide a robust and versatile foundation for\nsafety-critical, interactive traffic simulation, extending their utility across\nthe broader autonomous driving landscape. Project website:\nhttps://safe-sim.github.io/.\n","authors":["Wei-Jer Chang","Francesco Pittaluga","Masayoshi Tomizuka","Wei Zhan","Manmohan Chandraker"],"pdf_url":"https://arxiv.org/pdf/2401.00391v3.pdf","comment":"Accepted by ECCV2024; Project website: https://safe-sim.github.io/"},{"id":"http://arxiv.org/abs/2408.03480v1","updated":"2024-08-06T23:43:03Z","published":"2024-08-06T23:43:03Z","title":"Advancing EEG-Based Gaze Prediction Using Depthwise Separable\n  Convolution and Enhanced Pre-Processing","summary":"  In the field of EEG-based gaze prediction, the application of deep learning\nto interpret complex neural data poses significant challenges. This study\nevaluates the effectiveness of pre-processing techniques and the effect of\nadditional depthwise separable convolution on EEG vision transformers (ViTs) in\na pretrained model architecture. We introduce a novel method, the EEG Deeper\nClustered Vision Transformer (EEG-DCViT), which combines depthwise separable\nconvolutional neural networks (CNNs) with vision transformers, enriched by a\npre-processing strategy involving data clustering. The new approach\ndemonstrates superior performance, establishing a new benchmark with a Root\nMean Square Error (RMSE) of 51.6 mm. This achievement underscores the impact of\npre-processing and model refinement in enhancing EEG-based applications.\n","authors":["Matthew L Key","Tural Mehtiyev","Xiaodong Qu"],"pdf_url":"https://arxiv.org/pdf/2408.03480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03478v1","updated":"2024-08-06T23:34:49Z","published":"2024-08-06T23:34:49Z","title":"Effect of Kernel Size on CNN-Vision-Transformer-Based Gaze Prediction\n  Using Electroencephalography Data","summary":"  In this paper, we present an algorithm of gaze prediction from\nElectroencephalography (EEG) data. EEG-based gaze prediction is a new research\ntopic that can serve as an alternative to traditional video-based eye-tracking.\nCompared to the existing state-of-the-art (SOTA) method, we improved the root\nmean-squared-error of EEG-based gaze prediction to 53.06 millimeters, while\nreducing the training time to less than 33% of its original duration. Our\nsource code can be found at https://github.com/AmCh-Q/CSCI6907Project\n","authors":["Chuhui Qiu","Bugao Liang","Matthew L Key"],"pdf_url":"https://arxiv.org/pdf/2408.03478v1.pdf","comment":"International Conference on Human-Computer Interaction (HCII 2024)"},{"id":"http://arxiv.org/abs/2408.03475v1","updated":"2024-08-06T23:14:39Z","published":"2024-08-06T23:14:39Z","title":"Can LLMs Serve As Time Series Anomaly Detectors?","summary":"  An emerging topic in large language models (LLMs) is their application to\ntime series forecasting, characterizing mainstream and patternable\ncharacteristics of time series. A relevant but rarely explored and more\nchallenging question is whether LLMs can detect and explain time series\nanomalies, a critical task across various real-world applications. In this\npaper, we investigate the capabilities of LLMs, specifically GPT-4 and LLaMA3,\nin detecting and explaining anomalies in time series. Our studies reveal that:\n1) LLMs cannot be directly used for time series anomaly detection. 2) By\ndesigning prompt strategies such as in-context learning and chain-of-thought\nprompting, GPT-4 can detect time series anomalies with results competitive to\nbaseline methods. 3) We propose a synthesized dataset to automatically generate\ntime series anomalies with corresponding explanations. By applying instruction\nfine-tuning on this dataset, LLaMA3 demonstrates improved performance in time\nseries anomaly detection tasks. In summary, our exploration shows the promising\npotential of LLMs as time series anomaly detectors.\n","authors":["Manqing Dong","Hao Huang","Longbing Cao"],"pdf_url":"https://arxiv.org/pdf/2408.03475v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.08119v3","updated":"2024-08-06T23:09:06Z","published":"2024-01-16T05:23:34Z","title":"SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic\n  Spatio-Temporal Traffic Forecasting","summary":"  Traffic forecasting, a crucial application of spatio-temporal graph (STG)\nlearning, has traditionally relied on deterministic models for accurate point\nestimations. Yet, these models fall short of quantifying future uncertainties.\nRecently, many probabilistic methods, especially variants of diffusion models,\nhave been proposed to fill this gap. However, existing diffusion methods\ntypically deal with individual sensors separately when generating future time\nseries, resulting in limited usage of spatial information in the probabilistic\nlearning process. In this work, we propose SpecSTG, a novel spectral diffusion\nframework, to better leverage spatial dependencies and systematic patterns\ninherent in traffic data. More specifically, our method generates the Fourier\nrepresentation of future time series, transforming the learning process into\nthe spectral domain enriched with spatial information. Additionally, our\napproach incorporates a fast spectral graph convolution designed for Fourier\ninput, alleviating the computational burden associated with existing models.\nCompared with state-of-the-arts, SpecSTG achieves up to 8% improvements on\npoint estimations and up to 0.78% improvements on quantifying future\nuncertainties. Furthermore, SpecSTG's training and validation speed is 3.33X of\nthe most efficient existing diffusion method for STG forecasting. The source\ncode for SpecSTG is available at https://anonymous.4open.science/r/SpecSTG.\n","authors":["Lequan Lin","Dai Shi","Andi Han","Junbin Gao"],"pdf_url":"https://arxiv.org/pdf/2401.08119v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03472v1","updated":"2024-08-06T23:05:15Z","published":"2024-08-06T23:05:15Z","title":"Integrating HCI Datasets in Project-Based Machine Learning Courses: A\n  College-Level Review and Case Study","summary":"  This study explores the integration of real-world machine learning (ML)\nprojects using human-computer interfaces (HCI) datasets in college-level\ncourses to enhance both teaching and learning experiences. Employing a\ncomprehensive literature review, course websites analysis, and a detailed case\nstudy, the research identifies best practices for incorporating HCI datasets\ninto project-based ML education. Key f indings demonstrate increased student\nengagement, motivation, and skill development through hands-on projects, while\ninstructors benefit from effective tools for teaching complex concepts. The\nstudy also addresses challenges such as data complexity and resource\nallocation, offering recommendations for future improvements. These insights\nprovide a valuable framework for educators aiming to bridge the gap between\n","authors":["Xiaodong Qu","Matthew Key","Eric Luo","Chuhui Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.03472v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03464v1","updated":"2024-08-06T22:39:34Z","published":"2024-08-06T22:39:34Z","title":"AI Foundation Models in Remote Sensing: A Survey","summary":"  Artificial Intelligence (AI) technologies have profoundly transformed the\nfield of remote sensing, revolutionizing data collection, processing, and\nanalysis. Traditionally reliant on manual interpretation and task-specific\nmodels, remote sensing has been significantly enhanced by the advent of\nfoundation models--large-scale, pre-trained AI models capable of performing a\nwide array of tasks with unprecedented accuracy and efficiency. This paper\nprovides a comprehensive survey of foundation models in the remote sensing\ndomain, covering models released between June 2021 and June 2024. We categorize\nthese models based on their applications in computer vision and domain-specific\ntasks, offering insights into their architectures, pre-training datasets, and\nmethodologies. Through detailed performance comparisons, we highlight emerging\ntrends and the significant advancements achieved by these foundation models.\nAdditionally, we discuss the technical challenges, practical implications, and\nfuture research directions, addressing the need for high-quality data,\ncomputational resources, and improved model generalization. Our research also\nfinds that pre-training methods, particularly self-supervised learning\ntechniques like contrastive learning and masked autoencoders, significantly\nenhance the performance and robustness of foundation models in remote sensing\ntasks such as scene classification, object detection, and other applications.\nThis survey aims to serve as a resource for researchers and practitioners by\nproviding a panorama of advances and promising pathways for continued\ndevelopment and application of foundation models in remote sensing.\n","authors":["Siqi Lu","Junlin Guo","James R Zimmer-Dauphinee","Jordan M Nieusma","Xiao Wang","Parker VanValkenburgh","Steven A Wernke","Yuankai Huo"],"pdf_url":"https://arxiv.org/pdf/2408.03464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.07632v3","updated":"2024-08-06T22:37:21Z","published":"2024-03-12T13:12:24Z","title":"CardioGenAI: A Machine Learning-Based Framework for Re-Engineering Drugs\n  for Reduced hERG Liability","summary":"  The link between in vitro hERG ion channel inhibition and subsequent in vivo\nQT interval prolongation, a critical risk factor for the development of\narrythmias such as Torsade de Pointes, is so well established that in vitro\nhERG activity alone is often sufficient to end the development of an otherwise\npromising drug candidate. It is therefore of tremendous interest to develop\nadvanced methods for identifying hERG-active compounds in the early stages of\ndrug development, as well as for proposing redesigned compounds with reduced\nhERG liability and preserved on-target potency. In this work, we present\nCardioGenAI, a machine learning-based framework for re-engineering both\ndevelopmental and commercially available drugs for reduced hERG activity while\npreserving their pharmacological activity. The framework incorporates novel\nstate-of-the-art discriminative models for predicting hERG channel activity, as\nwell as activity against the voltage-gated NaV1.5 and CaV1.2 channels due to\ntheir potential implications in modulating the arrhythmogenic potential induced\nby hERG channel blockade. We applied the complete framework to pimozide, an\nFDA-approved antipsychotic agent that demonstrates high affinity to the hERG\nchannel, and generated 100 refined candidates. Remarkably, among the candidates\nis fluspirilene, a compound which is of the same class of drugs\n(diphenylmethanes) as pimozide and therefore has similar pharmacological\nactivity, yet exhibits over 700-fold weaker binding to hERG. We envision that\nthis method can effectively be applied to developmental compounds exhibiting\nhERG liabilities to provide a means of rescuing drug development programs that\nhave stalled due to hERG-related safety concerns. We have made all of our\nsoftware open-source to facilitate integration of the CardioGenAI framework for\nmolecular hypothesis generation into drug discovery workflows.\n","authors":["Gregory W. Kyro","Matthew T. Martin","Eric D. Watt","Victor S. Batista"],"pdf_url":"https://arxiv.org/pdf/2403.07632v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.11704v2","updated":"2024-08-06T22:37:06Z","published":"2024-06-17T16:25:04Z","title":"Nemotron-4 340B Technical Report","summary":"  We release the Nemotron-4 340B model family, including Nemotron-4-340B-Base,\nNemotron-4-340B-Instruct, and Nemotron-4-340B-Reward. Our models are open\naccess under the NVIDIA Open Model License Agreement, a permissive model\nlicense that allows distribution, modification, and use of the models and its\noutputs. These models perform competitively to open access models on a wide\nrange of evaluation benchmarks, and were sized to fit on a single DGX H100 with\n8 GPUs when deployed in FP8 precision. We believe that the community can\nbenefit from these models in various research studies and commercial\napplications, especially for generating synthetic data to train smaller\nlanguage models. Notably, over 98% of data used in our model alignment process\nis synthetically generated, showcasing the effectiveness of these models in\ngenerating synthetic data. To further support open research and facilitate\nmodel development, we are also open-sourcing the synthetic data generation\npipeline used in our model alignment process.\n","authors":[" Nvidia"," :","Bo Adler","Niket Agarwal","Ashwath Aithal","Dong H. Anh","Pallab Bhattacharya","Annika Brundyn","Jared Casper","Bryan Catanzaro","Sharon Clay","Jonathan Cohen","Sirshak Das","Ayush Dattagupta","Olivier Delalleau","Leon Derczynski","Yi Dong","Daniel Egert","Ellie Evans","Aleksander Ficek","Denys Fridman","Shaona Ghosh","Boris Ginsburg","Igor Gitman","Tomasz Grzegorzek","Robert Hero","Jining Huang","Vibhu Jawa","Joseph Jennings","Aastha Jhunjhunwala","John Kamalu","Sadaf Khan","Oleksii Kuchaiev","Patrick LeGresley","Hui Li","Jiwei Liu","Zihan Liu","Eileen Long","Ameya Sunil Mahabaleshwarkar","Somshubra Majumdar","James Maki","Miguel Martinez","Maer Rodrigues de Melo","Ivan Moshkov","Deepak Narayanan","Sean Narenthiran","Jesus Navarro","Phong Nguyen","Osvald Nitski","Vahid Noroozi","Guruprasad Nutheti","Christopher Parisien","Jupinder Parmar","Mostofa Patwary","Krzysztof Pawelec","Wei Ping","Shrimai Prabhumoye","Rajarshi Roy","Trisha Saar","Vasanth Rao Naik Sabavat","Sanjeev Satheesh","Jane Polak Scowcroft","Jason Sewall","Pavel Shamis","Gerald Shen","Mohammad Shoeybi","Dave Sizer","Misha Smelyanskiy","Felipe Soares","Makesh Narsimhan Sreedhar","Dan Su","Sandeep Subramanian","Shengyang Sun","Shubham Toshniwal","Hao Wang","Zhilin Wang","Jiaxuan You","Jiaqi Zeng","Jimmy Zhang","Jing Zhang","Vivienne Zhang","Yian Zhang","Chen Zhu"],"pdf_url":"https://arxiv.org/pdf/2406.11704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18742v5","updated":"2024-08-06T22:33:26Z","published":"2024-03-27T16:39:28Z","title":"Understanding the Learning Dynamics of Alignment with Human Feedback","summary":"  Aligning large language models (LLMs) with human intentions has become a\ncritical task for safely deploying models in real-world systems. While existing\nalignment approaches have seen empirical success, theoretically understanding\nhow these methods affect model behavior remains an open question. Our work\nprovides an initial attempt to theoretically analyze the learning dynamics of\nhuman preference alignment. We formally show how the distribution of preference\ndatasets influences the rate of model updates and provide rigorous guarantees\non the training accuracy. Our theory also reveals an intricate phenomenon where\nthe optimization is prone to prioritizing certain behaviors with higher\npreference distinguishability. We empirically validate our findings on\ncontemporary LLMs and alignment tasks, reinforcing our theoretical insights and\nshedding light on considerations for future alignment approaches. Disclaimer:\nThis paper contains potentially offensive text; reader discretion is advised.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2403.18742v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15750v2","updated":"2024-08-06T22:29:11Z","published":"2024-05-24T17:47:20Z","title":"Filtered Corpus Training (FiCT) Shows that Language Models can\n  Generalize from Indirect Evidence","summary":"  This paper introduces Filtered Corpus Training, a method that trains language\nmodels (LMs) on corpora with certain linguistic constructions filtered out from\nthe training data, and uses it to measure the ability of LMs to perform\nlinguistic generalization on the basis of indirect evidence. We apply the\nmethod to both LSTM and Transformer LMs (of roughly comparable size),\ndeveloping filtered corpora that target a wide range of linguistic phenomena.\nOur results show that while transformers are better qua LMs (as measured by\nperplexity), both models perform equally and surprisingly well on linguistic\ngeneralization measures, suggesting that they are capable of generalizing from\nindirect evidence.\n","authors":["Abhinav Patil","Jaap Jumelet","Yu Ying Chiu","Andy Lapastora","Peter Shen","Lexie Wang","Clevis Willrich","Shane Steinert-Threlkeld"],"pdf_url":"https://arxiv.org/pdf/2405.15750v2.pdf","comment":"Forthcoming in Transactions of the Association for Computational\n  Linguistics (TACL). This is a pre-MIT Press publication version. For code and\n  trained models, see http://github.com/CLMBRs/corpus-filtering"},{"id":"http://arxiv.org/abs/2401.12972v2","updated":"2024-08-06T22:28:25Z","published":"2024-01-23T18:58:35Z","title":"On the Efficacy of Text-Based Input Modalities for Action Anticipation","summary":"  Anticipating future actions is a highly challenging task due to the diversity\nand scale of potential future actions; yet, information from different\nmodalities help narrow down plausible action choices. Each modality can provide\ndiverse and often complementary context for the model to learn from. While\nprevious multi-modal methods leverage information from modalities such as video\nand audio, we primarily explore how text descriptions of actions and objects\ncan also lead to more accurate action anticipation by providing additional\ncontextual cues, e.g., about the environment and its contents. We propose a\nMulti-modal Contrastive Anticipative Transformer (M-CAT), a video transformer\narchitecture that jointly learns from multi-modal features and text\ndescriptions of actions and objects. We train our model in two stages, where\nthe model first learns to align video clips with descriptions of future\nactions, and is subsequently fine-tuned to predict future actions. Compared to\nexisting methods, M-CAT has the advantage of learning additional context from\ntwo types of text inputs: rich descriptions of future actions during\npre-training, and, text descriptions for detected objects and actions during\nmodality feature fusion. Through extensive experimental evaluation, we\ndemonstrate that our model outperforms previous methods on the EpicKitchens\ndatasets, and show that using simple text descriptions of actions and objects\naid in more effective action anticipation. In addition, we examine the impact\nof object and action information obtained via text, and perform extensive\nablations.\n","authors":["Apoorva Beedu","Karan Samel","Irfan Essa"],"pdf_url":"https://arxiv.org/pdf/2401.12972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03461v1","updated":"2024-08-06T22:14:54Z","published":"2024-08-06T22:14:54Z","title":"When does the mean network capture the topology of a sample of networks?","summary":"  The notion of Fr\\'echet mean (also known as \"barycenter\") network is the\nworkhorse of most machine learning algorithms that require the estimation of a\n\"location\" parameter to analyse network-valued data. In this context, it is\ncritical that the network barycenter inherits the topological structure of the\nnetworks in the training dataset. The metric - which measures the proximity\nbetween networks - controls the structural properties of the barycenter. This\nwork is significant because it provides for the first time analytical estimates\nof the sample Fr\\'echet mean for the stochastic blockmodel, which is at the\ncutting edge of rigorous probabilistic analysis of random networks. We show\nthat the mean network computed with the Hamming distance is unable to capture\nthe topology of the networks in the training sample, whereas the mean network\ncomputed using the effective resistance distance recovers the correct\npartitions and associated edge density. From a practical standpoint, our work\ninforms the choice of metrics in the context where the sample Fr\\'echet mean\nnetwork is used to characterise the topology of networks for network-valued\nmachine learning\n","authors":["Franois G Meyer"],"pdf_url":"https://arxiv.org/pdf/2408.03461v1.pdf","comment":"23 pages"},{"id":"http://arxiv.org/abs/2408.03459v1","updated":"2024-08-06T22:11:00Z","published":"2024-08-06T22:11:00Z","title":"On the Generalization of Preference Learning with DPO","summary":"  Large language models (LLMs) have demonstrated remarkable capabilities but\noften struggle to align with human preferences, leading to harmful or\nundesirable outputs. Preference learning, which trains models to distinguish\nbetween preferred and non-preferred responses based on human feedback, has\nbecome a crucial component for ensuring that LLMs align with human values.\nDespite the widespread adoption in real-world systems, a thorough theoretical\nunderstanding of the generalization guarantees for these models remain lacking.\nThis paper bridges that gap by introducing a new theoretical framework to\nanalyze the generalization guarantees of models trained with direct preference\noptimization (DPO). While existing generalization theory often focuses on\noverparameterized models achieving near-optimal loss or models independent of\nthe training process, our framework rigorously assesses how well models\ngeneralize after a finite number of gradient steps, reflecting real-world LLM\ntraining practices. By analyzing the reward margin associated with each sample\nand its trajectory throughout training, we can effectively bound the\ngeneralization error. We derive learning guarantees showing that, under\nspecific conditions, models trained with DPO can correctly discern preferred\nresponses on unseen data with high probability. These insights are empirically\nvalidated on contemporary LLMs, underscoring the practical relevance of our\ntheoretical findings.\n","authors":["Shawn Im","Yixuan Li"],"pdf_url":"https://arxiv.org/pdf/2408.03459v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.09753v2","updated":"2024-08-06T21:54:20Z","published":"2024-04-15T12:54:31Z","title":"Personalized Collaborative Fine-Tuning for On-Device Large Language\n  Models","summary":"  We explore on-device self-supervised collaborative fine-tuning of large\nlanguage models with limited local data availability. Taking inspiration from\nthe collaborative learning community, we introduce three distinct\ntrust-weighted gradient aggregation schemes: weight similarity-based,\nprediction similarity-based and validation performance-based. To minimize\ncommunication overhead, we integrate Low-Rank Adaptation (LoRA) and only\nexchange LoRA weight updates. Our protocols, driven by prediction and\nperformance metrics, surpass both FedAvg and local fine-tuning methods, which\nis particularly evident in realistic scenarios with more diverse local data\ndistributions. The results underscore the effectiveness of our approach in\naddressing heterogeneity and scarcity within local datasets.\n","authors":["Nicolas Wagner","Dongyang Fan","Martin Jaggi"],"pdf_url":"https://arxiv.org/pdf/2404.09753v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16266v2","updated":"2024-08-06T21:26:31Z","published":"2024-05-25T15:08:36Z","title":"Deep Reinforcement Learning with Enhanced PPO for Safe Mobile Robot\n  Navigation","summary":"  Collision-free motion is essential for mobile robots. Most approaches to\ncollision-free and efficient navigation with wheeled robots require parameter\ntuning by experts to obtain good navigation behavior. This study investigates\nthe application of deep reinforcement learning to train a mobile robot for\nautonomous navigation in a complex environment. The robot utilizes LiDAR sensor\ndata and a deep neural network to generate control signals guiding it toward a\nspecified target while avoiding obstacles. We employ two reinforcement learning\nalgorithms in the Gazebo simulation environment: Deep Deterministic Policy\nGradient and proximal policy optimization. The study introduces an enhanced\nneural network structure in the Proximal Policy Optimization algorithm to boost\nperformance, accompanied by a well-designed reward function to improve\nalgorithm efficacy. Experimental results conducted in both obstacle and\nobstacle-free environments underscore the effectiveness of the proposed\napproach. This research significantly contributes to the advancement of\nautonomous robotics in complex environments through the application of deep\nreinforcement learning.\n","authors":["Hamid Taheri","Seyed Rasoul Hosseini","Mohammad Ali Nekoui"],"pdf_url":"https://arxiv.org/pdf/2405.16266v2.pdf","comment":"This paper is under review by Int. J. of Intelligent Machines and\n  Robotics"},{"id":"http://arxiv.org/abs/2407.19943v2","updated":"2024-08-06T21:16:16Z","published":"2024-07-29T12:23:59Z","title":"Practical and Robust Safety Guarantees for Advanced Counterfactual\n  Learning to Rank","summary":"  Counterfactual learning to rank (CLTR) can be risky and, in various\ncircumstances, can produce sub-optimal models that hurt performance when\ndeployed. Safe CLTR was introduced to mitigate these risks when using inverse\npropensity scoring to correct for position bias. However, the existing safety\nmeasure for CLTR is not applicable to state-of-the-art CLTR methods, cannot\nhandle trust bias, and relies on specific assumptions about user behavior. Our\ncontributions are two-fold. First, we generalize the existing safe CLTR\napproach to make it applicable to state-of-the-art doubly robust CLTR and trust\nbias. Second, we propose a novel approach, proximal ranking policy optimization\n(PRPO), that provides safety in deployment without assumptions about user\nbehavior. PRPO removes incentives for learning ranking behavior that is too\ndissimilar to a safe ranking model. Thereby, PRPO imposes a limit on how much\nlearned models can degrade performance metrics, without relying on any specific\nuser assumptions. Our experiments show that both our novel safe doubly robust\nmethod and PRPO provide higher performance than the existing safe inverse\npropensity scoring approach. However, in unexpected circumstances, the safe\ndoubly robust approach can become unsafe and bring detrimental performance. In\ncontrast, PRPO always maintains safety, even in maximally adversarial\nsituations. By avoiding assumptions, PRPO is the first method with\nunconditional safety in deployment that translates to robust safety for\nreal-world applications.\n","authors":["Shashank Gupta","Harrie Oosterhuis","Maarten de Rijke"],"pdf_url":"https://arxiv.org/pdf/2407.19943v2.pdf","comment":"Accepted as full paper at CIKM 2024"},{"id":"http://arxiv.org/abs/2403.10795v2","updated":"2024-08-06T21:14:23Z","published":"2024-03-16T03:54:38Z","title":"Can Large Language Models Solve Robot Routing?","summary":"  Routing problems are common in mobile robotics, encompassing tasks such as\ninspection, surveillance, and coverage. Depending on the objective and\nconstraints, these problems often reduce to variants of the Traveling Salesman\nProblem (TSP), with solutions traditionally derived by translating high-level\nobjectives into an optimization formulation and using modern solvers to arrive\nat a solution. Here, we explore the potential of Large Language Models (LLMs)\nto replace the entire pipeline from tasks described in natural language to the\ngeneration of robot routes. We systematically investigate the performance of\nLLMs in robot routing by constructing a dataset with 80 unique robot routing\nproblems across 8 variants in both single and multi-robot settings. We evaluate\nLLMs through three frameworks: single attempt, self-debugging, and\nself-debugging with self-verification and various contexts, including\nmathematical formulations, pseudo-code, and related research papers. Our\nfindings reveal that both self-debugging and self-verification enhance success\nrates without significantly lowering the optimality gap. We observe\ncontext-sensitive behavior - providing mathematical formulations as context\ndecreases the optimality gap but significantly decreases success rates and\nproviding pseudo-code and related research papers as context does not\nconsistently improve success rates or decrease the optimality gap. We identify\nkey challenges and propose future directions to enhance LLM performance in\nsolving robot routing problems. Our source code is available on the project\nwebsite: https://sites.google.com/view/words-to-routes/.\n","authors":["Zhehui Huang","Guangyao Shi","Gaurav S. Sukhatme"],"pdf_url":"https://arxiv.org/pdf/2403.10795v2.pdf","comment":"Submitted to International Symposium of Robotics Research (ISRR 2024)"},{"id":"http://arxiv.org/abs/2407.19305v2","updated":"2024-08-06T21:07:17Z","published":"2024-07-27T17:27:05Z","title":"GP-VLS: A general-purpose vision language model for surgery","summary":"  Surgery requires comprehensive medical knowledge, visual assessment skills,\nand procedural expertise. While recent surgical AI models have focused on\nsolving task-specific problems, there is a need for general-purpose systems\nthat can understand surgical scenes and interact through natural language. This\npaper introduces GP-VLS, a general-purpose vision language model for surgery\nthat integrates medical and surgical knowledge with visual scene understanding.\nFor comprehensively evaluating general-purpose surgical models, we propose\nSurgiQual, which evaluates across medical and surgical knowledge benchmarks as\nwell as surgical vision-language questions. To train GP-VLS, we develop six new\ndatasets spanning medical knowledge, surgical textbooks, and vision-language\npairs for tasks like phase recognition and tool identification. We show that\nGP-VLS significantly outperforms existing open- and closed-source models on\nsurgical vision-language tasks, with 8-21% improvements in accuracy across\nSurgiQual benchmarks. GP-VLS also demonstrates strong performance on medical\nand surgical knowledge tests compared to open-source alternatives. Overall,\nGP-VLS provides an open-source foundation for developing AI assistants to\nsupport surgeons across a wide range of tasks and scenarios. The code and data\nfor this work is publicly available at gpvls-surgery-vlm.github.io.\n","authors":["Samuel Schmidgall","Joseph Cho","Cyril Zakka","William Hiesinger"],"pdf_url":"https://arxiv.org/pdf/2407.19305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03450v1","updated":"2024-08-06T21:03:16Z","published":"2024-08-06T21:03:16Z","title":"Probabilistic Surrogate Model for Accelerating the Design of Electric\n  Vehicle Battery Enclosures for Crash Performance","summary":"  This paper presents a probabilistic surrogate model for the accelerated\ndesign of electric vehicle battery enclosures with a focus on crash\nperformance. The study integrates high-throughput finite element simulations\nand Gaussian Process Regression to develop a surrogate model that predicts\ncrash parameters with high accuracy while providing uncertainty estimates. The\nmodel was trained using data generated from thermoforming and crash simulations\nover a range of material and process parameters. Validation against new\nsimulation data demonstrated the model's predictive accuracy with mean absolute\npercentage errors within 8.08% for all output variables. Additionally, a Monte\nCarlo uncertainty propagation study revealed the impact of input variability on\noutputs. The results highlight the efficacy of the Gaussian Process Regression\nmodel in capturing complex relationships within the dataset, offering a robust\nand efficient tool for the design optimization of composite battery enclosures.\n","authors":["Shadab Anwar Shaikh","Harish Cherukuri","Kranthi Balusu","Ram Devanathan","Ayoub Soulami"],"pdf_url":"https://arxiv.org/pdf/2408.03450v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03449v1","updated":"2024-08-06T21:02:27Z","published":"2024-08-06T21:02:27Z","title":"EEGMobile: Enhancing Speed and Accuracy in EEG-Based Gaze Prediction\n  with Advanced Mobile Architectures","summary":"  Electroencephalography (EEG) analysis is an important domain in the realm of\nBrain-Computer Interface (BCI) research. To ensure BCI devices are capable of\nproviding practical applications in the real world, brain signal processing\ntechniques must be fast, accurate, and resource-conscious to deliver\nlow-latency neural analytics. This study presents a model that leverages a\npre-trained MobileViT alongside Knowledge Distillation (KD) for EEG regression\ntasks. Our results showcase that this model is capable of performing at a level\ncomparable (only 3% lower) to the previous State-Of-The-Art (SOTA) on the\nEEGEyeNet Absolute Position Task while being 33% faster and 60% smaller. Our\nresearch presents a cost-effective model applicable to resource-constrained\ndevices and contributes to expanding future research on lightweight,\nmobile-friendly models for EEG regression.\n","authors":["Teng Liang","Andrews Damoah"],"pdf_url":"https://arxiv.org/pdf/2408.03449v1.pdf","comment":"Accepted HCI International 2024 - Late Breaking Work"},{"id":"http://arxiv.org/abs/2408.03445v1","updated":"2024-08-06T20:53:02Z","published":"2024-08-06T20:53:02Z","title":"Spacecraft inertial parameters estimation using time series clustering\n  and reinforcement learning","summary":"  This paper presents a machine learning approach to estimate the inertial\nparameters of a spacecraft in cases when those change during operations, e.g.\nmultiple deployments of payloads, unfolding of appendages and booms, propellant\nconsumption as well as during in-orbit servicing and active debris removal\noperations. The machine learning approach uses time series clustering together\nwith an optimised actuation sequence generated by reinforcement learning to\nfacilitate distinguishing among different inertial parameter sets. The\nperformance of the proposed strategy is assessed against the case of a\nmulti-satellite deployment system showing that the algorithm is resilient\ntowards common disturbances in such kinds of operations.\n","authors":["Konstantinos Platanitis","Miguel Arana-Catania","Leonardo Capicchiano","Saurabh Upadhyay","Leonard Felicetti"],"pdf_url":"https://arxiv.org/pdf/2408.03445v1.pdf","comment":"6 pages, 3 figures, 1 table. To be presented in ESA - AI for Space\n  (SPAICE)"},{"id":"http://arxiv.org/abs/2408.03441v1","updated":"2024-08-06T20:40:20Z","published":"2024-08-06T20:40:20Z","title":"Simple Perturbations Subvert Ethereum Phishing Transactions Detection:\n  An Empirical Analysis","summary":"  This paper explores the vulnerability of machine learning models,\nspecifically Random Forest, Decision Tree, and K-Nearest Neighbors, to very\nsimple single-feature adversarial attacks in the context of Ethereum fraudulent\ntransaction detection. Through comprehensive experimentation, we investigate\nthe impact of various adversarial attack strategies on model performance\nmetrics, such as accuracy, precision, recall, and F1-score. Our findings,\nhighlighting how prone those techniques are to simple attacks, are alarming,\nand the inconsistency in the attacks' effect on different algorithms promises\nways for attack mitigation. We examine the effectiveness of different\nmitigation strategies, including adversarial training and enhanced feature\nselection, in enhancing model robustness.\n","authors":["Ahod Alghureid","David Mohaisen"],"pdf_url":"https://arxiv.org/pdf/2408.03441v1.pdf","comment":"12 pages, 1 figure, 5 tables, accepted for presentation at WISA 2024"},{"id":"http://arxiv.org/abs/2408.03433v1","updated":"2024-08-06T20:19:06Z","published":"2024-08-06T20:19:06Z","title":"Hybrid diffusion models: combining supervised and generative pretraining\n  for label-efficient fine-tuning of segmentation models","summary":"  We are considering in this paper the task of label-efficient fine-tuning of\nsegmentation models: We assume that a large labeled dataset is available and\nallows to train an accurate segmentation model in one domain, and that we have\nto adapt this model on a related domain where only a few samples are available.\nWe observe that this adaptation can be done using two distinct methods: The\nfirst method, supervised pretraining, is simply to take the model trained on\nthe first domain using classical supervised learning, and fine-tune it on the\nsecond domain with the available labeled samples. The second method is to\nperform self-supervised pretraining on the first domain using a generic pretext\ntask in order to get high-quality representations which can then be used to\ntrain a model on the second domain in a label-efficient way. We propose in this\npaper to fuse these two approaches by introducing a new pretext task, which is\nto perform simultaneously image denoising and mask prediction on the first\ndomain. We motivate this choice by showing that in the same way that an image\ndenoiser conditioned on the noise level can be considered as a generative model\nfor the unlabeled image distribution using the theory of diffusion models, a\nmodel trained using this new pretext task can be considered as a generative\nmodel for the joint distribution of images and segmentation masks under the\nassumption that the mapping from images to segmentation masks is deterministic.\nWe then empirically show on several datasets that fine-tuning a model\npretrained using this approach leads to better results than fine-tuning a\nsimilar model trained using either supervised or unsupervised pretraining only.\n","authors":["Bruno Sauvalle","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2408.03433v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2406.17885v2","updated":"2024-08-06T20:06:11Z","published":"2024-06-25T18:47:50Z","title":"Enabling Regional Explainability by Automatic and Model-agnostic Rule\n  Extraction","summary":"  In Explainable AI, rule extraction translates model knowledge into logical\nrules, such as IF-THEN statements, crucial for understanding patterns learned\nby black-box models. This could significantly aid in fields like disease\ndiagnosis, disease progression estimation, or drug discovery. However, such\napplication domains often contain imbalanced data, with the class of interest\nunderrepresented. Existing methods inevitably compromise the performance of\nrules for the minor class to maximise the overall performance. As the first\nattempt in this field, we propose a model-agnostic approach for extracting\nrules from specific subgroups of data, featuring automatic rule generation for\nnumerical features. This method enhances the regional explainability of machine\nlearning models and offers wider applicability compared to existing methods. We\nadditionally introduce a new method for selecting features to compose rules,\nreducing computational costs in high-dimensional spaces. Experiments across\nvarious datasets and models demonstrate the effectiveness of our methods.\n","authors":["Yu Chen","Tianyu Cui","Alexander Capstick","Nan Fletcher-Loyd","Payam Barnaghi"],"pdf_url":"https://arxiv.org/pdf/2406.17885v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2408.03425v1","updated":"2024-08-06T20:02:57Z","published":"2024-08-06T20:02:57Z","title":"Sequential Conditional Transport on Probabilistic Graphs for\n  Interpretable Counterfactual Fairness","summary":"  In this paper, we link two existing approaches to derive counterfactuals:\nadaptations based on a causal graph, as suggested in Ple\\v{c}ko and Meinshausen\n(2020) and optimal transport, as in De Lara et al. (2024). We extend \"Knothe's\nrearrangement\" Bonnotte (2013) and \"triangular transport\" Zech and Marzouk\n(2022a) to probabilistic graphical models, and use this counterfactual\napproach, referred to as sequential transport, to discuss individual fairness.\nAfter establishing the theoretical foundations of the proposed method, we\ndemonstrate its application through numerical experiments on both synthetic and\nreal datasets.\n","authors":["Agathe Fernandes Machado","Arthur Charpentier","Ewen Gallic"],"pdf_url":"https://arxiv.org/pdf/2408.03425v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.05707v4","updated":"2024-08-06T19:53:17Z","published":"2023-10-09T13:29:37Z","title":"Guiding Language Model Reasoning with Planning Tokens","summary":"  Large language models (LLMs) have recently attracted considerable interest\nfor their ability to perform complex reasoning tasks, such as chain-of-thought\n(CoT) reasoning. However, most of the existing approaches to enhance this\nability rely heavily on data-driven methods, while neglecting the structural\naspects of the model's reasoning capacity. To encourage a more structural\ngeneration of CoT steps, we propose a hierarchical generation scheme: we let\nthe LM generate a planning token at the start of each reasoning step,\nintuitively serving as a high-level plan of the current step, and add their\nembeddings to the model parameters. Our approach requires a negligible increase\nin trainable parameters (0.001%) and can be applied through either full\nfine-tuning or a more parameter-efficient scheme. We demonstrate our method's\neffectiveness by applying it to three different LLMs, showing notable accuracy\nimprovements across three math word problem datasets and one multihop QA\ndataset with respect to standard fine-tuning baselines.\n","authors":["Xinyi Wang","Lucas Caccia","Oleksiy Ostapenko","Xingdi Yuan","William Yang Wang","Alessandro Sordoni"],"pdf_url":"https://arxiv.org/pdf/2310.05707v4.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2408.03421v1","updated":"2024-08-06T19:53:00Z","published":"2024-08-06T19:53:00Z","title":"Probabilistic Scores of Classifiers, Calibration is not Enough","summary":"  In binary classification tasks, accurate representation of probabilistic\npredictions is essential for various real-world applications such as predicting\npayment defaults or assessing medical risks. The model must then be\nwell-calibrated to ensure alignment between predicted probabilities and actual\noutcomes. However, when score heterogeneity deviates from the underlying data\nprobability distribution, traditional calibration metrics lose reliability,\nfailing to align score distribution with actual probabilities. In this study,\nwe highlight approaches that prioritize optimizing the alignment between\npredicted scores and true probability distributions over minimizing traditional\nperformance or calibration metrics. When employing tree-based models such as\nRandom Forest and XGBoost, our analysis emphasizes the flexibility these models\noffer in tuning hyperparameters to minimize the Kullback-Leibler (KL)\ndivergence between predicted and true distributions. Through extensive\nempirical analysis across 10 UCI datasets and simulations, we demonstrate that\noptimizing tree-based models based on KL divergence yields superior alignment\nbetween predicted scores and actual probabilities without significant\nperformance loss. In real-world scenarios, the reference probability is\ndetermined a priori as a Beta distribution estimated through maximum\nlikelihood. Conversely, minimizing traditional calibration metrics may lead to\nsuboptimal results, characterized by notable performance declines and inferior\nKL values. Our findings reveal limitations in traditional calibration metrics,\nwhich could undermine the reliability of predictive models for critical\ndecision-making.\n","authors":["Agathe Fernandes Machado","Arthur Charpentier","Emmanuel Flachaire","Ewen Gallic","Franois Hu"],"pdf_url":"https://arxiv.org/pdf/2408.03421v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03414v1","updated":"2024-08-06T19:23:42Z","published":"2024-08-06T19:23:42Z","title":"Logistic Regression makes small LLMs strong and explainable\n  \"tens-of-shot\" classifiers","summary":"  For simple classification tasks, we show that users can benefit from the\nadvantages of using small, local, generative language models instead of large\ncommercial models without a trade-off in performance or introducing extra\nlabelling costs. These advantages, including those around privacy,\navailability, cost, and explainability, are important both in commercial\napplications and in the broader democratisation of AI. Through experiments on\n17 sentence classification tasks (2-4 classes), we show that penalised logistic\nregression on the embeddings from a small LLM equals (and usually betters) the\nperformance of a large LLM in the \"tens-of-shot\" regime. This requires no more\nlabelled instances than are needed to validate the performance of the large\nLLM. Finally, we extract stable and sensible explanations for classification\ndecisions.\n","authors":["Marcus Buckmann","Edward Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03414v1.pdf","comment":"41 pages, 24 figures"},{"id":"http://arxiv.org/abs/2408.03413v1","updated":"2024-08-06T19:22:13Z","published":"2024-08-06T19:22:13Z","title":"A TVD neural network closure and application to turbulent combustion","summary":"  Trained neural networks (NN) have attractive features for closing governing\nequations, but in the absence of additional constraints, they can stray from\nphysical reality. A NN formulation is introduced to preclude spurious\noscillations that violate solution boundedness or positivity. It is embedded in\nthe discretized equations as a machine learning closure and strictly\nconstrained, inspired by total variation diminishing (TVD) methods for\nhyperbolic conservation laws. The constraint is exactly enforced during\ngradient-descent training by rescaling the NN parameters, which maps them onto\nan explicit feasible set. Demonstrations show that the constrained NN closure\nmodel usefully recovers linear and nonlinear hyperbolic phenomena and\nanti-diffusion while enforcing the non-oscillatory property. Finally, the model\nis applied to subgrid-scale (SGS) modeling of a turbulent reacting flow, for\nwhich it suppresses spurious oscillations in scalar fields that otherwise\nviolate the solution boundedness. It outperforms a simple penalization of\noscillations in the loss function.\n","authors":["Seung Won Suh","Jonathan F MacArt","Luke N Olson","Jonathan B Freund"],"pdf_url":"https://arxiv.org/pdf/2408.03413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03408v1","updated":"2024-08-06T19:10:25Z","published":"2024-08-06T19:10:25Z","title":"LLM-Aided Compilation for Tensor Accelerators","summary":"  Hardware accelerators, in particular accelerators for tensor processing, have\nmany potential application domains. However, they currently lack the software\ninfrastructure to support the majority of domains outside of deep learning.\nFurthermore, a compiler that can easily be updated to reflect changes at both\napplication and hardware levels would enable more agile development and design\nspace exploration of accelerators, allowing hardware designers to realize\ncloser-to-optimal performance. In this work, we discuss how large language\nmodels (LLMs) could be leveraged to build such a compiler. Specifically, we\ndemonstrate the ability of GPT-4 to achieve high pass rates in translating code\nto the Gemmini accelerator, and prototype a technique for decomposing\ntranslation into smaller, more LLM-friendly steps. Additionally, we propose a\n2-phase workflow for utilizing LLMs to generate hardware-optimized code.\n","authors":["Charles Hong","Sahil Bhatia","Altan Haan","Shengjun Kris Dong","Dima Nikiforov","Alvin Cheung","Yakun Sophia Shao"],"pdf_url":"https://arxiv.org/pdf/2408.03408v1.pdf","comment":"4 page workshop paper"},{"id":"http://arxiv.org/abs/2408.03407v1","updated":"2024-08-06T19:01:47Z","published":"2024-08-06T19:01:47Z","title":"Deep Clustering via Distribution Learning","summary":"  Distribution learning finds probability density functions from a set of data\nsamples, whereas clustering aims to group similar data points to form clusters.\nAlthough there are deep clustering methods that employ distribution learning\nmethods, past work still lacks theoretical analysis regarding the relationship\nbetween clustering and distribution learning. Thus, in this work, we provide a\ntheoretical analysis to guide the optimization of clustering via distribution\nlearning. To achieve better results, we embed deep clustering guided by a\ntheoretical analysis. Furthermore, the distribution learning method cannot\nalways be directly applied to data. To overcome this issue, we introduce a\nclustering-oriented distribution learning method called Monte-Carlo\nMarginalization for Clustering. We integrate Monte-Carlo Marginalization for\nClustering into Deep Clustering, resulting in Deep Clustering via Distribution\nLearning (DCDL). Eventually, the proposed DCDL achieves promising results\ncompared to state-of-the-art methods on popular datasets. Considering a\nclustering task, the new distribution learning method outperforms previous\nmethods as well.\n","authors":["Guanfang Dong","Zijie Tan","Chenqiu Zhao","Anup Basu"],"pdf_url":"https://arxiv.org/pdf/2408.03407v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03405v1","updated":"2024-08-06T18:56:29Z","published":"2024-08-06T18:56:29Z","title":"Combining Diverse Information for Coordinated Action: Stochastic Bandit\n  Algorithms for Heterogeneous Agents","summary":"  Stochastic multi-agent multi-armed bandits typically assume that the rewards\nfrom each arm follow a fixed distribution, regardless of which agent pulls the\narm. However, in many real-world settings, rewards can depend on the\nsensitivity of each agent to their environment. In medical screening, disease\ndetection rates can vary by test type; in preference matching, rewards can\ndepend on user preferences; and in environmental sensing, observation quality\ncan vary across sensors. Since past work does not specify how to allocate\nagents of heterogeneous but known sensitivity of these types in a stochastic\nbandit setting, we introduce a UCB-style algorithm, Min-Width, which aggregates\ninformation from diverse agents. In doing so, we address the joint challenges\nof (i) aggregating the rewards, which follow different distributions for each\nagent-arm pair, and (ii) coordinating the assignments of agents to arms.\nMin-Width facilitates efficient collaboration among heterogeneous agents,\nexploiting the known structure in the agents' reward functions to weight their\nrewards accordingly. We analyze the regret of Min-Width and conduct\npseudo-synthetic and fully synthetic experiments to study the performance of\ndifferent levels of information sharing. Our results confirm that the gains to\nmodeling agent heterogeneity tend to be greater when the sensitivities are more\nvaried across agents, while combining more information does not always improve\nperformance.\n","authors":["Lucia Gordon","Esther Rolf","Milind Tambe"],"pdf_url":"https://arxiv.org/pdf/2408.03405v1.pdf","comment":"19 pages, 6 figures, to be published in ECAI 2024"},{"id":"http://arxiv.org/abs/2408.03404v1","updated":"2024-08-06T18:55:31Z","published":"2024-08-06T18:55:31Z","title":"Set2Seq Transformer: Learning Permutation Aware Set Representations of\n  Artistic Sequences","summary":"  We propose Set2Seq Transformer, a novel sequential multiple instance\narchitecture, that learns to rank permutation aware set representations of\nsequences. First, we illustrate that learning temporal position-aware\nrepresentations of discrete timesteps can greatly improve static visual\nmultiple instance learning methods that do not regard temporality and\nconcentrate almost exclusively on visual content analysis. We further\ndemonstrate the significant advantages of end-to-end sequential multiple\ninstance learning, integrating visual content and temporal information in a\nmultimodal manner. As application we focus on fine art analysis related tasks.\nTo that end, we show that our Set2Seq Transformer can leverage visual set and\ntemporal position-aware representations for modelling visual artists' oeuvres\nfor predicting artistic success. Finally, through extensive quantitative and\nqualitative evaluation using a novel dataset, WikiArt-Seq2Rank, and a visual\nlearning-to-rank downstream task, we show that our Set2Seq Transformer captures\nessential temporal information improving the performance of strong static and\nsequential multiple instance learning methods for predicting artistic success.\n","authors":["Athanasios Efthymiou","Stevan Rudinac","Monika Kackovic","Nachoem Wijnberg","Marcel Worring"],"pdf_url":"https://arxiv.org/pdf/2408.03404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03400v1","updated":"2024-08-06T18:52:17Z","published":"2024-08-06T18:52:17Z","title":"Attacks and Defenses for Generative Diffusion Models: A Comprehensive\n  Survey","summary":"  Diffusion models (DMs) have achieved state-of-the-art performance on various\ngenerative tasks such as image synthesis, text-to-image, and text-guided\nimage-to-image generation. However, the more powerful the DMs, the more harmful\nthey potentially are. Recent studies have shown that DMs are prone to a wide\nrange of attacks, including adversarial attacks, membership inference, backdoor\ninjection, and various multi-modal threats. Since numerous pre-trained DMs are\npublished widely on the Internet, potential threats from these attacks are\nespecially detrimental to the society, making DM-related security a worth\ninvestigating topic. Therefore, in this paper, we conduct a comprehensive\nsurvey on the security aspect of DMs, focusing on various attack and defense\nmethods for DMs. First, we present crucial knowledge of DMs with five main\ntypes of DMs, including denoising diffusion probabilistic models, denoising\ndiffusion implicit models, noise conditioned score networks, stochastic\ndifferential equations, and multi-modal conditional DMs. We further survey a\nvariety of recent studies investigating different types of attacks that exploit\nthe vulnerabilities of DMs. Then, we thoroughly review potential\ncountermeasures to mitigate each of the presented threats. Finally, we discuss\nopen challenges of DM-related security and envision certain research directions\nfor this topic.\n","authors":["Vu Tuan Truong","Luan Ba Dang","Long Bao Le"],"pdf_url":"https://arxiv.org/pdf/2408.03400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03399v1","updated":"2024-08-06T18:52:15Z","published":"2024-08-06T18:52:15Z","title":"RHiOTS: A Framework for Evaluating Hierarchical Time Series Forecasting\n  Algorithms","summary":"  We introduce the Robustness of Hierarchically Organized Time Series (RHiOTS)\nframework, designed to assess the robustness of hierarchical time series\nforecasting models and algorithms on real-world datasets. Hierarchical time\nseries, where lower-level forecasts must sum to upper-level ones, are prevalent\nin various contexts, such as retail sales across countries. Current empirical\nevaluations of forecasting methods are often limited to a small set of\nbenchmark datasets, offering a narrow view of algorithm behavior. RHiOTS\naddresses this gap by systematically altering existing datasets and modifying\nthe characteristics of individual series and their interrelations. It uses a\nset of parameterizable transformations to simulate those changes in the data\ndistribution. Additionally, RHiOTS incorporates an innovative visualization\ncomponent, turning complex, multidimensional robustness evaluation results into\nintuitive, easily interpretable visuals. This approach allows an in-depth\nanalysis of algorithm and model behavior under diverse conditions. We\nillustrate the use of RHiOTS by analyzing the predictive performance of several\nalgorithms. Our findings show that traditional statistical methods are more\nrobust than state-of-the-art deep learning algorithms, except when the\ntransformation effect is highly disruptive. Furthermore, we found no\nsignificant differences in the robustness of the algorithms when applying\nspecific reconciliation methods, such as MinT. RHiOTS provides researchers with\na comprehensive tool for understanding the nuanced behavior of forecasting\nalgorithms, offering a more reliable basis for selecting the most appropriate\nmethod for a given problem.\n","authors":["Luis Roque","Carlos Soares","Lus Torgo"],"pdf_url":"https://arxiv.org/pdf/2408.03399v1.pdf","comment":"Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery\n  and Data Mining (KDD '24), August 25--29, 2024, Barcelona, Spain"},{"id":"http://arxiv.org/abs/2408.03397v1","updated":"2024-08-06T18:48:01Z","published":"2024-08-06T18:48:01Z","title":"HeTraX: Energy Efficient 3D Heterogeneous Manycore Architecture for\n  Transformer Acceleration","summary":"  Transformers have revolutionized deep learning and generative modeling to\nenable unprecedented advancements in natural language processing tasks and\nbeyond. However, designing hardware accelerators for executing transformer\nmodels is challenging due to the wide variety of computing kernels involved in\nthe transformer architecture. Existing accelerators are either inadequate to\naccelerate end-to-end transformer models or suffer notable thermal limitations.\nIn this paper, we propose the design of a three-dimensional heterogeneous\narchitecture referred to as HeTraX specifically optimized to accelerate\nend-to-end transformer models. HeTraX employs hardware resources aligned with\nthe computational kernels of transformers and optimizes both performance and\nenergy. Experimental results show that HeTraX outperforms existing\nstate-of-the-art by up to 5.6x in speedup and improves EDP by 14.5x while\nensuring thermally feasibility.\n","authors":["Pratyush Dhingra","Janardhan Rao Doppa","Partha Pratim Pande"],"pdf_url":"https://arxiv.org/pdf/2408.03397v1.pdf","comment":"Presented at ACM/IEEE International Symposium on Low Power\n  Electronics and Design (ISLPED-24)"},{"id":"http://arxiv.org/abs/2408.03388v1","updated":"2024-08-06T18:18:37Z","published":"2024-08-06T18:18:37Z","title":"A Non-negative VAE:the Generalized Gamma Belief Network","summary":"  The gamma belief network (GBN), often regarded as a deep topic model, has\ndemonstrated its potential for uncovering multi-layer interpretable latent\nrepresentations in text data. Its notable capability to acquire interpretable\nlatent factors is partially attributed to sparse and non-negative\ngamma-distributed latent variables. However, the existing GBN and its\nvariations are constrained by the linear generative model, thereby limiting\ntheir expressiveness and applicability. To address this limitation, we\nintroduce the generalized gamma belief network (Generalized GBN) in this paper,\nwhich extends the original linear generative model to a more expressive\nnon-linear generative model. Since the parameters of the Generalized GBN no\nlonger possess an analytic conditional posterior, we further propose an\nupward-downward Weibull inference network to approximate the posterior\ndistribution of the latent variables. The parameters of both the generative\nmodel and the inference network are jointly trained within the variational\ninference framework. Finally, we conduct comprehensive experiments on both\nexpressivity and disentangled representation learning tasks to evaluate the\nperformance of the Generalized GBN against state-of-the-art Gaussian\nvariational autoencoders serving as baselines.\n","authors":["Zhibin Duan","Tiansheng Wen","Muyao Wang","Bo Chen","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07346v3","updated":"2024-08-06T18:14:17Z","published":"2024-07-10T03:52:53Z","title":"INSIGHT: Universal Neural Simulator for Analog Circuits Harnessing\n  Autoregressive Transformers","summary":"  Analog front-end design heavily relies on specialized human expertise and\ncostly trial-and-error simulations, which motivated many prior works on analog\ndesign automation. However, efficient and effective exploration of the vast and\ncomplex design space remains constrained by the time-consuming nature of SPICE\nsimulations, making effective design automation a challenging endeavor. In this\npaper, we introduce INSIGHT, a GPU-powered, technology-agnostic, effective\nuniversal neural simulator in the analog front-end design automation loop.\nINSIGHT accurately predicts the performance metrics of analog circuits across\nvarious technologies with just a few microseconds of inference time. Notably,\nits autoregressive capabilities enable INSIGHT to accurately predict\nsimulation-costly critical transient specifications leveraging less expensive\nperformance metric information. The low cost and high fidelity feature make\nINSIGHT a good substitute for standard simulators in analog front-end\noptimization frameworks. INSIGHT is compatible with any optimization framework,\nfacilitating enhanced design space exploration for sample efficiency through\nsophisticated offline learning and adaptation techniques. Our experiments\ndemonstrate that INSIGHT-M, a model-based batch reinforcement learning sizing\nframework with INSIGHT as the accurate surrogate, only requires < 20 real-time\nsimulations with 100-1000x lower simulation costs and significant speedup over\nexisting sizing methods.\n","authors":["Souradip Poddar","Youngmin Oh","Yao Lai","Hanqing Zhu","Bosun Hwang","David Z. Pan"],"pdf_url":"https://arxiv.org/pdf/2407.07346v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.03284v1","updated":"2024-08-06T16:31:45Z","published":"2024-08-06T16:31:45Z","title":"ReSyncer: Rewiring Style-based Generator for Unified Audio-Visually\n  Synced Facial Performer","summary":"  Lip-syncing videos with given audio is the foundation for various\napplications including the creation of virtual presenters or performers. While\nrecent studies explore high-fidelity lip-sync with different techniques, their\ntask-orientated models either require long-term videos for clip-specific\ntraining or retain visible artifacts. In this paper, we propose a unified and\neffective framework ReSyncer, that synchronizes generalized audio-visual facial\ninformation. The key design is revisiting and rewiring the Style-based\ngenerator to efficiently adopt 3D facial dynamics predicted by a principled\nstyle-injected Transformer. By simply re-configuring the information insertion\nmechanisms within the noise and style space, our framework fuses motion and\nappearance with unified training. Extensive experiments demonstrate that\nReSyncer not only produces high-fidelity lip-synced videos according to audio,\nbut also supports multiple appealing properties that are suitable for creating\nvirtual presenters and performers, including fast personalized fine-tuning,\nvideo-driven lip-syncing, the transfer of speaking styles, and even face\nswapping. Resources can be found at\nhttps://guanjz20.github.io/projects/ReSyncer.\n","authors":["Jiazhi Guan","Zhiliang Xu","Hang Zhou","Kaisiyuan Wang","Shengyi He","Zhanwang Zhang","Borong Liang","Haocheng Feng","Errui Ding","Jingtuo Liu","Jingdong Wang","Youjian Zhao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03284v1.pdf","comment":"Accepted to European Conference on Computer Vision (ECCV), 2024.\n  Project page: https://guanjz20.github.io/projects/ReSyncer"},{"id":"http://arxiv.org/abs/2408.03185v1","updated":"2024-08-06T13:35:27Z","published":"2024-08-06T13:35:27Z","title":"MaskAnyone Toolkit: Offering Strategies for Minimizing Privacy Risks and\n  Maximizing Utility in Audio-Visual Data Archiving","summary":"  This paper introduces MaskAnyone, a novel toolkit designed to navigate some\nprivacy and ethical concerns of sharing audio-visual data in research.\nMaskAnyone offers a scalable, user-friendly solution for de-identifying\nindividuals in video and audio content through face-swapping and voice\nalteration, supporting multi-person masking and real-time bulk processing. By\nintegrating this tool within research practices, we aim to enhance data\nreproducibility and utility in social science research. Our approach draws on\nDesign Science Research, proposing that MaskAnyone can facilitate safer data\nsharing and potentially reduce the storage of fully identifiable data. We\ndiscuss the development and capabilities of MaskAnyone, explore its integration\ninto ethical research practices, and consider the broader implications of\naudio-visual data masking, including issues of consent and the risk of misuse.\nThe paper concludes with a preliminary evaluation framework for assessing the\neffectiveness and ethical integration of masking tools in such research\nsettings.\n","authors":["Babajide Alamu Owoyele","Martin Schilling","Rohan Sawahn","Niklas Kaemer","Pavel Zherebenkov","Bhuvanesh Verma","Wim Pouw","Gerard de Melo"],"pdf_url":"https://arxiv.org/pdf/2408.03185v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20962v2","updated":"2024-08-06T12:25:48Z","published":"2024-07-30T16:43:24Z","title":"MMTrail: A Multimodal Trailer Video Dataset with Language and Music\n  Descriptions","summary":"  Massive multi-modality datasets play a significant role in facilitating the\nsuccess of large video-language models. However, current video-language\ndatasets primarily provide text descriptions for visual frames, considering\naudio to be weakly related information. They usually overlook exploring the\npotential of inherent audio-visual correlation, leading to monotonous\nannotation within each modality instead of comprehensive and precise\ndescriptions. Such ignorance results in the difficulty of multiple\ncross-modality studies. To fulfill this gap, we present MMTrail, a large-scale\nmulti-modality video-language dataset incorporating more than 20M trailer clips\nwith visual captions, and 2M high-quality clips with multimodal captions.\nTrailers preview full-length video works and integrate context, visual frames,\nand background music. In particular, the trailer has two main advantages: (1)\nthe topics are diverse, and the content characters are of various types, e.g.,\nfilm, news, and gaming. (2) the corresponding background music is\ncustom-designed, making it more coherent with the visual context. Upon these\ninsights, we propose a systemic captioning framework, achieving various\nmodality annotations with more than 27.1k hours of trailer videos. Here, to\nensure the caption retains music perspective while preserving the authority of\nvisual context, we leverage the advanced LLM to merge all annotations\nadaptively. In this fashion, our MMtrail dataset potentially paves the path for\nfine-grained large multimodal-language model training. In experiments, we\nprovide evaluation metrics and benchmark results on our dataset, demonstrating\nthe high quality of our annotation and its effectiveness for model training.\n","authors":["Xiaowei Chi","Yatian Wang","Aosong Cheng","Pengjun Fang","Zeyue Tian","Yingqing He","Zhaoyang Liu","Xingqun Qi","Jiahao Pan","Rongyu Zhang","Mengfei Li","Ruibin Yuan","Yanbing Jiang","Wei Xue","Wenhan Luo","Qifeng Chen","Shanghang Zhang","Qifeng Liu","Yike Guo"],"pdf_url":"https://arxiv.org/pdf/2407.20962v2.pdf","comment":"15 Pages. Dataset report"},{"id":"http://arxiv.org/abs/2403.14972v2","updated":"2024-08-06T09:45:27Z","published":"2024-03-22T06:03:07Z","title":"A Picture Is Worth a Graph: A Blueprint Debate Paradigm for Multimodal\n  Reasoning","summary":"  This paper presents a pilot study aimed at introducing multi-agent debate\ninto multimodal reasoning. The study addresses two key challenges: the\ntrivialization of opinions resulting from excessive summarization and the\ndiversion of focus caused by distractor concepts introduced from images. These\nchallenges stem from the inductive (bottom-up) nature of existing debating\nschemes. To address the issue, we propose a deductive (top-down) debating\napproach called Blueprint Debate on Graphs (BDoG). In BDoG, debates are\nconfined to a blueprint graph to prevent opinion trivialization through\nworld-level summarization. Moreover, by storing evidence in branches within the\ngraph, BDoG mitigates distractions caused by frequent but irrelevant concepts.\nExtensive experiments validate that BDoG is able to achieve state-of-the-art\nresults in ScienceQA and MMBench with significant improvements over previous\nmethods. The source code can be accessed at https://github.com/thecharm/BDoG.\n","authors":["Changmeng Zheng","Dayong Liang","Wengyu Zhang","Xiao-Yong Wei","Tat-Seng Chua","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2403.14972v2.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.03001v1","updated":"2024-08-06T07:19:51Z","published":"2024-08-06T07:19:51Z","title":"Multitask and Multimodal Neural Tuning for Large Models","summary":"  In recent years, large-scale multimodal models have demonstrated impressive\ncapabilities across various domains. However, enabling these models to\neffectively perform multiple multimodal tasks simultaneously remains a\nsignificant challenge. To address this, we introduce a novel tuning method\ncalled neural tuning, designed to handle diverse multimodal tasks concurrently,\nincluding reasoning segmentation, referring segmentation, image captioning, and\ntext-to-image generation. Neural tuning emulates sparse distributed\nrepresentation in human brain, where only specific subsets of neurons are\nactivated for each task. Additionally, we present a new benchmark, MMUD, where\neach sample is annotated with multiple task labels. By applying neural tuning\nto pretrained large models on the MMUD benchmark, we achieve simultaneous task\nhandling in a streamlined and efficient manner. All models, code, and datasets\nwill be publicly available after publication, facilitating further research and\ndevelopment in this field.\n","authors":["Hao Sun","Yu Song","Jihong Hu","Yen-Wei Chen","Lanfen Lin"],"pdf_url":"https://arxiv.org/pdf/2408.03001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12629v2","updated":"2024-08-06T07:08:12Z","published":"2024-02-20T01:20:31Z","title":"Television Discourse Decoded: Comprehensive Multimodal Analytics at\n  Scale","summary":"  In this paper, we tackle the complex task of analyzing televised debates,\nwith a focus on a prime time news debate show from India. Previous methods,\nwhich often relied solely on text, fall short in capturing the multimodal\nessence of these debates. To address this gap, we introduce a comprehensive\nautomated toolkit that employs advanced computer vision and speech-to-text\ntechniques for large-scale multimedia analysis. Utilizing state-of-the-art\ncomputer vision algorithms and speech-to-text methods, we transcribe, diarize,\nand analyze thousands of YouTube videos of a prime-time television debate show\nin India. These debates are a central part of Indian media but have been\ncriticized for compromised journalistic integrity and excessive dramatization.\nOur toolkit provides concrete metrics to assess bias and incivility, capturing\na comprehensive multimedia perspective that includes text, audio utterances,\nand video frames. Our findings reveal significant biases in topic selection and\npanelist representation, along with alarming levels of incivility. This work\noffers a scalable, automated approach for future research in multimedia\nanalysis, with profound implications for the quality of public discourse and\ndemocratic debate. To catalyze further research in this area, we also release\nthe code, dataset collected and supplemental pdf.\n","authors":["Anmol Agarwal","Pratyush Priyadarshi","Shiven Sinha","Shrey Gupta","Hitkul Jangra","Ponnurangam Kumaraguru","Kiran Garimella"],"pdf_url":"https://arxiv.org/pdf/2402.12629v2.pdf","comment":"KDD 2024 [Updates for Camera Ready version]"},{"id":"http://arxiv.org/abs/2408.02978v1","updated":"2024-08-06T06:24:10Z","published":"2024-08-06T06:24:10Z","title":"ASR-enhanced Multimodal Representation Learning for Cross-Domain Product\n  Retrieval","summary":"  E-commerce is increasingly multimedia-enriched, with products exhibited in a\nbroad-domain manner as images, short videos, or live stream promotions. A\nunified and vectorized cross-domain production representation is essential. Due\nto large intra-product variance and high inter-product similarity in the\nbroad-domain scenario, a visual-only representation is inadequate. While\nAutomatic Speech Recognition (ASR) text derived from the short or live-stream\nvideos is readily accessible, how to de-noise the excessively noisy text for\nmultimodal representation learning is mostly untouched. We propose ASR-enhanced\nMultimodal Product Representation Learning (AMPere). In order to extract\nproduct-specific information from the raw ASR text, AMPere uses an\neasy-to-implement LLM-based ASR text summarizer. The LLM-summarized text,\ntogether with visual data, is then fed into a multi-branch network to generate\ncompact multimodal embeddings. Extensive experiments on a large-scale\ntri-domain dataset verify the effectiveness of AMPere in obtaining a unified\nmultimodal product representation that clearly improves cross-domain product\nretrieval.\n","authors":["Ruixiang Zhao","Jian Jia","Yan Li","Xuehan Bai","Quan Chen","Han Li","Peng Jiang","Xirong Li"],"pdf_url":"https://arxiv.org/pdf/2408.02978v1.pdf","comment":"10 pages, 5 figures"},{"id":"http://arxiv.org/abs/2404.18136v2","updated":"2024-08-06T06:04:41Z","published":"2024-04-28T10:16:35Z","title":"SafePaint: Anti-forensic Image Inpainting with Domain Adaptation","summary":"  Existing image inpainting methods have achieved remarkable accomplishments in\ngenerating visually appealing results, often accompanied by a trend toward\ncreating more intricate structural textures. However, while these models excel\nat creating more realistic image content, they often leave noticeable traces of\ntampering, posing a significant threat to security. In this work, we take the\nanti-forensic capabilities into consideration, firstly proposing an end-to-end\ntraining framework for anti-forensic image inpainting named SafePaint.\nSpecifically, we innovatively formulated image inpainting as two major tasks:\nsemantically plausible content completion and region-wise optimization. The\nformer is similar to current inpainting methods that aim to restore the missing\nregions of corrupted images. The latter, through domain adaptation, endeavors\nto reconcile the discrepancies between the inpainted region and the unaltered\narea to achieve anti-forensic goals. Through comprehensive theoretical\nanalysis, we validate the effectiveness of domain adaptation for anti-forensic\nperformance. Furthermore, we meticulously crafted a region-wise separated\nattention (RWSA) module, which not only aligns with our objective of\nanti-forensics but also enhances the performance of the model. Extensive\nqualitative and quantitative evaluations show our approach achieves comparable\nresults to existing image inpainting methods while offering anti-forensic\ncapabilities not available in other methods.\n","authors":["Dunyun Chen","Xin Liao","Xiaoshuai Wu","Shiwei Chen"],"pdf_url":"https://arxiv.org/pdf/2404.18136v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12794v2","updated":"2024-08-06T03:28:12Z","published":"2024-04-19T11:17:35Z","title":"MambaMOS: LiDAR-based 3D Moving Object Segmentation with Motion-aware\n  State Space Model","summary":"  LiDAR-based Moving Object Segmentation (MOS) aims to locate and segment\nmoving objects in point clouds of the current scan using motion information\nfrom previous scans. Despite the promising results achieved by previous MOS\nmethods, several key issues, such as the weak coupling of temporal and spatial\ninformation, still need further study. In this paper, we propose a novel\nLiDAR-based 3D Moving Object Segmentation with Motion-aware State Space Model,\ntermed MambaMOS. Firstly, we develop a novel embedding module, the Time Clue\nBootstrapping Embedding (TCBE), to enhance the coupling of temporal and spatial\ninformation in point clouds and alleviate the issue of overlooked temporal\nclues. Secondly, we introduce the Motion-aware State Space Model (MSSM) to\nendow the model with the capacity to understand the temporal correlations of\nthe same object across different time steps. Specifically, MSSM emphasizes the\nmotion states of the same object at different time steps through two distinct\ntemporal modeling and correlation steps. We utilize an improved state space\nmodel to represent these motion differences, significantly modeling the motion\nstates. Finally, extensive experiments on the SemanticKITTI-MOS and KITTI-Road\nbenchmarks demonstrate that the proposed MambaMOS achieves state-of-the-art\nperformance. The source code is publicly available at\nhttps://github.com/Terminal-K/MambaMOS.\n","authors":["Kang Zeng","Hao Shi","Jiacheng Lin","Siyu Li","Jintao Cheng","Kaiwei Wang","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2404.12794v2.pdf","comment":"Accepted to ACM MM 2024. The source code is publicly available at\n  https://github.com/Terminal-K/MambaMOS"},{"id":"http://arxiv.org/abs/2408.02901v1","updated":"2024-08-06T02:15:12Z","published":"2024-08-06T02:15:12Z","title":"Lighthouse: A User-Friendly Library for Reproducible Video Moment\n  Retrieval and Highlight Detection","summary":"  We propose Lighthouse, a user-friendly library for reproducible video moment\nretrieval and highlight detection (MR-HD). Although researchers proposed\nvarious MR-HD approaches, the research community holds two main issues. The\nfirst is a lack of comprehensive and reproducible experiments across various\nmethods, datasets, and video-text features. This is because no unified training\nand evaluation codebase covers multiple settings. The second is user-unfriendly\ndesign. Because previous works use different libraries, researchers set up\nindividual environments. In addition, most works release only the training\ncodes, requiring users to implement the whole inference process of MR-HD.\nLighthouse addresses these issues by implementing a unified reproducible\ncodebase that includes six models, three features, and five datasets. In\naddition, it provides an inference API and web demo to make these methods\neasily accessible for researchers and developers. Our experiments demonstrate\nthat Lighthouse generally reproduces the reported scores in the reference\npapers. The code is available at https://github.com/line/lighthouse.\n","authors":["Taichi Nishimura","Shota Nakada","Hokuto Munakata","Tatsuya Komatsu"],"pdf_url":"https://arxiv.org/pdf/2408.02901v1.pdf","comment":"6 pages; library tech report"},{"id":"http://arxiv.org/abs/2408.01532v2","updated":"2024-08-06T21:19:20Z","published":"2024-08-02T18:45:01Z","title":"Contextual Cross-Modal Attention for Audio-Visual Deepfake Detection and\n  Localization","summary":"  In the digital age, the emergence of deepfakes and synthetic media presents a\nsignificant threat to societal and political integrity. Deepfakes based on\nmulti-modal manipulation, such as audio-visual, are more realistic and pose a\ngreater threat. Current multi-modal deepfake detectors are often based on the\nattention-based fusion of heterogeneous data streams from multiple modalities.\nHowever, the heterogeneous nature of the data (such as audio and visual\nsignals) creates a distributional modality gap and poses a significant\nchallenge in effective fusion and hence multi-modal deepfake detection. In this\npaper, we propose a novel multi-modal attention framework based on recurrent\nneural networks (RNNs) that leverages contextual information for audio-visual\ndeepfake detection. The proposed approach applies attention to multi-modal\nmulti-sequence representations and learns the contributing features among them\nfor deepfake detection and localization. Thorough experimental validations on\naudio-visual deepfake datasets, namely FakeAVCeleb, AV-Deepfake1M, TVIL, and\nLAV-DF datasets, demonstrate the efficacy of our approach. Cross-comparison\nwith the published studies demonstrates superior performance of our approach\nwith an improved accuracy and precision by 3.47% and 2.05% in deepfake\ndetection and localization, respectively. Thus, obtaining state-of-the-art\nperformance. To facilitate reproducibility, the code and the datasets\ninformation is available at https://github.com/vcbsl/audiovisual-deepfake/.\n","authors":["Vinaya Sree Katamneni","Ajita Rattani"],"pdf_url":"https://arxiv.org/pdf/2408.01532v2.pdf","comment":null}]},"2024-08-07T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.03936v1","updated":"2024-08-07T17:54:21Z","published":"2024-08-07T17:54:21Z","title":"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature","summary":"  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n","authors":["Vincius Di Oliveira","Yuri Faanha Bezerra","Li Weigang","Pedro Carvalho Brom","Victor Rafael R. Celestino"],"pdf_url":"https://arxiv.org/pdf/2408.03936v1.pdf","comment":"13 pages, 1 figure, to be publish in International Conference on Web\n  Information Systems and Technologies - WEBIST 2024 proceedings"},{"id":"http://arxiv.org/abs/2408.03934v1","updated":"2024-08-07T17:52:02Z","published":"2024-08-07T17:52:02Z","title":"From Words to Worth: Newborn Article Impact Prediction with LLM","summary":"  As the academic landscape expands, the challenge of efficiently identifying\npotentially high-impact articles among the vast number of newly published works\nbecomes critical. This paper introduces a promising approach, leveraging the\ncapabilities of fine-tuned LLMs to predict the future impact of newborn\narticles solely based on titles and abstracts. Moving beyond traditional\nmethods heavily reliant on external information, the proposed method discerns\nthe shared semantic features of highly impactful papers from a large collection\nof title-abstract and potential impact pairs. These semantic features are\nfurther utilized to regress an improved metric, TNCSI_SP, which has been\nendowed with value, field, and time normalization properties. Additionally, a\ncomprehensive dataset has been constructed and released for fine-tuning the\nLLM, containing over 12,000 entries with corresponding titles, abstracts, and\nTNCSI_SP. The quantitative results, with an NDCG@20 of 0.901, demonstrate that\nthe proposed approach achieves state-of-the-art performance in predicting the\nimpact of newborn articles when compared to competitive counterparts. Finally,\nwe demonstrate a real-world application for predicting the impact of newborn\njournal articles to demonstrate its noteworthy practical value. Overall, our\nfindings challenge existing paradigms and propose a shift towards a more\ncontent-focused prediction of academic impact, offering new insights for\nassessing newborn article impact.\n","authors":["Penghai Zhao","Qinghua Xing","Kairan Dou","Jinyu Tian","Ying Tai","Jian Yang","Ming-Ming Cheng","Xiang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03934v1.pdf","comment":"7 pages for main sections, plus 3 additional pages for appendices.\n  Code, dataset are released at https://sway.cloud.microsoft/KOH09sPR21Ubojbc"},{"id":"http://arxiv.org/abs/2408.03910v1","updated":"2024-08-07T17:13:59Z","published":"2024-08-07T17:13:59Z","title":"CodexGraph: Bridging Large Language Models and Code Repositories via\n  Code Graph Databases","summary":"  Large Language Models (LLMs) excel in stand-alone code tasks like HumanEval\nand MBPP, but struggle with handling entire code repositories. This challenge\nhas prompted research on enhancing LLM-codebase interaction at a repository\nscale. Current solutions rely on similarity-based retrieval or manual tools and\nAPIs, each with notable drawbacks. Similarity-based retrieval often has low\nrecall in complex tasks, while manual tools and APIs are typically\ntask-specific and require expert knowledge, reducing their generalizability\nacross diverse code tasks and real-world applications. To mitigate these\nlimitations, we introduce \\framework, a system that integrates LLM agents with\ngraph database interfaces extracted from code repositories. By leveraging the\nstructural properties of graph databases and the flexibility of the graph query\nlanguage, \\framework enables the LLM agent to construct and execute queries,\nallowing for precise, code structure-aware context retrieval and code\nnavigation. We assess \\framework using three benchmarks: CrossCodeEval,\nSWE-bench, and EvoCodeBench. Additionally, we develop five real-world coding\napplications. With a unified graph database schema, \\framework demonstrates\ncompetitive performance and potential in both academic and real-world\nenvironments, showcasing its versatility and efficacy in software engineering.\nOur application demo:\nhttps://github.com/modelscope/modelscope-agent/tree/master/apps/codexgraph_agent.\n","authors":["Xiangyan Liu","Bo Lan","Zhiyuan Hu","Yang Liu","Zhicheng Zhang","Wenmeng Zhou","Fei Wang","Michael Shieh"],"pdf_url":"https://arxiv.org/pdf/2408.03910v1.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2408.03907v1","updated":"2024-08-07T17:11:34Z","published":"2024-08-07T17:11:34Z","title":"Decoding Biases: Automated Methods and LLM Judges for Gender Bias\n  Detection in Language Models","summary":"  Large Language Models (LLMs) have excelled at language understanding and\ngenerating human-level text. However, even with supervised training and human\nalignment, these LLMs are susceptible to adversarial attacks where malicious\nusers can prompt the model to generate undesirable text. LLMs also inherently\nencode potential biases that can cause various harmful effects during\ninteractions. Bias evaluation metrics lack standards as well as consensus and\nexisting methods often rely on human-generated templates and annotations which\nare expensive and labor intensive. In this work, we train models to\nautomatically create adversarial prompts to elicit biased responses from target\nLLMs. We present LLM- based bias evaluation metrics and also analyze several\nexisting automatic evaluation methods and metrics. We analyze the various\nnuances of model responses, identify the strengths and weaknesses of model\nfamilies, and assess where evaluation methods fall short. We compare these\nmetrics to human evaluation and validate that the LLM-as-a-Judge metric aligns\nwith human judgement on bias in response generation.\n","authors":["Shachi H Kumar","Saurav Sahay","Sahisnu Mazumder","Eda Okur","Ramesh Manuvinakurike","Nicole Beckage","Hsuan Su","Hung-yi Lee","Lama Nachman"],"pdf_url":"https://arxiv.org/pdf/2408.03907v1.pdf","comment":"6 pages paper content, 17 pages of appendix"},{"id":"http://arxiv.org/abs/2408.03900v1","updated":"2024-08-07T16:55:28Z","published":"2024-08-07T16:55:28Z","title":"Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond","summary":"  We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU)\ndataset comprising the speech counterpart for a portion of the MASSIVE textual\ncorpus. Speech-MASSIVE covers 12 languages from different families and inherits\nfrom MASSIVE the annotations for the intent prediction and slot-filling tasks.\nOur extension is prompted by the scarcity of massively multilingual SLU\ndatasets and the growing need for versatile speech datasets to assess\nfoundation models (LLMs, speech encoders) across languages and tasks. We\nprovide a multimodal, multitask, multilingual dataset and report SLU baselines\nusing both cascaded and end-to-end architectures in various training scenarios\n(zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the\nsuitability of Speech-MASSIVE for benchmarking other tasks such as speech\ntranscription, language identification, and speech translation. The dataset,\nmodels, and code are publicly available at:\nhttps://github.com/hlt-mt/Speech-MASSIVE\n","authors":["Beomseok Lee","Ioan Calapodescu","Marco Gaido","Matteo Negri","Laurent Besacier"],"pdf_url":"https://arxiv.org/pdf/2408.03900v1.pdf","comment":"Accepted at INTERSPEECH 2024. This version includes the same content\n  but with additional appendices"},{"id":"http://arxiv.org/abs/2408.03899v1","updated":"2024-08-07T16:55:00Z","published":"2024-08-07T16:55:00Z","title":"Simplifying Scholarly Abstracts for Accessible Digital Libraries","summary":"  Standing at the forefront of knowledge dissemination, digital libraries\ncurate vast collections of scientific literature. However, these scholarly\nwritings are often laden with jargon and tailored for domain experts rather\nthan the general public. As librarians, we strive to offer services to a\ndiverse audience, including those with lower reading levels. To extend our\nservices beyond mere access, we propose fine-tuning a language model to rewrite\nscholarly abstracts into more comprehensible versions, thereby making scholarly\nliterature more accessible when requested. We began by introducing a corpus\nspecifically designed for training models to simplify scholarly abstracts. This\ncorpus consists of over three thousand pairs of abstracts and significance\nstatements from diverse disciplines. We then fine-tuned four language models\nusing this corpus. The outputs from the models were subsequently examined both\nquantitatively for accessibility and semantic coherence, and qualitatively for\nlanguage quality, faithfulness, and completeness. Our findings show that the\nresulting models can improve readability by over three grade levels, while\nmaintaining fidelity to the original content. Although commercial\nstate-of-the-art models still hold an edge, our models are much more compact,\ncan be deployed locally in an affordable manner, and alleviate the privacy\nconcerns associated with using commercial models. We envision this work as a\nstep toward more inclusive and accessible libraries, improving our services for\nyoung readers and those without a college degree.\n","authors":["Haining Wang","Jason Clark"],"pdf_url":"https://arxiv.org/pdf/2408.03899v1.pdf","comment":"Initial submission to JCDL2024"},{"id":"http://arxiv.org/abs/2408.03874v1","updated":"2024-08-07T16:24:01Z","published":"2024-08-07T16:24:01Z","title":"Personalized Clinical Note Generation from Doctor-Patient Conversations","summary":"  In this work, we present a novel technique to improve the quality of draft\nclinical notes for physicians. This technique is concentrated on the ability to\nmodel implicit physician conversation styles and note preferences. We also\nintroduce a novel technique for the enrollment of new physicians when a limited\nnumber of clinical notes paired with conversations are available for that\nphysician, without the need to re-train a model to support them. We show that\nour technique outperforms the baseline model by improving the ROUGE-2 score of\nthe History of Present Illness section by 13.8%, the Physical Examination\nsection by 88.6%, and the Assessment & Plan section by 50.8%.\n","authors":["Nathan Brake","Thomas Schaaf"],"pdf_url":"https://arxiv.org/pdf/2408.03874v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03871v1","updated":"2024-08-07T16:21:41Z","published":"2024-08-07T16:21:41Z","title":"BeeManc at the PLABA Track of TAC-2023: Investigating LLMs and\n  Controllable Attributes for Improving Biomedical Text Readability","summary":"  In this system report, we describe the models and methods we used for our\nparticipation in the PLABA2023 task on biomedical abstract simplification, part\nof the TAC 2023 tracks. The system outputs we submitted come from the following\nthree categories: 1) domain fine-tuned T5-like models including Biomedical-T5\nand Lay-SciFive; 2) fine-tuned BARTLarge model with controllable attributes\n(via tokens) BART-w-CTs; 3) ChatGPTprompting. We also present the work we\ncarried out for this task on BioGPT finetuning. In the official automatic\nevaluation using SARI scores, BeeManc ranks 2nd among all teams and our model\nLaySciFive ranks 3rd among all 13 evaluated systems. In the official human\nevaluation, our model BART-w-CTs ranks 2nd on Sentence-Simplicity (score\n92.84), 3rd on Term-Simplicity (score 82.33) among all 7 evaluated systems; It\nalso produced a high score 91.57 on Fluency in comparison to the highest score\n93.53. In the second round of submissions, our team using ChatGPT-prompting\nranks the 2nd in several categories including simplified term accuracy score\n92.26 and completeness score 96.58, and a very similar score on faithfulness\nscore 95.3 to re-evaluated PLABA-base-1 (95.73) via human evaluations. Our\ncodes, fine-tuned models, prompts, and data splits from the system development\nstage will be available at https://github.com/ HECTA-UoM/PLABA-MU\n","authors":["Zihao Li","Samuel Belkadi","Nicolo Micheletti","Lifeng Han","Matthew Shardlow","Goran Nenadic"],"pdf_url":"https://arxiv.org/pdf/2408.03871v1.pdf","comment":"system report for PLABA-2023. arXiv admin note: substantial text\n  overlap with arXiv:2309.13202"},{"id":"http://arxiv.org/abs/2408.01852v2","updated":"2024-08-07T15:56:26Z","published":"2024-08-03T19:33:33Z","title":"Slo Escchame: Spanish Emotional Accompaniment Chatbot","summary":"  According to the World Health Organization (WHO), suicide was the fourth\nleading cause of death in the world for individuals aged 15 to 29 in 2019.\nGiven the rapid increase in mental health issues, providing psychological\nsupport is both crucial and urgent. In this paper: (1) we propose S\\'olo\nEsc\\'uchame, the first open-source Spanish emotional assistance chatbot, based\non LLaMA-2-7b-Chat. (2) We introduced the HEAR (Hispanic Emotional\nAccompaniment Responses) dataset, compiled from multiple English sources\ntranslated into Spanish, as well as generic data generated using\nChatGPT-3.5-Turbo. Finally, (3) we propose an evaluation metric based on two\nsemi-automatic assessment methods. Our system outperforms a range of\nstate-of-the-art models in providing psychological assistance in Spanish. Our\nmodels and datasets are publicly available to facilitate reproducibility.\n","authors":["Bruno Gil Ramrez","Jessica Lpez Espejel","Mara del Carmen Santiago Daz","Gustavo Trinidad Rubn Linares"],"pdf_url":"https://arxiv.org/pdf/2408.01852v2.pdf","comment":"Accepted at the 23rd Mexican International Conference on Artificial\n  Intelligence (MICAI) 2024"},{"id":"http://arxiv.org/abs/2408.03855v1","updated":"2024-08-07T15:52:46Z","published":"2024-08-07T15:52:46Z","title":"Why transformers are obviously good models of language","summary":"  Nobody knows how language works, but many theories abound. Transformers are a\nclass of neural networks that process language automatically with more success\nthan alternatives, both those based on neural computations and those that rely\non other (e.g. more symbolic) mechanisms. Here, I highlight direct connections\nbetween the transformer architecture and certain theoretical perspectives on\nlanguage. The empirical success of transformers relative to alternative models\nprovides circumstantial evidence that the linguistic approaches that\ntransformers embody should be, at least, evaluated with greater scrutiny by the\nlinguistics community and, at best, considered to be the currently best\navailable theories.\n","authors":["Felix Hill"],"pdf_url":"https://arxiv.org/pdf/2408.03855v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03849v1","updated":"2024-08-07T15:46:45Z","published":"2024-08-07T15:46:45Z","title":"Hate Speech Detection and Classification in Amharic Text with Deep\n  Learning","summary":"  Hate speech is a growing problem on social media. It can seriously impact\nsociety, especially in countries like Ethiopia, where it can trigger conflicts\namong diverse ethnic and religious groups. While hate speech detection in\nresource rich languages are progressing, for low resource languages such as\nAmharic are lacking. To address this gap, we develop Amharic hate speech data\nand SBi-LSTM deep learning model that can detect and classify text into four\ncategories of hate speech: racial, religious, gender, and non-hate speech. We\nhave annotated 5k Amharic social media post and comment data into four\ncategories. The data is annotated using a custom annotation tool by a total of\n100 native Amharic speakers. The model achieves a 94.8 F1-score performance.\nFuture improvements will include expanding the dataset and develop state-of-the\nart models.\n  Keywords: Amharic hate speech detection, classification, Amharic dataset,\nDeep Learning, SBi-LSTM\n","authors":["Samuel Minale Gashe","Seid Muhie Yimam","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2408.03849v1.pdf","comment":"Dataset: https://data.mendeley.com/datasets/p74pfhz3yx/1"},{"id":"http://arxiv.org/abs/2408.03837v1","updated":"2024-08-07T15:22:44Z","published":"2024-08-07T15:22:44Z","title":"WalledEval: A Comprehensive Safety Evaluation Toolkit for Large Language\n  Models","summary":"  WalledEval is a comprehensive AI safety testing toolkit designed to evaluate\nlarge language models (LLMs). It accommodates a diverse range of models,\nincluding both open-weight and API-based ones, and features over 35 safety\nbenchmarks covering areas such as multilingual safety, exaggerated safety, and\nprompt injections. The framework supports both LLM and judge benchmarking, and\nincorporates custom mutators to test safety against various text-style\nmutations such as future tense and paraphrasing. Additionally, WalledEval\nintroduces WalledGuard, a new, small and performant content moderation tool,\nand SGXSTest, a benchmark for assessing exaggerated safety in cultural\ncontexts. We make WalledEval publicly available at\nhttps://github.com/walledai/walledevalA.\n","authors":["Prannaya Gupta","Le Qi Yau","Hao Han Low","I-Shiang Lee","Hugo Maximus Lim","Yu Xin Teoh","Jia Hng Koh","Dar Win Liew","Rishabh Bhardwaj","Rajat Bhardwaj","Soujanya Poria"],"pdf_url":"https://arxiv.org/pdf/2408.03837v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2407.10853v2","updated":"2024-08-07T15:12:39Z","published":"2024-07-15T16:04:44Z","title":"An Actionable Framework for Assessing Bias and Fairness in Large\n  Language Model Use Cases","summary":"  Large language models (LLMs) can exhibit bias in a variety of ways. Such\nbiases can create or exacerbate unfair outcomes for certain groups within a\nprotected attribute, including, but not limited to sex, race, sexual\norientation, or age. This paper aims to provide a technical guide for\npractitioners to assess bias and fairness risks in LLM use cases. The main\ncontribution of this work is a decision framework that allows practitioners to\ndetermine which metrics to use for a specific LLM use case. To achieve this,\nthis study categorizes LLM bias and fairness risks, maps those risks to a\ntaxonomy of LLM use cases, and then formally defines various metrics to assess\neach type of risk. As part of this work, several new bias and fairness metrics\nare introduced, including innovative counterfactual metrics as well as metrics\nbased on stereotype classifiers. Instead of focusing solely on the model\nitself, the sensitivity of both prompt-risk and model-risk are taken into\naccount by defining evaluations at the level of an LLM use case, characterized\nby a model and a population of prompts. Furthermore, because all of the\nevaluation metrics are calculated solely using the LLM output, the proposed\nframework is highly practical and easily actionable for practitioners.\n","authors":["Dylan Bouchard"],"pdf_url":"https://arxiv.org/pdf/2407.10853v2.pdf","comment":"Comments: 21 pages, LaTeX; typos corrected, references added"},{"id":"http://arxiv.org/abs/2401.00763v2","updated":"2024-08-07T15:10:15Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v2.pdf","comment":"ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03819v1","updated":"2024-08-07T14:55:04Z","published":"2024-08-07T14:55:04Z","title":"Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning","summary":"  Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.\n","authors":["Simret Araya Gebreegziabher","Kuangshi Ai","Zheng Zhang","Elena L. Glassman","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2408.03819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03811v1","updated":"2024-08-07T14:42:13Z","published":"2024-08-07T14:42:13Z","title":"Generative Language Models with Retrieval Augmented Generation for\n  Automated Short Answer Scoring","summary":"  Automated Short Answer Scoring (ASAS) is a critical component in educational\nassessment. While traditional ASAS systems relied on rule-based algorithms or\ncomplex deep learning methods, recent advancements in Generative Language\nModels (GLMs) offer new opportunities for improvement. This study explores the\napplication of GLMs to ASAS, leveraging their off-the-shelf capabilities and\nperformance in various domains. We propose a novel pipeline that combines\nvector databases, transformer-based encoders, and GLMs to enhance short answer\nscoring accuracy. Our approach stores training responses in a vector database,\nretrieves semantically similar responses during inference, and employs a GLM to\nanalyze these responses and determine appropriate scores. We further optimize\nthe system through fine-tuned retrieval processes and prompt engineering.\nEvaluation on the SemEval 2013 dataset demonstrates a significant improvement\non the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,\nhighlighting the potential of GLMs in advancing ASAS technology.\n","authors":["Zifan Wang","Christopher Ormerod"],"pdf_url":"https://arxiv.org/pdf/2408.03811v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2407.18581v2","updated":"2024-08-07T14:19:00Z","published":"2024-07-26T08:03:07Z","title":"Dynamic Language Group-Based MoE: Enhancing Code-Switching Speech\n  Recognition with Hierarchical Routing","summary":"  The Mixture of Experts (MoE) approach is well-suited for multilingual and\ncode-switching (CS) tasks due to its multi-expert architecture. This work\nintroduces the DLG-MoE, a Dynamic Language Group-based MoE optimized for\nbilingual and CS scenarios. DLG-MoE operates based on a hierarchical routing\nmechanism. First, the language router explicitly models the language and\ndispatches the representations to the corresponding language expert groups.\nSubsequently, the unsupervised router within each language group implicitly\nmodels attributes beyond language, and coordinates expert routing and\ncollaboration. The model achieves state-of-the-art (SOTA) performance while\nalso having unparalleled flexibility. It supports different top-k inference and\nstreaming capabilities, and can also prune the model parameters to obtain a\nmonolingual sub-model. The Code will be released.\n","authors":["Hukai Huang","Shenghui Lu","Yahui Shan","He Qu","Wenhao Guan","Qingyang Hong","Lin Li"],"pdf_url":"https://arxiv.org/pdf/2407.18581v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.07705v3","updated":"2024-08-07T13:43:46Z","published":"2023-07-15T04:37:11Z","title":"CA-LoRA: Adapting Existing LoRA for Compressed LLMs to Enable Efficient\n  Multi-Tasking on Personal Devices","summary":"  Recently, there has been a demand to deploy Large Language Models (LLMs) on\npersonal devices such as laptops and smartphones. These LLMs have different\nmodel variants when handling different tasks. However, personal devices have\nlimited resources and require reduced storage overhead. To address this, there\nare two key methods available: the first is model compression, which compresses\nLLMs into smaller sizes; the second is LoRA, which can transfer an LLM to other\ntasks with very few parameters, avoiding the storage of multiple model variants\nin multi-task scenarios by only preserving LoRAs. However, our experiments show\nthat directly combining these two methods yields sub-optimal performance.\nConsidering that the open-source community has already contributed many LoRAs\nto LLMs, we propose to adapt these existing LoRAs from the LLMs to their\ncompressed version and introduce a Compression-Aware LoRA (CA-LoRA) framework.\nWe incorporate knowledge inheritance and recovery strategies to recover the\nlost knowledge caused by model compression. Experiment results demonstrate that\nCA-LoRA outperforms the vanilla LoRA methods applied to a compressed LLM and\nachieves comparable performance to the non-compressed LLM with existing LoRA\nmodules. The source code of CA-LoRA is available at\nhttps://github.com/thunlp/CA-LoRA.\n","authors":["Weilin Zhao","Yuxiang Huang","Xu Han","Zhiyuan Liu","Zhengyan Zhang","Kuai Li","Chen Chen","Tao Yang","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2307.07705v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03762v1","updated":"2024-08-07T13:31:44Z","published":"2024-08-07T13:31:44Z","title":"'Finance Wizard' at the FinLLM Challenge Task: Financial Text\n  Summarization","summary":"  This paper presents our participation under the team name `Finance Wizard' in\nthe FinNLP-AgentScen 2024 shared task #2: Financial Text Summarization. It\ndocuments our pipeline approach of fine-tuning a foundation model into a\ntask-specific model for Financial Text Summarization. It involves (1) adapting\nLlama3 8B, a foundation model, to the Finance domain via continued\npre-training, (2) multi-task instruction-tuning to further equip the model with\nmore finance-related capabilities, (3) finally fine-tuning the model into a\ntask-specific `expert'. Our model, FinLlama3\\_sum, yielded commendable results,\nsecuring the third position in its category with a ROUGE-1 score of 0.521.\n","authors":["Meisin Lee","Soon Lay-Ki"],"pdf_url":"https://arxiv.org/pdf/2408.03762v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03732v1","updated":"2024-08-07T12:38:23Z","published":"2024-08-07T12:38:23Z","title":"Question Rephrasing for Quantifying Uncertainty in Large Language\n  Models: Applications in Molecular Chemistry Tasks","summary":"  Uncertainty quantification enables users to assess the reliability of\nresponses generated by large language models (LLMs). We present a novel\nQuestion Rephrasing technique to evaluate the input uncertainty of LLMs, which\nrefers to the uncertainty arising from equivalent variations of the inputs\nprovided to LLMs. This technique is integrated with sampling methods that\nmeasure the output uncertainty of LLMs, thereby offering a more comprehensive\nuncertainty assessment. We validated our approach on property prediction and\nreaction prediction for molecular chemistry tasks.\n","authors":["Zizhang Chen","Pengyu Hong","Sandeep Madireddy"],"pdf_url":"https://arxiv.org/pdf/2408.03732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00655v4","updated":"2024-08-07T12:23:14Z","published":"2024-08-01T15:45:19Z","title":"SentenceVAE: Enable Next-sentence Prediction for Large Language Models\n  with Faster Speed, Higher Accuracy and Longer Context","summary":"  Current large language models (LLMs) primarily utilize next-token prediction\nmethod for inference, which significantly impedes their processing speed. In\nthis paper, we introduce a novel inference methodology termed next-sentence\nprediction, aimed at enhancing the inference efficiency of LLMs. We present\nSentence Variational Autoencoder (SentenceVAE), a tiny model consisting of a\nSentence Encoder and a Sentence Decoder. The Sentence Encoder can effectively\ncondense the information within a sentence into a singular token, while the\nSentence Decoder can reconstruct this compressed token back into sentence. By\nintegrating SentenceVAE into the input and output layers of LLMs, we develop\nSentence-level LLMs (SLLMs) that employ a sentence-by-sentence inference\nmethod. In addition, the SentenceVAE module of SLLMS can maintain the integrity\nof the original semantic content by segmenting the context into sentences,\nthereby improving accuracy while boosting inference speed. Moreover, compared\nto previous LLMs, SLLMs process fewer tokens over equivalent context length,\nsignificantly reducing memory demands for self-attention computation and\nfacilitating the handling of longer context. Extensive experiments on Wanjuan\ndataset have reveal that the proposed method can accelerate inference speed by\n204~365%, reduce perplexity (PPL) to 46~75% of its original metric, and\ndecrease memory overhead by 86~91% for the equivalent context length, compared\nto the token-by-token method.\n","authors":["Hongjun An","Yifan Chen","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.00655v4.pdf","comment":"update the article"},{"id":"http://arxiv.org/abs/2404.03818v2","updated":"2024-08-07T12:05:06Z","published":"2024-04-04T21:57:11Z","title":"PRobELM: Plausibility Ranking Evaluation for Language Models","summary":"  This paper introduces PRobELM (Plausibility Ranking Evaluation for Language\nModels), a benchmark designed to assess language models' ability to discern\nmore plausible from less plausible scenarios through their parametric\nknowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or\ntruthfulness, and others such as COPA explore plausible scenarios without\nexplicitly incorporating world knowledge, PRobELM seeks to bridge this gap by\nevaluating models' capabilities to prioritise plausible scenarios that leverage\nworld knowledge over less plausible alternatives. This design allows us to\nassess the potential of language models for downstream use cases such as\nliterature-based discovery where the focus is on identifying information that\nis likely but not yet known. Our benchmark is constructed from a dataset\ncurated from Wikidata edit histories, tailored to align the temporal bounds of\nthe training data for the evaluated models. PRobELM facilitates the evaluation\nof language models across multiple prompting types, including statement, text\ncompletion, and question-answering. Experiments with 10 models of various sizes\nand architectures on the relationship between model scales, training recency,\nand plausibility performance, reveal that factual accuracy does not directly\ncorrelate with plausibility performance and that up-to-date training data\nenhances plausibility assessment across different model architectures.\n","authors":["Zhangdie Yuan","Eric Chamoun","Rami Aly","Chenxi Whitehouse","Andreas Vlachos"],"pdf_url":"https://arxiv.org/pdf/2404.03818v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03706v1","updated":"2024-08-07T11:44:32Z","published":"2024-08-07T11:44:32Z","title":"Local Topology Measures of Contextual Language Model Latent Spaces With\n  Applications to Dialogue Term Extraction","summary":"  A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.\n","authors":["Benjamin Matthias Ruppik","Michael Heck","Carel van Niekerk","Renato Vukovic","Hsien-chin Lin","Shutong Feng","Marcus Zibrowius","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.03706v1.pdf","comment":"Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3\n  tables"},{"id":"http://arxiv.org/abs/2408.03675v1","updated":"2024-08-07T10:31:07Z","published":"2024-08-07T10:31:07Z","title":"NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time","summary":"  Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at https:\n//github.com/PaddlePaddle/Research/ tree/master/NLP/ACL2024-NACL.\n","authors":["Yilong Chen","Guoxia Wang","Junyuan Shang","Shiyao Cui","Zhenyu Zhang","Tingwen Liu","Shuohuan Wang","Yu Sun","Dianhai Yu","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2408.03675v1.pdf","comment":"Accepted by ACL 2024 (main conference, long paper)"},{"id":"http://arxiv.org/abs/2407.05721v2","updated":"2024-08-07T10:29:12Z","published":"2024-07-08T08:25:56Z","title":"PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation","summary":"  Mental health has attracted substantial attention in recent years and LLM can\nbe an effective technology for alleviating this problem owing to its capability\nin text understanding and dialogue. However, existing research in this domain\noften suffers from limitations, such as training on datasets lacking crucial\nprior knowledge and evidence, and the absence of comprehensive evaluation\nmethods. In this paper, we propose a specialized psychological large language\nmodel (LLM), named PsycoLLM, trained on a proposed high-quality psychological\ndataset, including single-turn QA, multi-turn dialogues and knowledge-based QA.\nSpecifically, we construct multi-turn dialogues through a three-step pipeline\ncomprising generation, evidence judgment, and refinement. We augment this\nprocess with real-world psychological case backgrounds extracted from online\nplatforms, enhancing the relevance and applicability of the generated data.\nAdditionally, to compare the performance of PsycoLLM with other LLMs, we\ndevelop a comprehensive psychological benchmark based on authoritative\npsychological counseling examinations in China, which includes assessments of\nprofessional ethics, theoretical proficiency, and case analysis. The\nexperimental results on the benchmark illustrates the effectiveness of\nPsycoLLM, which demonstrates superior performance compared to other LLMs.\n","authors":["Jinpeng Hu","Tengteng Dong","Luo Gang","Hui Ma","Peng Zou","Xiao Sun","Dan Guo","Meng Wang"],"pdf_url":"https://arxiv.org/pdf/2407.05721v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2407.00981v2","updated":"2024-08-07T09:52:44Z","published":"2024-07-01T05:35:30Z","title":"VisEval: A Benchmark for Data Visualization in the Era of Large Language\n  Models","summary":"  Translating natural language to visualization (NL2VIS) has shown great\npromise for visual data analysis, but it remains a challenging task that\nrequires multiple low-level implementations, such as natural language\nprocessing and visualization design. Recent advancements in pre-trained large\nlanguage models (LLMs) are opening new avenues for generating visualizations\nfrom natural language. However, the lack of a comprehensive and reliable\nbenchmark hinders our understanding of LLMs' capabilities in visualization\ngeneration. In this paper, we address this gap by proposing a new NL2VIS\nbenchmark called VisEval. Firstly, we introduce a high-quality and large-scale\ndataset. This dataset includes 2,524 representative queries covering 146\ndatabases, paired with accurately labeled ground truths. Secondly, we advocate\nfor a comprehensive automated evaluation methodology covering multiple\ndimensions, including validity, legality, and readability. By systematically\nscanning for potential issues with a number of heterogeneous checkers, VisEval\nprovides reliable and trustworthy evaluation outcomes. We run VisEval on a\nseries of state-of-the-art LLMs. Our evaluation reveals prevalent challenges\nand delivers essential insights for future advancements.\n","authors":["Nan Chen","Yuge Zhang","Jiahang Xu","Kan Ren","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2407.00981v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03652v1","updated":"2024-08-07T09:34:55Z","published":"2024-08-07T09:34:55Z","title":"mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search","summary":"  Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.\n","authors":["Ahmed Abdou","Tasneem Mohsen"],"pdf_url":"https://arxiv.org/pdf/2408.03652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03633v1","updated":"2024-08-07T08:44:44Z","published":"2024-08-07T08:44:44Z","title":"CARE: A Clue-guided Assistant for CSRs to Read User Manuals","summary":"  It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.\n","authors":["Weihong Du","Jia Liu","Zujie Wen","Dingnan Jin","Hongru Liang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2408.03633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03631v1","updated":"2024-08-07T08:43:32Z","published":"2024-08-07T08:43:32Z","title":"Large Language Models for Base Station Siting: Intelligent Deployment\n  based on Prompt or Agent","summary":"  Traditional base station siting (BSS) methods rely heavily on drive testing\nand user feedback, which are laborious and require extensive expertise in\ncommunication, networking, and optimization. As large language models (LLMs)\nand their associated technologies advance, particularly in the realms of prompt\nengineering and agent engineering, network optimization will witness a\nrevolutionary approach. This approach entails the strategic use of well-crafted\nprompts to infuse human experience and knowledge into these sophisticated LLMs,\nand the deployment of autonomous agents as a communication bridge to seamlessly\nconnect the machine language based LLMs with human users using natural\nlanguage. This integration represents the future paradigm of artificial\nintelligence (AI) as a service and AI for more ease. As a preliminary\nexploration, this research first develops a novel LLM-empowered BSS\noptimization framework, and heuristically proposes four different potential\nimplementations: the strategies based on Prompt-optimized LLM (PoL),\nhuman-in-the-Loop LLM (HiLL), LLM-empowered autonomous BSS agent (LaBa), and\nCooperative multiple LLM-based autonomous BSS agents (CLaBa). Through\nevaluation on real-world data, the experiments demonstrate that prompt-assisted\nLLMs and LLM-based agents can generate more efficient, cost-effective, and\nreliable network deployments, noticeably enhancing the efficiency of BSS\noptimization and reducing trivial manual participation.\n","authors":["Yanhu Wang","Muhammad Muzammil Afzal","Zhengyang Li","Jie Zhou","Chenyuan Feng","Shuaishuai Guo","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2408.03631v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03630v1","updated":"2024-08-07T08:43:18Z","published":"2024-08-07T08:43:18Z","title":"PAGED: A Benchmark for Procedural Graphs Extraction from Documents","summary":"  Automatic extraction of procedural graphs from documents creates a low-cost\nway for users to easily understand a complex procedure by skimming visual\ngraphs. Despite the progress in recent studies, it remains unanswered: whether\nthe existing studies have well solved this task (Q1) and whether the emerging\nlarge language models (LLMs) can bring new opportunities to this task (Q2). To\nthis end, we propose a new benchmark PAGED, equipped with a large high-quality\ndataset and standard evaluations. It investigates five state-of-the-art\nbaselines, revealing that they fail to extract optimal procedural graphs well\nbecause of their heavy reliance on hand-written rules and limited available\ndata. We further involve three advanced LLMs in PAGED and enhance them with a\nnovel self-refine strategy. The results point out the advantages of LLMs in\nidentifying textual elements and their gaps in building logical structures. We\nhope PAGED can serve as a major landmark for automatic procedural graph\nextraction and the investigations in PAGED can offer insights into the research\non logic reasoning among non-sequential elements.\n","authors":["Weihong Du","Wenrui Liao","Hongru Liang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2408.03630v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03622v1","updated":"2024-08-07T08:31:42Z","published":"2024-08-07T08:31:42Z","title":"Improving the quality of Persian clinical text with a novel spelling\n  correction system","summary":"  Background: The accuracy of spelling in Electronic Health Records (EHRs) is a\ncritical factor for efficient clinical care, research, and ensuring patient\nsafety. The Persian language, with its abundant vocabulary and complex\ncharacteristics, poses unique challenges for real-word error correction. This\nresearch aimed to develop an innovative approach for detecting and correcting\nspelling errors in Persian clinical text.\n  Methods: Our strategy employs a state-of-the-art pre-trained model that has\nbeen meticulously fine-tuned specifically for the task of spelling correction\nin the Persian clinical domain. This model is complemented by an innovative\northographic similarity matching algorithm, PERTO, which uses visual similarity\nof characters for ranking correction candidates.\n  Results: The evaluation of our approach demonstrated its robustness and\nprecision in detecting and rectifying word errors in Persian clinical text. In\nterms of non-word error correction, our model achieved an F1-Score of 90.0%\nwhen the PERTO algorithm was employed. For real-word error detection, our model\ndemonstrated its highest performance, achieving an F1-Score of 90.6%.\nFurthermore, the model reached its highest F1-Score of 91.5% for real-word\nerror correction when the PERTO algorithm was employed.\n  Conclusions: Despite certain limitations, our method represents a substantial\nadvancement in the field of spelling error detection and correction for Persian\nclinical text. By effectively addressing the unique challenges posed by the\nPersian language, our approach paves the way for more accurate and efficient\nclinical documentation, contributing to improved patient care and safety.\nFuture research could explore its use in other areas of the Persian medical\ndomain, enhancing its impact and utility.\n","authors":["Seyed Mohammad Sadegh Dashti","Seyedeh Fatemeh Dashti"],"pdf_url":"https://arxiv.org/pdf/2408.03622v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01185v5","updated":"2024-08-07T08:31:05Z","published":"2023-12-02T17:24:17Z","title":"A ripple in time: a discontinuity in American history","summary":"  In this technical note we suggest a novel approach to discover temporal\n(related and unrelated to language dilation) and personality (authorship\nattribution) in historical datasets. We exemplify our approach on the State of\nthe Union speeches given by the past 42 US presidents: this dataset is known\nfor its relatively small amount of data, and high variability of the amount and\nstyle of texts. Nevertheless we manage to achieve about 95\\% accuracy on the\nauthorship attribution task, and pin down the date of writing to a single\npresidential term.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2312.01185v5.pdf","comment":"6 pages, 8 figures; GitHub repository\n  (https://github.com/sashakolpakov/ripple_in_time); restructured manuscript"},{"id":"http://arxiv.org/abs/2405.14277v2","updated":"2024-08-07T08:21:58Z","published":"2024-05-23T07:53:04Z","title":"Improving Language Models Trained on Translated Data with Continual\n  Pre-Training and Dictionary Learning Analysis","summary":"  Training LLMs for low-resource languages usually utilizes data augmentation\nfrom English using machine translation (MT). This, however, brings a number of\nchallenges to LLM training: there are large costs attached to translating and\ncurating huge amounts of content with high-end machine translation solutions;\nthe translated content carries over cultural biases; and if the translation is\nnot faithful and accurate, data quality degrades causing issues in the trained\nmodel. In this work, we investigate the role of translation and synthetic data\nin training language models. We translate TinyStories, a dataset of 2.2M short\nstories for 3-4 year old children, from English to Arabic using the open\nNLLB-3B MT model. We train a number of story generation models of size 1M-33M\nparameters using this data. We identify a number of quality and task-specific\nissues in the resulting models. To rectify these issues, we further pre-train\nthe models with a small dataset of synthesized high-quality Arabic stories\ngenerated by a capable LLM, representing 1% of the original training data. We\nshow, using GPT-4 as a judge and Dictionary Learning Analysis from mechanistic\ninterpretability, that the suggested approach is a practical means to resolve\nsome of the machine translation pitfalls. We illustrate the improvements\nthrough case studies of linguistic and cultural bias issues.\n","authors":["Sabri Boughorbel","MD Rizwan Parvez","Majd Hawasly"],"pdf_url":"https://arxiv.org/pdf/2405.14277v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.03618v1","updated":"2024-08-07T08:19:44Z","published":"2024-08-07T08:19:44Z","title":"A Logical Fallacy-Informed Framework for Argument Generation","summary":"  Despite the remarkable performance of Large Language Models (LLMs), they\nstill struggle with generating logically sound arguments, resulting in\npotential risks such as spreading misinformation. An important factor\ncontributing to LLMs' suboptimal performance in generating coherent arguments\nis their oversight of logical fallacies. To address this issue, we introduce\nFIPO, a fallacy-informed framework that leverages preference optimization\nmethods to steer LLMs toward logically sound arguments. FIPO includes a\nclassification loss, to capture the fine-grained information on fallacy\ncategories. Our results on argumentation datasets show that our method reduces\nthe fallacy errors by up to 17.5%. Furthermore, our human evaluation results\nindicate that the quality of the generated arguments by our method\nsignificantly outperforms the fine-tuned baselines, as well as prior preference\noptimization methods, such as DPO. These findings highlight the importance of\nensuring models are aware of logical fallacies for effective argument\ngeneration.\n","authors":["Luca Mouchel","Debjit Paul","Shaobo Cui","Robert West","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2408.03618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03617v1","updated":"2024-08-07T08:18:51Z","published":"2024-08-07T08:18:51Z","title":"Is Child-Directed Speech Effective Training Data for Language Models?","summary":"  While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 models on 29M words of English-language child-directed\nspeech and a new matched, synthetic dataset (TinyDialogues), comparing to a\nheterogeneous blend of datasets from the BabyLM challenge. We evaluate both the\nsyntactic and semantic knowledge of these models using developmentally-inspired\nevaluations. Through pretraining experiments, we test whether the global\ndevelopmental ordering or the local discourse ordering of children's training\ndata support high performance relative to other datasets. The local properties\nof the data affect model results, but somewhat surprisingly, global properties\ndo not. Further, child language input is not uniquely valuable for training\nlanguage models. These findings support the hypothesis that, rather than\nproceeding from better data, children's learning is instead substantially more\nefficient than current language modeling techniques.\n","authors":["Steven Y. Feng","Noah D. Goodman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2408.03617v1.pdf","comment":"Preprint. Code and data will be released soon"},{"id":"http://arxiv.org/abs/2408.03615v1","updated":"2024-08-07T08:16:32Z","published":"2024-08-07T08:16:32Z","title":"Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in\n  Long-Horizon Tasks","summary":"  Building a general-purpose agent is a long-standing vision in the field of\nartificial intelligence. Existing agents have made remarkable progress in many\ndomains, yet they still struggle to complete long-horizon tasks in an open\nworld. We attribute this to the lack of necessary world knowledge and\nmultimodal experience that can guide agents through a variety of long-horizon\ntasks. In this paper, we propose a Hybrid Multimodal Memory module to address\nthe above challenges. It 1) transforms knowledge into Hierarchical Directed\nKnowledge Graph that allows agents to explicitly represent and learn world\nknowledge, and 2) summarises historical information into Abstracted Multimodal\nExperience Pool that provide agents with rich references for in-context\nlearning. On top of the Hybrid Multimodal Memory module, a multimodal agent,\nOptimus-1, is constructed with dedicated Knowledge-guided Planner and\nExperience-Driven Reflector, contributing to a better planning and reflection\nin the face of long-horizon tasks in Minecraft. Extensive experimental results\nshow that Optimus-1 significantly outperforms all existing agents on\nchallenging long-horizon task benchmarks, and exhibits near human-level\nperformance on many tasks. In addition, we introduce various Multimodal Large\nLanguage Models (MLLMs) as the backbone of Optimus-1. Experimental results show\nthat Optimus-1 exhibits strong generalization with the help of the Hybrid\nMultimodal Memory module, outperforming the GPT-4V baseline on many tasks.\n","authors":["Zaijing Li","Yuquan Xie","Rui Shao","Gongwei Chen","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2408.03615v1.pdf","comment":"30 pages, 13 figures"},{"id":"http://arxiv.org/abs/2401.06836v3","updated":"2024-08-07T08:09:53Z","published":"2024-01-12T16:42:10Z","title":"Enhancing Emotional Generation Capability of Large Language Models via\n  Emotional Chain-of-Thought","summary":"  Large Language Models (LLMs) have shown remarkable performance in various\nemotion recognition tasks, thereby piquing the research community's curiosity\nfor exploring their potential in emotional intelligence. However, several\nissues in the field of emotional generation tasks remain unresolved, including\nhuman preference alignment and emotional generation assessment. In this paper,\nwe propose the Emotional Chain-of-Thought (ECoT), a plug-and-play prompting\nmethod that enhances the performance of LLMs on various emotional generation\ntasks by aligning with human emotional intelligence guidelines. To assess the\nreliability of ECoT, we propose an automated model-based evaluation method\ncalled Emotional Generation Score (EGS). EGS incorporates Goleman's Emotional\nIntelligence Theory as a consensus of human experts, providing a new\nperspective on the evaluation of emotional generation tasks. Extensive\nexperimental results demonstrate the effectiveness of ECoT and EGS. Further, we\ndiscuss the promise of LLMs in the field of emotional intelligence and present\nkey insights into the LLMs with the ECoT in emotional generation tasks.\n","authors":["Zaijing Li","Gongwei Chen","Rui Shao","Yuquan Xie","Dongmei Jiang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2401.06836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18990v2","updated":"2024-08-07T07:46:39Z","published":"2024-07-25T12:07:55Z","title":"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications","summary":"  Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.\n","authors":["Alon Halfon","Shai Gretz","Ofir Arviv","Artem Spector","Orith Toledo-Ronen","Yoav Katz","Liat Ein-Dor","Michal Shmueli-Scheuer","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2407.18990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03603v1","updated":"2024-08-07T07:46:08Z","published":"2024-08-07T07:46:08Z","title":"EnJa: Ensemble Jailbreak on Large Language Models","summary":"  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n","authors":["Jiahao Zhang","Zilong Wang","Ruofan Wang","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.03603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v2","updated":"2024-08-07T07:20:23Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03573v1","updated":"2024-08-07T06:17:48Z","published":"2024-08-07T06:17:48Z","title":"Active Testing of Large Language Model via Multi-Stage Sampling","summary":"  Performance evaluation plays a crucial role in the development life cycle of\nlarge language models (LLMs). It estimates the model's capability, elucidates\nbehavior characteristics, and facilitates the identification of potential\nissues and limitations, thereby guiding further improvement. Given that LLMs'\ndiverse task-handling abilities stem from large volumes of training data, a\ncomprehensive evaluation also necessitates abundant, well-annotated, and\nrepresentative test data to assess LLM performance across various downstream\ntasks. However, the demand for high-quality test data often entails substantial\ntime, computational resources, and manual efforts, sometimes causing the\nevaluation to be inefficient or impractical. To address these challenges,\nresearchers propose active testing, which estimates the overall performance by\nselecting a subset of test data. Nevertheless, the existing active testing\nmethods tend to be inefficient, even inapplicable, given the unique new\nchallenges of LLMs (e.g., diverse task types, increased model complexity, and\nunavailability of training data). To mitigate such limitations and expedite the\ndevelopment cycle of LLMs, in this work, we introduce AcTracer, an active\ntesting framework tailored for LLMs that strategically selects a small subset\nof test data to achieve a nearly optimal performance estimation for LLMs.\nAcTracer utilizes both internal and external information from LLMs to guide the\ntest sampling process, reducing variance through a multi-stage pool-based\nactive selection. Our experiment results demonstrate that AcTracer achieves\nstate-of-the-art performance compared to existing methods across various tasks,\nwith up to 38.83% improvement over previous SOTA.\n","authors":["Yuheng Huang","Jiayang Song","Qiang Hu","Felix Juefei-Xu","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03573v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03567v1","updated":"2024-08-07T06:10:45Z","published":"2024-08-07T06:10:45Z","title":"Unlocking Exocentric Video-Language Data for Egocentric Video\n  Representation Learning","summary":"  We present EMBED (Egocentric Models Built with Exocentric Data), a method\ndesigned to transform exocentric video-language data for egocentric video\nrepresentation learning. Large-scale exocentric data covers diverse activities\nwith significant potential for egocentric learning, but inherent disparities\nbetween egocentric and exocentric data pose challenges in utilizing one view\nfor the other seamlessly. Egocentric videos predominantly feature close-up\nhand-object interactions, whereas exocentric videos offer a broader perspective\non human activities. Additionally, narratives in egocentric datasets are\ntypically more action-centric and closely linked with the visual content, in\ncontrast to the narrative styles found in exocentric datasets. To address these\nchallenges, we employ a data transformation framework to adapt exocentric data\nfor egocentric training, focusing on identifying specific video clips that\nemphasize hand-object interactions and transforming narration styles to align\nwith egocentric perspectives. By applying both vision and language style\ntransfer, our framework creates a new egocentric dataset derived from\nexocentric video-language data. Through extensive evaluations, we demonstrate\nthe effectiveness of EMBED, achieving state-of-the-art results across various\negocentric downstream tasks, including an absolute improvement of 4.7% on the\nEpic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification\nbenchmarks in zero-shot settings. Furthermore, EMBED enables egocentric\nvideo-language models to perform competitively in exocentric tasks. Finally, we\nshowcase EMBED's application across various exocentric datasets, exhibiting\nstrong generalization capabilities when applied to different exocentric\ndatasets.\n","authors":["Zi-Yi Dou","Xitong Yang","Tushar Nagarajan","Huiyu Wang","Jing Huang","Nanyun Peng","Kris Kitani","Fu-Jen Chu"],"pdf_url":"https://arxiv.org/pdf/2408.03567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02085v3","updated":"2024-08-07T06:04:31Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v3.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2408.03562v1","updated":"2024-08-07T05:52:00Z","published":"2024-08-07T05:52:00Z","title":"A Comparison of LLM Finetuning Methods & Evaluation Metrics with Travel\n  Chatbot Use Case","summary":"  This research compares large language model (LLM) fine-tuning methods,\nincluding Quantized Low Rank Adapter (QLoRA), Retrieval Augmented fine-tuning\n(RAFT), and Reinforcement Learning from Human Feedback (RLHF), and additionally\ncompared LLM evaluation methods including End to End (E2E) benchmark method of\n\"Golden Answers\", traditional natural language processing (NLP) metrics, RAG\nAssessment (Ragas), OpenAI GPT-4 evaluation metrics, and human evaluation,\nusing the travel chatbot use case. The travel dataset was sourced from the the\nReddit API by requesting posts from travel-related subreddits to get\ntravel-related conversation prompts and personalized travel experiences, and\naugmented for each fine-tuning method. We used two pretrained LLMs utilized for\nfine-tuning research: LLaMa 2 7B, and Mistral 7B. QLoRA and RAFT are applied to\nthe two pretrained models. The inferences from these models are extensively\nevaluated against the aforementioned metrics. The best model according to human\nevaluation and some GPT-4 metrics was Mistral RAFT, so this underwent a\nReinforcement Learning from Human Feedback (RLHF) training pipeline, and\nultimately was evaluated as the best model. Our main findings are that: 1)\nquantitative and Ragas metrics do not align with human evaluation, 2) Open AI\nGPT-4 evaluation most aligns with human evaluation, 3) it is essential to keep\nhumans in the loop for evaluation because, 4) traditional NLP metrics\ninsufficient, 5) Mistral generally outperformed LLaMa, 6) RAFT outperforms\nQLoRA, but still needs postprocessing, 7) RLHF improves model performance\nsignificantly. Next steps include improving data quality, increasing data\nquantity, exploring RAG methods, and focusing data collection on a specific\ncity, which would improve data quality by narrowing the focus, while creating a\nuseful product.\n","authors":["Sonia Meyer","Shreya Singh","Bertha Tam","Christopher Ton","Angel Ren"],"pdf_url":"https://arxiv.org/pdf/2408.03562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03554v1","updated":"2024-08-07T05:30:10Z","published":"2024-08-07T05:30:10Z","title":"Empirical Analysis of Large Vision-Language Models against Goal\n  Hijacking via Visual Prompt Injection","summary":"  We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.\n","authors":["Subaru Kimura","Ryota Tanaka","Shumpei Miyawaki","Jun Suzuki","Keisuke Sakaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03554v1.pdf","comment":"8 pages, 6 figures, Accepted to NAACL 2024 SRW"},{"id":"http://arxiv.org/abs/2408.03544v1","updated":"2024-08-07T04:49:38Z","published":"2024-08-07T04:49:38Z","title":"Unlocking the Non-Native Language Context Limitation: Native Language\n  Prompting Facilitates Knowledge Elicitation","summary":"  Multilingual large language models (MLLMs) struggle to answer questions posed\nin non-dominant languages, even though they have already acquired the relevant\nknowledge from their dominant language corpus. In contrast, human multilinguals\ncan overcome this issue by invoking the relatively rich knowledge acquired from\nnative language texts through Positive Native Language Transfer (PNLT).\nInspired by this, we analogize the dominant language of MLLMs to the native\nlanguage of human multilinguals, and propose Native Language Prompting (NatLan)\nto simulate the PNLT observed in human multilinguals. It explicitly creates\nnative language contexts for MLLMs to facilitate the elicitation of the rich\nnative language knowledge during question-answering, unlocking the limitations\nimposed by non-native language contexts on the effective application of\nknowledge. By employing multi-MLLM collaboration, NatLan reduces the workload\non each MLLM in simulating PNLT and refines semantic transfer. On the C-Eval\nbenchmark, NatLan provides up to a 10.1% average accuracy improvement and up to\na 5.0% increase in the hard-level subset across five MLLMs, surpassing all\ntop-notch related methods. Our code is available at\nhttps://github.com/AnonyNLP/NatLan.\n","authors":["Baixuan Li","Yunlong Fan","Zhiqiang Gao"],"pdf_url":"https://arxiv.org/pdf/2408.03544v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03541v1","updated":"2024-08-07T04:38:38Z","published":"2024-08-07T04:38:38Z","title":"EXAONE 3.0 7.8B Instruction Tuned Language Model","summary":"  We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\n","authors":["LG AI Research","Soyoung An","Kyunghoon Bae","Eunbi Choi","Stanley Jungkyu Choi","Yemuk Choi","Seokhee Hong","Yeonjung Hong","Junwon Hwang","Hyojin Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Yountae Jung","Euisoon Kim","Hyosang Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Youchul Kim","Edward Hwayoung Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Moontae Lee","Seungjun Lee","Woohyung Lim","Sangha Park","Sooyoun Park","Yongmin Park","Boseong Seo","Sihoon Yang","Heuiyeen Yeen","Kyungjae Yoo","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2408.03541v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06027v5","updated":"2024-08-07T04:17:16Z","published":"2024-07-08T15:25:33Z","title":"PAS: Data-Efficient Plug-and-Play Prompt Augmentation System","summary":"  In recent years, the rise of Large Language Models (LLMs) has spurred a\ngrowing demand for plug-and-play AI systems. Among the various AI techniques,\nprompt engineering stands out as particularly significant. However, users often\nface challenges in writing prompts due to the steep learning curve and\nsignificant time investment, and existing automatic prompt engineering (APE)\nmodels can be difficult to use. To address this issue, we propose PAS, an\nLLM-based plug-and-play APE system. PAS utilizes LLMs trained on high-quality,\nautomatically generated prompt complementary datasets, resulting in exceptional\nperformance. In comprehensive benchmarks, PAS achieves state-of-the-art (SoTA)\nresults compared to previous APE models, with an average improvement of 6.09\npoints. Moreover, PAS is highly efficient, achieving SoTA performance with only\n9000 data points. Additionally, PAS can autonomously generate prompt\naugmentation data without requiring additional human labor. Its flexibility\nalso allows it to be compatible with all existing LLMs and applicable to a wide\nrange of tasks. PAS excels in human evaluations, underscoring its suitability\nas a plug-in for users. This combination of high performance, efficiency, and\nflexibility makes PAS a valuable system for enhancing the usability and\neffectiveness of LLMs through improved prompt engineering.\n","authors":["Miao Zheng","Hao Liang","Fan Yang","Haoze Sun","Tianpeng Li","Lingchu Xiong","Yan Zhang","Youzhen Wu","Kun Li","Yanjun Shen","Mingan Lin","Tao Zhang","Guosheng Dong","Yujing Qiao","Kun Fang","Weipeng Chen","Bin Cui","Wentao Zhang","Zenan Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.06027v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01511v2","updated":"2024-08-07T03:46:30Z","published":"2024-05-02T17:44:41Z","title":"D2PO: Discriminator-Guided DPO with Response Evaluation Models","summary":"  Varied approaches for aligning language models have been proposed, including\nsupervised fine-tuning, RLHF, and direct optimization methods such as DPO.\nAlthough DPO has rapidly gained popularity due to its straightforward training\nprocess and competitive results, there is an open question of whether there\nremain practical advantages of using a discriminator, like a reward model, to\nevaluate responses. We propose D2PO, discriminator-guided DPO, an approach for\nthe online setting where preferences are being collected throughout learning.\nAs we collect gold preferences, we use these not only to train our policy, but\nto train a discriminative response evaluation model to silver-label even more\nsynthetic data for policy training. We explore this approach across a set of\ndiverse tasks, including a realistic chat setting, we find that our approach\nleads to higher-quality outputs compared to DPO with the same data budget, and\ngreater efficiency in terms of preference data requirements. Furthermore, we\nshow conditions under which silver labeling is most helpful: it is most\neffective when training the policy with DPO, outperforming traditional PPO, and\nbenefits from maintaining a separate discriminator from the policy model.\n","authors":["Prasann Singhal","Nathan Lambert","Scott Niekum","Tanya Goyal","Greg Durrett"],"pdf_url":"https://arxiv.org/pdf/2405.01511v2.pdf","comment":"20 pages, 12 figures, Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2404.04360v2","updated":"2024-08-07T03:36:51Z","published":"2024-04-05T19:14:14Z","title":"Prompt Public Large Language Models to Synthesize Data for Private\n  On-device Applications","summary":"  Pre-training on public data is an effective method to improve the performance\nfor federated learning (FL) with differential privacy (DP). This paper\ninvestigates how large language models (LLMs) trained on public data can\nimprove the quality of pre-training data for the on-device language models\ntrained with DP and FL. We carefully design LLM prompts to filter and transform\nexisting public data, and generate new data to resemble the real user data\ndistribution. The model pre-trained on our synthetic dataset achieves relative\nimprovement of 19.0% and 22.8% in next word prediction accuracy compared to the\nbaseline model pre-trained on a standard public dataset, when evaluated over\nthe real user data in Gboard (Google Keyboard, a production mobile keyboard\napplication). Furthermore, our method achieves evaluation accuracy better than\nor comparable to the baseline during the DP FL fine-tuning over millions of\nmobile devices, and our final model outperforms the baseline in production A/B\ntesting. Our experiments demonstrate the strengths of LLMs in synthesizing data\nclose to the private distribution even without accessing the private data, and\nalso suggest future research directions to further reduce the distribution gap.\n","authors":["Shanshan Wu","Zheng Xu","Yanxiang Zhang","Yuanbo Zhang","Daniel Ramage"],"pdf_url":"https://arxiv.org/pdf/2404.04360v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2408.03524v1","updated":"2024-08-07T03:23:55Z","published":"2024-08-07T03:23:55Z","title":"EgyBERT: A Large Language Model Pretrained on Egyptian Dialect Corpora","summary":"  This study presents EgyBERT, an Arabic language model pretrained on 10.4 GB\nof Egyptian dialectal texts. We evaluated EgyBERT's performance by comparing it\nwith five other multidialect Arabic language models across 10 evaluation\ndatasets. EgyBERT achieved the highest average F1-score of 84.25% and an\naccuracy of 87.33%, significantly outperforming all other comparative models,\nwith MARBERTv2 as the second best model achieving an F1-score 83.68% and an\naccuracy 87.19%. Additionally, we introduce two novel Egyptian dialectal\ncorpora: the Egyptian Tweets Corpus (ETC), containing over 34.33 million tweets\n(24.89 million sentences) amounting to 2.5 GB of text, and the Egyptian Forums\nCorpus (EFC), comprising over 44.42 million sentences (7.9 GB of text)\ncollected from various Egyptian online forums. Both corpora are used in\npretraining the new model, and they are the largest Egyptian dialectal corpora\nto date reported in the literature. Furthermore, this is the first study to\nevaluate the performance of various language models on Egyptian dialect\ndatasets, revealing significant differences in performance that highlight the\nneed for more dialect-specific models. The results confirm the effectiveness of\nEgyBERT model in processing and analyzing Arabic text expressed in Egyptian\ndialect, surpassing other language models included in the study. EgyBERT model\nis publicly available on \\url{https://huggingface.co/faisalq/EgyBERT}.\n","authors":["Faisal Qarah"],"pdf_url":"https://arxiv.org/pdf/2408.03524v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03511v1","updated":"2024-08-07T02:28:37Z","published":"2024-08-07T02:28:37Z","title":"MoExtend: Tuning New Experts for Modality and Task Extension","summary":"  Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.\n","authors":["Shanshan Zhong","Shanghua Gao","Zhongzhan Huang","Wushao Wen","Marinka Zitnik","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03511v1.pdf","comment":"ACL 2024 - SRW"},{"id":"http://arxiv.org/abs/2408.03506v1","updated":"2024-08-07T02:14:52Z","published":"2024-08-07T02:14:52Z","title":"1.5-Pints Technical Report: Pretraining in Days, Not Months -- Your\n  Language Model Thrives on Quality Data","summary":"  This paper presents a compute-efficient approach to pre-training a Language\nModel-the \"1.5-Pints\"-in only 9 days, while outperforming state-of-the-art\nmodels as an instruction-following assistant.Based on MT-Bench (a benchmark\nthat emulates human judgments), 1.5-Pints outperforms Apple's OpenELM and\nMicrosoft's Phi.This is achieved by a carefully curated pre-training dataset of\n57 billion tokens, using a mix of automated workflows and manual human review.\nThe selection of the dataset prioritizes content that is considered expository\nand \"textbook-like\" to aid the model in reasoning and logical deduction,\nculminating in its overall ability as a strong and versatile AI model. In terms\nof the model architecture, we employed a modified Mistral tokenizer, alongside\na Llama-2 architecture for wider compatibility. For training, we adopted the\nmethodologies used by StableLM, TinyLlama, and Huggingface Zephyr. 1.5-Pints\ndemonstrates that by focusing on data quality over quantity in LLM training, we\ncan significantly reduce training time and resources required. We believe this\napproach will not only make pre-training more accessible but also reduce our\ncarbon footprint. Our findings and resources from this research are\nopen-sourced, aiming to facilitate further advancements in the field. The\n1.5-Pints model is available in two versions: 2K and 16K context windows.\n","authors":["Calvin Tan","Jerome Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03506v1.pdf","comment":"Technical Report for 1.5-Pints"},{"id":"http://arxiv.org/abs/2408.03505v1","updated":"2024-08-07T02:08:29Z","published":"2024-08-07T02:08:29Z","title":"Optimus: Accelerating Large-Scale Multi-Modal LLM Training by Bubble\n  Exploitation","summary":"  Multimodal large language models (MLLMs) have extended the success of large\nlanguage models (LLMs) to multiple data types, such as image, text and audio,\nachieving significant performance in various domains, including multimodal\ntranslation, visual question answering and content generation. Nonetheless,\nexisting systems are inefficient to train MLLMs due to substantial GPU bubbles\ncaused by the heterogeneous modality models and complex data dependencies in 3D\nparallelism. This paper proposes Optimus, a distributed MLLM training system\nthat reduces end-to-end MLLM training time. Optimus is based on our principled\nanalysis that scheduling the encoder computation within the LLM bubbles can\nreduce bubbles in MLLM training. To make scheduling encoder computation\npossible for all GPUs, Optimus searches the separate parallel plans for encoder\nand LLM, and adopts a bubble scheduling algorithm to enable exploiting LLM\nbubbles without breaking the original data dependencies in the MLLM model\narchitecture. We further decompose encoder layer computation into a series of\nkernels, and analyze the common bubble pattern of 3D parallelism to carefully\noptimize the sub-millisecond bubble scheduling, minimizing the overall training\ntime. Our experiments in a production cluster show that Optimus accelerates\nMLLM training by 20.5%-21.3% with ViT-22B and GPT-175B model over 3072 GPUs\ncompared to baselines.\n","authors":["Weiqi Feng","Yangrui Chen","Shaoyu Wang","Yanghua Peng","Haibin Lin","Minlan Yu"],"pdf_url":"https://arxiv.org/pdf/2408.03505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03492v1","updated":"2024-08-07T01:03:56Z","published":"2024-08-07T01:03:56Z","title":"Automated Theorem Provers Help Improve Large Language Model Reasoning","summary":"  In this paper we demonstrate how logic programming systems and Automated\nfirst-order logic Theorem Provers (ATPs) can improve the accuracy of Large\nLanguage Models (LLMs) for logical reasoning tasks where the baseline\nperformance is given by direct LLM solutions. We first evaluate LLM reasoning\non steamroller problems using the PRONTOQA benchmark. We show how accuracy can\nbe improved with a neuro-symbolic architecture where the LLM acts solely as a\nfront-end for translating a given problem into a formal logic language and an\nautomated reasoning engine is called for solving it. However, this approach\ncritically hinges on the correctness of the LLM translation. To assess this\ntranslation correctness, we secondly define a framework of syntactic and\nsemantic error categories. We implemented the framework and used it to identify\nerrors that LLMs make in the benchmark domain. Based on these findings, we\nthirdly extended our method with capabilities for automatically correcting\nsyntactic and semantic errors. For semantic error correction we integrate\nfirst-order logic ATPs, which is our main and novel contribution. We\ndemonstrate that this approach reduces semantic errors significantly and\nfurther increases the accurracy of LLM logical reasoning.\n","authors":["Lachlan McGinness","Peter Baumgartner"],"pdf_url":"https://arxiv.org/pdf/2408.03492v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03281v2","updated":"2024-08-07T01:00:55Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v2.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval\n  ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.04127v1","updated":"2024-08-07T23:23:50Z","published":"2024-08-07T23:23:50Z","title":"Incorporating Spatial Awareness in Data-Driven Gesture Generation for\n  Virtual Agents","summary":"  This paper focuses on enhancing human-agent communication by integrating\nspatial context into virtual agents' non-verbal behaviors, specifically\ngestures. Recent advances in co-speech gesture generation have primarily\nutilized data-driven methods, which create natural motion but limit the scope\nof gestures to those performed in a void. Our work aims to extend these methods\nby enabling generative models to incorporate scene information into\nspeech-driven gesture synthesis. We introduce a novel synthetic gesture dataset\ntailored for this purpose. This development represents a critical step toward\ncreating embodied conversational agents that interact more naturally with their\nenvironment and users.\n","authors":["Anna Deichler","Simon Alexanderson","Jonas Beskow"],"pdf_url":"https://arxiv.org/pdf/2408.04127v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04121v1","updated":"2024-08-07T23:09:23Z","published":"2024-08-07T23:09:23Z","title":"Can Rule-Based Insights Enhance LLMs for Radiology Report\n  Classification? Introducing the RadPrompt Methodology","summary":"  Developing imaging models capable of detecting pathologies from chest X-rays\ncan be cost and time-prohibitive for large datasets as it requires supervision\nto attain state-of-the-art performance. Instead, labels extracted from\nradiology reports may serve as distant supervision since these are routinely\ngenerated as part of clinical practice. Despite their widespread use, current\nrule-based methods for label extraction rely on extensive rule sets that are\nlimited in their robustness to syntactic variability. To alleviate these\nlimitations, we introduce RadPert, a rule-based system that integrates an\nuncertainty-aware information schema with a streamlined set of rules, enhancing\nperformance. Additionally, we have developed RadPrompt, a multi-turn prompting\nstrategy that leverages RadPert to bolster the zero-shot predictive\ncapabilities of large language models, achieving a statistically significant\nimprovement in weighted average F1 score over GPT-4 Turbo. Most notably,\nRadPrompt surpasses both its underlying models, showcasing the synergistic\npotential of LLMs with rule-based models. We have evaluated our methods on two\nEnglish Corpora: the MIMIC-CXR gold-standard test set and a gold-standard\ndataset collected from the Cambridge University Hospitals.\n","authors":["Panagiotis Fytas","Anna Breger","Ian Selby","Simon Baker","Shahab Shahipasand","Anna Korhonen"],"pdf_url":"https://arxiv.org/pdf/2408.04121v1.pdf","comment":"Accepted at BioNLP, ACL 2024"},{"id":"http://arxiv.org/abs/2404.01536v2","updated":"2024-08-07T22:46:04Z","published":"2024-04-02T00:02:00Z","title":"Laying Anchors: Semantically Priming Numerals in Language Modeling","summary":"  Off-the-shelf pre-trained language models have become the de facto standard\nin NLP pipelines for a multitude of downstream tasks. However, the inability of\nthese models to properly encode numerals limits their performance on tasks\nrequiring numeric comprehension. We introduce strategies to semantically prime\nnumerals in any corpus by generating anchors governed by the distribution of\nnumerals in said corpus, thereby enabling mathematically grounded\nrepresentations of these numeral tokens. We establish the superiority of our\nproposed techniques through evaluation on a range of numeracy tasks for both\nin-domain (seen) and out-domain (unseen) numerals. Further, we expand our\nempirical evaluations to numerals ranging from 1 to 10 billion, a significantly\nbroader range compared to previous studies of the same nature, and we\ndemonstrate significant improvements in the mathematical grounding of our\nlearned embeddings.\n","authors":["Mandar Sharma","Rutuja Murlidhar Taware","Pravesh Koirala","Nikhil Muralidhar","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2404.01536v2.pdf","comment":"Accepted to the findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2408.04114v1","updated":"2024-08-07T22:32:19Z","published":"2024-08-07T22:32:19Z","title":"Zero-shot Factual Consistency Evaluation Across Domains","summary":"  This work addresses the challenge of factual consistency in text generation\nsystems. We unify the tasks of Natural Language Inference, Summarization\nEvaluation, Factuality Verification and Factual Consistency Evaluation to train\nmodels capable of evaluating the factual consistency of source-target pairs\nacross diverse domains. We rigorously evaluate these against eight baselines on\na comprehensive benchmark suite comprising 22 datasets that span various tasks,\ndomains, and document lengths. Results demonstrate that our method achieves\nstate-of-the-art performance on this heterogeneous benchmark while addressing\nefficiency concerns and attaining cross-domain generalization.\n","authors":["Raunak Agarwal"],"pdf_url":"https://arxiv.org/pdf/2408.04114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04112v1","updated":"2024-08-07T22:27:19Z","published":"2024-08-07T22:27:19Z","title":"Patchview: LLM-Powered Worldbuilding with Generative Dust and Magnet\n  Visualization","summary":"  Large language models (LLMs) can help writers build story worlds by\ngenerating world elements, such as factions, characters, and locations.\nHowever, making sense of many generated elements can be overwhelming. Moreover,\nif the user wants to precisely control aspects of generated elements that are\ndifficult to specify verbally, prompting alone may be insufficient. We\nintroduce Patchview, a customizable LLM-powered system that visually aids\nworldbuilding by allowing users to interact with story concepts and elements\nthrough the physical metaphor of magnets and dust. Elements in Patchview are\nvisually dragged closer to concepts with high relevance, facilitating\nsensemaking. The user can also steer the generation with verbally elusive\nconcepts by indicating the desired position of the element between concepts.\nWhen the user disagrees with the LLM's visualization and generation, they can\ncorrect those by repositioning the element. These corrections can be used to\nalign the LLM's future behaviors to the user's perception. With a user study,\nwe show that Patchview supports the sensemaking of world elements and steering\nof element generation, facilitating exploration during the worldbuilding\nprocess. Patchview provides insights on how customizable visual representation\ncan help sensemake, steer, and align generative AI model behaviors with the\nuser's intentions.\n","authors":["John Joon Young Chung","Max Kreminski"],"pdf_url":"https://arxiv.org/pdf/2408.04112v1.pdf","comment":"Accepted to UIST2024"},{"id":"http://arxiv.org/abs/2307.06865v3","updated":"2024-08-07T22:05:01Z","published":"2023-07-13T16:15:08Z","title":"Effective Prompt Extraction from Language Models","summary":"  The text generated by large language models is commonly controlled by\nprompting, where a prompt prepended to a user's query guides the model's\noutput. The prompts used by companies to guide their models are often treated\nas secrets, to be hidden from the user making the query. They have even been\ntreated as commodities to be bought and sold on marketplaces. However,\nanecdotal reports have shown adversarial users employing prompt extraction\nattacks to recover these prompts. In this paper, we present a framework for\nsystematically measuring the effectiveness of these attacks. In experiments\nwith 3 different sources of prompts and 11 underlying large language models, we\nfind that simple text-based attacks can in fact reveal prompts with high\nprobability. Our framework determines with high precision whether an extracted\nprompt is the actual secret prompt, rather than a model hallucination. Prompt\nextraction from real systems such as Claude 3 and ChatGPT further suggest that\nsystem prompts can be revealed by an adversary despite existing defenses in\nplace.\n","authors":["Yiming Zhang","Nicholas Carlini","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2307.06865v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.10859v2","updated":"2024-08-07T21:57:20Z","published":"2024-04-16T19:17:23Z","title":"Forcing Diffuse Distributions out of Language Models","summary":"  Despite being trained specifically to follow user instructions, today's\ninstructiontuned language models perform poorly when instructed to produce\nrandom outputs. For example, when prompted to pick a number uniformly between\none and ten Llama-2-13B-chat disproportionately favors the number five, and\nwhen tasked with picking a first name at random, Mistral-7B-Instruct chooses\nAvery 40 times more often than we would expect based on the U.S. population.\nWhen these language models are used for real-world tasks where diversity of\noutputs is crucial, such as language model assisted dataset construction, their\ninability to produce diffuse distributions over valid choices is a major\nhurdle. In this work, we propose a fine-tuning method that encourages language\nmodels to output distributions that are diffuse over valid outcomes. The\nmethods we introduce generalize across a variety of tasks and distributions and\nmake large language models practical for synthetic dataset generation with\nlittle human intervention.\n","authors":["Yiming Zhang","Avi Schwarzschild","Nicholas Carlini","Zico Kolter","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2404.10859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04093v1","updated":"2024-08-07T21:16:55Z","published":"2024-08-07T21:16:55Z","title":"Tree Attention: Topology-aware Decoding for Long-Context Attention on\n  GPU clusters","summary":"  Self-attention is the core mathematical operation of modern transformer\narchitectures and is also a significant computational bottleneck due to its\nquadratic complexity in the sequence length. In this work, we derive the scalar\nenergy function whose gradient computes the self-attention block, thus\nelucidating the theoretical underpinnings of self-attention, providing a\nBayesian interpretation of the operation and linking it closely with\nenergy-based models such as Hopfield Networks. Moreover, due to this\nformulation, we discover that we can use efficient and optimized\nautomatic-differentiation techniques to derive a highly efficient Tree\nAttention algorithm to compute the gradient of the energy and hence\nself-attention. Our formulation reveals that the reduction across the sequence\naxis can be efficiently computed in parallel through a tree reduction. Our\nalgorithm, for parallelizing attention computation across multiple GPUs,\nenables cross-device decoding to be performed asymptotically faster (up to 8x\nfaster) than alternative approaches such as Ring Attention, while also\nrequiring significantly less communication volume and incurring 2x less peak\nmemory. Our code is publicly available here:\n\\url{https://github.com/Zyphra/tree_attention}\n","authors":["Vasudev Shyam","Jonathan Pilault","Emily Shepperd","Quentin Anthony","Beren Millidge"],"pdf_url":"https://arxiv.org/pdf/2408.04093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.04845v2","updated":"2024-08-07T20:04:20Z","published":"2023-06-08T00:35:36Z","title":"Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with\n  Architecture-Routed Mixture-of-Experts","summary":"  Weight-sharing supernets are crucial for performance estimation in\ncutting-edge neural architecture search (NAS) frameworks. Despite their ability\nto generate diverse subnetworks without retraining, the quality of these\nsubnetworks is not guaranteed due to weight sharing. In NLP tasks like machine\ntranslation and pre-trained language modeling, there is a significant\nperformance gap between supernet and training from scratch for the same model\narchitecture, necessitating retraining post optimal architecture\nidentification.\n  This study introduces a solution called mixture-of-supernets, a generalized\nsupernet formulation leveraging mixture-of-experts (MoE) to enhance supernet\nmodel expressiveness with minimal training overhead. Unlike conventional\nsupernets, this method employs an architecture-based routing mechanism,\nenabling indirect sharing of model weights among subnetworks. This\ncustomization of weights for specific architectures, learned through gradient\ndescent, minimizes retraining time, significantly enhancing training efficiency\nin NLP. The proposed method attains state-of-the-art (SoTA) performance in NAS\nfor fast machine translation models, exhibiting a superior latency-BLEU\ntradeoff compared to HAT, the SoTA NAS framework for machine translation.\nFurthermore, it excels in NAS for building memory-efficient task-agnostic BERT\nmodels, surpassing NAS-BERT and AutoDistil across various model sizes. The code\ncan be found at: https://github.com/UBC-NLP/MoS.\n","authors":["Ganesh Jawahar","Haichuan Yang","Yunyang Xiong","Zechun Liu","Dilin Wang","Fei Sun","Meng Li","Aasish Pappu","Barlas Oguz","Muhammad Abdul-Mageed","Laks V. S. Lakshmanan","Raghuraman Krishnamoorthi","Vikas Chandra"],"pdf_url":"https://arxiv.org/pdf/2306.04845v2.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2310.16712v2","updated":"2024-08-07T19:59:00Z","published":"2023-10-25T15:34:30Z","title":"LLM Performance Predictors are good initializers for Architecture Search","summary":"  In this work, we utilize Large Language Models (LLMs) for a novel use case:\nconstructing Performance Predictors (PP) that estimate the performance of\nspecific deep neural network architectures on downstream tasks. We create PP\nprompts for LLMs, comprising (i) role descriptions, (ii) instructions for the\nLLM, (iii) hyperparameter definitions, and (iv) demonstrations presenting\nsample architectures with efficiency metrics and `training from scratch'\nperformance. In machine translation (MT) tasks, GPT-4 with our PP prompts\n(LLM-PP) achieves a SoTA mean absolute error and a slight degradation in rank\ncorrelation coefficient compared to baseline predictors. Additionally, we\ndemonstrate that predictions from LLM-PP can be distilled to a compact\nregression model (LLM-Distill-PP), which surprisingly retains much of the\nperformance of LLM-PP. This presents a cost-effective alternative for\nresource-intensive performance estimation. Specifically, for Neural\nArchitecture Search (NAS), we introduce a Hybrid-Search algorithm (HS-NAS)\nemploying LLM-Distill-PP for the initial search stages and reverting to the\nbaseline predictor later. HS-NAS performs similarly to SoTA NAS, reducing\nsearch hours by approximately 50%, and in some cases, improving latency,\nGFLOPs, and model size. The code can be found at:\nhttps://github.com/UBC-NLP/llmas.\n","authors":["Ganesh Jawahar","Muhammad Abdul-Mageed","Laks V. S. Lakshmanan","Dujian Ding"],"pdf_url":"https://arxiv.org/pdf/2310.16712v2.pdf","comment":"ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2404.16816v2","updated":"2024-08-07T19:47:21Z","published":"2024-04-25T17:57:36Z","title":"IndicGenBench: A Multilingual Benchmark to Evaluate Generation\n  Capabilities of LLMs on Indic Languages","summary":"  As large language models (LLMs) see increasing adoption across the globe, it\nis imperative for LLMs to be representative of the linguistic diversity of the\nworld. India is a linguistically diverse country of 1.4 Billion people. To\nfacilitate research on multilingual LLM evaluation, we release IndicGenBench -\nthe largest benchmark for evaluating LLMs on user-facing generation tasks\nacross a diverse set 29 of Indic languages covering 13 scripts and 4 language\nfamilies. IndicGenBench is composed of diverse generation tasks like\ncross-lingual summarization, machine translation, and cross-lingual question\nanswering. IndicGenBench extends existing benchmarks to many Indic languages\nthrough human curation providing multi-way parallel evaluation data for many\nunder-represented Indic languages for the first time. We evaluate a wide range\nof proprietary and open-source LLMs including GPT-3.5, GPT-4, PaLM-2, mT5,\nGemma, BLOOM and LLaMA on IndicGenBench in a variety of settings. The largest\nPaLM-2 models performs the best on most tasks, however, there is a significant\nperformance gap in all languages compared to English showing that further\nresearch is needed for the development of more inclusive multilingual language\nmodels. IndicGenBench is released at\nwww.github.com/google-research-datasets/indic-gen-bench\n","authors":["Harman Singh","Nitish Gupta","Shikhar Bharadwaj","Dinesh Tewari","Partha Talukdar"],"pdf_url":"https://arxiv.org/pdf/2404.16816v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2403.19154v3","updated":"2024-08-07T19:44:28Z","published":"2024-03-28T05:35:22Z","title":"STaR-GATE: Teaching Language Models to Ask Clarifying Questions","summary":"  When prompting language models to complete a task, users often leave\nimportant aspects unsaid. While asking questions could resolve this ambiguity\n(GATE; Li et al., 2023), models often struggle to ask good questions. We\nexplore a language model's ability to self-improve (STaR; Zelikman et al.,\n2022) by rewarding the model for generating useful questions-a simple method we\ndub STaR-GATE. We generate a synthetic dataset of 25,500 unique persona-task\nprompts to simulate conversations between a pretrained language model-the\nQuestioner-and a Roleplayer whose preferences are unknown to the Questioner. By\nasking questions, the Questioner elicits preferences from the Roleplayer. The\nQuestioner is iteratively finetuned on questions that increase the probability\nof high-quality responses to the task, which are generated by an Oracle with\naccess to the Roleplayer's latent preferences. After two iterations of\nself-improvement, the Questioner asks better questions, allowing it to generate\nresponses that are preferred over responses from the initial model on 72% of\ntasks. Our results indicate that teaching a language model to ask better\nquestions leads to better personalized responses.\n","authors":["Chinmaya Andukuri","Jan-Philipp Frnken","Tobias Gerstenberg","Noah D. Goodman"],"pdf_url":"https://arxiv.org/pdf/2403.19154v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09611v2","updated":"2024-08-07T19:27:53Z","published":"2024-02-14T22:57:03Z","title":"Towards Privacy-Aware Sign Language Translation at Scale","summary":"  A major impediment to the advancement of sign language translation (SLT) is\ndata scarcity. Much of the sign language data currently available on the web\ncannot be used for training supervised models due to the lack of aligned\ncaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bears\nprivacy risks due to the presence of biometric information, which the\nresponsible development of SLT technologies should account for. In this work,\nwe propose a two-stage framework for privacy-aware SLT at scale that addresses\nboth of these issues. We introduce SSVP-SLT, which leverages self-supervised\nvideo pretraining on anonymized and unannotated videos, followed by supervised\nSLT finetuning on a curated parallel dataset. SSVP-SLT achieves\nstate-of-the-art finetuned and zero-shot gloss-free SLT performance on the\nHow2Sign dataset, outperforming the strongest respective baselines by over 3\nBLEU-4. Based on controlled experiments, we further discuss the advantages and\nlimitations of self-supervised pretraining and anonymization via facial\nobfuscation for SLT.\n","authors":["Phillip Rust","Bowen Shi","Skyler Wang","Necati Cihan Camgz","Jean Maillard"],"pdf_url":"https://arxiv.org/pdf/2402.09611v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2404.04516v2","updated":"2024-08-07T19:08:32Z","published":"2024-04-06T06:12:13Z","title":"Language Models as Critical Thinking Tools: A Case Study of Philosophers","summary":"  Current work in language models (LMs) helps us speed up or even skip thinking\nby accelerating and automating cognitive work. But can LMs help us with\ncritical thinking -- thinking in deeper, more reflective ways which challenge\nassumptions, clarify ideas, and engineer new concepts? We treat philosophy as a\ncase study in critical thinking, and interview 21 professional philosophers\nabout how they engage in critical thinking and on their experiences with LMs.\nWe find that philosophers do not find LMs to be useful because they lack a\nsense of selfhood (memory, beliefs, consistency) and initiative (curiosity,\nproactivity). We propose the selfhood-initiative model for critical thinking\ntools to characterize this gap. Using the model, we formulate three roles LMs\ncould play as critical thinking tools: the Interlocutor, the Monitor, and the\nRespondent. We hope that our work inspires LM researchers to further develop\nLMs as critical thinking tools and philosophers and other 'critical thinkers'\nto imagine intellectually substantive uses of LMs.\n","authors":["Andre Ye","Jared Moore","Rose Novick","Amy X. Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.04516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02964v2","updated":"2024-08-07T18:50:48Z","published":"2024-08-06T05:21:13Z","title":"Accuracy and Consistency of LLMs in the Registered Dietitian Exam: The\n  Impact of Prompt Engineering and Knowledge Retrieval","summary":"  Large language models (LLMs) are fundamentally transforming human-facing\napplications in the health and well-being domains: boosting patient engagement,\naccelerating clinical decision-making, and facilitating medical education.\nAlthough state-of-the-art LLMs have shown superior performance in several\nconversational applications, evaluations within nutrition and diet applications\nare still insufficient. In this paper, we propose to employ the Registered\nDietitian (RD) exam to conduct a standard and comprehensive evaluation of\nstate-of-the-art LLMs, GPT-4o, Claude 3.5 Sonnet, and Gemini 1.5 Pro, assessing\nboth accuracy and consistency in nutrition queries. Our evaluation includes\n1050 RD exam questions encompassing several nutrition topics and proficiency\nlevels. In addition, for the first time, we examine the impact of Zero-Shot\n(ZS), Chain of Thought (CoT), Chain of Thought with Self Consistency (CoT-SC),\nand Retrieval Augmented Prompting (RAP) on both accuracy and consistency of the\nresponses. Our findings revealed that while these LLMs obtained acceptable\noverall performance, their results varied considerably with different prompts\nand question domains. GPT-4o with CoT-SC prompting outperformed the other\napproaches, whereas Gemini 1.5 Pro with ZS recorded the highest consistency.\nFor GPT-4o and Claude 3.5, CoT improved the accuracy, and CoT-SC improved both\naccuracy and consistency. RAP was particularly effective for GPT-4o to answer\nExpert level questions. Consequently, choosing the appropriate LLM and\nprompting technique, tailored to the proficiency level and specific domain, can\nmitigate errors and potential risks in diet and nutrition chatbots.\n","authors":["Iman Azimi","Mohan Qi","Li Wang","Amir M. Rahmani","Youlin Li"],"pdf_url":"https://arxiv.org/pdf/2408.02964v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07812v2","updated":"2024-08-07T18:27:59Z","published":"2024-02-12T17:17:50Z","title":"Retrieval Augmented Thought Process for Private Data Handling in\n  Healthcare","summary":"  Large Language Models (LLMs) have demonstrated the strong potential to assist\nboth clinicians and the general public with their extensive medical knowledge.\nHowever, their application in healthcare is constrained due to concerns about\nthe privacy of data used in training, which prevents the integration of private\nand personal information because of security and ethical issues. Moreover, if\ntheir capabilities can be enhanced with information retrieval to access\nup-to-date knowledge, the current integration of LLMs with Information\nretrieval lacks robustness to imperfect retrieval, which can hinder their\neffectiveness and even reduce overall performance. In this work, we address\nthis challenge by introducing the Retrieval-Augmented Thought Process (RATP).\nGiven access to external knowledge, RATP formulates the thought generation of\nLLMs as a multiple-step decision process. To optimise such a thought process,\nRATP leverages Monte-Carlo Tree Search and learns a proxy reward function that\npermits cost-efficient inference. On a private dataset of electronic medical\nrecords, deliberately excluded from any LLM training set, RATP achieves 35%\nadditional accuracy compared to in-context retrieval-augmented generation for\nthe question-answering task.\n","authors":["Thomas Pouplin","Hao Sun","Samuel Holt","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.07812v2.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.04029v1","updated":"2024-08-07T18:24:23Z","published":"2024-08-07T18:24:23Z","title":"Human Speech Perception in Noise: Can Large Language Models Paraphrase\n  to Improve It?","summary":"  Large Language Models (LLMs) can generate text by transferring style\nattributes like formality resulting in formal or informal text. However,\ninstructing LLMs to generate text that when spoken, is more intelligible in an\nacoustically difficult environment, is an under-explored topic. We conduct the\nfirst study to evaluate LLMs on a novel task of generating acoustically\nintelligible paraphrases for better human speech perception in noise. Our\nexperiments in English demonstrated that with standard prompting, LLMs struggle\nto control the non-textual attribute, i.e., acoustic intelligibility, while\nefficiently capturing the desired textual attributes like semantic equivalence.\nTo remedy this issue, we propose a simple prompting approach,\nprompt-and-select, which generates paraphrases by decoupling the desired\ntextual and non-textual attributes in the text generation pipeline. Our\napproach resulted in a 40% relative improvement in human speech perception, by\nparaphrasing utterances that are highly distorted in a listening condition with\nbabble noise at a signal-to-noise ratio (SNR) -5 dB. This study reveals the\nlimitation of LLMs in capturing non-textual attributes, and our proposed method\nshowcases the potential of using LLMs for better human speech perception in\nnoise.\n","authors":["Anupama Chingacham","Miaoran Zhang","Vera Demberg","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2408.04029v1.pdf","comment":"Accepted at HuCLLM @ ACL 2024"},{"id":"http://arxiv.org/abs/2403.00952v2","updated":"2024-08-07T18:14:12Z","published":"2024-03-01T20:03:44Z","title":"MediSwift: Efficient Sparse Pre-trained Biomedical Language Models","summary":"  Large language models (LLMs) are typically trained on general source data for\nvarious domains, but a recent surge in domain-specific LLMs has shown their\npotential to outperform general-purpose models in domain-specific tasks (e.g.,\nbiomedicine). Although domain-specific pre-training enhances efficiency and\nleads to smaller models, the computational costs of training these LLMs remain\nhigh, posing budgeting challenges. We introduce MediSwift, a suite of\nbiomedical LMs that leverage sparse pre-training on domain-specific biomedical\ntext data. By inducing up to 75% weight sparsity during the pre-training phase,\nMediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse\npre-training was performed on the Cerebras CS-2 system, which is specifically\ndesigned to realize the acceleration benefits from unstructured weight\nsparsity, thereby significantly enhancing the efficiency of the MediSwift\nmodels. Through subsequent dense fine-tuning and strategic soft prompting,\nMediSwift models outperform existing LLMs up to 7B parameters on biomedical\ntasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as\nPubMedQA. Our results show that sparse pre-training, along with dense\nfine-tuning and soft prompting, offers an effective method for creating\nhigh-performing, computationally efficient models in specialized domains.\n","authors":["Vithursan Thangarasa","Mahmoud Salem","Shreyas Saxena","Kevin Leong","Joel Hestness","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2403.00952v2.pdf","comment":"14 pages, 2 Figures, 5 Tables (Main Paper) + 3 pages (Supplementary\n  Material). Published at ACL 2024"},{"id":"http://arxiv.org/abs/2408.04023v1","updated":"2024-08-07T18:12:02Z","published":"2024-08-07T18:12:02Z","title":"Improving Large Language Model (LLM) fidelity through context-aware\n  grounding: A systematic approach to reliability and veracity","summary":"  As Large Language Models (LLMs) become increasingly sophisticated and\nubiquitous in natural language processing (NLP) applications, ensuring their\nrobustness, trustworthiness, and alignment with human values has become a\ncritical challenge. This paper presents a novel framework for contextual\ngrounding in textual models, with a particular emphasis on the Context\nRepresentation stage. Our approach aims to enhance the reliability and ethical\nalignment of these models through a comprehensive, context-aware methodology.\nBy explicitly capturing and representing relevant situational, cultural, and\nethical contexts in a machine-readable format, we lay the foundation for\nanchoring a model's behavior within these contexts. Our approach leverages\ntechniques from knowledge representation and reasoning, such as ontologies,\nsemantic web technologies, and logic-based formalisms. We evaluate our\nframework on real-world textual datasets, demonstrating its effectiveness in\nimproving model performance, fairness, and alignment with human expectations,\nwhile maintaining high accuracy. Furthermore, we discuss the other key\ncomponents of the framework, including context-aware encoding, context-aware\nlearning, interpretability and explainability, and continuous monitoring and\nadaptation. This research contributes to the growing body of work on\nresponsible AI, offering a practical approach to developing more reliable,\ntrustworthy, and ethically-aligned language models. Our findings have\nsignificant implications for the deployment of LLMs in sensitive domains such\nas healthcare, legal systems, and social services, where contextual\nunderstanding is paramount.\n","authors":["Wrick Talukdar","Anjanava Biswas"],"pdf_url":"https://arxiv.org/pdf/2408.04023v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.04015v1","updated":"2024-08-07T18:04:01Z","published":"2024-08-07T18:04:01Z","title":"Image-to-LaTeX Converter for Mathematical Formulas and Text","summary":"  In this project, we train a vision encoder-decoder model to generate LaTeX\ncode from images of mathematical formulas and text. Utilizing a diverse\ncollection of image-to-LaTeX data, we build two models: a base model with a\nSwin Transformer encoder and a GPT-2 decoder, trained on machine-generated\nimages, and a fine-tuned version enhanced with Low-Rank Adaptation (LoRA)\ntrained on handwritten formulas. We then compare the BLEU performance of our\nspecialized model on a handwritten test set with other similar models, such as\nPix2Text, TexTeller, and Sumen. Through this project, we contribute open-source\nmodels for converting images to LaTeX and provide from-scratch code for\nbuilding these models with distributed training and GPU optimizations.\n","authors":["Daniil Gurgurov","Aleksey Morshnev"],"pdf_url":"https://arxiv.org/pdf/2408.04015v1.pdf","comment":"4 pages"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.03940v1","updated":"2024-08-07T17:59:40Z","published":"2024-08-07T17:59:40Z","title":"How Well Can Vision Language Models See Image Details?","summary":"  Large Language Model-based Vision-Language Models (LLM-based VLMs) have\ndemonstrated impressive results in various vision-language understanding tasks.\nHowever, how well these VLMs can see image detail beyond the semantic level\nremains unclear. In our study, we introduce a pixel value prediction task (PVP)\nto explore \"How Well Can Vision Language Models See Image Details?\" and to\nassist VLMs in perceiving more details. Typically, these models comprise a\nfrozen CLIP visual encoder, a large language model, and a connecting module.\nAfter fine-tuning VLMs on the PVP task, we find: 1) existing VLMs struggle to\npredict precise pixel values by only fine-tuning the connection module and LLM;\nand 2) prediction precision is significantly improved when the vision encoder\nis also adapted. Additionally, our research reveals that incorporating pixel\nvalue prediction as one of the VLM pre-training tasks and vision encoder\nadaptation markedly boosts VLM performance on downstream image-language\nunderstanding tasks requiring detailed image perception, such as referring\nimage segmentation (with an average +10.19 cIoU improvement) and video game\ndecision making (with average score improvements of +80.34 and +70.54 on two\ngames, respectively).\n","authors":["Chenhui Gou","Abdulwahab Felemban","Faizan Farooq Khan","Deyao Zhu","Jianfei Cai","Hamid Rezatofighi","Mohamed Elhoseiny"],"pdf_url":"https://arxiv.org/pdf/2408.03940v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19674v3","updated":"2024-08-07T17:45:05Z","published":"2024-07-29T03:30:09Z","title":"Advancing Prompt Learning through an External Layer","summary":"  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n","authors":["Fangming Cui","Xun Yang","Chao Wu","Liang Xiao","Xinmei Tian"],"pdf_url":"https://arxiv.org/pdf/2407.19674v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03923v1","updated":"2024-08-07T17:30:59Z","published":"2024-08-07T17:30:59Z","title":"Fast Sprite Decomposition from Animated Graphics","summary":"  This paper presents an approach to decomposing animated graphics into\nsprites, a set of basic elements or layers. Our approach builds on the\noptimization of sprite parameters to fit the raster video. For efficiency, we\nassume static textures for sprites to reduce the search space while preventing\nartifacts using a texture prior model. To further speed up the optimization, we\nintroduce the initialization of the sprite parameters utilizing a pre-trained\nvideo object segmentation model and user input of single frame annotations. For\nour study, we construct the Crello Animation dataset from an online design\nservice and define quantitative metrics to measure the quality of the extracted\nsprites. Experiments show that our method significantly outperforms baselines\nfor similar decomposition tasks in terms of the quality/efficiency tradeoff.\n","authors":["Tomoyuki Suzuki","Kotaro Kikuchi","Kota Yamaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03923v1.pdf","comment":"To be published ECCV 2024, project page:\n  https://cyberagentailab.github.io/sprite-decompose/"},{"id":"http://arxiv.org/abs/2405.19450v2","updated":"2024-08-07T17:30:16Z","published":"2024-05-29T18:58:59Z","title":"FourierMamba: Fourier Learning Integration with State Space Models for\n  Image Deraining","summary":"  Image deraining aims to remove rain streaks from rainy images and restore\nclear backgrounds. Currently, some research that employs the Fourier transform\nhas proved to be effective for image deraining, due to it acting as an\neffective frequency prior for capturing rain streaks. However, despite there\nexists dependency of low frequency and high frequency in images, these\nFourier-based methods rarely exploit the correlation of different frequencies\nfor conjuncting their learning procedures, limiting the full utilization of\nfrequency information for image deraining. Alternatively, the recently emerged\nMamba technique depicts its effectiveness and efficiency for modeling\ncorrelation in various domains (e.g., spatial, temporal), and we argue that\nintroducing Mamba into its unexplored Fourier spaces to correlate different\nfrequencies would help improve image deraining. This motivates us to propose a\nnew framework termed FourierMamba, which performs image deraining with Mamba in\nthe Fourier space. Owning to the unique arrangement of frequency orders in\nFourier space, the core of FourierMamba lies in the scanning encoding of\ndifferent frequencies, where the low-high frequency order formats exhibit\ndifferently in the spatial dimension (unarranged in axis) and channel dimension\n(arranged in axis). Therefore, we design FourierMamba that correlates Fourier\nspace information in the spatial and channel dimensions with distinct designs.\nSpecifically, in the spatial dimension Fourier space, we introduce the zigzag\ncoding to scan the frequencies to rearrange the orders from low to high\nfrequencies, thereby orderly correlating the connections between frequencies;\nin the channel dimension Fourier space with arranged orders of frequencies in\naxis, we can directly use Mamba to perform frequency correlation and improve\nthe channel information representation.\n","authors":["Dong Li","Yidi Liu","Xueyang Fu","Senyan Xu","Zheng-Jun Zha"],"pdf_url":"https://arxiv.org/pdf/2405.19450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03922v1","updated":"2024-08-07T17:29:19Z","published":"2024-08-07T17:29:19Z","title":"FMiFood: Multi-modal Contrastive Learning for Food Image Classification","summary":"  Food image classification is the fundamental step in image-based dietary\nassessment, which aims to estimate participants' nutrient intake from eating\noccasion images. A common challenge of food images is the intra-class diversity\nand inter-class similarity, which can significantly hinder classification\nperformance. To address this issue, we introduce a novel multi-modal\ncontrastive learning framework called FMiFood, which learns more discriminative\nfeatures by integrating additional contextual information, such as food\ncategory text descriptions, to enhance classification accuracy. Specifically,\nwe propose a flexible matching technique that improves the similarity matching\nbetween text and image embeddings to focus on multiple key information.\nFurthermore, we incorporate the classification objectives into the framework\nand explore the use of GPT-4 to enrich the text descriptions and provide more\ndetailed context. Our method demonstrates improved performance on both the\nUPMC-101 and VFN datasets compared to existing methods.\n","authors":["Xinyue Pan","Jiangpeng He","Fengqing Zhu"],"pdf_url":"https://arxiv.org/pdf/2408.03922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03913v1","updated":"2024-08-07T17:19:15Z","published":"2024-08-07T17:19:15Z","title":"AdapMTL: Adaptive Pruning Framework for Multitask Learning Model","summary":"  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n","authors":["Mingcan Xiang","Steven Jiaxun Tang","Qizheng Yang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03913v1.pdf","comment":"13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03904v1","updated":"2024-08-07T17:08:46Z","published":"2024-08-07T17:08:46Z","title":"Lightweight Video Denoising Using a Classic Bayesian Backbone","summary":"  In recent years, state-of-the-art image and video denoising networks have\nbecome increasingly large, requiring millions of trainable parameters to\nachieve best-in-class performance. Improved denoising quality has come at the\ncost of denoising speed, where modern transformer networks are far slower to\nrun than smaller denoising networks such as FastDVDnet and classic Bayesian\ndenoisers such as the Wiener filter.\n  In this paper, we implement a hybrid Wiener filter which leverages small\nancillary networks to increase the original denoiser performance, while\nretaining fast denoising speeds. These networks are used to refine the Wiener\ncoring estimate, optimise windowing functions and estimate the unknown noise\nprofile. Using these methods, we outperform several popular denoisers and\nremain within 0.2 dB, on average, of the popular VRT transformer. Our method\nwas found to be over x10 faster than the transformer method, with a far lower\nparameter cost.\n","authors":["Clment Bled","Franois Piti"],"pdf_url":"https://arxiv.org/pdf/2408.03904v1.pdf","comment":"Paper accepted to ICME 2024"},{"id":"http://arxiv.org/abs/2407.14153v2","updated":"2024-08-07T17:04:53Z","published":"2024-07-19T09:32:30Z","title":"ESP-MedSAM: Efficient Self-Prompting SAM for Universal Image\n  Segmentation","summary":"  The Segment Anything Model (SAM) has demonstrated outstanding adaptation to\nmedical image segmentation but still faces three major challenges. Firstly, the\nhuge computational costs of SAM limit its real-world applicability. Secondly,\nSAM depends on manual annotations (e.g., points, boxes) as prompts, which are\nlaborious and impractical in clinical scenarios. Thirdly, SAM handles all\nsegmentation targets equally, which is suboptimal for diverse medical\nmodalities with inherent heterogeneity. To address these issues, we propose an\nEfficient Self-Prompting SAM for universal medical image segmentation, named\nESP-MedSAM. We devise a Multi-Modal Decoupled Knowledge Distillation (MMDKD)\nstrategy to distil common image knowledge and domain-specific medical knowledge\nfrom the foundation model to train a lightweight image encoder and a modality\ncontroller. Further, they combine with the additionally introduced Self-Patch\nPrompt Generator (SPPG) and Query-Decoupled Modality Decoder (QDMD) to\nconstruct ESP-MedSAM. Specifically, SPPG aims to generate a set of patch\nprompts automatically and QDMD leverages a one-to-one strategy to provide an\nindependent decoding channel for every modality. Extensive experiments indicate\nthat ESP-MedSAM outperforms state-of-the-arts in diverse medical imaging\nsegmentation takes, displaying superior zero-shot learning and modality\ntransfer ability. Especially, our framework uses only 31.4% parameters compared\nto SAM-Base.\n","authors":["Qing Xu","Jiaxuan Li","Xiangjian He","Ziyu Liu","Zhen Chen","Wenting Duan","Chenxin Li","Maggie M. He","Fiseha B. Tesema","Wooi P. Cheah","Yi Wang","Rong Qu","Jonathan M. Garibaldi"],"pdf_url":"https://arxiv.org/pdf/2407.14153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.15098v3","updated":"2024-08-07T17:03:30Z","published":"2024-03-22T10:36:50Z","title":"UniTraj: A Unified Framework for Scalable Vehicle Trajectory Prediction","summary":"  Vehicle trajectory prediction has increasingly relied on data-driven\nsolutions, but their ability to scale to different data domains and the impact\nof larger dataset sizes on their generalization remain under-explored. While\nthese questions can be studied by employing multiple datasets, it is\nchallenging due to several discrepancies, e.g., in data formats, map\nresolution, and semantic annotation types. To address these challenges, we\nintroduce UniTraj, a comprehensive framework that unifies various datasets,\nmodels, and evaluation criteria, presenting new opportunities for the vehicle\ntrajectory prediction field. In particular, using UniTraj, we conduct extensive\nexperiments and find that model performance significantly drops when\ntransferred to other datasets. However, enlarging data size and diversity can\nsubstantially improve performance, leading to a new state-of-the-art result for\nthe nuScenes dataset. We provide insights into dataset characteristics to\nexplain these findings. The code can be found here:\nhttps://github.com/vita-epfl/UniTraj\n","authors":["Lan Feng","Mohammadhossein Bahari","Kaouther Messaoud Ben Amor","loi Zablocki","Matthieu Cord","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2403.15098v3.pdf","comment":"Accepted in ECCV 2024"},{"id":"http://arxiv.org/abs/2403.11956v5","updated":"2024-08-07T17:02:00Z","published":"2024-03-18T16:52:49Z","title":"Subjective-Aligned Dataset and Metric for Text-to-Video Quality\n  Assessment","summary":"  With the rapid development of generative models, Artificial\nIntelligence-Generated Contents (AIGC) have exponentially increased in daily\nlives. Among them, Text-to-Video (T2V) generation has received widespread\nattention. Though many T2V models have been released for generating high\nperceptual quality videos, there is still lack of a method to evaluate the\nquality of these videos quantitatively. To solve this issue, we establish the\nlargest-scale Text-to-Video Quality Assessment DataBase (T2VQA-DB) to date. The\ndataset is composed of 10,000 videos generated by 9 different T2V models. We\nalso conduct a subjective study to obtain each video's corresponding mean\nopinion score. Based on T2VQA-DB, we propose a novel transformer-based model\nfor subjective-aligned Text-to-Video Quality Assessment (T2VQA). The model\nextracts features from text-video alignment and video fidelity perspectives,\nthen it leverages the ability of a large language model to give the prediction\nscore. Experimental results show that T2VQA outperforms existing T2V metrics\nand SOTA video quality assessment models. Quantitative analysis indicates that\nT2VQA is capable of giving subjective-align predictions, validating its\neffectiveness. The dataset and code will be released at\nhttps://github.com/QMME/T2VQA.\n","authors":["Tengchuan Kou","Xiaohong Liu","Zicheng Zhang","Chunyi Li","Haoning Wu","Xiongkuo Min","Guangtao Zhai","Ning Liu"],"pdf_url":"https://arxiv.org/pdf/2403.11956v5.pdf","comment":"Accepted by ACMMM 24"},{"id":"http://arxiv.org/abs/2408.03888v1","updated":"2024-08-07T16:39:16Z","published":"2024-08-07T16:39:16Z","title":"Dual-Modeling Decouple Distillation for Unsupervised Anomaly Detection","summary":"  Knowledge distillation based on student-teacher network is one of the\nmainstream solution paradigms for the challenging unsupervised Anomaly\nDetection task, utilizing the difference in representation capabilities of the\nteacher and student networks to implement anomaly localization. However,\nover-generalization of the student network to the teacher network may lead to\nnegligible differences in representation capabilities of anomaly, thus\naffecting the detection effectiveness. Existing methods address the possible\nover-generalization by using differentiated students and teachers from the\nstructural perspective or explicitly expanding distilled information from the\ncontent perspective, which inevitably result in an increased likelihood of\nunderfitting of the student network and poor anomaly detection capabilities in\nanomaly center or edge. In this paper, we propose Dual-Modeling Decouple\nDistillation (DMDD) for the unsupervised anomaly detection. In DMDD, a Decouple\nStudent-Teacher Network is proposed to decouple the initial student features\ninto normality and abnormality features. We further introduce Dual-Modeling\nDistillation based on normal-anomaly image pairs, fitting normality features of\nanomalous image and the teacher features of the corresponding normal image,\nwidening the distance between abnormality features and the teacher features in\nanomalous regions. Synthesizing these two distillation ideas, we achieve\nanomaly detection which focuses on both edge and center of anomaly. Finally, a\nMulti-perception Segmentation Network is proposed to achieve focused anomaly\nmap fusion based on multiple attention. Experimental results on MVTec AD show\nthat DMDD surpasses SOTA localization performance of previous knowledge\ndistillation-based methods, reaching 98.85% on pixel-level AUC and 96.13% on\nPRO.\n","authors":["Xinyue Liu","Jianyuan Wang","Biao Leng","Shuo Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03888v1.pdf","comment":"10 pages, 8 figures, Accepted to ACM MM '24"},{"id":"http://arxiv.org/abs/2408.03885v1","updated":"2024-08-07T16:34:32Z","published":"2024-08-07T16:34:32Z","title":"Global-Local Progressive Integration Network for Blind Image Quality\n  Assessment","summary":"  Vision transformers (ViTs) excel in computer vision for modeling long-term\ndependencies, yet face two key challenges for image quality assessment (IQA):\ndiscarding fine details during patch embedding, and requiring extensive\ntraining data due to lack of inductive biases. In this study, we propose a\nGlobal-Local progressive INTegration network for IQA, called GlintIQA, to\naddress these issues through three key components: 1) Hybrid feature extraction\ncombines ViT-based global feature extractor (VGFE) and convolutional neural\nnetworks (CNNs)-based local feature extractor (CLFE) to capture global\ncoarse-grained features and local fine-grained features, respectively. The\nincorporation of CNNs mitigates the patch-level information loss and inductive\nbias constraints inherent to ViT architectures. 2) Progressive feature\nintegration leverages diverse kernel sizes in embedding to spatially align\ncoarse- and fine-grained features, and progressively aggregate these features\nby interactively stacking channel-wise attention and spatial enhancement\nmodules to build effective quality-aware representations. 3) Content\nsimilarity-based labeling approach is proposed that automatically assigns\nquality labels to images with diverse content based on subjective quality\nscores. This addresses the scarcity of labeled training data in synthetic\ndatasets and bolsters model generalization. The experimental results\ndemonstrate the efficacy of our approach, yielding 5.04% average SROCC gains on\ncross-authentic dataset evaluations. Moreover, our model and its counterpart\npre-trained on the proposed dataset respectively exhibited 5.40% and 13.23%\nimprovements on across-synthetic datasets evaluation. The codes and proposed\ndataset will be released at https://github.com/XiaoqiWang/GlintIQA.\n","authors":["Xiaoqi Wang","Yun Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03885v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.00920v3","updated":"2024-08-07T16:30:56Z","published":"2023-10-02T06:17:24Z","title":"Every Dataset Counts: Scaling up Monocular 3D Object Detection with\n  Joint Datasets Training","summary":"  Monocular 3D object detection plays a crucial role in autonomous driving.\nHowever, existing monocular 3D detection algorithms depend on 3D labels derived\nfrom LiDAR measurements, which are costly to acquire for new datasets and\nchallenging to deploy in novel environments. Specifically, this study\ninvestigates the pipeline for training a monocular 3D object detection model on\na diverse collection of 3D and 2D datasets. The proposed framework comprises\nthree components: (1) a robust monocular 3D model capable of functioning across\nvarious camera settings, (2) a selective-training strategy to accommodate\ndatasets with differing class annotations, and (3) a pseudo 3D training\napproach using 2D labels to enhance detection performance in scenes containing\nonly 2D labels. With this framework, we could train models on a joint set of\nvarious open 3D/2D datasets to obtain models with significantly stronger\ngeneralization capability and enhanced performance on new dataset with only 2D\nlabels. We conduct extensive experiments on\nKITTI/nuScenes/ONCE/Cityscapes/BDD100K datasets to demonstrate the scaling\nability of the proposed method.\n","authors":["Fulong Ma","Xiaoyang Yan","Guoyang Zhao","Xiaojie Xu","Yuxuan Liu","Ming Liu"],"pdf_url":"https://arxiv.org/pdf/2310.00920v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03867v1","updated":"2024-08-07T16:16:31Z","published":"2024-08-07T16:16:31Z","title":"Surgformer: Surgical Transformer with Hierarchical Temporal Attention\n  for Surgical Phase Recognition","summary":"  Existing state-of-the-art methods for surgical phase recognition either rely\non the extraction of spatial-temporal features at a short-range temporal\nresolution or adopt the sequential extraction of the spatial and temporal\nfeatures across the entire temporal resolution. However, these methods have\nlimitations in modeling spatial-temporal dependency and addressing\nspatial-temporal redundancy: 1) These methods fail to effectively model\nspatial-temporal dependency, due to the lack of long-range information or joint\nspatial-temporal modeling. 2) These methods utilize dense spatial features\nacross the entire temporal resolution, resulting in significant\nspatial-temporal redundancy. In this paper, we propose the Surgical Transformer\n(Surgformer) to address the issues of spatial-temporal modeling and redundancy\nin an end-to-end manner, which employs divided spatial-temporal attention and\ntakes a limited set of sparse frames as input. Moreover, we propose a novel\nHierarchical Temporal Attention (HTA) to capture both global and local\ninformation within varied temporal resolutions from a target frame-centric\nperspective. Distinct from conventional temporal attention that primarily\nemphasizes dense long-range similarity, HTA not only captures long-term\ninformation but also considers local latent consistency among informative\nframes. HTA then employs pyramid feature aggregation to effectively utilize\ntemporal information across diverse temporal resolutions, thereby enhancing the\noverall temporal representation. Extensive experiments on two challenging\nbenchmark datasets verify that our proposed Surgformer performs favorably\nagainst the state-of-the-art methods. The code is released at\nhttps://github.com/isyangshu/Surgformer.\n","authors":["Shu Yang","Luyang Luo","Qiong Wang","Hao Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03867v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15636v3","updated":"2024-08-07T16:07:05Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03842v1","updated":"2024-08-07T15:35:25Z","published":"2024-08-07T15:35:25Z","title":"Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression","summary":"  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n","authors":["Hamidreza Soltani","Erfan Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2408.03842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03838v1","updated":"2024-08-07T15:24:25Z","published":"2024-08-07T15:24:25Z","title":"Using a Distance Sensor to Detect Deviations in a Planar Surface","summary":"  We investigate methods for determining if a planar surface contains geometric\ndeviations (e.g., protrusions, objects, divots, or cliffs) using only an\ninstantaneous measurement from a miniature optical time-of-flight sensor. The\nkey to our method is to utilize the entirety of information encoded in raw\ntime-of-flight data captured by off-the-shelf distance sensors. We provide an\nanalysis of the problem in which we identify the key ambiguity between geometry\nand surface photometrics. To overcome this challenging ambiguity, we fit a\nGaussian mixture model to a small dataset of planar surface measurements. This\nmodel implicitly captures the expected geometry and distribution of\nphotometrics of the planar surface and is used to identify measurements that\nare likely to contain deviations. We characterize our method on a variety of\nsurfaces and planar deviations across a range of scenarios. We find that our\nmethod utilizing raw time-of-flight data outperforms baselines which use only\nderived distance estimates. We build an example application in which our method\nenables mobile robot obstacle and cliff avoidance over a wide field-of-view.\n","authors":["Carter Sifferman","William Sun","Mohit Gupta","Michael Gleicher"],"pdf_url":"https://arxiv.org/pdf/2408.03838v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03834v1","updated":"2024-08-07T15:17:51Z","published":"2024-08-07T15:17:51Z","title":"Target Prompting for Information Extraction with Vision Language Model","summary":"  The recent trend in the Large Vision and Language model has brought a new\nchange in how information extraction systems are built. VLMs have set a new\nbenchmark with their State-of-the-art techniques in understanding documents and\nbuilding question-answering systems across various industries. They are\nsignificantly better at generating text from document images and providing\naccurate answers to questions. However, there are still some challenges in\neffectively utilizing these models to build a precise conversational system.\nGeneral prompting techniques used with large language models are often not\nsuitable for these specially designed vision language models. The output\ngenerated by such generic input prompts is ordinary and may contain information\ngaps when compared with the actual content of the document. To obtain more\naccurate and specific answers, a well-targeted prompt is required by the vision\nlanguage model, along with the document image. In this paper, a technique is\ndiscussed called Target prompting, which focuses on explicitly targeting parts\nof document images and generating related answers from those specific regions\nonly. The paper also covers the evaluation of response for each prompting\ntechnique using different user queries and input prompts.\n","authors":["Dipankar Medhi"],"pdf_url":"https://arxiv.org/pdf/2408.03834v1.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.19588v2","updated":"2024-08-07T15:11:01Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v2.pdf","comment":"ECCV 2024. Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2401.00763v2","updated":"2024-08-07T15:10:15Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v2.pdf","comment":"ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.03825v1","updated":"2024-08-07T15:01:08Z","published":"2024-08-07T15:01:08Z","title":"Towards Real-Time Gaussian Splatting: Accelerating 3DGS through\n  Photometric SLAM","summary":"  Initial applications of 3D Gaussian Splatting (3DGS) in Visual Simultaneous\nLocalization and Mapping (VSLAM) demonstrate the generation of high-quality\nvolumetric reconstructions from monocular video streams. However, despite these\npromising advancements, current 3DGS integrations have reduced tracking\nperformance and lower operating speeds compared to traditional VSLAM. To\naddress these issues, we propose integrating 3DGS with Direct Sparse Odometry,\na monocular photometric SLAM system. We have done preliminary experiments\nshowing that using Direct Sparse Odometry point cloud outputs, as opposed to\nstandard structure-from-motion methods, significantly shortens the training\ntime needed to achieve high-quality renders. Reducing 3DGS training time\nenables the development of 3DGS-integrated SLAM systems that operate in\nreal-time on mobile hardware. These promising initial findings suggest further\nexploration is warranted in combining traditional VSLAM systems with 3DGS.\n","authors":["Yan Song Hu","Dayou Mao","Yuhao Chen","John Zelek"],"pdf_url":"https://arxiv.org/pdf/2408.03825v1.pdf","comment":"This extended abstract has been submitted to be presented at an IEEE\n  conference. It will be made available online by IEEE but will not be\n  published in IEEE Xplore. Copyright may be transferred without notice, after\n  which this version may no longer be accessible"},{"id":"http://arxiv.org/abs/2408.03822v1","updated":"2024-08-07T14:56:34Z","published":"2024-08-07T14:56:34Z","title":"Compact 3D Gaussian Splatting for Static and Dynamic Radiance Fields","summary":"  3D Gaussian splatting (3DGS) has recently emerged as an alternative\nrepresentation that leverages a 3D Gaussian-based representation and introduces\nan approximated volumetric rendering, achieving very fast rendering speed and\npromising image quality. Furthermore, subsequent studies have successfully\nextended 3DGS to dynamic 3D scenes, demonstrating its wide range of\napplications. However, a significant drawback arises as 3DGS and its following\nmethods entail a substantial number of Gaussians to maintain the high fidelity\nof the rendered images, which requires a large amount of memory and storage. To\naddress this critical issue, we place a specific emphasis on two key\nobjectives: reducing the number of Gaussian points without sacrificing\nperformance and compressing the Gaussian attributes, such as view-dependent\ncolor and covariance. To this end, we propose a learnable mask strategy that\nsignificantly reduces the number of Gaussians while preserving high\nperformance. In addition, we propose a compact but effective representation of\nview-dependent color by employing a grid-based neural field rather than relying\non spherical harmonics. Finally, we learn codebooks to compactly represent the\ngeometric and temporal attributes by residual vector quantization. With model\ncompression techniques such as quantization and entropy coding, we consistently\nshow over 25x reduced storage and enhanced rendering speed compared to 3DGS for\nstatic scenes, while maintaining the quality of the scene representation. For\ndynamic scenes, our approach achieves more than 12x storage efficiency and\nretains a high-quality reconstruction compared to the existing state-of-the-art\nmethods. Our work provides a comprehensive framework for 3D scene\nrepresentation, achieving high performance, fast training, compactness, and\nreal-time rendering. Our project page is available at\nhttps://maincold2.github.io/c3dgs/.\n","authors":["Joo Chan Lee","Daniel Rho","Xiangyu Sun","Jong Hwan Ko","Eunbyung Park"],"pdf_url":"https://arxiv.org/pdf/2408.03822v1.pdf","comment":"Project page: https://maincold2.github.io/c3dgs/"},{"id":"http://arxiv.org/abs/2405.06342v2","updated":"2024-08-07T14:33:42Z","published":"2024-05-10T09:18:17Z","title":"Compression-Realized Deep Structural Network for Video Quality\n  Enhancement","summary":"  This paper focuses on the task of quality enhancement for compressed videos.\nAlthough deep network-based video restorers achieve impressive progress, most\nof the existing methods lack a structured design to optimally leverage the\npriors within compression codecs. Since the quality degradation of the video is\nprimarily induced by the compression algorithm, a new paradigm is urgently\nneeded for a more ``conscious'' process of quality enhancement. As a result, we\npropose the Compression-Realized Deep Structural Network (CRDS), introducing\nthree inductive biases aligned with the three primary processes in the classic\ncompression codec, merging the strengths of classical encoder architecture with\ndeep network capabilities. Inspired by the residual extraction and domain\ntransformation process in the codec, a pre-trained Latent Degradation Residual\nAuto-Encoder is proposed to transform video frames into a latent feature space,\nand the mutual neighborhood attention mechanism is integrated for precise\nmotion estimation and residual extraction. Furthermore, drawing inspiration\nfrom the quantization noise distribution of the codec, CRDS proposes a novel\nProgressive Denoising framework with intermediate supervision that decomposes\nthe quality enhancement into a series of simpler denoising sub-tasks.\nExperimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our\napproach surpasses state-of-the-art models. Codes are available at\nhttps://github.com/shc15522/CRDS.\n","authors":["Hanchi Sun","Xiaohong Liu","Xinyang Jiang","Yifei Shen","Dongsheng Li","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2405.06342v2.pdf","comment":"Accepted by ACM MM'24"},{"id":"http://arxiv.org/abs/2407.18520v2","updated":"2024-08-07T14:33:14Z","published":"2024-07-26T05:29:24Z","title":"Text-Region Matching for Multi-Label Image Recognition with Missing\n  Labels","summary":"  Recently, large-scale visual language pre-trained (VLP) models have\ndemonstrated impressive performance across various downstream tasks. Motivated\nby these advancements, pioneering efforts have emerged in multi-label image\nrecognition with missing labels, leveraging VLP prompt-tuning technology.\nHowever, they usually cannot match text and vision features well, due to\ncomplicated semantics gaps and missing labels in a multi-label image. To tackle\nthis challenge, we propose \\textbf{T}ext-\\textbf{R}egion \\textbf{M}atching for\noptimizing \\textbf{M}ulti-\\textbf{L}abel prompt tuning, namely TRM-ML, a novel\nmethod for enhancing meaningful cross-modal matching. Compared to existing\nmethods, we advocate exploring the information of category-aware regions rather\nthan the entire image or pixels, which contributes to bridging the semantic gap\nbetween textual and visual representations in a one-to-one matching manner.\nConcurrently, we further introduce multimodal contrastive learning to narrow\nthe semantic gap between textual and visual modalities and establish\nintra-class and inter-class relationships. Additionally, to deal with missing\nlabels, we propose a multimodal category prototype that leverages intra- and\ninter-category semantic relationships to estimate unknown labels, facilitating\npseudo-label generation. Extensive experiments on the MS-COCO, PASCAL VOC,\nVisual Genome, NUS-WIDE, and CUB-200-211 benchmark datasets demonstrate that\nour proposed framework outperforms the state-of-the-art methods by a\nsignificant margin. Our code is available\nhere\\href{https://github.com/yu-gi-oh-leilei/TRM-ML}{\\raisebox{-1pt}{\\faGithub}}.\n","authors":["Leilei Ma","Hongxing Xie","Lei Wang","Yanping Fu","Dengdi Sun","Haifeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.18520v2.pdf","comment":"Accepted to ACM International Conference on Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03790v1","updated":"2024-08-07T14:14:53Z","published":"2024-08-07T14:14:53Z","title":"Vision-Language Guidance for LiDAR-based Unsupervised 3D Object\n  Detection","summary":"  Accurate 3D object detection in LiDAR point clouds is crucial for autonomous\ndriving systems. To achieve state-of-the-art performance, the supervised\ntraining of detectors requires large amounts of human-annotated data, which is\nexpensive to obtain and restricted to predefined object categories. To mitigate\nmanual labeling efforts, recent unsupervised object detection approaches\ngenerate class-agnostic pseudo-labels for moving objects, subsequently serving\nas supervision signal to bootstrap a detector. Despite promising results, these\napproaches do not provide class labels or generalize well to static objects.\nFurthermore, they are mostly restricted to data containing multiple drives from\nthe same scene or images from a precisely calibrated and synchronized camera\nsetup. To overcome these limitations, we propose a vision-language-guided\nunsupervised 3D detection approach that operates exclusively on LiDAR point\nclouds. We transfer CLIP knowledge to classify point clusters of static and\nmoving objects, which we discover by exploiting the inherent spatio-temporal\ninformation of LiDAR point clouds for clustering, tracking, as well as box and\nlabel refinement. Our approach outperforms state-of-the-art unsupervised 3D\nobject detectors on the Waymo Open Dataset ($+23~\\text{AP}_{3D}$) and Argoverse\n2 ($+7.9~\\text{AP}_{3D}$) and provides class labels not solely based on object\nsize assumptions, marking a significant advancement in the field.\n","authors":["Christian Fruhwirth-Reisinger","Wei Lin","Duan Mali","Horst Bischof","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2408.03790v1.pdf","comment":"Accepted to BMVC 2024"},{"id":"http://arxiv.org/abs/2408.03789v1","updated":"2024-08-07T14:14:05Z","published":"2024-08-07T14:14:05Z","title":"Counterfactuals and Uncertainty-Based Explainable Paradigm for the\n  Automated Detection and Segmentation of Renal Cysts in Computed Tomography\n  Images: A Multi-Center Study","summary":"  Routine computed tomography (CT) scans often detect a wide range of renal\ncysts, some of which may be malignant. Early and precise localization of these\ncysts can significantly aid quantitative image analysis. Current segmentation\nmethods, however, do not offer sufficient interpretability at the feature and\npixel levels, emphasizing the necessity for an explainable framework that can\ndetect and rectify model inaccuracies. We developed an interpretable\nsegmentation framework and validated it on a multi-centric dataset. A\nVariational Autoencoder Generative Adversarial Network (VAE-GAN) was employed\nto learn the latent representation of 3D input patches and reconstruct input\nimages. Modifications in the latent representation using the gradient of the\nsegmentation model generated counterfactual explanations for varying dice\nsimilarity coefficients (DSC). Radiomics features extracted from these\ncounterfactual images, using a ground truth cyst mask, were analyzed to\ndetermine their correlation with segmentation performance. The DSCs for the\noriginal and VAE-GAN reconstructed images for counterfactual image generation\nshowed no significant differences. Counterfactual explanations highlighted how\nvariations in cyst image features influence segmentation outcomes and showed\nmodel discrepancies. Radiomics features correlating positively and negatively\nwith dice scores were identified. The uncertainty of the predicted segmentation\nmasks was estimated using posterior sampling of the weight space. The\ncombination of counterfactual explanations and uncertainty maps provided a\ndeeper understanding of the image features within the segmented renal cysts\nthat lead to high uncertainty. The proposed segmentation framework not only\nachieved high segmentation accuracy but also increased interpretability\nregarding how image features impact segmentation performance.\n","authors":["Zohaib Salahuddin","Abdalla Ibrahim","Sheng Kuang","Yousif Widaatalla","Razvan L. Miclea","Oliver Morin","Spencer Behr","Marnix P. M. Kop","Tom Marcelissen","Patricia Zondervan","Auke Jager","Philippe Lambin","Henry C Woodruff"],"pdf_url":"https://arxiv.org/pdf/2408.03789v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06646v2","updated":"2024-08-07T14:06:30Z","published":"2024-03-20T05:52:11Z","title":"Diffusion-based Human Motion Style Transfer with Semantic Guidance","summary":"  3D Human motion style transfer is a fundamental problem in computer graphic\nand animation processing. Existing AdaIN- based methods necessitate datasets\nwith balanced style distribution and content/style labels to train the\nclustered latent space. However, we may encounter a single unseen style example\nin practical scenarios, but not in sufficient quantity to constitute a style\ncluster for AdaIN-based methods. Therefore, in this paper, we propose a novel\ntwo-stage framework for few-shot style transfer learning based on the diffusion\nmodel. Specifically, in the first stage, we pre-train a diffusion-based\ntext-to-motion model as a generative prior so that it can cope with various\ncontent motion inputs. In the second stage, based on the single style example,\nwe fine-tune the pre-trained diffusion model in a few-shot manner to make it\ncapable of style transfer. The key idea is regarding the reverse process of\ndiffusion as a motion-style translation process since the motion styles can be\nviewed as special motion variations. During the fine-tuning for style transfer,\na simple yet effective semantic-guided style transfer loss coordinated with\nstyle example reconstruction loss is introduced to supervise the style transfer\nin CLIP semantic space. The qualitative and quantitative evaluations\ndemonstrate that our method can achieve state-of-the-art performance and has\npractical applications.\n","authors":["Lei Hu","Zihao Zhang","Yongjing Ye","Yiwen Xu","Shihong Xia"],"pdf_url":"https://arxiv.org/pdf/2405.06646v2.pdf","comment":"12 pages, 8 figures"},{"id":"http://arxiv.org/abs/2407.05771v2","updated":"2024-08-07T14:01:34Z","published":"2024-07-08T09:27:34Z","title":"Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction","summary":"  Inverse rendering methods have achieved remarkable performance in\nreconstructing high-fidelity 3D objects with disentangled geometries,\nmaterials, and environmental light. However, they still face huge challenges in\nreflective surface reconstruction. Although recent methods model the light\ntrace to learn specularity, the ignorance of indirect illumination makes it\nhard to handle inter-reflections among multiple smooth objects. In this work,\nwe propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which\ncomprehensively computes the environmental illumination and meanwhile considers\nthe reflective light from object surfaces. To address the computation challenge\nas the times of Monte Carlo sampling grow, we propose a specularity-adaptive\nsampling strategy, significantly reducing the computational complexity. Besides\nthe computational resource, higher geometry accuracy is also required because\ngeometric errors accumulate multiple times. Therefore, we further introduce a\nreflection-aware surface model to initialize the geometry and refine it during\ninverse rendering. We construct a challenging dataset containing scenes with\nmultiple objects and inter-reflections. Experiments show that our method\noutperforms other inverse rendering methods on various object groups. We also\nshow downstream applications, e.g., relighting and material editing, to\nillustrate the disentanglement ability of our method.\n","authors":["Tengjie Zhu","Zhuo Chen","Jingnan Gao","Yichao Yan","Xiaokang Yang"],"pdf_url":"https://arxiv.org/pdf/2407.05771v2.pdf","comment":"10 pages,6 figures,NeurIPS 2024 Submitted"},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2408.03771v1","updated":"2024-08-07T13:47:32Z","published":"2024-08-07T13:47:32Z","title":"Methodological Explainability Evaluation of an Interpretable Deep\n  Learning Model for Post-Hepatectomy Liver Failure Prediction Incorporating\n  Counterfactual Explanations and Layerwise Relevance Propagation: A\n  Prospective In Silico Trial","summary":"  Artificial intelligence (AI)-based decision support systems have demonstrated\nvalue in predicting post-hepatectomy liver failure (PHLF) in hepatocellular\ncarcinoma (HCC). However, they often lack transparency, and the impact of model\nexplanations on clinicians' decisions has not been thoroughly evaluated.\nBuilding on prior research, we developed a variational autoencoder-multilayer\nperceptron (VAE-MLP) model for preoperative PHLF prediction. This model\nintegrated counterfactuals and layerwise relevance propagation (LRP) to provide\ninsights into its decision-making mechanism. Additionally, we proposed a\nmethodological framework for evaluating the explainability of AI systems. This\nframework includes qualitative and quantitative assessments of explanations\nagainst recognized biomarkers, usability evaluations, and an in silico clinical\ntrial. Our evaluations demonstrated that the model's explanation correlated\nwith established biomarkers and exhibited high usability at both the case and\nsystem levels. Furthermore, results from the three-track in silico clinical\ntrial showed that clinicians' prediction accuracy and confidence increased when\nAI explanations were provided.\n","authors":["Xian Zhong","Zohaib Salahuddin","Yi Chen","Henry C Woodruff","Haiyi Long","Jianyun Peng","Nuwan Udawatte","Roberto Casale","Ayoub Mokhtari","Xiaoer Zhang","Jiayao Huang","Qingyu Wu","Li Tan","Lili Chen","Dongming Li","Xiaoyan Xie","Manxia Lin","Philippe Lambin"],"pdf_url":"https://arxiv.org/pdf/2408.03771v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.10428v5","updated":"2024-08-07T13:44:06Z","published":"2023-03-18T14:46:44Z","title":"RCA: Region Conditioned Adaptation for Visual Abductive Reasoning","summary":"  Visual abductive reasoning aims to make likely explanations for visual\nobservations. We propose a simple yet effective Region Conditioned Adaptation,\na hybrid parameter-efficient fine-tuning method that equips the frozen CLIP\nwith the ability to infer explanations from local visual cues. We encode\n``local hints'' and ``global contexts'' into visual prompts of the CLIP model\nseparately at fine and coarse-grained levels. Adapters are used for fine-tuning\nCLIP models for downstream tasks and we design a new attention adapter, that\ndirectly steers the focus of the attention map with trainable query and key\nprojections of a frozen CLIP model. Finally, we train our new model with a\nmodified contrastive loss to regress the visual feature simultaneously toward\nfeatures of literal description and plausible explanations. The loss enables\nCLIP to maintain both perception and reasoning abilities. Experiments on the\nSherlock visual abductive reasoning benchmark show that the RCA significantly\noutstands previous SOTAs, ranking the \\nth{1} on the leaderboards (e.g., Human\nAcc: RCA 31.74 \\textit{vs} CPT-CLIP 29.58, higher =better). We also validate\nthe RCA is generalizable to local perception benchmarks like RefCOCO. We\nopen-source our project at\n\\textit{\\color{magenta}{\\url{https://github.com/LUNAProject22/RPA}}}.\n","authors":["Hao Zhang","Yeo Keat Ee","Basura Fernando"],"pdf_url":"https://arxiv.org/pdf/2303.10428v5.pdf","comment":"13 pages, 11 figures, ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2406.18944v3","updated":"2024-08-07T13:37:41Z","published":"2024-06-27T07:14:14Z","title":"Investigating and Defending Shortcut Learning in Personalized Diffusion\n  Models","summary":"  Personalized diffusion models have gained popularity for adapting pre-trained\ntext-to-image models to generate images of specific topics with minimal\ntraining data. However, these models are vulnerable to minor adversarial\nperturbations, leading to degraded performance on corrupted datasets. Such\nvulnerabilities are further exploited to craft protective perturbations on\nsensitive images like portraits that prevent unauthorized generation. In\nresponse, diffusion-based purification methods have been proposed to remove\nthese perturbations and retain generation performance. However, existing works\nturn to over-purifying the images, which causes information loss. In this\npaper, we take a closer look at the fine-tuning process of personalized\ndiffusion models through the lens of shortcut learning. And we propose a\nhypothesis explaining the manipulation mechanisms of existing perturbation\nmethods, demonstrating that perturbed images significantly deviate from their\noriginal prompts in the CLIP-based latent space. This misalignment during\nfine-tuning causes models to associate noisy patterns with identifiers,\nresulting in performance degradation. Based on these insights, we introduce a\nsystematic approach to maintain training performance through purification. Our\nmethod first purifies the images to realign them with their original semantic\nmeanings in latent space. Then, we introduce contrastive learning with negative\ntokens to decouple the learning of clean identities from noisy patterns, which\nshows a strong potential capacity against adaptive perturbation. Our study\nuncovers shortcut learning vulnerabilities in personalized diffusion models and\nprovides a firm evaluation framework for future protective perturbation\nresearch. Code is available at https://github.com/liuyixin-louis/DiffShortcut.\n","authors":["Yixin Liu","Ruoxi Chen","Lichao Sun"],"pdf_url":"https://arxiv.org/pdf/2406.18944v3.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2408.03761v1","updated":"2024-08-07T13:30:58Z","published":"2024-08-07T13:30:58Z","title":"MMSummary: Multimodal Summary Generation for Fetal Ultrasound Video","summary":"  We present the first automated multimodal summary generation system,\nMMSummary, for medical imaging video, particularly with a focus on fetal\nultrasound analysis. Imitating the examination process performed by a human\nsonographer, MMSummary is designed as a three-stage pipeline, progressing from\nkeyframe detection to keyframe captioning and finally anatomy segmentation and\nmeasurement. In the keyframe detection stage, an innovative automated workflow\nis proposed to progressively select a concise set of keyframes, preserving\nsufficient video information without redundancy. Subsequently, we adapt a large\nlanguage model to generate meaningful captions for fetal ultrasound keyframes\nin the keyframe captioning stage. If a keyframe is captioned as fetal biometry,\nthe segmentation and measurement stage estimates biometric parameters by\nsegmenting the region of interest according to the textual prior. The MMSummary\nsystem provides comprehensive summaries for fetal ultrasound examinations and\nbased on reported experiments is estimated to reduce scanning time by\napproximately 31.5%, thereby suggesting the potential to enhance clinical\nworkflow efficiency.\n","authors":["Xiaoqing Guo","Qianhui Men","J. Alison Noble"],"pdf_url":"https://arxiv.org/pdf/2408.03761v1.pdf","comment":"MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2404.11317v2","updated":"2024-08-07T13:20:30Z","published":"2024-04-17T12:30:54Z","title":"Improving Composed Image Retrieval via Contrastive Learning with Scaling\n  Positives and Negatives","summary":"  The Composed Image Retrieval (CIR) task aims to retrieve target images using\na composed query consisting of a reference image and a modified text. Advanced\nmethods often utilize contrastive learning as the optimization objective, which\nbenefits from adequate positive and negative examples. However, the triplet for\nCIR incurs high manual annotation costs, resulting in limited positive\nexamples. Furthermore, existing methods commonly use in-batch negative\nsampling, which reduces the negative number available for the model. To address\nthe problem of lack of positives, we propose a data generation method by\nleveraging a multi-modal large language model to construct triplets for CIR. To\nintroduce more negatives during fine-tuning, we design a two-stage fine-tuning\nframework for CIR, whose second stage introduces plenty of static\nrepresentations of negatives to optimize the representation space rapidly. The\nabove two improvements can be effectively stacked and designed to be\nplug-and-play, easily applied to existing CIR models without changing their\noriginal architectures. Extensive experiments and ablation analysis demonstrate\nthat our method effectively scales positives and negatives and achieves\nstate-of-the-art results on both FashionIQ and CIRR datasets. In addition, our\nmethod also performs well in zero-shot composed image retrieval, providing a\nnew CIR solution for the low-resources scenario. Our code and data are released\nat https://github.com/BUAADreamer/SPN4CIR.\n","authors":["Zhangchi Feng","Richong Zhang","Zhijie Nie"],"pdf_url":"https://arxiv.org/pdf/2404.11317v2.pdf","comment":"Accepted to ACM MM 2024 Regular Papers"},{"id":"http://arxiv.org/abs/2408.03753v1","updated":"2024-08-07T13:06:29Z","published":"2024-08-07T13:06:29Z","title":"3iGS: Factorised Tensorial Illumination for 3D Gaussian Splatting","summary":"  The use of 3D Gaussians as representation of radiance fields has enabled high\nquality novel view synthesis at real-time rendering speed. However, the choice\nof optimising the outgoing radiance of each Gaussian independently as spherical\nharmonics results in unsatisfactory view dependent effects. In response to\nthese limitations, our work, Factorised Tensorial Illumination for 3D Gaussian\nSplatting, or 3iGS, improves upon 3D Gaussian Splatting (3DGS) rendering\nquality. Instead of optimising a single outgoing radiance parameter, 3iGS\nenhances 3DGS view-dependent effects by expressing the outgoing radiance as a\nfunction of a local illumination field and Bidirectional Reflectance\nDistribution Function (BRDF) features. We optimise a continuous incident\nillumination field through a Tensorial Factorisation representation, while\nseparately fine-tuning the BRDF features of each 3D Gaussian relative to this\nillumination field. Our methodology significantly enhances the rendering\nquality of specular view-dependent effects of 3DGS, while maintaining rapid\ntraining and rendering speeds.\n","authors":["Zhe Jun Tang","Tat-Jen Cham"],"pdf_url":"https://arxiv.org/pdf/2408.03753v1.pdf","comment":"The 18th European Conference on Computer Vision ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03748v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Data Generation Scheme for Thermal Modality with Edge-Guided Adversarial\n  Conditional Diffusion Model","summary":"  In challenging low light and adverse weather conditions,thermal vision\nalgorithms,especially object detection,have exhibited remarkable\npotential,contrasting with the frequent struggles encountered by visible vision\nalgorithms. Nevertheless,the efficacy of thermal vision algorithms driven by\ndeep learning models remains constrained by the paucity of available training\ndata samples. To this end,this paper introduces a novel approach termed the\nedge guided conditional diffusion model. This framework aims to produce\nmeticulously aligned pseudo thermal images at the pixel level,leveraging edge\ninformation extracted from visible images. By utilizing edges as contextual\ncues from the visible domain,the diffusion model achieves meticulous control\nover the delineation of objects within the generated images. To alleviate the\nimpacts of those visible-specific edge information that should not appear in\nthe thermal domain,a two-stage modality adversarial training strategy is\nproposed to filter them out from the generated images by differentiating the\nvisible and thermal modality. Extensive experiments on LLVIP demonstrate ECDM s\nsuperiority over existing state-of-the-art approaches in terms of image\ngeneration quality.\n","authors":["Guoqing Zhu","Honghu Pan","Qiang Wang","Chao Tian","Chao Yang","Zhenyu He"],"pdf_url":"https://arxiv.org/pdf/2408.03748v1.pdf","comment":"accepted by ACM MM 2024/ACM MM24"},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03745v1","updated":"2024-08-07T12:58:39Z","published":"2024-08-07T12:58:39Z","title":"Intuitionistic Fuzzy Cognitive Maps for Interpretable Image\n  Classification","summary":"  The interpretability of machine learning models is critical, as users may be\nreluctant to rely on their inferences. Intuitionistic FCMs (iFCMs) have been\nproposed as an extension of FCMs offering a natural mechanism to assess the\nquality of their output through the estimation of hesitancy, a concept\nresembling to human hesitation in decision making. To address the challenge of\ninterpretable image classification, this paper introduces a novel framework,\nnamed Interpretable Intuitionistic FCM (I2FCM) which is domain-independent,\nsimple to implement, and can be applied on Convolutional Neural Network (CNN)\nmodels, rendering them interpretable. To the best of our knowledge this is the\nfirst time iFCMs are applied for image classification. Further novel\ncontributions include: a feature extraction process focusing on the most\ninformative image regions; a learning algorithm for data-driven determination\nof the intuitionistic fuzzy interconnections of the iFCM; an inherently\ninterpretable classification approach based on image contents. In the context\nof image classification, hesitancy is considered as a degree of inconfidence\nwith which an image is categorized to a class. The constructed iFCM model\ndistinguishes the most representative image semantics and analyses them\nutilizing cause-and-effect relations. The effectiveness of the introduced\nframework is evaluated on publicly available datasets, and the experimental\nresults confirm that it can provide enhanced classification performance, while\nproviding interpretable inferences.\n","authors":["Georgia Sovatzidi","Michael D. Vasilakakis","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03745v1.pdf","comment":"This work has been submitted for possible journal publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.03734v1","updated":"2024-08-07T12:42:06Z","published":"2024-08-07T12:42:06Z","title":"Soft-Hard Attention U-Net Model and Benchmark Dataset for Multiscale\n  Image Shadow Removal","summary":"  Effective shadow removal is pivotal in enhancing the visual quality of images\nin various applications, ranging from computer vision to digital photography.\nDuring the last decades physics and machine learning -based methodologies have\nbeen proposed; however, most of them have limited capacity in capturing complex\nshadow patterns due to restrictive model assumptions, neglecting the fact that\nshadows usually appear at different scales. Also, current datasets used for\nbenchmarking shadow removal are composed of a limited number of images with\nsimple scenes containing mainly uniform shadows cast by single objects, whereas\nonly a few of them include both manual shadow annotations and paired\nshadow-free images. Aiming to address all these limitations in the context of\nnatural scene imaging, including urban environments with complex scenes, the\ncontribution of this study is twofold: a) it proposes a novel deep learning\narchitecture, named Soft-Hard Attention U-net (SHAU), focusing on multiscale\nshadow removal; b) it provides a novel synthetic dataset, named Multiscale\nShadow Removal Dataset (MSRD), containing complex shadow patterns of multiple\nscales, aiming to serve as a privacy-preserving dataset for a more\ncomprehensive benchmarking of future shadow removal methodologies. Key\narchitectural components of SHAU are the soft and hard attention modules, which\nalong with multiscale feature extraction blocks enable effective shadow removal\nof different scales and intensities. The results demonstrate the effectiveness\nof SHAU over the relevant state-of-the-art shadow removal methods across\nvarious benchmark datasets, improving the Peak Signal-to-Noise Ratio and Root\nMean Square Error for the shadow area by 25.1% and 61.3%, respectively.\n","authors":["Eirini Cholopoulou","Dimitrios E. Diamantis","Dimitra-Christina C. Koutsiou","Dimitris K. Iakovidis"],"pdf_url":"https://arxiv.org/pdf/2408.03734v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03717v1","updated":"2024-08-07T12:10:32Z","published":"2024-08-07T12:10:32Z","title":"Pick of the Bunch: Detecting Infrared Small Targets Beyond Hit-Miss\n  Trade-Offs via Selective Rank-Aware Attention","summary":"  Infrared small target detection faces the inherent challenge of precisely\nlocalizing dim targets amidst complex background clutter. Traditional\napproaches struggle to balance detection precision and false alarm rates. To\nbreak this dilemma, we propose SeRankDet, a deep network that achieves high\naccuracy beyond the conventional hit-miss trade-off, by following the ``Pick of\nthe Bunch'' principle. At its core lies our Selective Rank-Aware Attention\n(SeRank) module, employing a non-linear Top-K selection process that preserves\nthe most salient responses, preventing target signal dilution while maintaining\nconstant complexity. Furthermore, we replace the static concatenation typical\nin U-Net structures with our Large Selective Feature Fusion (LSFF) module, a\ndynamic fusion strategy that empowers SeRankDet with adaptive feature\nintegration, enhancing its ability to discriminate true targets from false\nalarms. The network's discernment is further refined by our Dilated Difference\nConvolution (DDC) module, which merges differential convolution aimed at\namplifying subtle target characteristics with dilated convolution to expand the\nreceptive field, thereby substantially improving target-background separation.\nDespite its lightweight architecture, the proposed SeRankDet sets new\nbenchmarks in state-of-the-art performance across multiple public datasets. The\ncode is available at https://github.com/GrokCV/SeRankDet.\n","authors":["Yimian Dai","Peiwen Pan","Yulei Qian","Yuxuan Li","Xiang Li","Jian Yang","Huan Wan"],"pdf_url":"https://arxiv.org/pdf/2408.03717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01669v2","updated":"2024-08-07T12:06:18Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v2.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.01334v2","updated":"2024-08-07T12:01:58Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-based Backbone Framework (TBBF) to enhance robot\ntask understanding and transferability. This framework uses therbligs (basic\naction elements) as the backbone to decompose high-level robot tasks into\nelemental robot configurations, which are then integrated with current\nfoundation models to improve task understanding. The approach consists of two\nstages: offline training and online testing. During the offline training stage,\nwe developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, the Large Language Model\n(LLM)-Alignment Policy for Visual Correction (LAP-VC) is employed to ensure\nprecise action execution, facilitating trajectory transfer in novel robot\nscenarios. Experimental results validate these methods, achieving 94.37% recall\nin therblig segmentation and success rates of 94.4% and 80% in real-world\nonline robot testing for simple and complex scenarios, respectively.\nSupplementary material is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v2.pdf","comment":"8 pages, 8 figures. This work is intended to be submitted to IEEE\n  Robotics and Automation Letters (RA-L) for possible publication"},{"id":"http://arxiv.org/abs/2305.12661v4","updated":"2024-08-07T11:37:02Z","published":"2023-05-22T03:04:22Z","title":"Semantic-guided modeling of spatial relation and object co-occurrence\n  for indoor scene recognition","summary":"  Exploring the semantic context in scene images is essential for indoor scene\nrecognition. However, due to the diverse intra-class spatial layouts and the\ncoexisting inter-class objects, modeling contextual relationships to adapt\nvarious image characteristics is a great challenge. Existing contextual\nmodeling methods for scene recognition exhibit two limitations: 1) They\ntypically model only one type of spatial relationship (order or metric) among\nobjects within scenes, with limited exploration of diverse spatial layouts. 2)\nThey often overlook the differences in coexisting objects across different\nscenes, suppressing scene recognition performance. To overcome these\nlimitations, we propose SpaCoNet, which simultaneously models Spatial relation\nand Co-occurrence of objects guided by semantic segmentation. Firstly, the\nSemantic Spatial Relation Module (SSRM) is constructed to model scene spatial\nfeatures. With the help of semantic segmentation, this module decouples spatial\ninformation from the scene image and thoroughly explores all spatial\nrelationships among objects in an end-to-end manner, thereby obtaining\nsemantic-based spatial features. Secondly, both spatial features from the SSRM\nand deep features from the Image Feature Extraction Module are allocated to\neach object, so as to distinguish the coexisting object across different\nscenes. Finally, utilizing the discriminative features above, we design a\nGlobal-Local Dependency Module to explore the long-range co-occurrence among\nobjects, and further generate a semantic-guided feature representation for\nindoor scene recognition. Experimental results on three widely used scene\ndatasets demonstrate the effectiveness and generality of the proposed method.\n","authors":["Chuanxin Song","Hanbo Wu","Xin Ma"],"pdf_url":"https://arxiv.org/pdf/2305.12661v4.pdf","comment":"Under second review at Expert Systems with Applications"},{"id":"http://arxiv.org/abs/2408.03703v1","updated":"2024-08-07T11:33:46Z","published":"2024-08-07T11:33:46Z","title":"CAS-ViT: Convolutional Additive Self-attention Vision Transformers for\n  Efficient Mobile Applications","summary":"  Vision Transformers (ViTs) mark a revolutionary advance in neural networks\nwith their token mixer's powerful global context capability. However, the\npairwise token affinity and complex matrix operations limit its deployment on\nresource-constrained scenarios and real-time applications, such as mobile\ndevices, although considerable efforts have been made in previous works. In\nthis paper, we introduce CAS-ViT: Convolutional Additive Self-attention Vision\nTransformers, to achieve a balance between efficiency and performance in mobile\napplications. Firstly, we argue that the capability of token mixers to obtain\nglobal contextual information hinges on multiple information interactions, such\nas spatial and channel domains. Subsequently, we construct a novel additive\nsimilarity function following this paradigm and present an efficient\nimplementation named Convolutional Additive Token Mixer (CATM). This\nsimplification leads to a significant reduction in computational overhead. We\nevaluate CAS-ViT across a variety of vision tasks, including image\nclassification, object detection, instance segmentation, and semantic\nsegmentation. Our experiments, conducted on GPUs, ONNX, and iPhones,\ndemonstrate that CAS-ViT achieves a competitive performance when compared to\nother state-of-the-art backbones, establishing it as a viable option for\nefficient mobile vision applications. Our code and model are available at:\n\\url{https://github.com/Tianfang-Zhang/CAS-ViT}\n","authors":["Tianfang Zhang","Lei Li","Yang Zhou","Wentao Liu","Chen Qian","Xiangyang Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05278v2","updated":"2024-08-07T11:33:21Z","published":"2024-07-07T06:36:09Z","title":"HyperKAN: Kolmogorov-Arnold Networks make Hyperspectral Image\n  Classificators Smarter","summary":"  In traditional neural network architectures, a multilayer perceptron (MLP) is\ntypically employed as a classification block following the feature extraction\nstage. However, the Kolmogorov-Arnold Network (KAN) presents a promising\nalternative to MLP, offering the potential to enhance prediction accuracy. In\nthis paper, we propose the replacement of linear and convolutional layers of\ntraditional networks with KAN-based counterparts. These modifications allowed\nus to significantly increase the per-pixel classification accuracy for\nhyperspectral remote-sensing images. We modified seven different neural network\narchitectures for hyperspectral image classification and observed a substantial\nimprovement in the classification accuracy across all the networks. The\narchitectures considered in the paper include baseline MLP, state-of-the-art 1D\n(1DCNN) and 3D convolutional (two different 3DCNN, NM3DCNN), and transformer\n(SSFTT) architectures, as well as newly proposed M1DCNN. The greatest effect\nwas achieved for convolutional networks working exclusively on spectral data,\nand the best classification quality was achieved using a KAN-based transformer\narchitecture. All the experiments were conducted using seven openly available\nhyperspectral datasets. Our code is available at\nhttps://github.com/f-neumann77/HyperKAN.\n","authors":["Valeriy Lobanov","Nikita Firsov","Evgeny Myasnikov","Roman Khabibullin","Artem Nikonorov"],"pdf_url":"https://arxiv.org/pdf/2407.05278v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.10271v2","updated":"2024-08-07T11:32:19Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.03695v1","updated":"2024-08-07T11:20:37Z","published":"2024-08-07T11:20:37Z","title":"Openstory++: A Large-scale Dataset and Benchmark for Instance-aware\n  Open-domain Visual Storytelling","summary":"  Recent image generation models excel at creating high-quality images from\nbrief captions. However, they fail to maintain consistency of multiple\ninstances across images when encountering lengthy contexts. This inconsistency\nis largely due to in existing training datasets the absence of granular\ninstance feature labeling in existing training datasets. To tackle these\nissues, we introduce Openstory++, a large-scale dataset combining additional\ninstance-level annotations with both images and text. Furthermore, we develop a\ntraining methodology that emphasizes entity-centric image-text generation,\nensuring that the models learn to effectively interweave visual and textual\ninformation. Specifically, Openstory++ streamlines the process of keyframe\nextraction from open-domain videos, employing vision-language models to\ngenerate captions that are then polished by a large language model for\nnarrative continuity. It surpasses previous datasets by offering a more\nexpansive open-domain resource, which incorporates automated captioning,\nhigh-resolution imagery tailored for instance count, and extensive frame\nsequences for temporal consistency. Additionally, we present Cohere-Bench, a\npioneering benchmark framework for evaluating the image generation tasks when\nlong multimodal context is provided, including the ability to keep the\nbackground, style, instances in the given context coherent. Compared to\nexisting benchmarks, our work fills critical gaps in multi-modal generation,\npropelling the development of models that can adeptly generate and interpret\ncomplex narratives in open-domain environments. Experiments conducted within\nCohere-Bench confirm the superiority of Openstory++ in nurturing high-quality\nvisual storytelling models, enhancing their ability to address open-domain\ngeneration tasks. More details can be found at https://openstorypp.github.io/\n","authors":["Zilyu Ye","Jinxiu Liu","Ruotian Peng","Jinjin Cao","Zhiyang Chen","Yiyang Zhang","Ziwei Xuan","Mingyuan Zhou","Xiaoqian Shen","Mohamed Elhoseiny","Qi Liu","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2408.03695v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.12670v3","updated":"2024-08-07T10:45:03Z","published":"2024-03-19T12:11:57Z","title":"Driving Animatronic Robot Facial Expression From Speech","summary":"  Animatronic robots hold the promise of enabling natural human-robot\ninteraction through lifelike facial expressions. However, generating realistic,\nspeech-synchronized robot expressions poses significant challenges due to the\ncomplexities of facial biomechanics and the need for responsive motion\nsynthesis. This paper introduces a novel, skinning-centric approach to drive\nanimatronic robot facial expressions from speech input. At its core, the\nproposed approach employs linear blend skinning (LBS) as a unifying\nrepresentation, guiding innovations in both embodiment design and motion\nsynthesis. LBS informs the actuation topology, facilitates human expression\nretargeting, and enables efficient speech-driven facial motion generation. This\napproach demonstrates the capability to produce highly realistic facial\nexpressions on an animatronic face in real-time at over 4000 fps on a single\nNvidia RTX 4090, significantly advancing robots' ability to replicate nuanced\nhuman expressions for natural interaction. To foster further research and\ndevelopment in this field, the code has been made publicly available at:\n\\url{https://github.com/library87/OpenRoboExp}.\n","authors":["Boren Li","Hang Li","Hangxin Liu"],"pdf_url":"https://arxiv.org/pdf/2403.12670v3.pdf","comment":"8 pages, 6 figures, accepted to IROS 2024. For associated project\n  page, see https://library87.github.io/animatronic-face-iros24"},{"id":"http://arxiv.org/abs/2408.03677v1","updated":"2024-08-07T10:36:26Z","published":"2024-08-07T10:36:26Z","title":"L4DR: LiDAR-4DRadar Fusion for Weather-Robust 3D Object Detection","summary":"  LiDAR-based vision systems are integral for 3D object detection, which is\ncrucial for autonomous navigation. However, they suffer from performance\ndegradation in adverse weather conditions due to the quality deterioration of\nLiDAR point clouds. Fusing LiDAR with the weather-robust 4D radar sensor is\nexpected to solve this problem. However, the fusion of LiDAR and 4D radar is\nchallenging because they differ significantly in terms of data quality and the\ndegree of degradation in adverse weather. To address these issues, we introduce\nL4DR, a weather-robust 3D object detection method that effectively achieves\nLiDAR and 4D Radar fusion. Our L4DR includes Multi-Modal Encoding (MME) and\nForeground-Aware Denoising (FAD) technique to reconcile sensor gaps, which is\nthe first exploration of the complementarity of early fusion between LiDAR and\n4D radar. Additionally, we design an Inter-Modal and Intra-Modal ({IM}2 )\nparallel feature extraction backbone coupled with a Multi-Scale Gated Fusion\n(MSGF) module to counteract the varying degrees of sensor degradation under\nadverse weather conditions. Experimental evaluation on a VoD dataset with\nsimulated fog proves that L4DR is more adaptable to changing weather\nconditions. It delivers a significant performance increase under different fog\nlevels, improving the 3D mAP by up to 18.17% over the traditional LiDAR-only\napproach. Moreover, the results on the K-Radar dataset validate the consistent\nperformance improvement of L4DR in real-world adverse weather conditions.\n","authors":["Xun Huang","Ziyu Xu","Hai Wu","Jinlong Wang","Qiming Xia","Yan Xia","Jonathan Li","Kyle Gao","Chenglu Wen","Cheng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03677v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.14068v2","updated":"2024-08-07T10:28:18Z","published":"2024-06-20T07:44:56Z","title":"Classifying Dry Eye Disease Patients from Healthy Controls Using Machine\n  Learning and Metabolomics Data","summary":"  Dry eye disease is a common disorder of the ocular surface, leading patients\nto seek eye care. Clinical signs and symptoms are currently used to diagnose\ndry eye disease. Metabolomics, a method for analyzing biological systems, has\nbeen found helpful in identifying distinct metabolites in patients and in\ndetecting metabolic profiles that may indicate dry eye disease at early stages.\nIn this study, we explored using machine learning and metabolomics information\nto identify which cataract patients suffered from dry eye disease. As there is\nno one-size-fits-all machine learning model for metabolomics data, choosing the\nmost suitable model can significantly affect the quality of predictions and\nsubsequent metabolomics analyses. To address this challenge, we conducted a\ncomparative analysis of nine machine learning models on three metabolomics data\nsets from cataract patients with and without dry eye disease. The models were\nevaluated and optimized using nested k-fold cross-validation. To assess the\nperformance of these models, we selected a set of suitable evaluation metrics\ntailored to the data set's challenges. The logistic regression model overall\nperformed the best, achieving the highest area under the curve score of 0.8378,\nbalanced accuracy of 0.735, Matthew's correlation coefficient of 0.5147, an\nF1-score of 0.8513, and a specificity of 0.5667. Additionally, following the\nlogistic regression, the XGBoost and Random Forest models also demonstrated\ngood performance.\n","authors":["Sajad Amouei Sheshkal","Morten Gundersen","Michael Alexander Riegler","ygunn Aass Utheim","Kjell Gunnar Gundersen","Hugo Lewi Hammer"],"pdf_url":"https://arxiv.org/pdf/2406.14068v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01126v2","updated":"2024-08-07T10:25:08Z","published":"2024-08-02T09:07:31Z","title":"IG-SLAM: Instant Gaussian SLAM","summary":"  3D Gaussian Splatting has recently shown promising results as an alternative\nscene representation in SLAM systems to neural implicit representations.\nHowever, current methods either lack dense depth maps to supervise the mapping\nprocess or detailed training designs that consider the scale of the\nenvironment. To address these drawbacks, we present IG-SLAM, a dense RGB-only\nSLAM system that employs robust Dense-SLAM methods for tracking and combines\nthem with Gaussian Splatting. A 3D map of the environment is constructed using\naccurate pose and dense depth provided by tracking. Additionally, we utilize\ndepth uncertainty in map optimization to improve 3D reconstruction. Our decay\nstrategy in map optimization enhances convergence and allows the system to run\nat 10 fps in a single process. We demonstrate competitive performance with\nstate-of-the-art RGB-only SLAM systems while achieving faster operation speeds.\nWe present our experiments on the Replica, TUM-RGBD, ScanNet, and EuRoC\ndatasets. The system achieves photo-realistic 3D reconstruction in large-scale\nsequences, particularly in the EuRoC dataset.\n","authors":["F. Aykut Sarikamis","A. Aydin Alatan"],"pdf_url":"https://arxiv.org/pdf/2408.01126v2.pdf","comment":"8 pages, 3 page ref, 5 figures"},{"id":"http://arxiv.org/abs/2408.03663v1","updated":"2024-08-07T10:04:04Z","published":"2024-08-07T10:04:04Z","title":"Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks","summary":"  In this paper, we introduce a memory-efficient CNN (convolutional neural\nnetwork), which enables resource-constrained low-end embedded and IoT devices\nto perform on-device vision tasks, such as image classification and object\ndetection, using extremely low memory, i.e., only 63 KB on ImageNet\nclassification. Based on the bottleneck block of MobileNet, we propose three\ndesign principles that significantly curtail the peak memory usage of a CNN so\nthat it can fit the limited KB memory of the low-end device. First, 'input\nsegmentation' divides an input image into a set of patches, including the\ncentral patch overlapped with the others, reducing the size (and memory\nrequirement) of a large input image. Second, 'patch tunneling' builds\nindependent tunnel-like paths consisting of multiple bottleneck blocks per\npatch, penetrating through the entire model from an input patch to the last\nlayer of the network, maintaining lightweight memory usage throughout the whole\nnetwork. Lastly, 'bottleneck reordering' rearranges the execution order of\nconvolution operations inside the bottleneck block such that the memory usage\nremains constant regardless of the size of the convolution output channels. The\nexperiment result shows that the proposed network classifies ImageNet with\nextremely low memory (i.e., 63 KB) while achieving competitive top-1 accuracy\n(i.e., 61.58\\%). To the best of our knowledge, the memory usage of the proposed\nnetwork is far smaller than state-of-the-art memory-efficient networks, i.e.,\nup to 89x and 3.1x smaller than MobileNet (i.e., 5.6 MB) and MCUNet (i.e., 196\nKB), respectively.\n","authors":["Jaewook Lee","Yoel Park","Seulki Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03663v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03657v1","updated":"2024-08-07T09:52:30Z","published":"2024-08-07T09:52:30Z","title":"PHOCUS: Physics-Based Deconvolution for Ultrasound Resolution\n  Enhancement","summary":"  Ultrasound is widely used in medical diagnostics allowing for accessible and\npowerful imaging but suffers from resolution limitations due to diffraction and\nthe finite aperture of the imaging system, which restricts diagnostic use. The\nimpulse function of an ultrasound imaging system is called the point spread\nfunction (PSF), which is convolved with the spatial distribution of reflectors\nin the image formation process. Recovering high-resolution reflector\ndistributions by removing image distortions induced by the convolution process\nimproves image clarity and detail. Conventionally, deconvolution techniques\nattempt to rectify the imaging system's dependent PSF, working directly on the\nradio-frequency (RF) data. However, RF data is often not readily accessible.\nTherefore, we introduce a physics-based deconvolution process using a modeled\nPSF, working directly on the more commonly available B-mode images. By\nleveraging Implicit Neural Representations (INRs), we learn a continuous\nmapping from spatial locations to their respective echogenicity values,\neffectively compensating for the discretized image space. Our contribution\nconsists of a novel methodology for retrieving a continuous echogenicity map\ndirectly from a B-mode image through a differentiable physics-based rendering\npipeline for ultrasound resolution enhancement. We qualitatively and\nquantitatively evaluate our approach on synthetic data, demonstrating\nimprovements over traditional methods in metrics such as PSNR and SSIM.\nFurthermore, we show qualitative enhancements on an ultrasound phantom and an\nin-vivo acquisition of a carotid artery.\n","authors":["Felix Duelmer","Walter Simson","Mohammad Farid Azampour","Magdalena Wysocki","Angelos Karlas","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2408.03657v1.pdf","comment":"Accepted at the Workshop of Advances in Simplifying Medical\n  Ultrasound at MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.03654v1","updated":"2024-08-07T09:40:26Z","published":"2024-08-07T09:40:26Z","title":"Unsupervised Detection of Fetal Brain Anomalies using Denoising\n  Diffusion Models","summary":"  Congenital malformations of the brain are among the most common fetal\nabnormalities that impact fetal development. Previous anomaly detection methods\non ultrasound images are based on supervised learning, rely on manual\nannotations, and risk missing underrepresented categories. In this work, we\nframe fetal brain anomaly detection as an unsupervised task using diffusion\nmodels. To this end, we employ an inpainting-based Noise Agnostic Anomaly\nDetection approach that identifies the abnormality using\ndiffusion-reconstructed fetal brain images from multiple noise levels. Our\napproach only requires normal fetal brain ultrasound images for training,\naddressing the limited availability of abnormal data. Our experiments on a\nreal-world clinical dataset show the potential of using unsupervised methods\nfor fetal brain anomaly detection. Additionally, we comprehensively evaluate\nhow different noise types affect diffusion models in the fetal anomaly\ndetection domain.\n","authors":["Markus Ditlev Sjgren Olsen","Jakob Ambsdorf","Manxi Lin","Caroline Takse-Vester","Morten Bo Sndergaard Svendsen","Anders Nymark Christensen","Mads Nielsen","Martin Grnnebk Tolsgaard","Aasa Feragen","Paraskevas Pegios"],"pdf_url":"https://arxiv.org/pdf/2408.03654v1.pdf","comment":"Accepted at ASMUS@MICCAI 2024"},{"id":"http://arxiv.org/abs/2405.00354v2","updated":"2024-08-07T09:36:55Z","published":"2024-05-01T07:16:03Z","title":"CrossMatch: Enhance Semi-Supervised Medical Image Segmentation with\n  Perturbation Strategies and Knowledge Distillation","summary":"  Semi-supervised learning for medical image segmentation presents a unique\nchallenge of efficiently using limited labeled data while leveraging abundant\nunlabeled data. Despite advancements, existing methods often do not fully\nexploit the potential of the unlabeled data for enhancing model robustness and\naccuracy. In this paper, we introduce CrossMatch, a novel framework that\nintegrates knowledge distillation with dual perturbation strategies-image-level\nand feature-level-to improve the model's learning from both labeled and\nunlabeled data. CrossMatch employs multiple encoders and decoders to generate\ndiverse data streams, which undergo self-knowledge distillation to enhance\nconsistency and reliability of predictions across varied perturbations. Our\nmethod significantly surpasses other state-of-the-art techniques in standard\nbenchmarks by effectively minimizing the gap between training on labeled and\nunlabeled data and improving edge accuracy and generalization in medical image\nsegmentation. The efficacy of CrossMatch is demonstrated through extensive\nexperimental validations, showing remarkable performance improvements without\nincreasing computational costs. Code for this implementation is made available\nat https://github.com/AiEson/CrossMatch.git.\n","authors":["Bin Zhao","Chunshi Wang","Shuxue Ding"],"pdf_url":"https://arxiv.org/pdf/2405.00354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15838v2","updated":"2024-08-07T09:34:25Z","published":"2024-07-22T17:55:22Z","title":"MMInstruct: A High-Quality Multi-Modal Instruction Tuning Dataset with\n  Extensive Diversity","summary":"  Despite the effectiveness of vision-language supervised fine-tuning in\nenhancing the performance of Vision Large Language Models (VLLMs). However,\nexisting visual instruction tuning datasets include the following limitations:\n(1) Instruction annotation quality: despite existing VLLMs exhibiting strong\nperformance, instructions generated by those advanced VLLMs may still suffer\nfrom inaccuracies, such as hallucinations. (2) Instructions and image\ndiversity: the limited range of instruction types and the lack of diversity in\nimage data may impact the model's ability to generate diversified and closer to\nreal-world scenarios outputs. To address these challenges, we construct a\nhigh-quality, diverse visual instruction tuning dataset MMInstruct, which\nconsists of 973K instructions from 24 domains. There are four instruction\ntypes: Judgement, Multiple-Choice, Long Visual Question Answering and Short\nVisual Question Answering. To construct MMInstruct, we propose an instruction\ngeneration data engine that leverages GPT-4V, GPT-3.5, and manual correction.\nOur instruction generation engine enables semi-automatic, low-cost, and\nmulti-domain instruction generation at 1/6 the cost of manual construction.\nThrough extensive experiment validation and ablation experiments, we\ndemonstrate that MMInstruct could significantly improve the performance of\nVLLMs, e.g., the model fine-tuning on MMInstruct achieves new state-of-the-art\nperformance on 10 out of 12 benchmarks. The code and data shall be available at\nhttps://github.com/yuecao0119/MMInstruct.\n","authors":["Yangzhou Liu","Yue Cao","Zhangwei Gao","Weiyun Wang","Zhe Chen","Wenhai Wang","Hao Tian","Lewei Lu","Xizhou Zhu","Tong Lu","Yu Qiao","Jifeng Dai"],"pdf_url":"https://arxiv.org/pdf/2407.15838v2.pdf","comment":"18 pages, 8 figures, technical report"},{"id":"http://arxiv.org/abs/2408.03651v1","updated":"2024-08-07T09:30:51Z","published":"2024-08-07T09:30:51Z","title":"SAM2-PATH: A better segment anything model for semantic segmentation in\n  digital pathology","summary":"  The semantic segmentation task in pathology plays an indispensable role in\nassisting physicians in determining the condition of tissue lesions. Foundation\nmodels, such as the SAM (Segment Anything Model) and SAM2, exhibit exceptional\nperformance in instance segmentation within everyday natural scenes. SAM-PATH\nhas also achieved impressive results in semantic segmentation within the field\nof pathology. However, in computational pathology, the models mentioned above\nstill have the following limitations. The pre-trained encoder models suffer\nfrom a scarcity of pathology image data; SAM and SAM2 are not suitable for\nsemantic segmentation. In this paper, we have designed a trainable\nKolmogorov-Arnold Networks(KAN) classification module within the SAM2 workflow,\nand we have introduced the largest pretrained vision encoder for histopathology\n(UNI) to date. Our proposed framework, SAM2-PATH, augments SAM2's capability to\nperform semantic segmentation in digital pathology autonomously, eliminating\nthe need for human provided input prompts. The experimental results demonstrate\nthat, after fine-tuning the KAN classification module and decoder, Our dataset\nhas achieved competitive results on publicly available pathology data. The code\nhas been open-sourced and can be found at the following address:\nhttps://github.com/simzhangbest/SAM2PATH.\n","authors":["Mingya Zhang","Liang Wang","Limei Gu","Zhao Li","Yaohui Wang","Tingshen Ling","Xianping Tao"],"pdf_url":"https://arxiv.org/pdf/2408.03651v1.pdf","comment":"6 pages , 3 figures"},{"id":"http://arxiv.org/abs/2408.02164v2","updated":"2024-08-07T09:23:36Z","published":"2024-08-04T23:21:46Z","title":"Rethinking Affect Analysis: A Protocol for Ensuring Fairness and\n  Consistency","summary":"  Evaluating affect analysis methods presents challenges due to inconsistencies\nin database partitioning and evaluation protocols, leading to unfair and biased\nresults. Previous studies claim continuous performance improvements, but our\nfindings challenge such assertions. Using these insights, we propose a unified\nprotocol for database partitioning that ensures fairness and comparability. We\nprovide detailed demographic annotations (in terms of race, gender and age),\nevaluation metrics, and a common framework for expression recognition, action\nunit detection and valence-arousal estimation. We also rerun the methods with\nthe new protocol and introduce a new leaderboards to encourage future research\nin affect recognition with a fairer comparison. Our annotations, code, and\npre-trained models are available on\n\\hyperlink{https://github.com/dkollias/Fair-Consistent-Affect-Analysis}{Github}.\n","authors":["Guanyu Hu","Dimitrios Kollias","Eleni Papadopoulou","Paraskevi Tzouveli","Jie Wei","Xinyu Yang"],"pdf_url":"https://arxiv.org/pdf/2408.02164v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2405.06841"},{"id":"http://arxiv.org/abs/2405.10885v2","updated":"2024-08-07T09:11:38Z","published":"2024-05-17T16:22:52Z","title":"FA-Depth: Toward Fast and Accurate Self-supervised Monocular Depth\n  Estimation","summary":"  Most existing methods often rely on complex models to predict scene depth\nwith high accuracy, resulting in slow inference that is not conducive to\ndeployment. To better balance precision and speed, we first designed SmallDepth\nbased on sparsity. Second, to enhance the feature representation ability of\nSmallDepth during training under the condition of equal complexity during\ninference, we propose an equivalent transformation module(ETM). Third, to\nimprove the ability of each layer in the case of a fixed SmallDepth to perceive\ndifferent context information and improve the robustness of SmallDepth to the\nleft-right direction and illumination changes, we propose pyramid loss. Fourth,\nto further improve the accuracy of SmallDepth, we utilized the proposed\nfunction approximation loss (APX) to transfer knowledge in the pretrained\nHQDecv2, obtained by optimizing the previous HQDec to address grid artifacts in\nsome regions, to SmallDepth. Extensive experiments demonstrate that each\nproposed component improves the precision of SmallDepth without changing the\ncomplexity of SmallDepth during inference, and the developed approach achieves\nstate-of-the-art results on KITTI at an inference speed of more than 500 frames\nper second and with approximately 2 M parameters. The code and models will be\npublicly available at https://github.com/fwucas/FA-Depth.\n","authors":["Fei Wang","Jun Cheng"],"pdf_url":"https://arxiv.org/pdf/2405.10885v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.14973v2","updated":"2024-08-07T08:53:25Z","published":"2024-03-22T06:04:11Z","title":"Pose-Aware Self-Supervised Learning with Viewpoint Trajectory\n  Regularization","summary":"  Learning visual features from unlabeled images has proven successful for\nsemantic categorization, often by mapping different $views$ of the same object\nto the same feature to achieve recognition invariance. However, visual\nrecognition involves not only identifying $what$ an object is but also\nunderstanding $how$ it is presented. For example, seeing a car from the side\nversus head-on is crucial for deciding whether to stay put or jump out of the\nway. While unsupervised feature learning for downstream viewpoint reasoning is\nimportant, it remains under-explored, partly due to the lack of a standardized\nevaluation method and benchmarks.\n  We introduce a new dataset of adjacent image triplets obtained from a\nviewpoint trajectory, without any semantic or pose labels. We benchmark both\nsemantic classification and pose estimation accuracies on the same visual\nfeature. Additionally, we propose a viewpoint trajectory regularization loss\nfor learning features from unlabeled image triplets. Our experiments\ndemonstrate that this approach helps develop a visual representation that\nencodes object identity and organizes objects by their poses, retaining\nsemantic classification accuracy while achieving emergent global pose awareness\nand better generalization to novel objects. Our dataset and code are available\nat http://pwang.pw/trajSSL/.\n","authors":["Jiayun Wang","Yubei Chen","Stella X. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14973v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03637v1","updated":"2024-08-07T08:52:21Z","published":"2024-08-07T08:52:21Z","title":"TALE: Training-free Cross-domain Image Composition via Adaptive Latent\n  Manipulation and Energy-guided Optimization","summary":"  We present TALE, a novel training-free framework harnessing the generative\ncapabilities of text-to-image diffusion models to address the cross-domain\nimage composition task that focuses on flawlessly incorporating user-specified\nobjects into a designated visual contexts regardless of domain disparity.\nPrevious methods often involve either training auxiliary networks or finetuning\ndiffusion models on customized datasets, which are expensive and may undermine\nthe robust textual and visual priors of pre-trained diffusion models. Some\nrecent works attempt to break the barrier by proposing training-free\nworkarounds that rely on manipulating attention maps to tame the denoising\nprocess implicitly. However, composing via attention maps does not necessarily\nyield desired compositional outcomes. These approaches could only retain some\nsemantic information and usually fall short in preserving identity\ncharacteristics of input objects or exhibit limited background-object style\nadaptation in generated images. In contrast, TALE is a novel method that\noperates directly on latent space to provide explicit and effective guidance\nfor the composition process to resolve these problems. Specifically, we equip\nTALE with two mechanisms dubbed Adaptive Latent Manipulation and Energy-guided\nLatent Optimization. The former formulates noisy latents conducive to\ninitiating and steering the composition process by directly leveraging\nbackground and foreground latents at corresponding timesteps, and the latter\nexploits designated energy functions to further optimize intermediate latents\nconforming to specific conditions that complement the former to generate\ndesired final results. Our experiments demonstrate that TALE surpasses prior\nbaselines and attains state-of-the-art performance in image-guided composition\nacross various photorealistic and artistic domains.\n","authors":["Kien T. Pham","Jingye Chen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03637v1.pdf","comment":"The 32nd ACM Multimedia Conference (MM '24)"},{"id":"http://arxiv.org/abs/2408.03632v1","updated":"2024-08-07T08:43:58Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v1.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2408.03627v1","updated":"2024-08-07T08:39:33Z","published":"2024-08-07T08:39:33Z","title":"Weakly Contrastive Learning via Batch Instance Discrimination and\n  Feature Clustering for Small Sample SAR ATR","summary":"  In recent years, impressive performance of deep learning technology has been\nrecognized in Synthetic Aperture Radar (SAR) Automatic Target Recognition\n(ATR). Since a large amount of annotated data is required in this technique, it\nposes a trenchant challenge to the issue of obtaining a high recognition rate\nthrough less labeled data. To overcome this problem, inspired by the\ncontrastive learning, we proposed a novel framework named Batch Instance\nDiscrimination and Feature Clustering (BIDFC). In this framework, different\nfrom that of the objective of general contrastive learning methods, embedding\ndistance between samples should be moderate because of the high similarity\nbetween samples in the SAR images. Consequently, our flexible framework is\nequipped with adjustable distance between embedding, which we term as weakly\ncontrastive learning. Technically, instance labels are assigned to the\nunlabeled data in per batch and random augmentation and training are performed\nfew times on these augmented data. Meanwhile, a novel Dynamic-Weighted Variance\nloss (DWV loss) function is also posed to cluster the embedding of enhanced\nversions for each sample. Experimental results on the moving and stationary\ntarget acquisition and recognition (MSTAR) database indicate a 91.25%\nclassification accuracy of our method fine-tuned on only 3.13% training data.\nEven though a linear evaluation is performed on the same training data, the\naccuracy can still reach 90.13%. We also verified the effectiveness of BIDFC in\nOpenSarShip database, indicating that our method can be generalized to other\ndatasets. Our code is avaliable at:\nhttps://github.com/Wenlve-Zhou/BIDFC-master.\n","authors":["Yikui Zhai","Wenlve Zhou","Bing Sun","Jingwen Li","Qirui Ke","Zilu Ying","Junying Gan","Chaoyun Mai","Ruggero Donida Labati","Vincenzo Piuri","Fabio Scotti"],"pdf_url":"https://arxiv.org/pdf/2408.03627v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03624v1","updated":"2024-08-07T08:34:48Z","published":"2024-08-07T08:34:48Z","title":"AgentsCoMerge: Large Language Model Empowered Collaborative Decision\n  Making for Ramp Merging","summary":"  Ramp merging is one of the bottlenecks in traffic systems, which commonly\ncause traffic congestion, accidents, and severe carbon emissions. In order to\naddress this essential issue and enhance the safety and efficiency of connected\nand autonomous vehicles (CAVs) at multi-lane merging zones, we propose a novel\ncollaborative decision-making framework, named AgentsCoMerge, to leverage large\nlanguage models (LLMs). Specifically, we first design a scene observation and\nunderstanding module to allow an agent to capture the traffic environment. Then\nwe propose a hierarchical planning module to enable the agent to make decisions\nand plan trajectories based on the observation and the agent's own state. In\naddition, in order to facilitate collaboration among multiple agents, we\nintroduce a communication module to enable the surrounding agents to exchange\nnecessary information and coordinate their actions. Finally, we develop a\nreinforcement reflection guided training paradigm to further enhance the\ndecision-making capability of the framework. Extensive experiments are\nconducted to evaluate the performance of our proposed method, demonstrating its\nsuperior efficiency and effectiveness for multi-agent collaborative\ndecision-making under various ramp merging scenarios.\n","authors":["Senkang Hu","Zhengru Fang","Zihan Fang","Yiqin Deng","Xianhao Chen","Yuguang Fang","Sam Kwong"],"pdf_url":"https://arxiv.org/pdf/2408.03624v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2304.09914v4","updated":"2024-08-07T08:20:43Z","published":"2023-04-19T18:32:49Z","title":"The Face of Populism: Examining Differences in Facial Emotional\n  Expressions of Political Leaders Using Machine Learning","summary":"  Populist rhetoric employed on online media is characterized as deeply\nimpassioned and often imbued with strong emotions. The aim of this paper is to\nempirically investigate the differences in affective nonverbal communication of\npolitical leaders. We use a deep-learning approach to process a sample of 220\nYouTube videos of political leaders from 15 different countries, analyze their\nfacial expressions of emotion and then examine differences in average emotion\nscores representing the relative presence of 6 emotional states (anger,\ndisgust, fear, happiness, sadness, and surprise) and a neutral expression for\neach frame of the YouTube video. Based on a sample of manually coded images, we\nfind that this deep-learning approach has 53-60\\% agreement with human labels.\nWe observe statistically significant differences in the average score of\nnegative emotions between groups of leaders with varying degrees of populist\nrhetoric.\n","authors":["Sara Major","Aleksandar Tomaevi"],"pdf_url":"https://arxiv.org/pdf/2304.09914v4.pdf","comment":"Version 4.0: Annotation study added, supplementary information\n  extended"},{"id":"http://arxiv.org/abs/2408.03616v1","updated":"2024-08-07T08:17:34Z","published":"2024-08-07T08:17:34Z","title":"Distillation Learning Guided by Image Reconstruction for One-Shot\n  Medical Image Segmentation","summary":"  Traditional one-shot medical image segmentation (MIS) methods use\nregistration networks to propagate labels from a reference atlas or rely on\ncomprehensive sampling strategies to generate synthetic labeled data for\ntraining. However, these methods often struggle with registration errors and\nlow-quality synthetic images, leading to poor performance and generalization.\nTo overcome this, we introduce a novel one-shot MIS framework based on\nknowledge distillation, which allows the network to directly 'see' real images\nthrough a distillation process guided by image reconstruction. It focuses on\nanatomical structures in a single labeled image and a few unlabeled ones. A\nregistration-based data augmentation network creates realistic, labeled\nsamples, while a feature distillation module helps the student network learn\nsegmentation from these samples, guided by the teacher network. During\ninference, the streamlined student network accurately segments new images.\nEvaluations on three public datasets (OASIS for T1 brain MRI, BCV for abdomen\nCT, and VerSe for vertebrae CT) show superior segmentation performance and\ngeneralization across different medical image datasets and modalities compared\nto leading methods. Our code is available at\nhttps://github.com/NoviceFodder/OS-MedSeg.\n","authors":["Feng Zhou","Yanjie Zhou","Longjie Wang","Yun Peng","David E. Carlson","Liyun Tu"],"pdf_url":"https://arxiv.org/pdf/2408.03616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03612v1","updated":"2024-08-07T08:08:08Z","published":"2024-08-07T08:08:08Z","title":"JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling","summary":"  Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.\n","authors":["Seok Hwan Lee","Taein Son","Soo Won Seo","Jisong Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03612v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2408.03608v1","updated":"2024-08-07T07:54:19Z","published":"2024-08-07T07:54:19Z","title":"InPer: Whole-Process Domain Generalization via Causal Intervention and\n  Perturbation","summary":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03608v1.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2408.03598v1","updated":"2024-08-07T07:35:17Z","published":"2024-08-07T07:35:17Z","title":"PRISM: PRogressive dependency maxImization for Scale-invariant image\n  Matching","summary":"  Image matching aims at identifying corresponding points between a pair of\nimages. Currently, detector-free methods have shown impressive performance in\nchallenging scenarios, thanks to their capability of generating dense matches\nand global receptive field. However, performing feature interaction and\nproposing matches across the entire image is unnecessary, because not all image\nregions contribute to the matching process. Interacting and matching in\nunmatchable areas can introduce errors, reducing matching accuracy and\nefficiency. Meanwhile, the scale discrepancy issue still troubles existing\nmethods. To address above issues, we propose PRogressive dependency\nmaxImization for Scale-invariant image Matching (PRISM), which jointly prunes\nirrelevant patch features and tackles the scale discrepancy. To do this, we\nfirstly present a Multi-scale Pruning Module (MPM) to adaptively prune\nirrelevant features by maximizing the dependency between the two feature sets.\nMoreover, we design the Scale-Aware Dynamic Pruning Attention (SADPA) to\naggregate information from different scales via a hierarchical design. Our\nmethod's superior matching performance and generalization capability are\nconfirmed by leading accuracy across various evaluation benchmarks and\ndownstream tasks. The code is publicly available at\nhttps://github.com/Master-cai/PRISM.\n","authors":["Xudong Cai","Yongcai Wang","Lun Luo","Minhang Wang","Deying Li","Jintao Xu","Weihao Gu","Rui Ai"],"pdf_url":"https://arxiv.org/pdf/2408.03598v1.pdf","comment":"15 pages, 8 figures, ACM MM 2024. Supplementary materials are\n  included"},{"id":"http://arxiv.org/abs/2405.19722v2","updated":"2024-08-07T07:28:02Z","published":"2024-05-30T06:07:57Z","title":"QClusformer: A Quantum Transformer-based Framework for Unsupervised\n  Visual Clustering","summary":"  Unsupervised vision clustering, a cornerstone in computer vision, has been\nstudied for decades, yielding significant outcomes across numerous vision\ntasks. However, these algorithms involve substantial computational demands when\nconfronted with vast amounts of unlabeled data. Conversely, quantum computing\nholds promise in expediting unsupervised algorithms when handling large-scale\ndatabases. In this study, we introduce QClusformer, a pioneering\nTransformer-based framework leveraging quantum machines to tackle unsupervised\nvision clustering challenges. Specifically, we design the Transformer\narchitecture, including the self-attention module and transformer blocks, from\na quantum perspective to enable execution on quantum hardware. In addition, we\npresent QClusformer, a variant based on the Transformer architecture, tailored\nfor unsupervised vision clustering tasks. By integrating these elements into an\nend-to-end framework, QClusformer consistently outperforms previous methods\nrunning on classical computers. Empirical evaluations across diverse\nbenchmarks, including MS-Celeb-1M and DeepFashion, underscore the superior\nperformance of QClusformer compared to state-of-the-art methods.\n","authors":["Xuan-Bac Nguyen","Hoang-Quan Nguyen","Samuel Yen-Chi Chen","Samee U. Khan","Hugh Churchill","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2405.19722v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03596v1","updated":"2024-08-07T07:24:15Z","published":"2024-08-07T07:24:15Z","title":"Hierarchical Quantum Control Gates for Functional MRI Understanding","summary":"  Quantum computing has emerged as a powerful tool for solving complex problems\nintractable for classical computers, particularly in popular fields such as\ncryptography, optimization, and neurocomputing. In this paper, we present a new\nquantum-based approach named the Hierarchical Quantum Control Gates (HQCG)\nmethod for efficient understanding of Functional Magnetic Resonance Imaging\n(fMRI) data. This approach includes two novel modules: the Local Quantum\nControl Gate (LQCG) and the Global Quantum Control Gate (GQCG), which are\ndesigned to extract local and global features of fMRI signals, respectively.\nOur method operates end-to-end on a quantum machine, leveraging quantum\nmechanics to learn patterns within extremely high-dimensional fMRI signals,\nsuch as 30,000 samples which is a challenge for classical computers. Empirical\nresults demonstrate that our approach significantly outperforms classical\nmethods. Additionally, we found that the proposed quantum model is more stable\nand less prone to overfitting than the classical methods.\n","authors":["Xuan-Bac Nguyen","Hoang-Quan Nguyen","Hugh Churchill","Samee U. Khan","Khoa Luu"],"pdf_url":"https://arxiv.org/pdf/2408.03596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03592v1","updated":"2024-08-07T07:12:52Z","published":"2024-08-07T07:12:52Z","title":"HistoSPACE: Histology-Inspired Spatial Transcriptome Prediction And\n  Characterization Engine","summary":"  Spatial transcriptomics (ST) enables the visualization of gene expression\nwithin the context of tissue morphology. This emerging discipline has the\npotential to serve as a foundation for developing tools to design precision\nmedicines. However, due to the higher costs and expertise required for such\nexperiments, its translation into a regular clinical practice might be\nchallenging. Despite the implementation of modern deep learning to enhance\ninformation obtained from histological images using AI, efforts have been\nconstrained by limitations in the diversity of information. In this paper, we\ndeveloped a model, HistoSPACE that explore the diversity of histological images\navailable with ST data to extract molecular insights from tissue image. Our\nproposed study built an image encoder derived from universal image autoencoder.\nThis image encoder was connected to convolution blocks to built the final\nmodel. It was further fine tuned with the help of ST-Data. This model is\nnotably lightweight in compared to traditional histological models. Our\ndeveloped model demonstrates significant efficiency compared to contemporary\nalgorithms, revealing a correlation of 0.56 in leave-one-out cross-validation.\nFinally, its robustness was validated through an independent dataset, showing a\nwell matched preditction with predefined disease pathology.\n","authors":["Shivam Kumar","Samrat Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2408.03592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Bjrn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02922v2","updated":"2024-08-07T06:44:24Z","published":"2024-08-06T03:15:18Z","title":"Pose Magic: Efficient and Temporally Consistent Human Pose Estimation\n  with a Hybrid Mamba-GCN Network","summary":"  Current state-of-the-art (SOTA) methods in 3D Human Pose Estimation (HPE) are\nprimarily based on Transformers. However, existing Transformer-based 3D HPE\nbackbones often encounter a trade-off between accuracy and computational\nefficiency. To resolve the above dilemma, in this work, we leverage recent\nadvances in state space models and utilize Mamba for high-quality and efficient\nlong-range modeling. Nonetheless, Mamba still faces challenges in precisely\nexploiting local dependencies between joints. To address these issues, we\npropose a new attention-free hybrid spatiotemporal architecture named Hybrid\nMamba-GCN (Pose Magic). This architecture introduces local enhancement with GCN\nby capturing relationships between neighboring joints, thus producing new\nrepresentations to complement Mamba's outputs. By adaptively fusing\nrepresentations from Mamba and GCN, Pose Magic demonstrates superior capability\nin learning the underlying 3D structure. To meet the requirements of real-time\ninference, we also provide a fully causal version. Extensive experiments show\nthat Pose Magic achieves new SOTA results ($\\downarrow 0.9 mm$) while saving\n$74.1\\%$ FLOPs. In addition, Pose Magic exhibits optimal motion consistency and\nthe ability to generalize to unseen sequence lengths.\n","authors":["Xinyi Zhang","Qiqi Bao","Qinpeng Cui","Wenming Yang","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2408.02922v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11936v2","updated":"2024-08-07T06:43:51Z","published":"2024-07-16T17:26:50Z","title":"Thermal Imaging and Radar for Remote Sleep Monitoring of Breathing and\n  Apnea","summary":"  Polysomnography (PSG), the current gold standard method for monitoring and\ndetecting sleep disorders, is cumbersome and costly. At-home testing solutions,\nknown as home sleep apnea testing (HSAT), exist. However, they are\ncontact-based, a feature which limits the ability of some patient populations\nto tolerate testing and discourages widespread deployment. Previous work on\nnon-contact sleep monitoring for sleep apnea detection either estimates\nrespiratory effort using radar or nasal airflow using a thermal camera, but has\nnot compared the two or used them together. We conducted a study on 10\nparticipants, ages 34 - 78, with suspected sleep disorders using a hardware\nsetup with a synchronized radar and thermal camera. We show the first\ncomparison of radar and thermal imaging for sleep monitoring, and find that our\nthermal imaging method outperforms radar significantly. Our thermal imaging\nmethod detects apneas with an accuracy of 0.99, a precision of 0.68, a recall\nof 0.74, an F1 score of 0.71, and an intra-class correlation of 0.70; our radar\nmethod detects apneas with an accuracy of 0.83, a precision of 0.13, a recall\nof 0.86, an F1 score of 0.22, and an intra-class correlation of 0.13. We also\npresent a novel proposal for classifying obstructive and central sleep apnea by\nleveraging a multimodal setup. This method could be used accurately detect and\nclassify apneas during sleep with non-contact sensors, thereby improving\ndiagnostic capacities in patient populations unable to tolerate current\ntechnology.\n","authors":["Kai Del Regno","Alexander Vilesov","Adnan Armouti","Anirudh Bindiganavale Harish","Selim Emir Can","Ashley Kita","Achuta Kadambi"],"pdf_url":"https://arxiv.org/pdf/2407.11936v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03568v1","updated":"2024-08-07T06:11:25Z","published":"2024-08-07T06:11:25Z","title":"A comparative study of generative adversarial networks for image\n  recognition algorithms based on deep learning and traditional methods","summary":"  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n","authors":["Yihao Zhong","Yijing Wei","Yingbin Liang","Xiqing Liu","Rongwei Ji","Yiru Cang"],"pdf_url":"https://arxiv.org/pdf/2408.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03567v1","updated":"2024-08-07T06:10:45Z","published":"2024-08-07T06:10:45Z","title":"Unlocking Exocentric Video-Language Data for Egocentric Video\n  Representation Learning","summary":"  We present EMBED (Egocentric Models Built with Exocentric Data), a method\ndesigned to transform exocentric video-language data for egocentric video\nrepresentation learning. Large-scale exocentric data covers diverse activities\nwith significant potential for egocentric learning, but inherent disparities\nbetween egocentric and exocentric data pose challenges in utilizing one view\nfor the other seamlessly. Egocentric videos predominantly feature close-up\nhand-object interactions, whereas exocentric videos offer a broader perspective\non human activities. Additionally, narratives in egocentric datasets are\ntypically more action-centric and closely linked with the visual content, in\ncontrast to the narrative styles found in exocentric datasets. To address these\nchallenges, we employ a data transformation framework to adapt exocentric data\nfor egocentric training, focusing on identifying specific video clips that\nemphasize hand-object interactions and transforming narration styles to align\nwith egocentric perspectives. By applying both vision and language style\ntransfer, our framework creates a new egocentric dataset derived from\nexocentric video-language data. Through extensive evaluations, we demonstrate\nthe effectiveness of EMBED, achieving state-of-the-art results across various\negocentric downstream tasks, including an absolute improvement of 4.7% on the\nEpic-Kitchens-100 multi-instance retrieval and 6.2% on the EGTEA classification\nbenchmarks in zero-shot settings. Furthermore, EMBED enables egocentric\nvideo-language models to perform competitively in exocentric tasks. Finally, we\nshowcase EMBED's application across various exocentric datasets, exhibiting\nstrong generalization capabilities when applied to different exocentric\ndatasets.\n","authors":["Zi-Yi Dou","Xitong Yang","Tushar Nagarajan","Huiyu Wang","Jing Huang","Nanyun Peng","Kris Kitani","Fu-Jen Chu"],"pdf_url":"https://arxiv.org/pdf/2408.03567v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2408.02085v3","updated":"2024-08-07T06:04:31Z","published":"2024-08-04T16:50:07Z","title":"Unleashing the Power of Data Tsunami: A Comprehensive Survey on Data\n  Assessment and Selection for Instruction Tuning of Language Models","summary":"  Instruction tuning plays a critical role in aligning large language models\n(LLMs) with human preference. Despite the vast amount of open instruction\ndatasets, naively training a LLM on all existing instructions may not be\noptimal and practical. To pinpoint the most beneficial datapoints, data\nassessment and selection methods have been proposed in the fields of natural\nlanguage processing (NLP) and deep learning. However, under the context of\ninstruction tuning, there still exists a gap in knowledge on what kind of data\nevaluation metrics can be employed and how they can be integrated into the\nselection mechanism. To bridge this gap, we present a comprehensive review on\nexisting literature of data assessment and selection especially for instruction\ntuning of LLMs. We systematically categorize all applicable methods into\nquality-based, diversity-based, and importance-based ones where a unified,\nfine-grained taxonomy is structured. For each category, representative methods\nare elaborated to describe the landscape of relevant research. In addition,\ncomparison between latest methods is conducted on their officially reported\nresults to provide in-depth discussions on their limitations. Finally, we\nsummarize the open challenges and propose the promosing avenues for future\nstudies. All related contents are available at\nhttps://github.com/yuleiqin/fantastic-data-engineering.\n","authors":["Yulei Qin","Yuncheng Yang","Pengcheng Guo","Gang Li","Hang Shao","Yuchen Shi","Zihan Xu","Yun Gu","Ke Li","Xing Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02085v3.pdf","comment":"review, survey, 28 pages, 2 figures, 4 tables"},{"id":"http://arxiv.org/abs/2402.14461v2","updated":"2024-08-07T06:01:18Z","published":"2024-02-22T11:40:49Z","title":"S^2Former-OR: Single-Stage Bi-Modal Transformer for Scene Graph\n  Generation in OR","summary":"  Scene graph generation (SGG) of surgical procedures is crucial in enhancing\nholistically cognitive intelligence in the operating room (OR). However,\nprevious works have primarily relied on multi-stage learning, where the\ngenerated semantic scene graphs depend on intermediate processes with pose\nestimation and object detection. This pipeline may potentially compromise the\nflexibility of learning multimodal representations, consequently constraining\nthe overall effectiveness. In this study, we introduce a novel single-stage\nbi-modal transformer framework for SGG in the OR, termed S^2Former-OR, aimed to\ncomplementally leverage multi-view 2D scenes and 3D point clouds for SGG in an\nend-to-end manner. Concretely, our model embraces a View-Sync Transfusion\nscheme to encourage multi-view visual information interaction. Concurrently, a\nGeometry-Visual Cohesion operation is designed to integrate the synergic 2D\nsemantic features into 3D point cloud features. Moreover, based on the\naugmented feature, we propose a novel relation-sensitive transformer decoder\nthat embeds dynamic entity-pair queries and relational trait priors, which\nenables the direct prediction of entity-pair relations for graph generation\nwithout intermediate steps. Extensive experiments have validated the superior\nSGG performance and lower computational cost of S^2Former-OR on 4D-OR\nbenchmark, compared with current OR-SGG methods, e.g., 3 percentage points\nPrecision increase and 24.2M reduction in model parameters. We further compared\nour method with generic single-stage SGG methods with broader metrics for a\ncomprehensive evaluation, with consistently better performance achieved.\n","authors":["Jialun Pei","Diandian Guo","Jingyang Zhang","Manxi Lin","Yueming Jin","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2402.14461v2.pdf","comment":"This work has been accepted by TMI2024"},{"id":"http://arxiv.org/abs/2408.03564v1","updated":"2024-08-07T05:56:05Z","published":"2024-08-07T05:56:05Z","title":"Underwater litter monitoring using consumer-grade aerial-aquatic speedy\n  scanner (AASS) and deep learning based super-resolution reconstruction and\n  detection network","summary":"  Underwater litter is widely spread across aquatic environments such as lakes,\nrivers, and oceans, significantly impacting natural ecosystems. Current\nmonitoring technologies for detecting underwater litter face limitations in\nsurvey efficiency, cost, and environmental conditions, highlighting the need\nfor efficient, consumer-grade technologies for automatic detection. This\nresearch introduces the Aerial-Aquatic Speedy Scanner (AASS) combined with\nSuper-Resolution Reconstruction (SRR) and an improved YOLOv8 detection network.\nAASS enhances data acquisition efficiency over traditional methods, capturing\nhigh-quality images that accurately identify underwater waste. SRR improves\nimage-resolution by mitigating motion blur and insufficient resolution, thereby\nenhancing detection tasks. Specifically, the RCAN model achieved the highest\nmean average precision (mAP) of 78.6% for detection accuracy on reconstructed\nimages among the tested SRR models. With a magnification factor of 4, the SRR\ntest set shows an improved mAP compared to the conventional bicubic set. These\nresults demonstrate the effectiveness of the proposed method in detecting\nunderwater litter.\n","authors":["Fan Zhao","Yongying Liu","Jiaqi Wang","Yijia Chen","Dianhan Xi","Xinlei Shao","Shigeru Tabeta","Katsunori Mizuno"],"pdf_url":"https://arxiv.org/pdf/2408.03564v1.pdf","comment":"The earlier version of this conference paper was accepted at OCEANS\n  2024-Halifax, Canada and was selected for inclusion in the Student Poster\n  Competition (SPC) Program"},{"id":"http://arxiv.org/abs/2403.19026v3","updated":"2024-08-07T05:54:04Z","published":"2024-03-27T21:43:12Z","title":"EgoNav: Egocentric Scene-aware Human Trajectory Prediction","summary":"  Wearable collaborative robots stand to assist human wearers who need fall\nprevention assistance or wear exoskeletons. Such a robot needs to be able to\nconstantly adapt to the surrounding scene based on egocentric vision, and\npredict the ego motion of the wearer. In this work, we leveraged body-mounted\ncameras and sensors to anticipate the trajectory of human wearers through\ncomplex surroundings. To facilitate research in ego-motion prediction, we have\ncollected a comprehensive walking scene navigation dataset centered on the\nuser's perspective. We then present a method to predict human motion\nconditioning on the surrounding static scene. Our method leverages a diffusion\nmodel to produce a distribution of potential future trajectories, taking into\naccount the user's observation of the environment. To that end, we introduce a\ncompact representation to encode the user's visual memory of the surroundings,\nas well as an efficient sample-generating technique to speed up real-time\ninference of a diffusion model. We ablate our model and compare it to\nbaselines, and results show that our model outperforms existing methods on key\nmetrics of collision avoidance and trajectory mode coverage.\n","authors":["Weizhuo Wang","C. Karen Liu","Monroe Kennedy III"],"pdf_url":"https://arxiv.org/pdf/2403.19026v3.pdf","comment":"13 pages, 9 figures"},{"id":"http://arxiv.org/abs/2408.03559v1","updated":"2024-08-07T05:47:15Z","published":"2024-08-07T05:47:15Z","title":"Monitoring of Hermit Crabs Using drone-captured imagery and Deep\n  Learning based Super-Resolution Reconstruction and Improved YOLOv8","summary":"  Hermit crabs play a crucial role in coastal ecosystems by dispersing seeds,\ncleaning up debris, and disturbing soil. They serve as vital indicators of\nmarine environmental health, responding to climate change and pollution.\nTraditional survey methods, like quadrat sampling, are labor-intensive,\ntime-consuming, and environmentally dependent. This study presents an\ninnovative approach combining UAV-based remote sensing with Super-Resolution\nReconstruction (SRR) and the CRAB-YOLO detection network, a modification of\nYOLOv8s, to monitor hermit crabs. SRR enhances image quality by addressing\nissues such as motion blur and insufficient resolution, significantly improving\ndetection accuracy over conventional low-resolution fuzzy images. The CRAB-YOLO\nnetwork integrates three improvements for detection accuracy, hermit crab\ncharacteristics, and computational efficiency, achieving state-of-the-art\n(SOTA) performance compared to other mainstream detection models. The RDN\nnetworks demonstrated the best image reconstruction performance, and CRAB-YOLO\nachieved a mean average precision (mAP) of 69.5% on the SRR test set, a 40%\nimprovement over the conventional Bicubic method with a magnification factor of\n4. These results indicate that the proposed method is effective in detecting\nhermit crabs, offering a cost-effective and automated solution for extensive\nhermit crab monitoring, thereby aiding coastal benthos conservation.\n","authors":["Fan Zhao","Yijia Chen","Dianhan Xi","Yongying Liu","Jiaqi Wang","Shigeru Tabeta","Katsunori Mizuno"],"pdf_url":"https://arxiv.org/pdf/2408.03559v1.pdf","comment":"The earlier version of this conference paper was presented at OCEANS\n  2024-Singapore and was selected for inclusion in the Student Poster\n  Competition (SPC) Program"},{"id":"http://arxiv.org/abs/2408.03558v1","updated":"2024-08-07T05:47:06Z","published":"2024-08-07T05:47:06Z","title":"D2Styler: Advancing Arbitrary Style Transfer with Discrete Diffusion\n  Methods","summary":"  In image processing, one of the most challenging tasks is to render an\nimage's semantic meaning using a variety of artistic approaches. Existing\ntechniques for arbitrary style transfer (AST) frequently experience\nmode-collapse, over-stylization, or under-stylization due to a disparity\nbetween the style and content images. We propose a novel framework called\nD$^2$Styler (Discrete Diffusion Styler) that leverages the discrete\nrepresentational capability of VQ-GANs and the advantages of discrete\ndiffusion, including stable training and avoidance of mode collapse. Our method\nuses Adaptive Instance Normalization (AdaIN) features as a context guide for\nthe reverse diffusion process. This makes it easy to move features from the\nstyle image to the content image without bias. The proposed method\nsubstantially enhances the visual quality of style-transferred images, allowing\nthe combination of content and style in a visually appealing manner. We take\nstyle images from the WikiArt dataset and content images from the COCO dataset.\nExperimental results demonstrate that D$^2$Styler produces high-quality\nstyle-transferred images and outperforms twelve existing methods on nearly all\nthe metrics. The qualitative results and ablation studies provide further\ninsights into the efficacy of our technique. The code is available at\nhttps://github.com/Onkarsus13/D2Styler.\n","authors":["Onkar Susladkar","Gayatri Deshmukh","Sparsh Mittal","Parth Shastri"],"pdf_url":"https://arxiv.org/pdf/2408.03558v1.pdf","comment":"Paper accepted at 27th International Conference on Pattern\n  Recognition (ICPR), 2024"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2408.03551v1","updated":"2024-08-07T05:23:52Z","published":"2024-08-07T05:23:52Z","title":"VPOcc: Exploiting Vanishing Point for Monocular 3D Semantic Occupancy\n  Prediction","summary":"  Monocular 3D semantic occupancy prediction is becoming important in robot\nvision due to the compactness of using a single RGB camera. However, existing\nmethods often do not adequately account for camera perspective geometry,\nresulting in information imbalance along the depth range of the image. To\naddress this issue, we propose a vanishing point (VP) guided monocular 3D\nsemantic occupancy prediction framework named VPOcc. Our framework consists of\nthree novel modules utilizing VP. First, in the VPZoomer module, we initially\nutilize VP in feature extraction to achieve information balanced feature\nextraction across the scene by generating a zoom-in image based on VP. Second,\nwe perform perspective geometry-aware feature aggregation by sampling points\ntowards VP using a VP-guided cross-attention (VPCA) module. Finally, we create\nan information-balanced feature volume by effectively fusing original and\nzoom-in voxel feature volumes with a balanced feature volume fusion (BVFV)\nmodule. Experiments demonstrate that our method achieves state-of-the-art\nperformance for both IoU and mIoU on SemanticKITTI and SSCBench-KITTI360. These\nresults are obtained by effectively addressing the information imbalance in\nimages through the utilization of VP. Our code will be available at\nwww.github.com/anonymous.\n","authors":["Junsu Kim","Junhee Lee","Ukcheol Shin","Jean Oh","Kyungdon Joo"],"pdf_url":"https://arxiv.org/pdf/2408.03551v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.01089v4","updated":"2024-08-07T05:22:57Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v4.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2404.06605v3","updated":"2024-08-07T05:10:43Z","published":"2024-04-09T20:24:29Z","title":"RoadBEV: Road Surface Reconstruction in Bird's Eye View","summary":"  Road surface conditions, especially geometry profiles, enormously affect\ndriving performance of autonomous vehicles. Vision-based online road\nreconstruction promisingly captures road information in advance. Existing\nsolutions like monocular depth estimation and stereo matching suffer from\nmodest performance. The recent technique of Bird's-Eye-View (BEV) perception\nprovides immense potential to more reliable and accurate reconstruction. This\npaper uniformly proposes two simple yet effective models for road elevation\nreconstruction in BEV named RoadBEV-mono and RoadBEV-stereo, which estimate\nroad elevation with monocular and stereo images, respectively. The former\ndirectly fits elevation values based on voxel features queried from image view,\nwhile the latter efficiently recognizes road elevation patterns based on BEV\nvolume representing correlation between left and right voxel features.\nInsightful analyses reveal their consistence and difference with the\nperspective view. Experiments on real-world dataset verify the models'\neffectiveness and superiority. Elevation errors of RoadBEV-mono and\nRoadBEV-stereo achieve 1.83 cm and 0.50 cm, respectively. Our models are\npromising for practical road preview, providing essential information for\npromoting safety and comfort of autonomous vehicles. The code is released at\nhttps://github.com/ztsrxh/RoadBEV\n","authors":["Tong Zhao","Lei Yang","Yichen Xie","Mingyu Ding","Masayoshi Tomizuka","Yintao Wei"],"pdf_url":"https://arxiv.org/pdf/2404.06605v3.pdf","comment":"Accepted by IEEE TITS https://ieeexplore.ieee.org/document/10618926"},{"id":"http://arxiv.org/abs/2305.10856v3","updated":"2024-08-07T04:59:01Z","published":"2023-05-18T10:18:59Z","title":"Spatial-Frequency Discriminability for Revealing Adversarial\n  Perturbations","summary":"  The vulnerability of deep neural networks to adversarial perturbations has\nbeen widely perceived in the computer vision community. From a security\nperspective, it poses a critical risk for modern vision systems, e.g., the\npopular Deep Learning as a Service (DLaaS) frameworks. For protecting deep\nmodels while not modifying them, current algorithms typically detect\nadversarial patterns through discriminative decomposition for natural and\nadversarial data. However, these decompositions are either biased towards\nfrequency resolution or spatial resolution, thus failing to capture adversarial\npatterns comprehensively. Also, when the detector relies on few fixed features,\nit is practical for an adversary to fool the model while evading the detector\n(i.e., defense-aware attack). Motivated by such facts, we propose a\ndiscriminative detector relying on a spatial-frequency Krawtchouk\ndecomposition. It expands the above works from two aspects: 1) the introduced\nKrawtchouk basis provides better spatial-frequency discriminability, capturing\nthe differences between natural and adversarial data comprehensively in both\nspatial and frequency distributions, w.r.t. the common trigonometric or wavelet\nbasis; 2) the extensive features formed by the Krawtchouk decomposition allows\nfor adaptive feature selection and secrecy mechanism, significantly increasing\nthe difficulty of the defense-aware attack, w.r.t. the detector with few fixed\nfeatures. Theoretical and numerical analyses demonstrate the uniqueness and\nusefulness of our detector, exhibiting competitive scores on several deep\nmodels and image sets against a variety of adversarial attacks.\n","authors":["Chao Wang","Shuren Qi","Zhiqiu Huang","Yushu Zhang","Rushi Lan","Xiaochun Cao","Feng-Lei Fan"],"pdf_url":"https://arxiv.org/pdf/2305.10856v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03545v1","updated":"2024-08-07T04:50:05Z","published":"2024-08-07T04:50:05Z","title":"CLIP-based Point Cloud Classification via Point Cloud to Image\n  Translation","summary":"  Point cloud understanding is an inherently challenging problem because of the\nsparse and unordered structure of the point cloud in the 3D space. Recently,\nContrastive Vision-Language Pre-training (CLIP) based point cloud\nclassification model i.e. PointCLIP has added a new direction in the point\ncloud classification research domain. In this method, at first multi-view depth\nmaps are extracted from the point cloud and passed through the CLIP visual\nencoder. To transfer the 3D knowledge to the network, a small network called an\nadapter is fine-tuned on top of the CLIP visual encoder. PointCLIP has two\nlimitations. Firstly, the point cloud depth maps lack image information which\nis essential for tasks like classification and recognition. Secondly, the\nadapter only relies on the global representation of the multi-view features.\nMotivated by this observation, we propose a Pretrained Point Cloud to Image\nTranslation Network (PPCITNet) that produces generalized colored images along\nwith additional salient visual cues to the point cloud depth maps so that it\ncan achieve promising performance on point cloud classification and\nunderstanding. In addition, we propose a novel viewpoint adapter that combines\nthe view feature processed by each viewpoint as well as the global intertwined\nknowledge that exists across the multi-view features. The experimental results\ndemonstrate the superior performance of the proposed model over existing\nstate-of-the-art CLIP-based models on ModelNet10, ModelNet40, and ScanobjectNN\ndatasets.\n","authors":["Shuvozit Ghose","Manyi Li","Yiming Qian","Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03545v1.pdf","comment":"Accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2408.03542v1","updated":"2024-08-07T04:42:10Z","published":"2024-08-07T04:42:10Z","title":"Automatic identification of the area covered by acorn trees in the\n  dehesa (pastureland) Extremadura of Spain","summary":"  The acorn is the fruit of the oak and is an important crop in the Spanish\ndehesa extreme\\~na, especially for the value it provides in the Iberian pig\nfood to obtain the \"acorn\" certification. For this reason, we want to maximise\nthe production of Iberian pigs with the appropriate weight. Hence the need to\nknow the area covered by the crowns of the acorn trees, to determine the\ncovered wooded area (CWA, from the Spanish Superficie Arbolada Cubierta SAC)\nand thereby estimate the number of Iberian pigs that can be released per\nhectare, as indicated by the royal decree 4/2014. In this work, we propose the\nautomatic estimation of the CWA, through aerial digital images (orthophotos) of\nthe pastureland of Extremadura, and with this, to offer the possibility of\ndetermining the number of Iberian pigs to be released in a specific plot of\nland. Among the main issues for automatic detection are, first, the correct\nidentification of acorn trees, secondly, correctly discriminating the shades of\nthe acorn trees and, finally, detect the arbuscles (young acorn trees not yet\nproductive, or shrubs that are not oaks). These difficulties represent a real\nchallenge, both for the automatic segmentation process and for manual\nsegmentation. In this work, the proposed method for automatic segmentation is\nbased on the clustering algorithm proposed by Gustafson-Kessel (GK) but the\nmodified version of Babuska (GK-B) and on the use of real orthophotos. The\nobtained results are promising both in their comparison with the real images\nand when compared with the images segmented by hand. The whole set of\northophotos used in this work correspond to an approximate area of 142\nhectares, and the results are of great interest to producers of certified\n\"acorn\" pork.\n","authors":["Ojeda-Magaa Benjamin","Ruelas Ruben","Quintanilla-Dominguez Joel","Gomez-Barba Leopoldo","Lopez de Herrera Juan","Robledo-Hernandez Jose","Tarquis Ana"],"pdf_url":"https://arxiv.org/pdf/2408.03542v1.pdf","comment":"22 pages, 15 Figures, 2 Tables"},{"id":"http://arxiv.org/abs/2408.03540v1","updated":"2024-08-07T04:38:03Z","published":"2024-08-07T04:38:03Z","title":"PoseMamba: Monocular 3D Human Pose Estimation with Bidirectional\n  Global-Local Spatio-Temporal State Space Model","summary":"  Transformers have significantly advanced the field of 3D human pose\nestimation (HPE). However, existing transformer-based methods primarily use\nself-attention mechanisms for spatio-temporal modeling, leading to a quadratic\ncomplexity, unidirectional modeling of spatio-temporal relationships, and\ninsufficient learning of spatial-temporal correlations. Recently, the Mamba\narchitecture, utilizing the state space model (SSM), has exhibited superior\nlong-range modeling capabilities in a variety of vision tasks with linear\ncomplexity. In this paper, we propose PoseMamba, a novel purely SSM-based\napproach with linear complexity for 3D human pose estimation in monocular\nvideo. Specifically, we propose a bidirectional global-local spatio-temporal\nSSM block that comprehensively models human joint relations within individual\nframes as well as temporal correlations across frames. Within this\nbidirectional global-local spatio-temporal SSM block, we introduce a reordering\nstrategy to enhance the local modeling capability of the SSM. This strategy\nprovides a more logical geometric scanning order and integrates it with the\nglobal SSM, resulting in a combined global-local spatial scan. We have\nquantitatively and qualitatively evaluated our approach using two benchmark\ndatasets: Human3.6M and MPI-INF-3DHP. Extensive experiments demonstrate that\nPoseMamba achieves state-of-the-art performance on both datasets while\nmaintaining a smaller model size and reducing computational costs. The code and\nmodels will be released.\n","authors":["Yunlong Huang","Junshuo Liu","Ke Xian","Robert Caiming Qiu"],"pdf_url":"https://arxiv.org/pdf/2408.03540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03538v1","updated":"2024-08-07T04:35:06Z","published":"2024-08-07T04:35:06Z","title":"PRTGS: Precomputed Radiance Transfer of Gaussian Splats for Real-Time\n  High-Quality Relighting","summary":"  We proposed Precomputed RadianceTransfer of GaussianSplats (PRTGS), a\nreal-time high-quality relighting method for Gaussian splats in low-frequency\nlighting environments that captures soft shadows and interreflections by\nprecomputing 3D Gaussian splats' radiance transfer. Existing studies have\ndemonstrated that 3D Gaussian splatting (3DGS) outperforms neural fields'\nefficiency for dynamic lighting scenarios. However, the current relighting\nmethod based on 3DGS still struggles to compute high-quality shadow and\nindirect illumination in real time for dynamic light, leading to unrealistic\nrendering results. We solve this problem by precomputing the expensive\ntransport simulations required for complex transfer functions like shadowing,\nthe resulting transfer functions are represented as dense sets of vectors or\nmatrices for every Gaussian splat. We introduce distinct precomputing methods\ntailored for training and rendering stages, along with unique ray tracing and\nindirect lighting precomputation techniques for 3D Gaussian splats to\naccelerate training speed and compute accurate indirect lighting related to\nenvironment light. Experimental analyses demonstrate that our approach achieves\nstate-of-the-art visual quality while maintaining competitive training times\nand allows high-quality real-time (30+ fps) relighting for dynamic light and\nrelatively complex scenes at 1080p resolution.\n","authors":["Yijia Guo","Yuanxi Bai","Liwen Hu","Ziyi Guo","Mianzhi Liu","Yu Cai","Tiejun Huang","Lei Ma"],"pdf_url":"https://arxiv.org/pdf/2408.03538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04346v2","updated":"2024-08-07T04:00:57Z","published":"2024-07-05T08:37:10Z","title":"MobileFlow: A Multimodal LLM For Mobile GUI Agent","summary":"  Currently, the integration of mobile Graphical User Interfaces (GUIs) is\nubiquitous in most people's daily lives. And the ongoing evolution of\nmultimodal large-scale models, such as GPT-4v, Qwen-VL-Max, has significantly\nbolstered the capabilities of GUI comprehension and user action analysis,\nshowcasing the potentiality of intelligent GUI assistants. However, current GUI\nAgents often need to access page layout information through calling system\nAPIs, which may pose privacy risks. Fixing GUI (such as mobile interfaces) to a\ncertain low resolution might result in the loss of fine-grained image details.\nAt the same time, the multimodal large models built for GUI Agents currently\nhave poor understanding and decision-making abilities for Chinese GUI\ninterfaces, making them difficult to apply to a large number of Chinese apps.\nThis paper introduces MobileFlow, a multimodal large language model\nmeticulously crafted for mobile GUI agents. Transforming from the open-source\nmodel Qwen-VL-Chat into GUI domain, MobileFlow contains approximately 21\nbillion parameters and is equipped with novel hybrid visual encoders, making it\npossible for variable resolutions of image inputs and good support for\nmultilingual GUI. By incorporating Mixture of Experts (MoE) expansions and\npioneering alignment training strategies, MobileFlow has the capacity to fully\ninterpret image data and comprehend user instructions for GUI interaction\ntasks. Finally, MobileFlow outperforms Qwen-VL-Max and GPT-4v in terms of task\nexecution by GUI agents on both public and our proposed evaluation metrics, and\nhas been successfully deployed in real-world business contexts, proving its\neffectiveness for practical applications.\n","authors":["Songqin Nong","Jiali Zhu","Rui Wu","Jiongchao Jin","Shuo Shan","Xiutian Huang","Wenhao Xu"],"pdf_url":"https://arxiv.org/pdf/2407.04346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18403v5","updated":"2024-08-07T03:30:30Z","published":"2023-05-28T15:15:48Z","title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient\n  Fine-Tuning","summary":"  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional\nperformance across various tasks through fine-tuning. Although low-rank\nadaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream\ntasks, their deployment is still hindered by the vast model scale and\ncomputational costs. Post-training model pruning offers a way to compress LLMs.\nHowever, the current pruning methods designed for LLMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LLMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate structured pruned model in a highly memory-efficient manner.\nSpecifically, we first design a LoRA-guided pruning criterion, which uses the\nweights and gradients of LoRA, rather than the gradients of pre-trained weights\nfor importance estimation. We subsequently integrate this criterion into an\niterative pruning process, effectively removing redundant channels and heads.\nExtensive experimental results demonstrate the superior performance of our\nLoRAPrune over existing approaches on the LLaMA series models. At a 50\\%\ncompression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,\nachieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while\nalso decreasing memory usage by 52.6%. Besides, LoRAPrune also matches\nsemi-structural pruning across multiple LLMs, proving its wide applicability.\nThe code is available at https://github.com/aim-uofa/LoRAPrune.\n","authors":["Mingyang Zhang","Hao Chen","Chunhua Shen","Zhen Yang","Linlin Ou","Xinyi Yu","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2305.18403v5.pdf","comment":"accepted by acl 2024 findings"},{"id":"http://arxiv.org/abs/2408.03521v1","updated":"2024-08-07T03:16:33Z","published":"2024-08-07T03:16:33Z","title":"SwinShadow: Shifted Window for Ambiguous Adjacent Shadow Detection","summary":"  Shadow detection is a fundamental and challenging task in many computer\nvision applications. Intuitively, most shadows come from the occlusion of light\nby the object itself, resulting in the object and its shadow being contiguous\n(referred to as the adjacent shadow in this paper). In this case, when the\ncolor of the object is similar to that of the shadow, existing methods struggle\nto achieve accurate detection. To address this problem, we present SwinShadow,\na transformer-based architecture that fully utilizes the powerful shifted\nwindow mechanism for detecting adjacent shadows. The mechanism operates in two\nsteps. Initially, it applies local self-attention within a single window,\nenabling the network to focus on local details. Subsequently, it shifts the\nattention windows to facilitate inter-window attention, enabling the capture of\na broader range of adjacent information. These combined steps significantly\nimprove the network's capacity to distinguish shadows from nearby objects. And\nthe whole process can be divided into three parts: encoder, decoder, and\nfeature integration. During encoding, we adopt Swin Transformer to acquire\nhierarchical features. Then during decoding, for shallow layers, we propose a\ndeep supervision (DS) module to suppress the false positives and boost the\nrepresentation capability of shadow features for subsequent processing, while\nfor deep layers, we leverage a double attention (DA) module to integrate local\nand shifted window in one stage to achieve a larger receptive field and enhance\nthe continuity of information. Ultimately, a new multi-level aggregation (MLA)\nmechanism is applied to fuse the decoded features for mask prediction.\nExtensive experiments on three shadow detection benchmark datasets, SBU, UCF,\nand ISTD, demonstrate that our network achieves good performance in terms of\nbalance error rate (BER).\n","authors":["Yonghui Wang","Shaokai Liu","Li Li","Wengang Zhou","Houqiang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03521v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03516v1","updated":"2024-08-07T02:54:43Z","published":"2024-08-07T02:54:43Z","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","summary":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12223v2","updated":"2024-08-07T02:51:50Z","published":"2024-04-06T03:02:47Z","title":"Cascaded Multi-path Shortcut Diffusion Model for Medical Image\n  Translation","summary":"  Image-to-image translation is a vital component in medical imaging\nprocessing, with many uses in a wide range of imaging modalities and clinical\nscenarios. Previous methods include Generative Adversarial Networks (GANs) and\nDiffusion Models (DMs), which offer realism but suffer from instability and\nlack uncertainty estimation. Even though both GAN and DM methods have\nindividually exhibited their capability in medical image translation tasks, the\npotential of combining a GAN and DM to further improve translation performance\nand to enable uncertainty estimation remains largely unexplored. In this work,\nwe address these challenges by proposing a Cascade Multi-path Shortcut\nDiffusion Model (CMDM) for high-quality medical image translation and\nuncertainty estimation. To reduce the required number of iterations and ensure\nrobust performance, our method first obtains a conditional GAN-generated prior\nimage that will be used for the efficient reverse translation with a DM in the\nsubsequent step. Additionally, a multi-path shortcut diffusion strategy is\nemployed to refine translation results and estimate uncertainty. A cascaded\npipeline further enhances translation quality, incorporating residual averaging\nbetween cascades. We collected three different medical image datasets with two\nsub-tasks for each dataset to test the generalizability of our approach. Our\nexperimental results found that CMDM can produce high-quality translations\ncomparable to state-of-the-art methods while providing reasonable uncertainty\nestimations that correlate well with the translation error.\n","authors":["Yinchi Zhou","Tianqi Chen","Jun Hou","Huidong Xie","Nicha C. Dvornek","S. Kevin Zhou","David L. Wilson","James S. Duncan","Chi Liu","Bo Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.12223v2.pdf","comment":"Accepted at Medical Image Analysis Journal"},{"id":"http://arxiv.org/abs/2304.06345v3","updated":"2024-08-07T02:46:46Z","published":"2023-04-13T08:52:34Z","title":"ASR: Attention-alike Structural Re-parameterization","summary":"  The structural re-parameterization (SRP) technique is a novel deep learning\ntechnique that achieves interconversion between different network architectures\nthrough equivalent parameter transformations. This technique enables the\nmitigation of the extra costs for performance improvement during training, such\nas parameter size and inference time, through these transformations during\ninference, and therefore SRP has great potential for industrial and practical\napplications. The existing SRP methods have successfully considered many\ncommonly used architectures, such as normalizations, pooling methods, and\nmulti-branch convolution. However, the widely used attention modules which\ndrastically slow inference speed cannot be directly implemented by SRP due to\nthese modules usually act on the backbone network in a multiplicative manner\nand the modules' output is input-dependent during inference, which limits the\napplication scenarios of SRP. In this paper, we conduct extensive experiments\nfrom a statistical perspective and discover an interesting phenomenon Stripe\nObservation, which reveals that channel attention values quickly approach some\nconstant vectors during training. This observation inspires us to propose a\nsimple-yet-effective attention-alike structural re-parameterization (ASR) that\nallows us to achieve SRP for a given network while enjoying the effectiveness\nof the attention mechanism. Extensive experiments conducted on several standard\nbenchmarks demonstrate the effectiveness of ASR in generally improving the\nperformance of existing backbone networks, attention modules, and SRP methods\nwithout any elaborated model crafting. We also analyze the limitations and\nprovide experimental and theoretical evidence for the strong robustness of the\nproposed ASR.\n","authors":["Shanshan Zhong","Zhongzhan Huang","Wushao Wen","Jinghui Qin","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2304.06345v3.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.03867v2","updated":"2024-08-07T02:32:40Z","published":"2023-11-07T10:31:41Z","title":"Supervised domain adaptation for building extraction from off-nadir\n  aerial images","summary":"  Building extraction $-$ needed for inventory management and planning of urban\nenvironment $-$ is affected by the misalignment between labels and off-nadir\nsource imagery in training data. Teacher-Student learning of noise-tolerant\nconvolutional neural networks (CNNs) is the existing solution, but the Student\nnetworks typically have lower accuracy and cannot surpass the Teacher's\nperformance. This paper proposes a supervised domain adaptation (SDA) of\nencoder-decoder networks (EDNs) between noisy and clean datasets to tackle the\nproblem. EDNs are configured with high-performing lightweight encoders such as\nEfficientNet, ResNeSt, and MobileViT. The proposed method is compared against\nthe existing Teacher-Student learning methods like knowledge distillation (KD)\nand deep mutual learning (DML) with three newly developed datasets. The methods\nare evaluated for different urban buildings (low-rise, mid-rise, high-rise, and\nskyscrapers), where misalignment increases with the increase in building height\nand spatial resolution. For a robust experimental design, 43 lightweight CNNs,\nfive optimisers, nine loss functions, and seven EDNs are benchmarked to obtain\nthe best-performing EDN for SDA. The SDA of the best-performing EDN from our\nstudy significantly outperformed KD and DML with up to 0.943, 0.868, 0.912, and\n0.697 F1 scores in the low-rise, mid-rise, high-rise, and skyscrapers\nrespectively. The proposed method and the experimental findings will be\nbeneficial in training robust CNNs for building extraction.\n","authors":["Bipul Neupane","Jagannath Aryal","Abbas Rajabifard"],"pdf_url":"https://arxiv.org/pdf/2311.03867v2.pdf","comment":"This work has been submitted to Elsevier for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03511v1","updated":"2024-08-07T02:28:37Z","published":"2024-08-07T02:28:37Z","title":"MoExtend: Tuning New Experts for Modality and Task Extension","summary":"  Large language models (LLMs) excel in various tasks but are primarily trained\non text data, limiting their application scope. Expanding LLM capabilities to\ninclude vision-language understanding is vital, yet training them on multimodal\ndata from scratch is challenging and costly. Existing instruction tuning\nmethods, e.g., LLAVA, often connects a pretrained CLIP vision encoder and LLMs\nvia fully fine-tuning LLMs to bridge the modality gap. However, full\nfine-tuning is plagued by catastrophic forgetting, i.e., forgetting previous\nknowledge, and high training costs particularly in the era of increasing tasks\nand modalities. To solve this issue, we introduce MoExtend, an effective\nframework designed to streamline the modality adaptation and extension of\nMixture-of-Experts (MoE) models. MoExtend seamlessly integrates new experts\ninto pre-trained MoE models, endowing them with novel knowledge without the\nneed to tune pretrained models such as MoE and vision encoders. This approach\nenables rapid adaptation and extension to new modal data or tasks, effectively\naddressing the challenge of accommodating new modalities within LLMs.\nFurthermore, MoExtend avoids tuning pretrained models, thus mitigating the risk\nof catastrophic forgetting. Experimental results demonstrate the efficacy and\nefficiency of MoExtend in enhancing the multimodal capabilities of LLMs,\ncontributing to advancements in multimodal AI research. Code:\nhttps://github.com/zhongshsh/MoExtend.\n","authors":["Shanshan Zhong","Shanghua Gao","Zhongzhan Huang","Wushao Wen","Marinka Zitnik","Pan Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03511v1.pdf","comment":"ACL 2024 - SRW"},{"id":"http://arxiv.org/abs/2408.03507v1","updated":"2024-08-07T02:18:39Z","published":"2024-08-07T02:18:39Z","title":"GUI Element Detection Using SOTA YOLO Deep Learning Models","summary":"  Detection of Graphical User Interface (GUI) elements is a crucial task for\nautomatic code generation from images and sketches, GUI testing, and GUI\nsearch. Recent studies have leveraged both old-fashioned and modern computer\nvision (CV) techniques. Oldfashioned methods utilize classic image processing\nalgorithms (e.g. edge detection and contour detection) and modern methods use\nmature deep learning solutions for general object detection tasks. GUI element\ndetection, however, is a domain-specific case of object detection, in which\nobjects overlap more often, and are located very close to each other, plus the\nnumber of object classes is considerably lower, yet there are more objects in\nthe images compared to natural images. Hence, the studies that have been\ncarried out on comparing various object detection models, might not apply to\nGUI element detection. In this study, we evaluate the performance of the four\nmost recent successful YOLO models for general object detection tasks on GUI\nelement detection and investigate their accuracy performance in detecting\nvarious GUI elements.\n","authors":["Seyed Shayan Daneshvar","Shaowei Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03507v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03503v1","updated":"2024-08-07T02:03:32Z","published":"2024-08-07T02:03:32Z","title":"Opening the Black Box of 3D Reconstruction Error Analysis with VECTOR","summary":"  Reconstruction of 3D scenes from 2D images is a technical challenge that\nimpacts domains from Earth and planetary sciences and space exploration to\naugmented and virtual reality. Typically, reconstruction algorithms first\nidentify common features across images and then minimize reconstruction errors\nafter estimating the shape of the terrain. This bundle adjustment (BA) step\noptimizes around a single, simplifying scalar value that obfuscates many\npossible causes of reconstruction errors (e.g., initial estimate of the\nposition and orientation of the camera, lighting conditions, ease of feature\ndetection in the terrain). Reconstruction errors can lead to inaccurate\nscientific inferences or endanger a spacecraft exploring a remote environment.\nTo address this challenge, we present VECTOR, a visual analysis tool that\nimproves error inspection for stereo reconstruction BA. VECTOR provides\nanalysts with previously unavailable visibility into feature locations, camera\npose, and computed 3D points. VECTOR was developed in partnership with the\nPerseverance Mars Rover and Ingenuity Mars Helicopter terrain reconstruction\nteam at the NASA Jet Propulsion Laboratory. We report on how this tool was used\nto debug and improve terrain reconstruction for the Mars 2020 mission.\n","authors":["Racquel Fygenson","Kazi Jawad","Isabel Li","Francois Ayoub","Robert G. Deen","Scott Davidoff","Dominik Moritz","Mauricio Hess-Flores"],"pdf_url":"https://arxiv.org/pdf/2408.03503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03500v1","updated":"2024-08-07T01:59:25Z","published":"2024-08-07T01:59:25Z","title":"e-Health CSIRO at RRG24: Entropy-Augmented Self-Critical Sequence\n  Training for Radiology Report Generation","summary":"  The Shared Task on Large-Scale Radiology Report Generation (RRG24) aims to\nexpedite the development of assistive systems for interpreting and reporting on\nchest X-ray (CXR) images. This task challenges participants to develop models\nthat generate the findings and impression sections of radiology reports from\nCXRs from a patient's study, using five different datasets. This paper outlines\nthe e-Health CSIRO team's approach, which achieved multiple first-place\nfinishes in RRG24. The core novelty of our approach lies in the addition of\nentropy regularisation to self-critical sequence training, to maintain a higher\nentropy in the token distribution. This prevents overfitting to common phrases\nand ensures a broader exploration of the vocabulary during training, essential\nfor handling the diversity of the radiology reports in the RRG24 datasets. Our\nmodel is available on Hugging Face https://huggingface.co/aehrc/cxrmate-rrg24.\n","authors":["Aaron Nicolson","Jinghui Liu","Jason Dowling","Anthony Nguyen","Bevan Koopman"],"pdf_url":"https://arxiv.org/pdf/2408.03500v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03499v1","updated":"2024-08-07T01:50:34Z","published":"2024-08-07T01:50:34Z","title":"FacialPulse: An Efficient RNN-based Depression Detection via Temporal\n  Facial Landmarks","summary":"  Depression is a prevalent mental health disorder that significantly impacts\nindividuals' lives and well-being. Early detection and intervention are crucial\nfor effective treatment and management of depression. Recently, there are many\nend-to-end deep learning methods leveraging the facial expression features for\nautomatic depression detection. However, most current methods overlook the\ntemporal dynamics of facial expressions. Although very recent 3DCNN methods\nremedy this gap, they introduce more computational cost due to the selection of\nCNN-based backbones and redundant facial features.\n  To address the above limitations, by considering the timing correlation of\nfacial expressions, we propose a novel framework called FacialPulse, which\nrecognizes depression with high accuracy and speed. By harnessing the\nbidirectional nature and proficiently addressing long-term dependencies, the\nFacial Motion Modeling Module (FMMM) is designed in FacialPulse to fully\ncapture temporal features. Since the proposed FMMM has parallel processing\ncapabilities and has the gate mechanism to mitigate gradient vanishing, this\nmodule can also significantly boost the training speed.\n  Besides, to effectively use facial landmarks to replace original images to\ndecrease information redundancy, a Facial Landmark Calibration Module (FLCM) is\ndesigned to eliminate facial landmark errors to further improve recognition\naccuracy. Extensive experiments on the AVEC2014 dataset and MMDA dataset (a\ndepression dataset) demonstrate the superiority of FacialPulse on recognition\naccuracy and speed, with the average MAE (Mean Absolute Error) decreased by 21%\ncompared to baselines, and the recognition speed increased by 100% compared to\nstate-of-the-art methods. Codes are released at\nhttps://github.com/volatileee/FacialPulse.\n","authors":["Ruiqi Wang","Jinyang Huang","Jie Zhang","Xin Liu","Xiang Zhang","Zhi Liu","Peng Zhao","Sigui Chen","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.00875v3","updated":"2024-08-07T01:50:29Z","published":"2024-04-01T03:10:36Z","title":"DPA-Net: Structured 3D Abstraction from Sparse Views via Differentiable\n  Primitive Assembly","summary":"  We present a differentiable rendering framework to learn structured 3D\nabstractions in the form of primitive assemblies from sparse RGB images\ncapturing a 3D object. By leveraging differentiable volume rendering, our\nmethod does not require 3D supervision. Architecturally, our network follows\nthe general pipeline of an image-conditioned neural radiance field (NeRF)\nexemplified by pixelNeRF for color prediction. As our core contribution, we\nintroduce differential primitive assembly (DPA) into NeRF to output a 3D\noccupancy field in place of density prediction, where the predicted occupancies\nserve as opacity values for volume rendering. Our network, coined DPA-Net,\nproduces a union of convexes, each as an intersection of convex quadric\nprimitives, to approximate the target 3D object, subject to an abstraction loss\nand a masking loss, both defined in the image space upon volume rendering. With\ntest-time adaptation and additional sampling and loss designs aimed at\nimproving the accuracy and compactness of the obtained assemblies, our method\ndemonstrates superior performance over state-of-the-art alternatives for 3D\nprimitive abstraction from sparse views.\n","authors":["Fenggen Yu","Yiming Qian","Xu Zhang","Francisca Gil-Ureta","Brian Jackson","Eric Bennett","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.00875v3.pdf","comment":"14 pages, accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2308.15068v4","updated":"2024-08-07T01:04:29Z","published":"2023-08-29T07:00:35Z","title":"A Comprehensive Augmentation Framework for Anomaly Detection","summary":"  Data augmentation methods are commonly integrated into the training of\nanomaly detection models. Previous approaches have primarily focused on\nreplicating real-world anomalies or enhancing diversity, without considering\nthat the standard of anomaly varies across different classes, potentially\nleading to a biased training distribution.This paper analyzes crucial traits of\nsimulated anomalies that contribute to the training of reconstructive networks\nand condenses them into several methods, thus creating a comprehensive\nframework by selectively utilizing appropriate combinations.Furthermore, we\nintegrate this framework with a reconstruction-based approach and concurrently\npropose a split training strategy that alleviates the issue of overfitting\nwhile avoiding introducing interference to the reconstruction process. The\nevaluations conducted on the MVTec anomaly detection dataset demonstrate that\nour method outperforms the previous state-of-the-art approach, particularly in\nterms of object classes. To evaluate generalizability, we generate a simulated\ndataset comprising anomalies with diverse characteristics since the original\ntest samples only include specific types of anomalies and may lead to biased\nevaluations. Experimental results demonstrate that our approach exhibits\npromising potential for generalizing effectively to various unforeseen\nanomalies encountered in real-world scenarios.\n","authors":["Jiang Lin","Yaping Yan"],"pdf_url":"https://arxiv.org/pdf/2308.15068v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.09305v2","updated":"2024-08-07T23:45:13Z","published":"2024-06-13T16:40:39Z","title":"Toffee: Efficient Million-Scale Dataset Construction for Subject-Driven\n  Text-to-Image Generation","summary":"  In subject-driven text-to-image generation, recent works have achieved\nsuperior performance by training the model on synthetic datasets containing\nnumerous image pairs. Trained on these datasets, generative models can produce\ntext-aligned images for specific subject from arbitrary testing image in a\nzero-shot manner. They even outperform methods which require additional\nfine-tuning on testing images. However, the cost of creating such datasets is\nprohibitive for most researchers. To generate a single training pair, current\nmethods fine-tune a pre-trained text-to-image model on the subject image to\ncapture fine-grained details, then use the fine-tuned model to create images\nfor the same subject based on creative text prompts. Consequently, constructing\na large-scale dataset with millions of subjects can require hundreds of\nthousands of GPU hours. To tackle this problem, we propose Toffee, an efficient\nmethod to construct datasets for subject-driven editing and generation.\nSpecifically, our dataset construction does not need any subject-level\nfine-tuning. After pre-training two generative models, we are able to generate\ninfinite number of high-quality samples. We construct the first large-scale\ndataset for subject-driven image editing and generation, which contains 5\nmillion image pairs, text prompts, and masks. Our dataset is 5 times the size\nof previous largest dataset, yet our cost is tens of thousands of GPU hours\nlower. To test the proposed dataset, we also propose a model which is capable\nof both subject-driven image editing and generation. By simply training the\nmodel on our proposed dataset, it obtains competitive results, illustrating the\neffectiveness of the proposed dataset construction framework.\n","authors":["Yufan Zhou","Ruiyi Zhang","Kaizhi Zheng","Nanxuan Zhao","Jiuxiang Gu","Zichao Wang","Xin Eric Wang","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2406.09305v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04121v1","updated":"2024-08-07T23:09:23Z","published":"2024-08-07T23:09:23Z","title":"Can Rule-Based Insights Enhance LLMs for Radiology Report\n  Classification? Introducing the RadPrompt Methodology","summary":"  Developing imaging models capable of detecting pathologies from chest X-rays\ncan be cost and time-prohibitive for large datasets as it requires supervision\nto attain state-of-the-art performance. Instead, labels extracted from\nradiology reports may serve as distant supervision since these are routinely\ngenerated as part of clinical practice. Despite their widespread use, current\nrule-based methods for label extraction rely on extensive rule sets that are\nlimited in their robustness to syntactic variability. To alleviate these\nlimitations, we introduce RadPert, a rule-based system that integrates an\nuncertainty-aware information schema with a streamlined set of rules, enhancing\nperformance. Additionally, we have developed RadPrompt, a multi-turn prompting\nstrategy that leverages RadPert to bolster the zero-shot predictive\ncapabilities of large language models, achieving a statistically significant\nimprovement in weighted average F1 score over GPT-4 Turbo. Most notably,\nRadPrompt surpasses both its underlying models, showcasing the synergistic\npotential of LLMs with rule-based models. We have evaluated our methods on two\nEnglish Corpora: the MIMIC-CXR gold-standard test set and a gold-standard\ndataset collected from the Cambridge University Hospitals.\n","authors":["Panagiotis Fytas","Anna Breger","Ian Selby","Simon Baker","Shahab Shahipasand","Anna Korhonen"],"pdf_url":"https://arxiv.org/pdf/2408.04121v1.pdf","comment":"Accepted at BioNLP, ACL 2024"},{"id":"http://arxiv.org/abs/2403.00816v2","updated":"2024-08-07T22:40:27Z","published":"2024-02-26T01:17:50Z","title":"Read and Think: An Efficient Step-wise Multimodal Language Model for\n  Document Understanding and Reasoning","summary":"  Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities.\n","authors":["Jinxu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.00816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04110v1","updated":"2024-08-07T22:23:13Z","published":"2024-08-07T22:23:13Z","title":"PaveCap: The First Multimodal Framework for Comprehensive Pavement\n  Condition Assessment with Dense Captioning and PCI Estimation","summary":"  This research introduces the first multimodal approach for pavement condition\nassessment, providing both quantitative Pavement Condition Index (PCI)\npredictions and qualitative descriptions. We introduce PaveCap, a novel\nframework for automated pavement condition assessment. The framework consists\nof two main parts: a Single-Shot PCI Estimation Network and a Dense Captioning\nNetwork. The PCI Estimation Network uses YOLOv8 for object detection, the\nSegment Anything Model (SAM) for zero-shot segmentation, and a four-layer\nconvolutional neural network to predict PCI. The Dense Captioning Network uses\na YOLOv8 backbone, a Transformer encoder-decoder architecture, and a\nconvolutional feed-forward module to generate detailed descriptions of pavement\nconditions. To train and evaluate these networks, we developed a pavement\ndataset with bounding box annotations, textual annotations, and PCI values. The\nresults of our PCI Estimation Network showed a strong positive correlation\n(0.70) between predicted and actual PCIs, demonstrating its effectiveness in\nautomating condition assessment. Also, the Dense Captioning Network produced\naccurate pavement condition descriptions, evidenced by high BLEU (0.7445), GLEU\n(0.5893), and METEOR (0.7252) scores. Additionally, the dense captioning model\nhandled complex scenarios well, even correcting some errors in the ground truth\ndata. The framework developed here can greatly improve infrastructure\nmanagement and decision18 making in pavement maintenance.\n","authors":["Blessing Agyei Kyem","Eugene Kofi Okrah Denteh","Joshua Kofi Asamoah","Armstrong Aboah"],"pdf_url":"https://arxiv.org/pdf/2408.04110v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05285v2","updated":"2024-08-07T21:47:41Z","published":"2024-06-07T22:41:39Z","title":"VISTA3D: Versatile Imaging SegmenTation and Annotation model for 3D\n  Computed Tomography","summary":"  Medical image segmentation is a core component of precision medicine, and 3D\ncomputed tomography (CT) is one of the most important imaging techniques. A\nhighly accurate and clinically applicable segmentation foundation model will\ngreatly facilitate clinicians and researchers using CT images. Although\nexisting foundation models have attracted great interest, none are adequate for\n3D CT, either because they lack accurate automatic segmentation for large\ncohort analysis or the ability to segment novel classes. An ideal segmentation\nsolution should possess two features: accurate out-of-the-box performance\ncovering major organ classes, and effective adaptation or zero-shot ability to\nnovel structures. To achieve this goal, we introduce Versatile Imaging\nSegmenTation and Annotation model (VISTA3D). VISTA3D is trained systematically\non 11454 volumes and provides accurate out-of-the-box segmentation for 127\ncommon types of human anatomical structures and various lesions. Additionally,\nVISTA3D supports 3D interactive segmentation, allowing convenient editing of\nautomatic results and achieving state-of-the-art annotation results on unseen\nclasses. The novel model design and training recipe represent a promising step\ntoward developing a versatile medical image foundation model and will serve as\na valuable foundation for CT image analysis. Code and model weights are\navailable at https://github.com/Project-MONAI/VISTA\n","authors":["Yufan He","Pengfei Guo","Yucheng Tang","Andriy Myronenko","Vishwesh Nath","Ziyue Xu","Dong Yang","Can Zhao","Benjamin Simon","Mason Belue","Stephanie Harmon","Baris Turkbey","Daguang Xu","Wenqi Li"],"pdf_url":"https://arxiv.org/pdf/2406.05285v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04103v1","updated":"2024-08-07T21:44:56Z","published":"2024-08-07T21:44:56Z","title":"Decoding Visual Sentiment of Political Imagery","summary":"  How can we define visual sentiment when viewers systematically disagree on\ntheir perspectives? This study introduces a novel approach to visual sentiment\nanalysis by integrating attitudinal differences into visual sentiment\nclassification. Recognizing that societal divides, such as partisan\ndifferences, heavily influence sentiment labeling, we developed a dataset that\nreflects these divides. We then trained a deep learning multi-task multi-class\nmodel to predict visual sentiment from different ideological viewpoints.\nApplied to immigration-related images, our approach captures perspectives from\nboth Democrats and Republicans. By incorporating diverse perspectives into the\nlabeling and model training process, our strategy addresses the limitation of\nlabel ambiguity and demonstrates improved accuracy in visual sentiment\npredictions. Overall, our study advocates for a paradigm shift in decoding\nvisual sentiment toward creating classifiers that more accurately reflect the\nsentiments generated by humans.\n","authors":["Olga Gasparyan","Elena Sirotkina"],"pdf_url":"https://arxiv.org/pdf/2408.04103v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04102v1","updated":"2024-08-07T21:44:29Z","published":"2024-08-07T21:44:29Z","title":"ArtVLM: Attribute Recognition Through Vision-Based Prefix Language\n  Modeling","summary":"  Recognizing and disentangling visual attributes from objects is a foundation\nto many computer vision applications. While large vision language\nrepresentations like CLIP had largely resolved the task of zero-shot object\nrecognition, zero-shot visual attribute recognition remains a challenge because\nCLIP's contrastively-learned vision-language representation cannot effectively\ncapture object-attribute dependencies. In this paper, we target this weakness\nand propose a sentence generation-based retrieval formulation for attribute\nrecognition that is novel in 1) explicitly modeling a to-be-measured and\nretrieved object-attribute relation as a conditional probability graph, which\nconverts the recognition problem into a dependency-sensitive language-modeling\nproblem, and 2) applying a large pretrained Vision-Language Model (VLM) on this\nreformulation and naturally distilling its knowledge of image-object-attribute\nrelations to use towards attribute recognition. Specifically, for each\nattribute to be recognized on an image, we measure the visual-conditioned\nprobability of generating a short sentence encoding the attribute's relation to\nobjects on the image. Unlike contrastive retrieval, which measures likelihood\nby globally aligning elements of the sentence to the image, generative\nretrieval is sensitive to the order and dependency of objects and attributes in\nthe sentence. We demonstrate through experiments that generative retrieval\nconsistently outperforms contrastive retrieval on two visual reasoning\ndatasets, Visual Attribute in the Wild (VAW), and our newly-proposed Visual\nGenome Attribute Ranking (VGARank).\n","authors":["William Y. Zhu","Keren Ye","Junjie Ke","Jiahui Yu","Leonidas Guibas","Peyman Milanfar","Feng Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04102v1.pdf","comment":"Accepted at ECCV 2024"},{"id":"http://arxiv.org/abs/2408.04091v1","updated":"2024-08-07T21:13:49Z","published":"2024-08-07T21:13:49Z","title":"The Quest for Early Detection of Retinal Disease: 3D CycleGAN-based\n  Translation of Optical Coherence Tomography into Confocal Microscopy","summary":"  Optical coherence tomography (OCT) and confocal microscopy are pivotal in\nretinal imaging, offering distinct advantages and limitations. In vivo OCT\noffers rapid, non-invasive imaging but can suffer from clarity issues and\nmotion artifacts, while ex vivo confocal microscopy, providing high-resolution,\ncellular-detailed color images, is invasive and raises ethical concerns. To\nbridge the benefits of both modalities, we propose a novel framework based on\nunsupervised 3D CycleGAN for translating unpaired in vivo OCT to ex vivo\nconfocal microscopy images. This marks the first attempt to exploit the\ninherent 3D information of OCT and translate it into the rich, detailed color\ndomain of confocal microscopy. We also introduce a unique dataset,\nOCT2Confocal, comprising mouse OCT and confocal retinal images, facilitating\nthe development of and establishing a benchmark for cross-modal image\ntranslation research. Our model has been evaluated both quantitatively and\nqualitatively, achieving Fr\\'echet Inception Distance (FID) scores of 0.766 and\nKernel Inception Distance (KID) scores as low as 0.153, and leading subjective\nMean Opinion Scores (MOS). Our model demonstrated superior image fidelity and\nquality with limited data over existing methods. Our approach effectively\nsynthesizes color information from 3D confocal images, closely approximating\ntarget outcomes and suggesting enhanced potential for diagnostic and monitoring\napplications in ophthalmology.\n","authors":["Xin Tian","Nantheera Anantrasirichai","Lindsay Nicholson","Alin Achim"],"pdf_url":"https://arxiv.org/pdf/2408.04091v1.pdf","comment":"30 pages, 11 figures, 5 tables"},{"id":"http://arxiv.org/abs/2407.19547v2","updated":"2024-08-07T20:43:10Z","published":"2024-07-28T17:46:15Z","title":"Temporal Feature Matters: A Framework for Diffusion Model Quantization","summary":"  The Diffusion models, widely used for image generation, face significant\nchallenges related to their broad applicability due to prolonged inference\ntimes and high memory demands. Efficient Post-Training Quantization (PTQ) is\ncrucial to address these issues. However, unlike traditional models, diffusion\nmodels critically rely on the time-step for the multi-round denoising.\nTypically, each time-step is encoded into a hypersensitive temporal feature by\nseveral modules. Despite this, existing PTQ methods do not optimize these\nmodules individually. Instead, they employ unsuitable reconstruction objectives\nand complex calibration methods, leading to significant disturbances in the\ntemporal feature and denoising trajectory, as well as reduced compression\nefficiency. To address these challenges, we introduce a novel quantization\nframework that includes three strategies: 1) TIB-based Maintenance: Based on\nour innovative Temporal Information Block (TIB) definition, Temporal\nInformation-aware Reconstruction (TIAR) and Finite Set Calibration (FSC) are\ndeveloped to efficiently align original temporal features. 2) Cache-based\nMaintenance: Instead of indirect and complex optimization for the related\nmodules, pre-computing and caching quantized counterparts of temporal features\nare developed to minimize errors. 3) Disturbance-aware Selection: Employ\ntemporal feature errors to guide a fine-grained selection between the two\nmaintenance strategies for further disturbance reduction. This framework\npreserves most of the temporal information and ensures high-quality end-to-end\ngeneration. Extensive testing on various datasets, diffusion models and\nhardware confirms our superior performance and acceleration..\n","authors":["Yushi Huang","Ruihao Gong","Xianglong Liu","Jing Liu","Yuhang Li","Jiwen Lu","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2407.19547v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2311.16503"},{"id":"http://arxiv.org/abs/2408.04077v1","updated":"2024-08-07T20:36:20Z","published":"2024-08-07T20:36:20Z","title":"PushPull-Net: Inhibition-driven ResNet robust to image corruptions","summary":"  We introduce a novel computational unit, termed PushPull-Conv, in the first\nlayer of a ResNet architecture, inspired by the anti-phase inhibition\nphenomenon observed in the primary visual cortex. This unit redefines the\ntraditional convolutional layer by implementing a pair of complementary\nfilters: a trainable push kernel and its counterpart, the pull kernel. The push\nkernel (analogous to traditional convolution) learns to respond to specific\nstimuli, while the pull kernel reacts to the same stimuli but of opposite\ncontrast. This configuration enhances stimulus selectivity and effectively\ninhibits response in regions lacking preferred stimuli. This effect is\nattributed to the push and pull kernels, which produce responses of comparable\nmagnitude in such regions, thereby neutralizing each other. The incorporation\nof the PushPull-Conv into ResNets significantly increases their robustness to\nimage corruption. Our experiments with benchmark corruption datasets show that\nthe PushPull-Conv can be combined with other data augmentation techniques to\nfurther improve model robustness. We set a new robustness benchmark on ResNet50\nachieving an $mCE$ of 49.95$\\%$ on ImageNet-C when combining PRIME augmentation\nwith PushPull inhibition.\n","authors":["Guru Swaroop Bennabhaktula","Enrique Alegre","Nicola Strisciuglio","George Azzopardi"],"pdf_url":"https://arxiv.org/pdf/2408.04077v1.pdf","comment":"Accepted at ICPR 2024, code available at\n  https://github.com/bgswaroop/pushpull-conv"},{"id":"http://arxiv.org/abs/2403.05466v2","updated":"2024-08-07T20:33:27Z","published":"2024-03-08T17:29:51Z","title":"Grasping Trajectory Optimization with Point Clouds","summary":"  We introduce a new trajectory optimization method for robotic grasping based\non a point-cloud representation of robots and task spaces. In our method,\nrobots are represented by 3D points on their link surfaces. The task space of a\nrobot is represented by a point cloud that can be obtained from depth sensors.\nUsing the point-cloud representation, goal reaching in grasping can be\nformulated as point matching, while collision avoidance can be efficiently\nachieved by querying the signed distance values of the robot points in the\nsigned distance field of the scene points. Consequently, a constrained\nnonlinear optimization problem is formulated to solve the joint motion and\ngrasp planning problem. The advantage of our method is that the point-cloud\nrepresentation is general to be used with any robot in any environment. We\ndemonstrate the effectiveness of our method by performing experiments on a\ntabletop scene and a shelf scene for grasping with a Fetch mobile manipulator\nand a Franka Panda arm. The project page is available at\n\\url{https://irvlutd.github.io/GraspTrajOpt}\n","authors":["Yu Xiang","Sai Haneesh Allu","Rohith Peddi","Tyler Summers","Vibhav Gogate"],"pdf_url":"https://arxiv.org/pdf/2403.05466v2.pdf","comment":"Published in IROS 2024"},{"id":"http://arxiv.org/abs/2405.06786v2","updated":"2024-08-07T20:27:41Z","published":"2024-05-10T19:26:17Z","title":"SAM3D: Zero-Shot Semi-Automatic Segmentation in 3D Medical Images with\n  the Segment Anything Model","summary":"  We introduce SAM3D, a new approach to semi-automatic zero-shot segmentation\nof 3D images building on the existing Segment Anything Model. We achieve fast\nand accurate segmentations in 3D images with a four-step strategy involving:\nuser prompting with 3D polylines, volume slicing along multiple axes,\nslice-wide inference with a pretrained model, and recomposition and refinement\nin 3D. We evaluated SAM3D performance qualitatively on an array of imaging\nmodalities and anatomical structures and quantify performance for specific\nstructures in abdominal pelvic CT and brain MRI. Notably, our method achieves\ngood performance with zero model training or finetuning, making it particularly\nuseful for tasks with a scarcity of preexisting labeled data. By enabling users\nto create 3D segmentations of unseen data quickly and with dramatically reduced\nmanual input, these methods have the potential to aid surgical planning and\neducation, diagnostic imaging, and scientific research.\n","authors":["Trevor J. Chan","Aarush Sahni","Yijin Fang","Jie Li","Alisha Luthra","Alison Pouch","Chamith S. Rajapakse"],"pdf_url":"https://arxiv.org/pdf/2405.06786v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04076v1","updated":"2024-08-07T20:26:35Z","published":"2024-08-07T20:26:35Z","title":"Multi-scale structural complexity as a quantitative measure of visual\n  complexity","summary":"  While intuitive for humans, the concept of visual complexity is hard to\ndefine and quantify formally. We suggest adopting the multi-scale structural\ncomplexity (MSSC) measure, an approach that defines structural complexity of an\nobject as the amount of dissimilarities between distinct scales in its\nhierarchical organization. In this work, we apply MSSC to the case of visual\nstimuli, using an open dataset of images with subjective complexity scores\nobtained from human participants (SAVOIAS). We demonstrate that MSSC correlates\nwith subjective complexity on par with other computational complexity measures,\nwhile being more intuitive by definition, consistent across categories of\nimages, and easier to compute. We discuss objective and subjective elements\ninherently present in human perception of complexity and the domains where the\ntwo are more likely to diverge. We show how the multi-scale nature of MSSC\nallows further investigation of complexity as it is perceived by humans.\n","authors":["Anna Kravchenko","Andrey A. Bagrov","Mikhail I. Katsnelson","Veronica Dudarev"],"pdf_url":"https://arxiv.org/pdf/2408.04076v1.pdf","comment":"16 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.04072v1","updated":"2024-08-07T20:19:20Z","published":"2024-08-07T20:19:20Z","title":"AEye: A Visualization Tool for Image Datasets","summary":"  Image datasets serve as the foundation for machine learning models in\ncomputer vision, significantly influencing model capabilities, performance, and\nbiases alongside architectural considerations. Therefore, understanding the\ncomposition and distribution of these datasets has become increasingly crucial.\nTo address the need for intuitive exploration of these datasets, we propose\nAEye, an extensible and scalable visualization tool tailored to image datasets.\nAEye utilizes a contrastively trained model to embed images into semantically\nmeaningful high-dimensional representations, facilitating data clustering and\norganization. To visualize the high-dimensional representations, we project\nthem onto a two-dimensional plane and arrange images in layers so users can\nseamlessly navigate and explore them interactively. AEye facilitates semantic\nsearch functionalities for both text and image queries, enabling users to\nsearch for content. We open-source the codebase for AEye, and provide a simple\nconfiguration to add datasets.\n","authors":["Florian Grtschla","Luca A. Lanzendrfer","Marco Calzavara","Roger Wattenhofer"],"pdf_url":"https://arxiv.org/pdf/2408.04072v1.pdf","comment":"Accepted at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.04065v1","updated":"2024-08-07T20:07:25Z","published":"2024-08-07T20:07:25Z","title":"Do Sharpness-based Optimizers Improve Generalization in Medical Image\n  Analysis?","summary":"  Effective clinical deployment of deep learning models in healthcare demands\nhigh generalization performance to ensure accurate diagnosis and treatment\nplanning. In recent years, significant research has focused on improving the\ngeneralization of deep learning models by regularizing the sharpness of the\nloss landscape. Among the optimization approaches that explicitly minimize\nsharpness, Sharpness-Aware Minimization (SAM) has shown potential in enhancing\ngeneralization performance on general domain image datasets. This success has\nled to the development of several advanced sharpness-based algorithms aimed at\naddressing the limitations of SAM, such as Adaptive SAM, surrogate-Gap SAM,\nWeighted SAM, and Curvature Regularized SAM. These sharpness-based optimizers\nhave shown improvements in model generalization compared to conventional\nstochastic gradient descent optimizers and their variants on general domain\nimage datasets, but they have not been thoroughly evaluated on medical images.\nThis work provides a review of recent sharpness-based methods for improving the\ngeneralization of deep learning networks and evaluates the methods performance\non medical breast ultrasound images. Our findings indicate that the initial SAM\nmethod successfully enhances the generalization of various deep learning\nmodels. While Adaptive SAM improves generalization of convolutional neural\nnetworks, it fails to do so for vision transformers. Other sharpness-based\noptimizers, however, do not demonstrate consistent results. The results reveal\nthat, contrary to findings in the non-medical domain, SAM is the only\nrecommended sharpness-based optimizer that consistently improves generalization\nin medical image analysis, and further research is necessary to refine the\nvariants of SAM to enhance generalization performance in this field\n","authors":["Mohamed Hassan","Aleksander Vakanski","Min Xian"],"pdf_url":"https://arxiv.org/pdf/2408.04065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13717v4","updated":"2024-08-07T19:31:30Z","published":"2023-11-22T22:21:26Z","title":"Feature Extraction for Generative Medical Imaging Evaluation: New\n  Evidence Against an Evolving Trend","summary":"  Fr\\'echet Inception Distance (FID) is a widely used metric for assessing\nsynthetic image quality. It relies on an ImageNet-based feature extractor,\nmaking its applicability to medical imaging unclear. A recent trend is to adapt\nFID to medical imaging through feature extractors trained on medical images.\nOur study challenges this practice by demonstrating that ImageNet-based\nextractors are more consistent and aligned with human judgment than their\nRadImageNet counterparts. We evaluated sixteen StyleGAN2 networks across four\nmedical imaging modalities and four data augmentation techniques with Fr\\'echet\ndistances (FDs) computed using eleven ImageNet or RadImageNet-trained feature\nextractors. Comparison with human judgment via visual Turing tests revealed\nthat ImageNet-based extractors produced rankings consistent with human\njudgment, with the FD derived from the ImageNet-trained SwAV extractor\nsignificantly correlating with expert evaluations. In contrast,\nRadImageNet-based rankings were volatile and inconsistent with human judgment.\nOur findings challenge prevailing assumptions, providing novel evidence that\nmedical image-trained feature extractors do not inherently improve FDs and can\neven compromise their reliability. Our code is available at\nhttps://github.com/mckellwoodland/fid-med-eval.\n","authors":["McKell Woodland","Austin Castelo","Mais Al Taie","Jessica Albuquerque Marques Silva","Mohamed Eltaher","Frank Mohn","Alexander Shieh","Suprateek Kundu","Joshua P. Yung","Ankit B. Patel","Kristy K. Brock"],"pdf_url":"https://arxiv.org/pdf/2311.13717v4.pdf","comment":"Preprint of manuscript early accepted to MICCAI 2024"},{"id":"http://arxiv.org/abs/2402.09611v2","updated":"2024-08-07T19:27:53Z","published":"2024-02-14T22:57:03Z","title":"Towards Privacy-Aware Sign Language Translation at Scale","summary":"  A major impediment to the advancement of sign language translation (SLT) is\ndata scarcity. Much of the sign language data currently available on the web\ncannot be used for training supervised models due to the lack of aligned\ncaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bears\nprivacy risks due to the presence of biometric information, which the\nresponsible development of SLT technologies should account for. In this work,\nwe propose a two-stage framework for privacy-aware SLT at scale that addresses\nboth of these issues. We introduce SSVP-SLT, which leverages self-supervised\nvideo pretraining on anonymized and unannotated videos, followed by supervised\nSLT finetuning on a curated parallel dataset. SSVP-SLT achieves\nstate-of-the-art finetuned and zero-shot gloss-free SLT performance on the\nHow2Sign dataset, outperforming the strongest respective baselines by over 3\nBLEU-4. Based on controlled experiments, we further discuss the advantages and\nlimitations of self-supervised pretraining and anonymization via facial\nobfuscation for SLT.\n","authors":["Phillip Rust","Bowen Shi","Skyler Wang","Necati Cihan Camgz","Jean Maillard"],"pdf_url":"https://arxiv.org/pdf/2402.09611v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2307.07176v3","updated":"2024-08-07T19:08:36Z","published":"2023-07-14T06:00:08Z","title":"SafeDreamer: Safe Reinforcement Learning with World Models","summary":"  The deployment of Reinforcement Learning (RL) in real-world applications is\nconstrained by its failure to satisfy safety criteria. Existing Safe\nReinforcement Learning (SafeRL) methods, which rely on cost functions to\nenforce safety, often fail to achieve zero-cost performance in complex\nscenarios, especially vision-only tasks. These limitations are primarily due to\nmodel inaccuracies and inadequate sample efficiency. The integration of the\nworld model has proven effective in mitigating these shortcomings. In this\nwork, we introduce SafeDreamer, a novel algorithm incorporating\nLagrangian-based methods into world model planning processes within the\nsuperior Dreamer framework. Our method achieves nearly zero-cost performance on\nvarious tasks, spanning low-dimensional and vision-only input, within the\nSafety-Gymnasium benchmark, showcasing its efficacy in balancing performance\nand safety in RL tasks. Further details can be found in the code repository:\n\\url{https://github.com/PKU-Alignment/SafeDreamer}.\n","authors":["Weidong Huang","Jiaming Ji","Chunhe Xia","Borong Zhang","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2307.07176v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2408.04034v1","updated":"2024-08-07T18:30:18Z","published":"2024-08-07T18:30:18Z","title":"Task-oriented Sequential Grounding in 3D Scenes","summary":"  Grounding natural language in physical 3D environments is essential for the\nadvancement of embodied artificial intelligence. Current datasets and models\nfor 3D visual grounding predominantly focus on identifying and localizing\nobjects from static, object-centric descriptions. These approaches do not\nadequately address the dynamic and sequential nature of task-oriented grounding\nnecessary for practical applications. In this work, we propose a new task:\nTask-oriented Sequential Grounding in 3D scenes, wherein an agent must follow\ndetailed step-by-step instructions to complete daily activities by locating a\nsequence of target objects in indoor scenes. To facilitate this task, we\nintroduce SG3D, a large-scale dataset containing 22,346 tasks with 112,236\nsteps across 4,895 real-world 3D scenes. The dataset is constructed using a\ncombination of RGB-D scans from various 3D scene datasets and an automated task\ngeneration pipeline, followed by human verification for quality assurance. We\nadapted three state-of-the-art 3D visual grounding models to the sequential\ngrounding task and evaluated their performance on SG3D. Our results reveal that\nwhile these models perform well on traditional benchmarks, they face\nsignificant challenges with task-oriented sequential grounding, underscoring\nthe need for further research in this area.\n","authors":["Zhuofan Zhang","Ziyu Zhu","Pengxiang Li","Tengyu Liu","Xiaojian Ma","Yixin Chen","Baoxiong Jia","Siyuan Huang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2408.04034v1.pdf","comment":"website: https://sg-3d.github.io/"},{"id":"http://arxiv.org/abs/2312.02976v2","updated":"2024-08-07T18:11:51Z","published":"2023-12-05T18:59:45Z","title":"SPOC: Imitating Shortest Paths in Simulation Enables Effective\n  Navigation and Manipulation in the Real World","summary":"  Reinforcement learning (RL) with dense rewards and imitation learning (IL)\nwith human-generated trajectories are the most widely used approaches for\ntraining modern embodied agents. RL requires extensive reward shaping and\nauxiliary losses and is often too slow and ineffective for long-horizon tasks.\nWhile IL with human supervision is effective, collecting human trajectories at\nscale is extremely expensive. In this work, we show that imitating\nshortest-path planners in simulation produces agents that, given a language\ninstruction, can proficiently navigate, explore, and manipulate objects in both\nsimulation and in the real world using only RGB sensors (no depth map or GPS\ncoordinates). This surprising result is enabled by our end-to-end,\ntransformer-based, SPOC architecture, powerful visual encoders paired with\nextensive image augmentation, and the dramatic scale and diversity of our\ntraining data: millions of frames of shortest-path-expert trajectories\ncollected inside approximately 200,000 procedurally generated houses containing\n40,000 unique 3D assets. Our models, data, training code, and newly proposed\n10-task benchmarking suite CHORES are available in\nhttps://spoc-robot.github.io.\n","authors":["Kiana Ehsani","Tanmay Gupta","Rose Hendrix","Jordi Salvador","Luca Weihs","Kuo-Hao Zeng","Kunal Pratap Singh","Yejin Kim","Winson Han","Alvaro Herrasti","Ranjay Krishna","Dustin Schwenk","Eli VanderBilt","Aniruddha Kembhavi"],"pdf_url":"https://arxiv.org/pdf/2312.02976v2.pdf","comment":"First six authors contributed equally. Project page:\n  https://spoc-robot.github.io/"},{"id":"http://arxiv.org/abs/2408.04015v1","updated":"2024-08-07T18:04:01Z","published":"2024-08-07T18:04:01Z","title":"Image-to-LaTeX Converter for Mathematical Formulas and Text","summary":"  In this project, we train a vision encoder-decoder model to generate LaTeX\ncode from images of mathematical formulas and text. Utilizing a diverse\ncollection of image-to-LaTeX data, we build two models: a base model with a\nSwin Transformer encoder and a GPT-2 decoder, trained on machine-generated\nimages, and a fine-tuned version enhanced with Low-Rank Adaptation (LoRA)\ntrained on handwritten formulas. We then compare the BLEU performance of our\nspecialized model on a handwritten test set with other similar models, such as\nPix2Text, TexTeller, and Sumen. Through this project, we contribute open-source\nmodels for converting images to LaTeX and provide from-scratch code for\nbuilding these models with distributed training and GPU optimizations.\n","authors":["Daniil Gurgurov","Aleksey Morshnev"],"pdf_url":"https://arxiv.org/pdf/2408.04015v1.pdf","comment":"4 pages"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2402.06859v2","updated":"2024-08-07T16:54:23Z","published":"2024-02-10T01:47:10Z","title":"LiRank: Industrial Large Scale Ranking Models at LinkedIn","summary":"  We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.\n","authors":["Fedor Borisyuk","Mingzhou Zhou","Qingquan Song","Siyu Zhu","Birjodh Tiwana","Ganesh Parameswaran","Siddharth Dangi","Lars Hertel","Qiang Xiao","Xiaochen Hou","Yunbo Ouyang","Aman Gupta","Sheallika Singh","Dan Liu","Hailing Cheng","Lei Le","Jonathan Hung","Sathiya Keerthi","Ruoyan Wang","Fengyu Zhang","Mohit Kothari","Chen Zhu","Daqi Sun","Yun Dai","Xun Luan","Sirou Zhu","Zhiwei Wang","Neil Daftary","Qianqi Shen","Chengming Jiang","Haichao Wei","Maneesh Varshney","Amol Ghoting","Souvik Ghosh"],"pdf_url":"https://arxiv.org/pdf/2402.06859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03886v1","updated":"2024-08-07T16:35:10Z","published":"2024-08-07T16:35:10Z","title":"Retrieval Augmentation via User Interest Clustering","summary":"  Many existing industrial recommender systems are sensitive to the patterns of\nuser-item engagement. Light users, who interact less frequently, correspond to\na data sparsity problem, making it difficult for the system to accurately learn\nand represent their preferences. On the other hand, heavy users with rich\ninteraction history often demonstrate a variety of niche interests that are\nhard to be precisely captured under the standard \"user-item\" similarity\nmeasurement. Moreover, implementing these systems in an industrial environment\nnecessitates that they are resource-efficient and scalable to process web-scale\ndata under strict latency constraints. In this paper, we address these\nchallenges by introducing an intermediate \"interest\" layer between users and\nitems. We propose a novel approach that efficiently constructs user interest\nand facilitates low computational cost inference by clustering engagement\ngraphs and incorporating user-interest attention. This method enhances the\nunderstanding of light users' preferences by linking them with heavy users. By\nintegrating user-interest attention, our approach allows a more personalized\nsimilarity metric, adept at capturing the complex dynamics of user-item\ninteractions. The use of interest as an intermediary layer fosters a balance\nbetween scalability and expressiveness in the model. Evaluations on two public\ndatasets reveal that our method not only achieves improved recommendation\nperformance but also demonstrates enhanced computational efficiency compared to\nitem-level attention models. Our approach has also been deployed in multiple\nproducts at Meta, facilitating short-form video related recommendation.\n","authors":["Hanjia Lyu","Hanqing Zeng","Yinglong Xia","Ren Chen","Jiebo Luo"],"pdf_url":"https://arxiv.org/pdf/2408.03886v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03873v1","updated":"2024-08-07T16:23:29Z","published":"2024-08-07T16:23:29Z","title":"A Reproducible Analysis of Sequential Recommender Systems","summary":"  Sequential Recommender Systems (SRSs) have emerged as a highly efficient\napproach to recommendation systems. By leveraging sequential data, SRSs can\nidentify temporal patterns in user behaviour, significantly improving\nrecommendation accuracy and relevance.Ensuring the reproducibility of these\nmodels is paramount for advancing research and facilitating comparisons between\nthem. Existing works exhibit shortcomings in reproducibility and replicability\nof results, leading to inconsistent statements across papers. Our work fills\nthese gaps by standardising data pre-processing and model implementations,\nproviding a comprehensive code resource, including a framework for developing\nSRSs and establishing a foundation for consistent and reproducible\nexperimentation. We conduct extensive experiments on several benchmark\ndatasets, comparing various SRSs implemented in our resource. We challenge\nprevailing performance benchmarks, offering new insights into the SR domain.\nFor instance, SASRec does not consistently outperform GRU4Rec. On the contrary,\nwhen the number of model parameters becomes substantial, SASRec starts to\nclearly dominate all the other SRSs. This discrepancy underscores the\nsignificant impact that experimental configuration has on the outcomes and the\nimportance of setting it up to ensure precise and comprehensive results.\nFailure to do so can lead to significantly flawed conclusions, highlighting the\nneed for rigorous experimental design and analysis in SRS research. Our code is\navailable at https://github.com/antoniopurificato/recsys_repro_conf.\n","authors":["Filippo Betello","Antonio Purificato","Federico Siciliano","Giovanni Trappolini","Andrea Bacciu","Nicola Tonellotto","Fabrizio Silvestri"],"pdf_url":"https://arxiv.org/pdf/2408.03873v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2405.13795v2","updated":"2024-08-07T15:13:45Z","published":"2024-05-22T16:19:52Z","title":"A general framework for distributed approximate similarity search with\n  arbitrary distances","summary":"  Similarity search is a central problem in domains such as information\nmanagement and retrieval or data analysis. Many similarity search algorithms\nare designed or specifically adapted to metric distances. Thus, they are\nunsuitable for alternatives like the cosine distance, which has become quite\ncommon, for example, with embeddings and in text mining. This paper presents\nGDASC (General Distributed Approximate Similarity search with Clustering), a\ngeneral framework for distributed approximate similarity search that accepts\narbitrary distances. This framework can build a multilevel index structure, by\nselecting a clustering algorithm, the number of prototypes in each cluster and\nany arbitrary distance function. As a result, this framework effectively\novercomes the limitation of using metric distances and can address situations\ninvolving cosine similarity or other non-standard similarity measures.\nExperimental results using k-medoids clustering in GDASC with real datasets\nconfirm the applicability of this approach for approximate similarity search,\nimproving the performance of extant algorithms for this purpose.\n","authors":["Elena Garcia-Morato","Maria Jesus Algar","Cesar Alfaro","Felipe Ortega","Javier Gomez","Javier M. Moguerza"],"pdf_url":"https://arxiv.org/pdf/2405.13795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03811v1","updated":"2024-08-07T14:42:13Z","published":"2024-08-07T14:42:13Z","title":"Generative Language Models with Retrieval Augmented Generation for\n  Automated Short Answer Scoring","summary":"  Automated Short Answer Scoring (ASAS) is a critical component in educational\nassessment. While traditional ASAS systems relied on rule-based algorithms or\ncomplex deep learning methods, recent advancements in Generative Language\nModels (GLMs) offer new opportunities for improvement. This study explores the\napplication of GLMs to ASAS, leveraging their off-the-shelf capabilities and\nperformance in various domains. We propose a novel pipeline that combines\nvector databases, transformer-based encoders, and GLMs to enhance short answer\nscoring accuracy. Our approach stores training responses in a vector database,\nretrieves semantically similar responses during inference, and employs a GLM to\nanalyze these responses and determine appropriate scores. We further optimize\nthe system through fine-tuned retrieval processes and prompt engineering.\nEvaluation on the SemEval 2013 dataset demonstrates a significant improvement\non the SCIENTSBANK 3-way and 2-way tasks compared to existing methods,\nhighlighting the potential of GLMs in advancing ASAS technology.\n","authors":["Zifan Wang","Christopher Ormerod"],"pdf_url":"https://arxiv.org/pdf/2408.03811v1.pdf","comment":"20 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.03772v1","updated":"2024-08-07T13:48:24Z","published":"2024-08-07T13:48:24Z","title":"Relevance meets Diversity: A User-Centric Framework for Knowledge\n  Exploration through Recommendations","summary":"  Providing recommendations that are both relevant and diverse is a key\nconsideration of modern recommender systems. Optimizing both of these measures\npresents a fundamental trade-off, as higher diversity typically comes at the\ncost of relevance, resulting in lower user engagement. Existing recommendation\nalgorithms try to resolve this trade-off by combining the two measures,\nrelevance and diversity, into one aim and then seeking recommendations that\noptimize the combined objective, for a given number of items to recommend.\nTraditional approaches, however, do not consider the user interaction with the\nrecommended items.\n  In this paper, we put the user at the central stage, and build on the\ninterplay between relevance, diversity, and user behavior. In contrast to\napplications where the goal is solely to maximize engagement, we focus on\nscenarios aiming at maximizing the total amount of knowledge encountered by the\nuser. We use diversity as a surrogate of the amount of knowledge obtained by\nthe user while interacting with the system, and we seek to maximize diversity.\nWe propose a probabilistic user-behavior model in which users keep interacting\nwith the recommender system as long as they receive relevant recommendations,\nbut they may stop if the relevance of the recommended items drops. Thus, for a\nrecommender system to achieve a high-diversity measure, it will need to produce\nrecommendations that are both relevant and diverse.\n  Finally, we propose a novel recommendation strategy that combines relevance\nand diversity by a copula function. We conduct an extensive evaluation of the\nproposed methodology over multiple datasets, and we show that our strategy\noutperforms several state-of-the-art competitors. Our implementation is\npublicly available at https://github.com/EricaCoppolillo/EXPLORE.\n","authors":["Erica Coppolillo","Giuseppe Manco","Aristides Gionis"],"pdf_url":"https://arxiv.org/pdf/2408.03772v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03655v1","updated":"2024-08-07T09:45:24Z","published":"2024-08-07T09:45:24Z","title":"Consumer Transactions Simulation through Generative Adversarial Networks","summary":"  In the rapidly evolving domain of large-scale retail data systems,\nenvisioning and simulating future consumer transactions has become a crucial\narea of interest. It offers significant potential to fortify demand forecasting\nand fine-tune inventory management. This paper presents an innovative\napplication of Generative Adversarial Networks (GANs) to generate synthetic\nretail transaction data, specifically focusing on a novel system architecture\nthat combines consumer behavior modeling with stock-keeping unit (SKU)\navailability constraints to address real-world assortment optimization\nchallenges. We diverge from conventional methodologies by integrating SKU data\ninto our GAN architecture and using more sophisticated embedding methods (e.g.,\nhyper-graphs). This design choice enables our system to generate not only\nsimulated consumer purchase behaviors but also reflects the dynamic interplay\nbetween consumer behavior and SKU availability -- an aspect often overlooked,\namong others, because of data scarcity in legacy retail simulation models. Our\nGAN model generates transactions under stock constraints, pioneering a\nresourceful experimental system with practical implications for real-world\nretail operation and strategy. Preliminary results demonstrate enhanced realism\nin simulated transactions measured by comparing generated items with real ones\nusing methods employed earlier in related studies. This underscores the\npotential for more accurate predictive modeling.\n","authors":["Sergiy Tkachuk","Szymon ukasik","Anna Wrblewska"],"pdf_url":"https://arxiv.org/pdf/2408.03655v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2404.15939v3","updated":"2024-08-07T09:36:15Z","published":"2024-04-24T15:58:59Z","title":"Telco-RAG: Navigating the Challenges of Retrieval-Augmented Language\n  Models for Telecommunications","summary":"  The application of Large Language Models (LLMs) and Retrieval-Augmented\nGeneration (RAG) systems in the telecommunication domain presents unique\nchallenges, primarily due to the complex nature of telecom standard documents\nand the rapid evolution of the field. The paper introduces Telco-RAG, an\nopen-source RAG framework designed to handle the specific needs of\ntelecommunications standards, particularly 3rd Generation Partnership Project\n(3GPP) documents. Telco-RAG addresses the critical challenges of implementing a\nRAG pipeline on highly technical content, paving the way for applying LLMs in\ntelecommunications and offering guidelines for RAG implementation in other\ntechnical domains.\n","authors":["Andrei-Laurentiu Bornea","Fadhel Ayed","Antonio De Domenico","Nicola Piovesan","Ali Maatouk"],"pdf_url":"https://arxiv.org/pdf/2404.15939v3.pdf","comment":"6 pages, 5 Figure, 4 Tables, accepted to IEEE Globecom 2024 (see\n  https://github.com/netop-team/telco-rag)"},{"id":"http://arxiv.org/abs/2408.02854v2","updated":"2024-08-07T04:55:45Z","published":"2024-08-05T22:34:28Z","title":"Wiping out the limitations of Large Language Models -- A Taxonomy for\n  Retrieval Augmented Generation","summary":"  Current research on RAGs is distributed across various disciplines, and since\nthe technology is evolving very quickly, its unit of analysis is mostly on\ntechnological innovations, rather than applications in business contexts. Thus,\nin this research, we aim to create a taxonomy to conceptualize a comprehensive\noverview of the constituting characteristics that define RAG applications,\nfacilitating the adoption of this technology in the IS community. To the best\nof our knowledge, no RAG application taxonomies have been developed so far. We\ndescribe our methodology for developing the taxonomy, which includes the\ncriteria for selecting papers, an explanation of our rationale for employing a\nLarge Language Model (LLM)-supported approach to extract and identify initial\ncharacteristics, and a concise overview of our systematic process for\nconceptualizing the taxonomy. Our systematic taxonomy development process\nincludes four iterative phases designed to refine and enhance our understanding\nand presentation of RAG's core dimensions. We have developed a total of five\nmeta-dimensions and sixteen dimensions to comprehensively capture the concept\nof Retrieval-Augmented Generation (RAG) applications. When discussing our\nfindings, we also detail the specific research areas and pose key research\nquestions to guide future information system researchers as they explore the\nemerging topics of RAG systems.\n","authors":["Mahei Manhai Li","Irina Nikishina","zge Sevgili","Martin Semman"],"pdf_url":"https://arxiv.org/pdf/2408.02854v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03091v2","updated":"2024-08-07T04:34:24Z","published":"2024-08-06T10:45:14Z","title":"Modeling User Intent Beyond Trigger: Incorporating Uncertainty for\n  Trigger-Induced Recommendation","summary":"  To cater to users' desire for an immersive browsing experience, numerous\ne-commerce platforms provide various recommendation scenarios, with a focus on\nTrigger-Induced Recommendation (TIR) tasks. However, the majority of current\nTIR methods heavily rely on the trigger item to understand user intent, lacking\na higher-level exploration and exploitation of user intent (e.g., popular items\nand complementary items), which may result in an overly convergent\nunderstanding of users' short-term intent and can be detrimental to users'\nlong-term purchasing experiences. Moreover, users' short-term intent shows\nuncertainty and is affected by various factors such as browsing context and\nhistorical behaviors, which poses challenges to user intent modeling. To\naddress these challenges, we propose a novel model called Deep Uncertainty\nIntent Network (DUIN), comprising three essential modules: i) Explicit Intent\nExploit Module extracting explicit user intent using the contrastive learning\nparadigm; ii) Latent Intent Explore Module exploring latent user intent by\nleveraging the multi-view relationships between items; iii) Intent Uncertainty\nMeasurement Module offering a distributional estimation and capturing the\nuncertainty associated with user intent. Experiments on three real-world\ndatasets demonstrate the superior performance of DUIN compared to existing\nbaselines. Notably, DUIN has been deployed across all TIR scenarios in our\ne-commerce platform, with online A/B testing results conclusively validating\nits superiority.\n","authors":["Jianxing Ma","Zhibo Xiao","Luwei Yang","Hansheng Xue","Xuanzhou Liu","Wen Jiang","Wei Ning","Guannan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03091v2.pdf","comment":"Accepted at CIKM 2024"},{"id":"http://arxiv.org/abs/2408.03533v1","updated":"2024-08-07T04:20:28Z","published":"2024-08-07T04:20:28Z","title":"Lifelong Personalized Low-Rank Adaptation of Large Language Models for\n  Recommendation","summary":"  We primarily focus on the field of large language models (LLMs) for\nrecommendation, which has been actively explored recently and poses a\nsignificant challenge in effectively enhancing recommender systems with logical\nreasoning abilities and open-world knowledge. Current mainstream efforts mainly\ncenter around injecting personalized information from recommendation models\ninto LLMs by customizing input templates or aligning representations between\nsemantic and recommendation spaces at the prediction layer. However, they face\nthree significant limitations: (1) LoRA is mostly used as a core component in\nexisting works, but personalization is not well established in LoRA parameters\nas the LoRA matrix shared by every user may not cater to different users'\ncharacteristics, leading to suboptimal performance. (2) Although lifelong\npersonalized behavior sequences are ideal for personalization, their use raises\neffectiveness and efficiency issues since LLMs require escalating training and\ninference time to extend text lengths. (3) Existing approaches aren't scalable\nfor large datasets due to training efficiency constraints. Thus, LLMs only see\na small fraction of the datasets (e.g., less than 10%) instead of the whole\ndatasets, limiting their exposure to the full training space. To address these\nproblems, we propose RecLoRA. This model incorporates a Personalized LoRA\nmodule that maintains independent LoRAs for different users and a Long-Short\nModality Retriever that retrieves different history lengths for different\nmodalities, significantly improving performance while adding minimal time cost.\nFurthermore, we design a Few2Many Learning Strategy, using a conventional\nrecommendation model as a lens to magnify small training spaces to full spaces.\nExtensive experiments on public datasets demonstrate the efficacy of our\nRecLoRA compared to existing baseline models.\n","authors":["Jiachen Zhu","Jianghao Lin","Xinyi Dai","Bo Chen","Rong Shan","Jieming Zhu","Ruiming Tang","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03533v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00816v2","updated":"2024-08-07T22:40:27Z","published":"2024-02-26T01:17:50Z","title":"Read and Think: An Efficient Step-wise Multimodal Language Model for\n  Document Understanding and Reasoning","summary":"  Understanding the contents of multimodal documents is essential to accurately\nextract relevant evidence and use it for reasoning. Existing document\nunderstanding models tend to generate answers with a single word or phrase\ndirectly, ignoring the source document's evidence and lacking interpretability.\nIn this work, we address the lack of step-wise capabilities through data\naugmentation and extension. Specifically, We use Multi-modal Large Language\nModels (MLLMs), which have strong visual understanding and reasoning abilities,\nas data generators to generate step-wise question-and-answer pairs for document\nimages and use a high-performance LLM as the error detector to filter out noisy\ndata. This step-wise data generation pipeline is implemented using both\ntemplate-based and few-shot methods. We then use the generated high-quality\ndata to train a humanized document understanding and reasoning model,\nspecifically designed to solve complex questions that require reasoning or\nmulti-hop question answering, dubbed DocAssistant. Experimental results\ndemonstrate the effectiveness and application value of step-wise generation,\nshowing a 5 improvement on InfoVQA with complex layouts and a 7 improvement on\nChartQA with complex reasoning, compared to directly generated answers. We hope\nour work highlights the potential of synthetic data and encourages further\nexploration of multi-modal document reasoning capabilities.\n","authors":["Jinxu Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.00816v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10078v2","updated":"2024-08-07T21:05:44Z","published":"2024-07-14T04:53:36Z","title":"Data Imputation using Large Language Model to Accelerate Recommendation\n  System","summary":"  This paper aims to address the challenge of sparse and missing data in\nrecommendation systems, a significant hurdle in the age of big data.\nTraditional imputation methods struggle to capture complex relationships within\nthe data. We propose a novel approach that fine-tune Large Language Model (LLM)\nand use it impute missing data for recommendation systems. LLM which is trained\non vast amounts of text, is able to understand complex relationship among data\nand intelligently fill in missing information. This enriched data is then used\nby the recommendation system to generate more accurate and personalized\nsuggestions, ultimately enhancing the user experience. We evaluate our\nLLM-based imputation method across various tasks within the recommendation\nsystem domain, including single classification, multi-classification, and\nregression compared to traditional data imputation methods. By demonstrating\nthe superiority of LLM imputation over traditional methods, we establish its\npotential for improving recommendation system performance.\n","authors":["Zhicheng Ding","Jiahao Tian","Zhenkai Wang","Jinman Zhao","Siyang Li"],"pdf_url":"https://arxiv.org/pdf/2407.10078v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07812v2","updated":"2024-08-07T18:27:59Z","published":"2024-02-12T17:17:50Z","title":"Retrieval Augmented Thought Process for Private Data Handling in\n  Healthcare","summary":"  Large Language Models (LLMs) have demonstrated the strong potential to assist\nboth clinicians and the general public with their extensive medical knowledge.\nHowever, their application in healthcare is constrained due to concerns about\nthe privacy of data used in training, which prevents the integration of private\nand personal information because of security and ethical issues. Moreover, if\ntheir capabilities can be enhanced with information retrieval to access\nup-to-date knowledge, the current integration of LLMs with Information\nretrieval lacks robustness to imperfect retrieval, which can hinder their\neffectiveness and even reduce overall performance. In this work, we address\nthis challenge by introducing the Retrieval-Augmented Thought Process (RATP).\nGiven access to external knowledge, RATP formulates the thought generation of\nLLMs as a multiple-step decision process. To optimise such a thought process,\nRATP leverages Monte-Carlo Tree Search and learns a proxy reward function that\npermits cost-efficient inference. On a private dataset of electronic medical\nrecords, deliberately excluded from any LLM training set, RATP achieves 35%\nadditional accuracy compared to in-context retrieval-augmented generation for\nthe question-answering task.\n","authors":["Thomas Pouplin","Hao Sun","Samuel Holt","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.07812v2.pdf","comment":"17 pages, 18 figures"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.03936v1","updated":"2024-08-07T17:54:21Z","published":"2024-08-07T17:54:21Z","title":"SLIM-RAFT: A Novel Fine-Tuning Approach to Improve Cross-Linguistic\n  Performance for Mercosur Common Nomenclature","summary":"  Natural language processing (NLP) has seen significant advancements with the\nadvent of large language models (LLMs). However, substantial improvements are\nstill needed for languages other than English, especially for specific domains\nlike the applications of Mercosur Common Nomenclature (NCM), a Brazilian\nHarmonized System (HS). To address this gap, this study uses TeenyTineLLaMA, a\nfoundational Portuguese LLM, as an LLM source to implement the NCM application\nprocessing. Additionally, a simplified Retrieval-Augmented Fine-Tuning (RAFT)\ntechnique, termed SLIM-RAFT, is proposed for task-specific fine-tuning of LLMs.\nThis approach retains the chain-of-thought (CoT) methodology for prompt\ndevelopment in a more concise and streamlined manner, utilizing brief and\nfocused documents for training. The proposed model demonstrates an efficient\nand cost-effective alternative for fine-tuning smaller LLMs, significantly\noutperforming TeenyTineLLaMA and ChatGPT-4 in the same task. Although the\nresearch focuses on NCM applications, the methodology can be easily adapted for\nHS applications worldwide.\n","authors":["Vincius Di Oliveira","Yuri Faanha Bezerra","Li Weigang","Pedro Carvalho Brom","Victor Rafael R. Celestino"],"pdf_url":"https://arxiv.org/pdf/2408.03936v1.pdf","comment":"13 pages, 1 figure, to be publish in International Conference on Web\n  Information Systems and Technologies - WEBIST 2024 proceedings"},{"id":"http://arxiv.org/abs/2310.12294v3","updated":"2024-08-07T17:46:32Z","published":"2023-10-18T19:55:11Z","title":"Open-Set Multivariate Time-Series Anomaly Detection","summary":"  Numerous methods for time-series anomaly detection (TSAD) have emerged in\nrecent years, most of which are unsupervised and assume that only normal\nsamples are available during the training phase, due to the challenge of\nobtaining abnormal data in real-world scenarios. Still, limited samples of\nabnormal data are often available, albeit they are far from representative of\nall possible anomalies. Supervised methods can be utilized to classify normal\nand seen anomalies, but they tend to overfit to the seen anomalies present\nduring training, hence, they fail to generalize to unseen anomalies. We propose\nthe first algorithm to address the open-set TSAD problem, called Multivariate\nOpen-Set Time-Series Anomaly Detector (MOSAD), that leverages only a few shots\nof labeled anomalies during the training phase in order to achieve superior\nanomaly detection performance compared to both supervised and unsupervised TSAD\nalgorithms. MOSAD is a novel multi-head TSAD framework with a shared\nrepresentation space and specialized heads, including the Generative head, the\nDiscriminative head, and the Anomaly-Aware Contrastive head. The latter\nproduces a superior representation space for anomaly detection compared to\nconventional supervised contrastive learning. Extensive experiments on three\nreal-world datasets establish MOSAD as a new state-of-the-art in the TSAD\nfield.\n","authors":["Thomas Lai","Thi Kieu Khanh Ho","Narges Armanfard"],"pdf_url":"https://arxiv.org/pdf/2310.12294v3.pdf","comment":"Accepted to ECAI-2024"},{"id":"http://arxiv.org/abs/2408.03915v1","updated":"2024-08-07T17:20:52Z","published":"2024-08-07T17:20:52Z","title":"Hard to Explain: On the Computational Hardness of In-Distribution Model\n  Interpretation","summary":"  The ability to interpret Machine Learning (ML) models is becoming\nincreasingly essential. However, despite significant progress in the field,\nthere remains a lack of rigorous characterization regarding the innate\ninterpretability of different models. In an attempt to bridge this gap, recent\nwork has demonstrated that it is possible to formally assess interpretability\nby studying the computational complexity of explaining the decisions of various\nmodels. In this setting, if explanations for a particular model can be obtained\nefficiently, the model is considered interpretable (since it can be explained\n``easily''). However, if generating explanations over an ML model is\ncomputationally intractable, it is considered uninterpretable. Prior research\nidentified two key factors that influence the complexity of interpreting an ML\nmodel: (i) the type of the model (e.g., neural networks, decision trees, etc.);\nand (ii) the form of explanation (e.g., contrastive explanations, Shapley\nvalues, etc.). In this work, we claim that a third, important factor must also\nbe considered for this analysis -- the underlying distribution over which the\nexplanation is obtained. Considering the underlying distribution is key in\navoiding explanations that are socially misaligned, i.e., convey information\nthat is biased and unhelpful to users. We demonstrate the significant influence\nof the underlying distribution on the resulting overall interpretation\ncomplexity, in two settings: (i) prediction models paired with an external\nout-of-distribution (OOD) detector; and (ii) prediction models designed to\ninherently generate socially aligned explanations. Our findings prove that the\nexpressiveness of the distribution can significantly influence the overall\ncomplexity of interpretation, and identify essential prerequisites that a model\nmust possess to generate socially aligned explanations.\n","authors":["Guy Amir","Shahaf Bassan","Guy Katz"],"pdf_url":"https://arxiv.org/pdf/2408.03915v1.pdf","comment":"To appear in ECAI 2024"},{"id":"http://arxiv.org/abs/2408.03913v1","updated":"2024-08-07T17:19:15Z","published":"2024-08-07T17:19:15Z","title":"AdapMTL: Adaptive Pruning Framework for Multitask Learning Model","summary":"  In the domain of multimedia and multimodal processing, the efficient handling\nof diverse data streams such as images, video, and sensor data is paramount.\nModel compression and multitask learning (MTL) are crucial in this field,\noffering the potential to address the resource-intensive demands of processing\nand interpreting multiple forms of media simultaneously. However, effectively\ncompressing a multitask model presents significant challenges due to the\ncomplexities of balancing sparsity allocation and accuracy performance across\nmultiple tasks. To tackle these challenges, we propose AdapMTL, an adaptive\npruning framework for MTL models. AdapMTL leverages multiple learnable soft\nthresholds independently assigned to the shared backbone and the task-specific\nheads to capture the nuances in different components' sensitivity to pruning.\nDuring training, it co-optimizes the soft thresholds and MTL model weights to\nautomatically determine the suitable sparsity level at each component to\nachieve both high task accuracy and high overall sparsity. It further\nincorporates an adaptive weighting mechanism that dynamically adjusts the\nimportance of task-specific losses based on each task's robustness to pruning.\nWe demonstrate the effectiveness of AdapMTL through comprehensive experiments\non popular multitask datasets, namely NYU-v2 and Tiny-Taskonomy, with different\narchitectures, showcasing superior performance compared to state-of-the-art\npruning methods.\n","authors":["Mingcan Xiang","Steven Jiaxun Tang","Qizheng Yang","Hui Guan","Tongping Liu"],"pdf_url":"https://arxiv.org/pdf/2408.03913v1.pdf","comment":"13 pages, 9 figures, Published at ACM Multimedia (ACM MM) 2024"},{"id":"http://arxiv.org/abs/2408.03909v1","updated":"2024-08-07T17:13:46Z","published":"2024-08-07T17:13:46Z","title":"LaFA: Latent Feature Attacks on Non-negative Matrix Factorization","summary":"  As Machine Learning (ML) applications rapidly grow, concerns about\nadversarial attacks compromising their reliability have gained significant\nattention. One unsupervised ML method known for its resilience to such attacks\nis Non-negative Matrix Factorization (NMF), an algorithm that decomposes input\ndata into lower-dimensional latent features. However, the introduction of\npowerful computational tools such as Pytorch enables the computation of\ngradients of the latent features with respect to the original data, raising\nconcerns about NMF's reliability. Interestingly, naively deriving the\nadversarial loss for NMF as in the case of ML would result in the\nreconstruction loss, which can be shown theoretically to be an ineffective\nattacking objective. In this work, we introduce a novel class of attacks in NMF\ntermed Latent Feature Attacks (LaFA), which aim to manipulate the latent\nfeatures produced by the NMF process. Our method utilizes the Feature Error\n(FE) loss directly on the latent features. By employing FE loss, we generate\nperturbations in the original data that significantly affect the extracted\nlatent features, revealing vulnerabilities akin to those found in other ML\ntechniques. To handle large peak-memory overhead from gradient back-propagation\nin FE attacks, we develop a method based on implicit differentiation which\nenables their scaling to larger datasets. We validate NMF vulnerabilities and\nFE attacks effectiveness through extensive experiments on synthetic and\nreal-world data.\n","authors":["Minh Vu","Ben Nebgen","Erik Skau","Geigh Zollicoffer","Juan Castorena","Kim Rasmussen","Boian Alexandrov","Manish Bhattarai"],"pdf_url":"https://arxiv.org/pdf/2408.03909v1.pdf","comment":"LA-UR-24-26951"},{"id":"http://arxiv.org/abs/2402.14049v2","updated":"2024-08-07T17:09:10Z","published":"2024-02-21T18:25:04Z","title":"Generative Adversarial Models for Extreme Geospatial Downscaling","summary":"  Addressing the challenges of climate change requires accurate and\nhigh-resolution mapping of geospatial data, especially climate and weather\nvariables. However, many existing geospatial datasets, such as the gridded\noutputs of the state-of-the-art numerical climate models (e.g., general\ncirculation models), are only available at very coarse spatial resolutions due\nto the model complexity and extremely high computational demand.\nDeep-learning-based methods, particularly generative adversarial networks\n(GANs) and their variants, have proved effective for refining natural images\nand have shown great promise in improving geospatial datasets. This paper\ndescribes a conditional GAN-based stochastic geospatial downscaling method that\ncan accommodates very high scaling factors. Compared to most existing methods,\nthe method can generate high-resolution accurate climate datasets from very\nlow-resolution inputs. More importantly, the method explicitly considers the\nuncertainty inherent to the downscaling process that tends to be ignored in\nexisting methods. Given an input, the method can produce a multitude of\nplausible high-resolution samples instead of one single deterministic result.\nThese samples allow for an empirical exploration and inferences of model\nuncertainty and robustness. With a case study of gridded climate datasets (wind\nvelocity and solar irradiance), we demonstrate the performances of the\nframework in downscaling tasks with large scaling factors (up to $64\\times$)\nand highlight the advantages of the framework with a comprehensive comparison\nwith commonly used and most recent downscaling methods, including area-to-point\n(ATP) kriging, deep image prior (DIP), enhanced super-resolution generative\nadversarial networks (ESRGAN), physics-informed resolution-enhancing GAN (PhIRE\nGAN), and an efficient diffusion model for remote sensing image\nsuper-resolution (EDiffSR).\n","authors":["Guiye Li","Guofeng Cao"],"pdf_url":"https://arxiv.org/pdf/2402.14049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.13218v3","updated":"2024-08-07T16:57:06Z","published":"2024-07-18T07:04:33Z","title":"LiNR: Model Based Neural Retrieval on GPUs at LinkedIn","summary":"  This paper introduces LiNR, LinkedIn's large-scale, GPU-based retrieval\nsystem. LiNR supports a billion-sized index on GPU models. We discuss our\nexperiences and challenges in creating scalable, differentiable search indexes\nusing TensorFlow and PyTorch at production scale. In LiNR, both items and model\nweights are integrated into the model binary. Viewing index construction as a\nform of model training, we describe scaling our system for large indexes,\nincorporating full scans and efficient filtering. A key focus is on enabling\nattribute-based pre-filtering for exhaustive GPU searches, addressing the\ncommon challenge of post-filtering in KNN searches that often reduces system\nquality. We further provide multi-embedding retrieval algorithms and strategies\nfor tackling cold start issues in retrieval. Our advancements in supporting\nlarger indexes through quantization are also discussed. We believe LiNR\nrepresents one of the industry's first Live-updated model-based retrieval\nindexes. Applied to out-of-network post recommendations on LinkedIn Feed, LiNR\nhas contributed to a 3% relative increase in professional daily active users.\nWe envisage LiNR as a step towards integrating retrieval and ranking into a\nsingle GPU model, simplifying complex infrastructures and enabling end-to-end\noptimization of the entire differentiable infrastructure through gradient\ndescent.\n","authors":["Fedor Borisyuk","Qingquan Song","Mingzhou Zhou","Ganesh Parameswaran","Madhu Arun","Siva Popuri","Tugrul Bingol","Zhuotao Pei","Kuang-Hsuan Lee","Lu Zheng","Qizhan Shao","Ali Naqvi","Sen Zhou","Aman Gupta"],"pdf_url":"https://arxiv.org/pdf/2407.13218v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2302.04262v3","updated":"2024-08-07T16:54:40Z","published":"2023-02-08T18:55:49Z","title":"Algorithmic Collective Action in Machine Learning","summary":"  We initiate a principled study of algorithmic collective action on digital\nplatforms that deploy machine learning algorithms. We propose a simple\ntheoretical model of a collective interacting with a firm's learning algorithm.\nThe collective pools the data of participating individuals and executes an\nalgorithmic strategy by instructing participants how to modify their own data\nto achieve a collective goal. We investigate the consequences of this model in\nthree fundamental learning-theoretic settings: the case of a nonparametric\noptimal learning algorithm, a parametric risk minimizer, and gradient-based\noptimization. In each setting, we come up with coordinated algorithmic\nstrategies and characterize natural success criteria as a function of the\ncollective's size. Complementing our theory, we conduct systematic experiments\non a skill classification task involving tens of thousands of resumes from a\ngig platform for freelancers. Through more than two thousand model training\nruns of a BERT-like language model, we see a striking correspondence emerge\nbetween our empirical observations and the predictions made by our theory.\nTaken together, our theory and experiments broadly support the conclusion that\nalgorithmic collectives of exceedingly small fractional size can exert\nsignificant control over a platform's learning algorithm.\n","authors":["Moritz Hardt","Eric Mazumdar","Celestine Mendler-Dnner","Tijana Zrnic"],"pdf_url":"https://arxiv.org/pdf/2302.04262v3.pdf","comment":"Published at ICML 2023; Revision corrects epsilon-dependence in the\n  analysis"},{"id":"http://arxiv.org/abs/2402.06859v2","updated":"2024-08-07T16:54:23Z","published":"2024-02-10T01:47:10Z","title":"LiRank: Industrial Large Scale Ranking Models at LinkedIn","summary":"  We present LiRank, a large-scale ranking framework at LinkedIn that brings to\nproduction state-of-the-art modeling architectures and optimization methods. We\nunveil several modeling improvements, including Residual DCN, which adds\nattention and residual connections to the famous DCNv2 architecture. We share\ninsights into combining and tuning SOTA architectures to create a unified\nmodel, including Dense Gating, Transformers and Residual DCN. We also propose\nnovel techniques for calibration and describe how we productionalized deep\nlearning based explore/exploit methods. To enable effective, production-grade\nserving of large ranking models, we detail how to train and compress models\nusing quantization and vocabulary compression. We provide details about the\ndeployment setup for large-scale use cases of Feed ranking, Jobs\nRecommendations, and Ads click-through rate (CTR) prediction. We summarize our\nlearnings from various A/B tests by elucidating the most effective technical\napproaches. These ideas have contributed to relative metrics improvements\nacross the board at LinkedIn: +0.5% member sessions in the Feed, +1.76%\nqualified job applications for Jobs search and recommendations, and +4.3% for\nAds CTR. We hope this work can provide practical insights and solutions for\npractitioners interested in leveraging large-scale deep ranking systems.\n","authors":["Fedor Borisyuk","Mingzhou Zhou","Qingquan Song","Siyu Zhu","Birjodh Tiwana","Ganesh Parameswaran","Siddharth Dangi","Lars Hertel","Qiang Xiao","Xiaochen Hou","Yunbo Ouyang","Aman Gupta","Sheallika Singh","Dan Liu","Hailing Cheng","Lei Le","Jonathan Hung","Sathiya Keerthi","Ruoyan Wang","Fengyu Zhang","Mohit Kothari","Chen Zhu","Daqi Sun","Yun Dai","Xun Luan","Sirou Zhu","Zhiwei Wang","Neil Daftary","Qianqi Shen","Chengming Jiang","Haichao Wei","Maneesh Varshney","Amol Ghoting","Souvik Ghosh"],"pdf_url":"https://arxiv.org/pdf/2402.06859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03877v1","updated":"2024-08-07T16:27:45Z","published":"2024-08-07T16:27:45Z","title":"Knowledge Probing for Graph Representation Learning","summary":"  Graph learning methods have been extensively applied in diverse application\nareas. However, what kind of inherent graph properties e.g. graph proximity,\ngraph structural information has been encoded into graph representation\nlearning for downstream tasks is still under-explored. In this paper, we\npropose a novel graph probing framework (GraphProbe) to investigate and\ninterpret whether the family of graph learning methods has encoded different\nlevels of knowledge in graph representation learning. Based on the intrinsic\nproperties of graphs, we design three probes to systematically investigate the\ngraph representation learning process from different perspectives, respectively\nthe node-wise level, the path-wise level, and the structural level. We\nconstruct a thorough evaluation benchmark with nine representative graph\nlearning methods from random walk based approaches, basic graph neural networks\nand self-supervised graph methods, and probe them on six benchmark datasets for\nnode classification, link prediction and graph classification. The experimental\nevaluation verify that GraphProbe can estimate the capability of graph\nrepresentation learning. Remaking results have been concluded: GCN and\nWeightedGCN methods are relatively versatile methods achieving better results\nwith respect to different tasks.\n","authors":["Mingyu Zhao","Xingyu Huang","Ziyu Lyu","Yanlin Wang","Lixin Cui","Lu Bai"],"pdf_url":"https://arxiv.org/pdf/2408.03877v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03872v1","updated":"2024-08-07T16:22:21Z","published":"2024-08-07T16:22:21Z","title":"Inter-Series Transformer: Attending to Products in Time Series\n  Forecasting","summary":"  Time series forecasting is an important task in many fields ranging from\nsupply chain management to weather forecasting. Recently, Transformer neural\nnetwork architectures have shown promising results in forecasting on common\ntime series benchmark datasets. However, application to supply chain demand\nforecasting, which can have challenging characteristics such as sparsity and\ncross-series effects, has been limited.\n  In this work, we explore the application of Transformer-based models to\nsupply chain demand forecasting. In particular, we develop a new\nTransformer-based forecasting approach using a shared, multi-task per-time\nseries network with an initial component applying attention across time series,\nto capture interactions and help address sparsity. We provide a case study\napplying our approach to successfully improve demand prediction for a medical\ndevice manufacturing company. To further validate our approach, we also apply\nit to public demand forecasting datasets as well and demonstrate competitive to\nsuperior performance compared to a variety of baseline and state-of-the-art\nforecast methods across the private and public datasets.\n","authors":["Rares Cristian","Pavithra Harsha","Clemente Ocejo","Georgia Perakis","Brian Quanz","Ioannis Spantidakis","Hamza Zerhouni"],"pdf_url":"https://arxiv.org/pdf/2408.03872v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08973v2","updated":"2024-08-07T16:21:25Z","published":"2024-04-13T11:40:05Z","title":"PraFFL: A Preference-Aware Scheme in Fair Federated Learning","summary":"  Fairness in federated learning has emerged as a critical concern, aiming to\ndevelop an unbiased model for any special group (e.g., male or female) of\nsensitive features. However, there is a trade-off between model performance and\nfairness, i.e., improving model fairness will decrease model performance.\nExisting approaches have characterized such a trade-off by introducing\nhyperparameters to quantify client's preferences for model fairness and model\nperformance. Nevertheless, these approaches are limited to scenarios where each\nclient has only a single pre-defined preference, and fail to work in practical\nsystems where each client generally have multiple preferences. The key\nchallenge is to design a method that allows the model to adapt to diverse\npreferences of each client in real time. To this end, we propose a\nPreference-aware scheme in Fair Federated Learning paradigm (called PraFFL) to\ngenerate preference-wise model in real time. PraFFL can adaptively adjust the\nmodel based on each client's preferences to meet their needs. We theoretically\nprove that PraFFL can offer the optimal model tailored to an arbitrary\npreference of each client, and show its linear convergence. Experimental\nresults show that our proposed PraFFL outperforms five fair federated learning\nalgorithms in terms of the model's capability of adapting to clients' different\npreferences.\n","authors":["Rongguang Ye","Wei-Bin Kou","Ming Tang"],"pdf_url":"https://arxiv.org/pdf/2404.08973v2.pdf","comment":"14 pages, 10 figures"},{"id":"http://arxiv.org/abs/2403.02426v2","updated":"2024-08-07T16:20:56Z","published":"2024-03-04T19:18:53Z","title":"Digital Twins and Civil Engineering Phases: Reorienting Adoption\n  Strategies","summary":"  Digital twin (DT) technology has received immense attention over the years\ndue to the promises it presents to various stakeholders in science and\nengineering. As a result, different thematic areas of DT have been explored.\nThis is no different in specific fields such as manufacturing, automation, oil\nand gas, and civil engineering, leading to fragmented approaches for\nfield-specific applications. The civil engineering industry is further\ndisadvantaged in this regard as it relies on external techniques by other\nengineering fields for its DT adoption. A rising consequence of these\nextensions is a concentrated application of DT to the operations and\nmaintenance phase. On another spectrum, Building Information Modeling (BIM) is\npervasively utilized in the planning/design phase, and the transient nature of\nthe construction phase remains a challenge for its DT adoption. In this paper,\nwe present a phase-based development of DT in the Architecture, Engineering,\nand Construction industry. We commence by presenting succinct expositions on DT\nas a concept and as a service, and establish a five-level scale system.\nFurthermore, we present separately a systematic literature review of the\nconventional techniques employed at each civil engineering phase. In this\nregard, we identified enabling technologies such as computer vision for\nextended sensing and the Internet of Things for reliable integration.\nUltimately, we attempt to reveal DT as an important tool across the entire life\ncycle of civil engineering projects, and nudge researchers to think more\nholistically in their quest for the integration of DT for civil engineering\napplications.\n","authors":["Taiwo A. Adebiyi","Nafeezat A. Ajenifuja","Ruda Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.02426v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03865v1","updated":"2024-08-07T16:13:43Z","published":"2024-08-07T16:13:43Z","title":"PackMamba: Efficient Processing of Variable-Length Sequences in Mamba\n  training","summary":"  With the evolution of large language models, traditional Transformer models\nbecome computationally demanding for lengthy sequences due to the quadratic\ngrowth in computation with respect to the sequence length. Mamba, emerging as a\ngroundbreaking architecture in the field of generative AI, demonstrates\nremarkable proficiency in handling elongated sequences with reduced\ncomputational and memory complexity. Nevertheless, the existing training\nframework of Mamba presents inefficiency with variable-length sequence inputs.\nEither single-sequence training results in low GPU utilization, or batched\nprocessing of variable-length sequences to a maximum length incurs considerable\nmemory and computational overhead. To address this problem, we analyze the\nperformance of bottleneck operators in Mamba under diverse tensor shapes and\nproposed PackMamba, a high-throughput Mamba that efficiently handles\nvariable-length sequences. Diving deep into state-space models (SSMs), we\nmodify the parallel operators to avoid passing information between individual\nsequences while maintaining high performance. Experimental results on an NVIDIA\nA100 GPU demonstrate throughput exceeding the baseline single-sequence\nprocessing scheme: 3.06x speedup on the 1.4B model and 2.62x on the 2.8B model.\n","authors":["Haoran Xu","Ziqian Liu","Rong Fu","Zhongling Su","Zerui Wang","Zheng Cai","Zhilin Pei","Xingcheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.03865v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15636v3","updated":"2024-08-07T16:07:05Z","published":"2024-05-24T15:22:58Z","title":"Visualize and Paint GAN Activations","summary":"  We investigate how generated structures of GANs correlate with their\nactivations in hidden layers, with the purpose of better understanding the\ninner workings of those models and being able to paint structures with\nunconditionally trained GANs. This gives us more control over the generated\nimages, allowing to generate them from a semantic segmentation map while not\nrequiring such a segmentation in the training data. To this end we introduce\nthe concept of tileable features, allowing us to identify activations that work\nwell for painting.\n","authors":["Rudolf Herdt","Peter Maass"],"pdf_url":"https://arxiv.org/pdf/2405.15636v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.05941v3","updated":"2024-08-07T15:58:17Z","published":"2023-11-10T08:54:51Z","title":"Out-of-Distribution-Aware Electric Vehicle Charging","summary":"  We tackle the challenge of learning to charge Electric Vehicles (EVs) with\nOut-of-Distribution (OOD) data. Traditional scheduling algorithms typically\nfail to balance near-optimal average performance with worst-case guarantees,\nparticularly with OOD data. Model Predictive Control (MPC) is often too\nconservative and data-independent, whereas Reinforcement Learning (RL) tends to\nbe overly aggressive and fully trusts the data, hindering their ability to\nconsistently achieve the best-of-both-worlds. To bridge this gap, we introduce\na novel OOD-aware scheduling algorithm, denoted OOD-Charging. This algorithm\nemploys a dynamic \"awareness radius\", which updates in real-time based on the\nTemporal Difference (TD)-error that reflects the severity of OOD. The\nOOD-Charging algorithm allows for a more effective balance between consistency\nand robustness in EV charging schedules, thereby significantly enhancing\nadaptability and efficiency in real-world charging environments. Our results\ndemonstrate that this approach improves the scheduling reward reliably under\nreal OOD scenarios with remarkable shifts of EV charging behaviors caused by\nCOVID-19 in the Caltech ACN-Data.\n","authors":["Tongxin Li","Chenxi Sun"],"pdf_url":"https://arxiv.org/pdf/2311.05941v3.pdf","comment":"12 pages, 7 figures"},{"id":"http://arxiv.org/abs/2408.03849v1","updated":"2024-08-07T15:46:45Z","published":"2024-08-07T15:46:45Z","title":"Hate Speech Detection and Classification in Amharic Text with Deep\n  Learning","summary":"  Hate speech is a growing problem on social media. It can seriously impact\nsociety, especially in countries like Ethiopia, where it can trigger conflicts\namong diverse ethnic and religious groups. While hate speech detection in\nresource rich languages are progressing, for low resource languages such as\nAmharic are lacking. To address this gap, we develop Amharic hate speech data\nand SBi-LSTM deep learning model that can detect and classify text into four\ncategories of hate speech: racial, religious, gender, and non-hate speech. We\nhave annotated 5k Amharic social media post and comment data into four\ncategories. The data is annotated using a custom annotation tool by a total of\n100 native Amharic speakers. The model achieves a 94.8 F1-score performance.\nFuture improvements will include expanding the dataset and develop state-of-the\nart models.\n  Keywords: Amharic hate speech detection, classification, Amharic dataset,\nDeep Learning, SBi-LSTM\n","authors":["Samuel Minale Gashe","Seid Muhie Yimam","Yaregal Assabie"],"pdf_url":"https://arxiv.org/pdf/2408.03849v1.pdf","comment":"Dataset: https://data.mendeley.com/datasets/p74pfhz3yx/1"},{"id":"http://arxiv.org/abs/2306.07350v3","updated":"2024-08-07T15:36:15Z","published":"2023-06-12T18:16:33Z","title":"G-invariant diffusion maps","summary":"  The diffusion maps embedding of data lying on a manifold has shown success in\ntasks such as dimensionality reduction, clustering, and data visualization. In\nthis work, we consider embedding data sets that were sampled from a manifold\nwhich is closed under the action of a continuous matrix group. An example of\nsuch a data set is images whose planar rotations are arbitrary. The G-invariant\ngraph Laplacian, introduced in Part I of this work, admits eigenfunctions in\nthe form of tensor products between the elements of the irreducible unitary\nrepresentations of the group and eigenvectors of certain matrices. We employ\nthese eigenfunctions to derive diffusion maps that intrinsically account for\nthe group action on the data. In particular, we construct both equivariant and\ninvariant embeddings, which can be used to cluster and align the data points.\nWe demonstrate the utility of our construction in the problem of random\ncomputerized tomography.\n","authors":["Eitan Rosen","Xiuyuan Cheng","Yoel Shkolnisky"],"pdf_url":"https://arxiv.org/pdf/2306.07350v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03842v1","updated":"2024-08-07T15:35:25Z","published":"2024-08-07T15:35:25Z","title":"Bi-Level Spatial and Channel-aware Transformer for Learned Image\n  Compression","summary":"  Recent advancements in learned image compression (LIC) methods have\ndemonstrated superior performance over traditional hand-crafted codecs. These\nlearning-based methods often employ convolutional neural networks (CNNs) or\nTransformer-based architectures. However, these nonlinear approaches frequently\noverlook the frequency characteristics of images, which limits their\ncompression efficiency. To address this issue, we propose a novel\nTransformer-based image compression method that enhances the transformation\nstage by considering frequency components within the feature map. Our method\nintegrates a novel Hybrid Spatial-Channel Attention Transformer Block (HSCATB),\nwhere a spatial-based branch independently handles high and low frequencies at\nthe attention layer, and a Channel-aware Self-Attention (CaSA) module captures\ninformation across channels, significantly improving compression performance.\nAdditionally, we introduce a Mixed Local-Global Feed Forward Network (MLGFFN)\nwithin the Transformer block to enhance the extraction of diverse and rich\ninformation, which is crucial for effective compression. These innovations\ncollectively improve the transformation's ability to project data into a more\ndecorrelated latent space, thereby boosting overall compression efficiency.\nExperimental results demonstrate that our framework surpasses state-of-the-art\nLIC methods in rate-distortion performance.\n","authors":["Hamidreza Soltani","Erfan Ghasemi"],"pdf_url":"https://arxiv.org/pdf/2408.03842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.19588v2","updated":"2024-08-07T15:11:01Z","published":"2024-03-28T17:12:39Z","title":"DenseNets Reloaded: Paradigm Shift Beyond ResNets and ViTs","summary":"  This paper revives Densely Connected Convolutional Networks (DenseNets) and\nreveals the underrated effectiveness over predominant ResNet-style\narchitectures. We believe DenseNets' potential was overlooked due to untouched\ntraining methods and traditional design elements not fully revealing their\ncapabilities. Our pilot study shows dense connections through concatenation are\nstrong, demonstrating that DenseNets can be revitalized to compete with modern\narchitectures. We methodically refine suboptimal components - architectural\nadjustments, block redesign, and improved training recipes towards widening\nDenseNets and boosting memory efficiency while keeping concatenation shortcuts.\nOur models, employing simple architectural elements, ultimately surpass Swin\nTransformer, ConvNeXt, and DeiT-III - key architectures in the residual\nlearning lineage. Furthermore, our models exhibit near state-of-the-art\nperformance on ImageNet-1K, competing with the very recent models and\ndownstream tasks, ADE20k semantic segmentation, and COCO object\ndetection/instance segmentation. Finally, we provide empirical analyses that\nuncover the merits of the concatenation over additive shortcuts, steering a\nrenewed preference towards DenseNet-style designs. Our code is available at\nhttps://github.com/naver-ai/rdnet.\n","authors":["Donghyun Kim","Byeongho Heo","Dongyoon Han"],"pdf_url":"https://arxiv.org/pdf/2403.19588v2.pdf","comment":"ECCV 2024. Code at https://github.com/naver-ai/rdnet"},{"id":"http://arxiv.org/abs/2301.04660v2","updated":"2024-08-07T15:07:41Z","published":"2023-01-11T19:00:00Z","title":"Anomalies, Representations, and Self-Supervision","summary":"  We develop a self-supervised method for density-based anomaly detection using\ncontrastive learning, and test it using event-level anomaly data from CMS\nADC2021. The AnomalyCLR technique is data-driven and uses augmentations of the\nbackground data to mimic non-Standard-Model events in a model-agnostic way. It\nuses a permutation-invariant Transformer Encoder architecture to map the\nobjects measured in a collider event to the representation space, where the\ndata augmentations define a representation space which is sensitive to\npotential anomalous features. An AutoEncoder trained on background\nrepresentations then computes anomaly scores for a variety of signals in the\nrepresentation space. With AnomalyCLR we find significant improvements on\nperformance metrics for all signals when compared to the raw data baseline.\n","authors":["Barry M. Dillon","Luigi Favaro","Friedrich Feiden","Tanmoy Modak","Tilman Plehn"],"pdf_url":"https://arxiv.org/pdf/2301.04660v2.pdf","comment":"19 pages, 3 figures, journal version"},{"id":"http://arxiv.org/abs/2408.03819v1","updated":"2024-08-07T14:55:04Z","published":"2024-08-07T14:55:04Z","title":"Leveraging Variation Theory in Counterfactual Data Augmentation for\n  Optimized Active Learning","summary":"  Active Learning (AL) allows models to learn interactively from user feedback.\nThis paper introduces a counterfactual data augmentation approach to AL,\nparticularly addressing the selection of datapoints for user querying, a\npivotal concern in enhancing data efficiency. Our approach is inspired by\nVariation Theory, a theory of human concept learning that emphasizes the\nessential features of a concept by focusing on what stays the same and what\nchanges. Instead of just querying with existing datapoints, our approach\nsynthesizes artificial datapoints that highlight potential key similarities and\ndifferences among labels using a neuro-symbolic pipeline combining large\nlanguage models (LLMs) and rule-based models. Through an experiment in the\nexample domain of text classification, we show that our approach achieves\nsignificantly higher performance when there are fewer annotated data. As the\nannotated training data gets larger the impact of the generated data starts to\ndiminish showing its capability to address the cold start problem in AL. This\nresearch sheds light on integrating theories of human learning into the\noptimization of AL.\n","authors":["Simret Araya Gebreegziabher","Kuangshi Ai","Zheng Zhang","Elena L. Glassman","Toby Jia-Jun Li"],"pdf_url":"https://arxiv.org/pdf/2408.03819v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03816v1","updated":"2024-08-07T14:52:06Z","published":"2024-08-07T14:52:06Z","title":"Early Prediction of Causes (not Effects) in Healthcare by Long-Term\n  Clinical Time Series Forecasting","summary":"  Machine learning for early syndrome diagnosis aims to solve the intricate\ntask of predicting a ground truth label that most often is the outcome (effect)\nof a medical consensus definition applied to observed clinical measurements\n(causes), given clinical measurements observed several hours before. Instead of\nfocusing on the prediction of the future effect, we propose to directly predict\nthe causes via time series forecasting (TSF) of clinical variables and\ndetermine the effect by applying the gold standard consensus definition to the\nforecasted values. This method has the invaluable advantage of being\nstraightforwardly interpretable to clinical practitioners, and because model\ntraining does not rely on a particular label anymore, the forecasted data can\nbe used to predict any consensus-based label. We exemplify our method by means\nof long-term TSF with Transformer models, with a focus on accurate prediction\nof sparse clinical variables involved in the SOFA-based Sepsis-3 definition and\nthe new Simplified Acute Physiology Score (SAPS-II) definition. Our experiments\nare conducted on two datasets and show that contrary to recent proposals which\nadvocate set function encoders for time series and direct multi-step decoders,\nbest results are achieved by a combination of standard dense encoders with\niterative multi-step decoders. The key for success of iterative multi-step\ndecoding can be attributed to its ability to capture cross-variate dependencies\nand to a student forcing training strategy that teaches the model to rely on\nits own previous time step predictions for the next time step prediction.\n","authors":["Michael Staniek","Marius Fracarolli","Michael Hagmann","Stefan Riezler"],"pdf_url":"https://arxiv.org/pdf/2408.03816v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03806v1","updated":"2024-08-07T14:32:36Z","published":"2024-08-07T14:32:36Z","title":"Trustworthy Image Semantic Communication with GenAI: Explainablity,\n  Controllability, and Efficiency","summary":"  Image semantic communication (ISC) has garnered significant attention for its\npotential to achieve high efficiency in visual content transmission. However,\nexisting ISC systems based on joint source-channel coding face challenges in\ninterpretability, operability, and compatibility. To address these limitations,\nwe propose a novel trustworthy ISC framework. This approach leverages text\nextraction and segmentation mapping techniques to convert images into\nexplainable semantics, while employing Generative Artificial Intelligence\n(GenAI) for multiple downstream inference tasks. We also introduce a multi-rate\nISC transmission protocol that dynamically adapts to both the received\nexplainable semantic content and specific task requirements at the receiver.\nSimulation results demonstrate that our framework achieves explainable\nlearning, decoupled training, and compatible transmission in various\napplication scenarios. Finally, some intriguing research directions and\napplication scenarios are identified.\n","authors":["Xijun Wang","Dongshan Ye","Chenyuan Feng","Howard H. Yang","Xiang Chen","Tony Q. S. Quek"],"pdf_url":"https://arxiv.org/pdf/2408.03806v1.pdf","comment":"8 pages, 4 figures, 2 tables"},{"id":"http://arxiv.org/abs/2407.11075v2","updated":"2024-08-07T14:05:28Z","published":"2024-07-13T04:29:36Z","title":"A Comprehensive Survey on Kolmogorov Arnold Networks (KAN)","summary":"  Through this comprehensive survey of Kolmogorov-Arnold Networks(KAN), we have\ngained a thorough understanding of its theoretical foundation, architectural\ndesign, application scenarios, and current research progress. KAN, with its\nunique architecture and flexible activation functions, excels in handling\ncomplex data patterns and nonlinear relationships, demonstrating wide-ranging\napplication potential. While challenges remain, KAN is poised to pave the way\nfor innovative solutions in various fields, potentially revolutionizing how we\napproach complex computational problems.\n","authors":["Yuntian Hou","Di Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.11075v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15793v2","updated":"2024-08-07T13:59:46Z","published":"2024-07-22T16:51:28Z","title":"CLIP with Generative Latent Replay: a Strong Baseline for Incremental\n  Learning","summary":"  With the emergence of Transformers and Vision-Language Models (VLMs) such as\nCLIP, large pre-trained models have become a common strategy to enhance\nperformance in Continual Learning scenarios. This led to the development of\nnumerous prompting strategies to effectively fine-tune transformer-based models\nwithout succumbing to catastrophic forgetting. However, these methods struggle\nto specialize the model on domains significantly deviating from the\npre-training and preserving its zero-shot capabilities. In this work, we\npropose Continual Generative training for Incremental prompt-Learning, a novel\napproach to mitigate forgetting while adapting a VLM, which exploits generative\nreplay to align prompts to tasks. We also introduce a new metric to evaluate\nzero-shot capabilities within CL benchmarks. Through extensive experiments on\ndifferent domains, we demonstrate the effectiveness of our framework in\nadapting to new tasks while improving zero-shot capabilities. Further analysis\nreveals that our approach can bridge the gap with joint prompt tuning. The\ncodebase is available at https://github.com/aimagelab/mammoth.\n","authors":["Emanuele Frascaroli","Aniello Panariello","Pietro Buzzega","Lorenzo Bonicelli","Angelo Porrello","Simone Calderara"],"pdf_url":"https://arxiv.org/pdf/2407.15793v2.pdf","comment":"15 pages, 1 figure. Accepted at the The 35th British Machine Vision\n  Conference 2024 (BMVC 2024), Glasgow, UK"},{"id":"http://arxiv.org/abs/2404.09657v3","updated":"2024-08-07T13:44:01Z","published":"2024-04-15T10:45:12Z","title":"Sampling for Model Predictive Trajectory Planning in Autonomous Driving\n  using Normalizing Flows","summary":"  Alongside optimization-based planners, sampling-based approaches are often\nused in trajectory planning for autonomous driving due to their simplicity.\nModel predictive path integral control is a framework that builds upon\noptimization principles while incorporating stochastic sampling of input\ntrajectories. This paper investigates several sampling approaches for\ntrajectory generation. In this context, normalizing flows originating from the\nfield of variational inference are considered for the generation of sampling\ndistributions, as they model transformations of simple to more complex\ndistributions. Accordingly, learning-based normalizing flow models are trained\nfor a more efficient exploration of the input domain for the task at hand. The\ndeveloped algorithm and the proposed sampling distributions are evaluated in\ntwo simulation scenarios.\n","authors":["Georg Rabenstein","Lars Ullrich","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.09657v3.pdf","comment":"Accepted to be published as part of the 2024 IEEE Intelligent\n  Vehicles Symposium (IV), Jeju Shinhwa World, Jeju Island, Korea, June 2-5,\n  2024"},{"id":"http://arxiv.org/abs/2408.03765v1","updated":"2024-08-07T13:36:03Z","published":"2024-08-07T13:36:03Z","title":"Reliable Node Similarity Matrix Guided Contrastive Graph Clustering","summary":"  Graph clustering, which involves the partitioning of nodes within a graph\ninto disjoint clusters, holds significant importance for numerous subsequent\napplications. Recently, contrastive learning, known for utilizing supervisory\ninformation, has demonstrated encouraging results in deep graph clustering.\nThis methodology facilitates the learning of favorable node representations for\nclustering by attracting positively correlated node pairs and distancing\nnegatively correlated pairs within the representation space. Nevertheless, a\nsignificant limitation of existing methods is their inadequacy in thoroughly\nexploring node-wise similarity. For instance, some hypothesize that the node\nsimilarity matrix within the representation space is identical, ignoring the\ninherent semantic relationships among nodes. Given the fundamental role of\ninstance similarity in clustering, our research investigates contrastive graph\nclustering from the perspective of the node similarity matrix. We argue that an\nideal node similarity matrix within the representation space should accurately\nreflect the inherent semantic relationships among nodes, ensuring the\npreservation of semantic similarities in the learned representations. In\nresponse to this, we introduce a new framework, Reliable Node Similarity Matrix\nGuided Contrastive Graph Clustering (NS4GC), which estimates an approximately\nideal node similarity matrix within the representation space to guide\nrepresentation learning. Our method introduces node-neighbor alignment and\nsemantic-aware sparsification, ensuring the node similarity matrix is both\naccurate and efficiently sparse. Comprehensive experiments conducted on $8$\nreal-world datasets affirm the efficacy of learning the node similarity matrix\nand the superior performance of NS4GC.\n","authors":["Yunhui Liu","Xinyi Gao","Tieke He","Tao Zheng","Jianhua Zhao","Hongzhi Yin"],"pdf_url":"https://arxiv.org/pdf/2408.03765v1.pdf","comment":"Accepted by IEEE Transactions on Knowledge and Data Engineering\n  (TKDE)"},{"id":"http://arxiv.org/abs/2408.01953v2","updated":"2024-08-07T13:24:38Z","published":"2024-08-04T07:59:17Z","title":"EqvAfford: SE(3) Equivariance for Point-Level Affordance Learning","summary":"  Humans perceive and interact with the world with the awareness of\nequivariance, facilitating us in manipulating different objects in diverse\nposes. For robotic manipulation, such equivariance also exists in many\nscenarios. For example, no matter what the pose of a drawer is (translation,\nrotation and tilt), the manipulation strategy is consistent (grasp the handle\nand pull in a line). While traditional models usually do not have the awareness\nof equivariance for robotic manipulation, which might result in more data for\ntraining and poor performance in novel object poses, we propose our EqvAfford\nframework, with novel designs to guarantee the equivariance in point-level\naffordance learning for downstream robotic manipulation, with great performance\nand generalization ability on representative tasks on objects in diverse poses.\n","authors":["Yue Chen","Chenrui Tie","Ruihai Wu","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2408.01953v2.pdf","comment":"Accept to CVPRWorkshop on Equivariant Vision: From Theory to Practice\n  2024"},{"id":"http://arxiv.org/abs/2207.03927v2","updated":"2024-08-07T13:15:55Z","published":"2022-07-08T14:27:52Z","title":"BAST: Binaural Audio Spectrogram Transformer for Binaural Sound\n  Localization","summary":"  Accurate sound localization in a reverberation environment is essential for\nhuman auditory perception. Recently, Convolutional Neural Networks (CNNs) have\nbeen utilized to model the binaural human auditory pathway. However, CNN shows\nbarriers in capturing the global acoustic features. To address this issue, we\npropose a novel end-to-end Binaural Audio Spectrogram Transformer (BAST) model\nto predict the sound azimuth in both anechoic and reverberation environments.\nTwo modes of implementation, i.e. BAST-SP and BAST-NSP corresponding to BAST\nmodel with shared and non-shared parameters respectively, are explored. Our\nmodel with subtraction interaural integration and hybrid loss achieves an\nangular distance of 1.29 degrees and a Mean Square Error of 1e-3 at all\nazimuths, significantly surpassing CNN based model. The exploratory analysis of\nthe BAST's performance on the left-right hemifields and anechoic and\nreverberation environments shows its generalization ability as well as the\nfeasibility of binaural Transformers in sound localization. Furthermore, the\nanalysis of the attention maps is provided to give additional insights on the\ninterpretation of the localization process in a natural reverberant\nenvironment.\n","authors":["Sheng Kuang","Jie Shi","Kiki van der Heijden","Siamak Mehrkanoon"],"pdf_url":"https://arxiv.org/pdf/2207.03927v2.pdf","comment":"9 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.07118v2","updated":"2024-08-07T13:14:00Z","published":"2024-02-11T07:27:01Z","title":"Next-Generation Teleophthalmology: AI-enabled Quality Assessment Aiding\n  Remote Smartphone-based Consultation","summary":"  Blindness and other eye diseases are a global health concern, particularly in\nlow- and middle-income countries like India. In this regard, during the\nCOVID-19 pandemic, teleophthalmology became a lifeline, and the Grabi\nattachment for smartphone-based eye imaging gained in use. However, quality of\nuser-captured image often remained inadequate, requiring clinician vetting and\ndelays. In this backdrop, we propose an AI-based quality assessment system with\ninstant feedback mimicking clinicians' judgments and tested on patient-captured\nimages. Dividing the complex problem hierarchically, here we tackle a\nnontrivial part, and demonstrate a proof of the concept.\n","authors":["Dhruv Srikanth","Jayang Gurung","N Satya Deepika","Vineet Joshi","Lopamudra Giri","Pravin Vaddavalli","Soumya Jana"],"pdf_url":"https://arxiv.org/pdf/2402.07118v2.pdf","comment":"4 pages, Presented at IEEE EMBC 2024"},{"id":"http://arxiv.org/abs/2404.15213v2","updated":"2024-08-07T13:11:14Z","published":"2024-03-28T10:15:10Z","title":"Automatic Classification of Subjective Time Perception Using Multi-modal\n  Physiological Data of Air Traffic Controllers","summary":"  In high-pressure environments where human individuals must simultaneously\nmonitor multiple entities, communicate effectively, and maintain intense focus,\nthe perception of time becomes a critical factor influencing performance and\nwell-being. One indicator of well-being can be the person's subjective time\nperception. In our project $ChronoPilot$, we aim to develop a device that\nmodulates human subjective time perception. In this study, we present a method\nto automatically assess the subjective time perception of air traffic\ncontrollers, a group often faced with demanding conditions, using their\nphysiological data and eleven state-of-the-art machine learning classifiers.\nThe physiological data consist of photoplethysmogram, electrodermal activity,\nand temperature data. We find that the support vector classifier works best\nwith an accuracy of 79 % and electrodermal activity provides the most\ndescriptive biomarker. These findings are an important step towards closing the\nfeedback loop of our $ChronoPilot$-device to automatically modulate the user's\nsubjective time perception. This technological advancement may promise\nimprovements in task management, stress reduction, and overall productivity in\nhigh-stakes professions.\n","authors":["Till Aust","Eirini Balta","Argiro Vatakis","Heiko Hamann"],"pdf_url":"https://arxiv.org/pdf/2404.15213v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03747v1","updated":"2024-08-07T13:01:10Z","published":"2024-08-07T13:01:10Z","title":"Online Model-based Anomaly Detection in Multivariate Time Series:\n  Taxonomy, Survey, Research Challenges and Future Directions","summary":"  Time-series anomaly detection plays an important role in engineering\nprocesses, like development, manufacturing and other operations involving\ndynamic systems. These processes can greatly benefit from advances in the\nfield, as state-of-the-art approaches may aid in cases involving, for example,\nhighly dimensional data. To provide the reader with understanding of the\nterminology, this survey introduces a novel taxonomy where a distinction\nbetween online and offline, and training and inference is made. Additionally,\nit presents the most popular data sets and evaluation metrics used in the\nliterature, as well as a detailed analysis. Furthermore, this survey provides\nan extensive overview of the state-of-the-art model-based online semi- and\nunsupervised anomaly detection approaches for multivariate time-series data,\ncategorising them into different model families and other properties. The\nbiggest research challenge revolves around benchmarking, as currently there is\nno reliable way to compare different approaches against one another. This\nproblem is two-fold: on the one hand, public data sets suffers from at least\none fundamental flaw, while on the other hand, there is a lack of intuitive and\nrepresentative evaluation metrics in the field. Moreover, the way most\npublications choose a detection threshold disregards real-world conditions,\nwhich hinders the application in the real world. To allow for tangible advances\nin the field, these issues must be addressed in future work.\n","authors":["Lucas Correia","Jan-Christoph Goos","Philipp Klein","Thomas Bck","Anna V. Kononova"],"pdf_url":"https://arxiv.org/pdf/2408.03747v1.pdf","comment":"Submitted to Engineering Applications of Artificial Intelligence\n  journal"},{"id":"http://arxiv.org/abs/2408.03746v1","updated":"2024-08-07T12:59:58Z","published":"2024-08-07T12:59:58Z","title":"Flexible Bayesian Last Layer Models Using Implicit Priors and Diffusion\n  Posterior Sampling","summary":"  Bayesian Last Layer (BLL) models focus solely on uncertainty in the output\nlayer of neural networks, demonstrating comparable performance to more complex\nBayesian models. However, the use of Gaussian priors for last layer weights in\nBayesian Last Layer (BLL) models limits their expressive capacity when faced\nwith non-Gaussian, outlier-rich, or high-dimensional datasets. To address this\nshortfall, we introduce a novel approach that combines diffusion techniques and\nimplicit priors for variational learning of Bayesian last layer weights. This\nmethod leverages implicit distributions for modeling weight priors in BLL,\ncoupled with diffusion samplers for approximating true posterior predictions,\nthereby establishing a comprehensive Bayesian prior and posterior estimation\nstrategy. By delivering an explicit and computationally efficient variational\nlower bound, our method aims to augment the expressive abilities of BLL models,\nenhancing model accuracy, calibration, and out-of-distribution detection\nproficiency. Through detailed exploration and experimental validation, We\nshowcase the method's potential for improving predictive accuracy and\nuncertainty quantification while ensuring computational efficiency.\n","authors":["Jian Xu","Zhiqi Lin","Shigui Li","Min Chen","Junmei Yang","Delu Zeng","John Paisley"],"pdf_url":"https://arxiv.org/pdf/2408.03746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.18381v4","updated":"2024-08-07T12:59:31Z","published":"2023-05-28T06:53:41Z","title":"Distill Gold from Massive Ores: Bi-level Data Pruning towards Efficient\n  Dataset Distillation","summary":"  Data-efficient learning has garnered significant attention, especially given\nthe current trend of large multi-modal models. Recently, dataset distillation\nhas become an effective approach by synthesizing data samples that are\nessential for network training. However, it remains to be explored which\nsamples are essential for the dataset distillation process itself. In this\nwork, we study the data efficiency and selection for the dataset distillation\ntask. By re-formulating the dynamics of distillation, we provide insight into\nthe inherent redundancy in the real dataset, both theoretically and\nempirically. We propose to use the empirical loss value as a static data\npruning criterion. To further compensate for the variation of the data value in\ntraining, we find the most contributing samples based on their causal effects\non the distillation. The proposed selection strategy can efficiently exploit\nthe training dataset, outperform the previous SOTA distillation algorithms, and\nconsistently enhance the distillation algorithms, even on much larger-scale and\nmore heterogeneous datasets, e.g., full ImageNet-1K and Kinetics-400. We\nbelieve this paradigm will open up new avenues in the dynamics of distillation\nand pave the way for efficient dataset distillation. Our code is available on\nhttps://github.com/silicx/GoldFromOres-BiLP.\n","authors":["Yue Xu","Yong-Lu Li","Kaitong Cui","Ziyu Wang","Cewu Lu","Yu-Wing Tai","Chi-Keung Tang"],"pdf_url":"https://arxiv.org/pdf/2305.18381v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2307.12754v4","updated":"2024-08-07T12:51:46Z","published":"2023-07-24T12:52:55Z","title":"Nonparametric Linear Feature Learning in Regression Through\n  Regularisation","summary":"  Representation learning plays a crucial role in automated feature selection,\nparticularly in the context of high-dimensional data, where non-parametric\nmethods often struggle. In this study, we focus on supervised learning\nscenarios where the pertinent information resides within a lower-dimensional\nlinear subspace of the data, namely the multi-index model. If this subspace\nwere known, it would greatly enhance prediction, computation, and\ninterpretation. To address this challenge, we propose a novel method for joint\nlinear feature learning and non-parametric function estimation, aimed at more\neffectively leveraging hidden features for learning. Our approach employs\nempirical risk minimisation, augmented with a penalty on function derivatives,\nensuring versatility. Leveraging the orthogonality and rotation invariance\nproperties of Hermite polynomials, we introduce our estimator, named RegFeaL.\nBy using alternative minimisation, we iteratively rotate the data to improve\nalignment with leading directions. We establish that the expected risk of our\nmethod converges in high-probability to the minimal risk under minimal\nassumptions and with explicit rates. Additionally, we provide empirical results\ndemonstrating the performance of RegFeaL in various experiments.\n","authors":["Bertille Follain","Francis Bach"],"pdf_url":"https://arxiv.org/pdf/2307.12754v4.pdf","comment":"45 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.03735v1","updated":"2024-08-07T12:42:09Z","published":"2024-08-07T12:42:09Z","title":"Advancing Multimodal Large Language Models with Quantization-Aware Scale\n  Learning for Efficient Adaptation","summary":"  This paper presents the first study to explore the potential of parameter\nquantization for multimodal large language models to alleviate the significant\nresource constraint encountered during vision-language instruction tuning. We\nintroduce a Quantization-aware Scale LeArning method based on multimodal\nWarmup, termed QSLAW. This method is grounded in two key innovations: (1) The\nlearning of group-wise scale factors for quantized LLM weights to mitigate the\nquantization error arising from activation outliers and achieve more effective\nvision-language instruction tuning; (2) The implementation of a multimodal\nwarmup that progressively integrates linguistic and multimodal training\nsamples, thereby preventing overfitting of the quantized model to multimodal\ndata while ensuring stable adaptation of multimodal large language models to\ndownstream vision-language tasks. Extensive experiments demonstrate that models\nquantized by QSLAW perform on par with, or even surpass, their full-precision\ncounterparts, while facilitating up to 1.4 times reduction in VL tuning time\nand GPU consumption. Our code is released at https://github.com/xjjxmu/QSLAW.\n","authors":["Jingjing Xie","Yuxin Zhang","Mingbao Lin","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2408.03735v1.pdf","comment":"Accepted by ACMMM2024"},{"id":"http://arxiv.org/abs/2408.03733v1","updated":"2024-08-07T12:41:56Z","published":"2024-08-07T12:41:56Z","title":"Bayes-optimal learning of an extensive-width neural network from\n  quadratically many samples","summary":"  We consider the problem of learning a target function corresponding to a\nsingle hidden layer neural network, with a quadratic activation function after\nthe first layer, and random weights. We consider the asymptotic limit where the\ninput dimension and the network width are proportionally large. Recent work\n[Cui & al '23] established that linear regression provides Bayes-optimal test\nerror to learn such a function when the number of available samples is only\nlinear in the dimension. That work stressed the open challenge of theoretically\nanalyzing the optimal test error in the more interesting regime where the\nnumber of samples is quadratic in the dimension. In this paper, we solve this\nchallenge for quadratic activations and derive a closed-form expression for the\nBayes-optimal test error. We also provide an algorithm, that we call GAMP-RIE,\nwhich combines approximate message passing with rotationally invariant matrix\ndenoising, and that asymptotically achieves the optimal performance.\nTechnically, our result is enabled by establishing a link with recent works on\noptimal denoising of extensive-rank matrices and on the ellipsoid fitting\nproblem. We further show empirically that, in the absence of noise,\nrandomly-initialized gradient descent seems to sample the space of weights,\nleading to zero training loss, and averaging over initialization leads to a\ntest error equal to the Bayes-optimal one.\n","authors":["Antoine Maillard","Emanuele Troiani","Simon Martin","Florent Krzakala","Lenka Zdeborov"],"pdf_url":"https://arxiv.org/pdf/2408.03733v1.pdf","comment":"47 pages"},{"id":"http://arxiv.org/abs/2408.03732v1","updated":"2024-08-07T12:38:23Z","published":"2024-08-07T12:38:23Z","title":"Question Rephrasing for Quantifying Uncertainty in Large Language\n  Models: Applications in Molecular Chemistry Tasks","summary":"  Uncertainty quantification enables users to assess the reliability of\nresponses generated by large language models (LLMs). We present a novel\nQuestion Rephrasing technique to evaluate the input uncertainty of LLMs, which\nrefers to the uncertainty arising from equivalent variations of the inputs\nprovided to LLMs. This technique is integrated with sampling methods that\nmeasure the output uncertainty of LLMs, thereby offering a more comprehensive\nuncertainty assessment. We validated our approach on property prediction and\nreaction prediction for molecular chemistry tasks.\n","authors":["Zizhang Chen","Pengyu Hong","Sandeep Madireddy"],"pdf_url":"https://arxiv.org/pdf/2408.03732v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03728v1","updated":"2024-08-07T12:33:46Z","published":"2024-08-07T12:33:46Z","title":"A Convex-optimization-based Layer-wise Post-training Pruner for Large\n  Language Models","summary":"  Pruning is a critical strategy for compressing trained large language models\n(LLMs), aiming at substantial memory conservation and computational\nacceleration without compromising performance. However, existing pruning\nmethods often necessitate inefficient retraining for billion-scale LLMs or rely\non heuristic methods such as the optimal brain surgeon framework, which degrade\nperformance. In this paper, we introduce FISTAPruner, the first post-training\npruner based on convex optimization models and algorithms. Specifically, we\npropose a convex optimization model incorporating $\\ell_1$ norm to induce\nsparsity and utilize the FISTA solver for optimization. FISTAPruner\nincorporates an intra-layer cumulative error correction mechanism and supports\nparallel pruning. We comprehensively evaluate FISTAPruner on models such as\nOPT, LLaMA, LLaMA-2, and LLaMA-3 with 125M to 70B parameters under unstructured\nand 2:4 semi-structured sparsity, demonstrating superior performance over\nexisting state-of-the-art methods across various language benchmarks.\n","authors":["Pengxiang Zhao","Hanyu Hu","Ping Li","Yi Zheng","Zhefeng Wang","Xiaoming Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.03728v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03706v1","updated":"2024-08-07T11:44:32Z","published":"2024-08-07T11:44:32Z","title":"Local Topology Measures of Contextual Language Model Latent Spaces With\n  Applications to Dialogue Term Extraction","summary":"  A common approach for sequence tagging tasks based on contextual word\nrepresentations is to train a machine learning classifier directly on these\nembedding vectors. This approach has two shortcomings. First, such methods\nconsider single input sequences in isolation and are unable to put an\nindividual embedding vector in relation to vectors outside the current local\ncontext of use. Second, the high performance of these models relies on\nfine-tuning the embedding model in conjunction with the classifier, which may\nnot always be feasible due to the size or inaccessibility of the underlying\nfeature-generation model. It is thus desirable, given a collection of embedding\nvectors of a corpus, i.e., a datastore, to find features of each vector that\ndescribe its relation to other, similar vectors in the datastore. With this in\nmind, we introduce complexity measures of the local topology of the latent\nspace of a contextual language model with respect to a given datastore. The\neffectiveness of our features is demonstrated through their application to\ndialogue term extraction. Our work continues a line of research that explores\nthe manifold hypothesis for word embeddings, demonstrating that local structure\nin the space carved out by word embeddings can be exploited to infer semantic\nproperties.\n","authors":["Benjamin Matthias Ruppik","Michael Heck","Carel van Niekerk","Renato Vukovic","Hsien-chin Lin","Shutong Feng","Marcus Zibrowius","Milica Gai"],"pdf_url":"https://arxiv.org/pdf/2408.03706v1.pdf","comment":"Accepted as a long paper to SIGDIAL 2024. 9 pages, 2 figures, 3\n  tables"},{"id":"http://arxiv.org/abs/2312.10271v2","updated":"2024-08-07T11:32:19Z","published":"2023-12-16T00:23:21Z","title":"Robustness of Deep Learning for Accelerated MRI: Benefits of Diverse\n  Training Data","summary":"  Deep learning based methods for image reconstruction are state-of-the-art for\na variety of imaging tasks. However, neural networks often perform worse if the\ntraining data differs significantly from the data they are applied to. For\nexample, a model trained for accelerated magnetic resonance imaging (MRI) on\none scanner performs worse on another scanner. In this work, we investigate the\nimpact of the training data on a model's performance and robustness for\naccelerated MRI. We find that models trained on the combination of various data\ndistributions, such as those obtained from different MRI scanners and\nanatomies, exhibit robustness equal or superior to models trained on the best\nsingle distribution for a specific target distribution. Thus training on such\ndiverse data tends to improve robustness. Furthermore, training on such a\ndiverse dataset does not compromise in-distribution performance, i.e., a model\ntrained on diverse data yields in-distribution performance at least as good as\nmodels trained on the more narrow individual distributions. Our results suggest\nthat training a model for imaging on a variety of distributions tends to yield\na more effective and robust model than maintaining separate models for\nindividual distributions.\n","authors":["Kang Lin","Reinhard Heckel"],"pdf_url":"https://arxiv.org/pdf/2312.10271v2.pdf","comment":"ICML 2024"},{"id":"http://arxiv.org/abs/2408.03694v1","updated":"2024-08-07T11:14:18Z","published":"2024-08-07T11:14:18Z","title":"A Blockchain-based Reliable Federated Meta-learning for Metaverse: A\n  Dual Game Framework","summary":"  The metaverse, envisioned as the next digital frontier for avatar-based\nvirtual interaction, involves high-performance models. In this dynamic\nenvironment, users' tasks frequently shift, requiring fast model\npersonalization despite limited data. This evolution consumes extensive\nresources and requires vast data volumes. To address this, meta-learning\nemerges as an invaluable tool for metaverse users, with federated meta-learning\n(FML), offering even more tailored solutions owing to its adaptive\ncapabilities. However, the metaverse is characterized by users heterogeneity\nwith diverse data structures, varied tasks, and uneven sample sizes,\npotentially undermining global training outcomes due to statistical difference.\nGiven this, an urgent need arises for smart coalition formation that accounts\nfor these disparities. This paper introduces a dual game-theoretic framework\nfor metaverse services involving meta-learners as workers to manage FML. A\nblockchain-based cooperative coalition formation game is crafted, grounded on a\nreputation metric, user similarity, and incentives. We also introduce a novel\nreputation system based on users' historical contributions and potential\ncontributions to present tasks, leveraging correlations between past and new\ntasks. Finally, a Stackelberg game-based incentive mechanism is presented to\nattract reliable workers to participate in meta-learning, minimizing users'\nenergy costs, increasing payoffs, boosting FML efficacy, and improving\nmetaverse utility. Results show that our dual game framework outperforms\nbest-effort, random, and non-uniform clustering schemes - improving training\nperformance by up to 10%, cutting completion times by as much as 30%, enhancing\nmetaverse utility by more than 25%, and offering up to 5% boost in training\nefficiency over non-blockchain systems, effectively countering misbehaving\nusers.\n","authors":["Emna Baccour","Aiman Erbad","Amr Mohamed","Mounir Hamdi","Mohsen Guizani"],"pdf_url":"https://arxiv.org/pdf/2408.03694v1.pdf","comment":"Accepted in IEEE Internet of Things Journal"},{"id":"http://arxiv.org/abs/2408.03691v1","updated":"2024-08-07T11:13:19Z","published":"2024-08-07T11:13:19Z","title":"Generative Design of Periodic Orbits in the Restricted Three-Body\n  Problem","summary":"  The Three-Body Problem has fascinated scientists for centuries and it has\nbeen crucial in the design of modern space missions. Recent developments in\nGenerative Artificial Intelligence hold transformative promise for addressing\nthis longstanding problem. This work investigates the use of Variational\nAutoencoder (VAE) and its internal representation to generate periodic orbits.\nWe utilize a comprehensive dataset of periodic orbits in the Circular\nRestricted Three-Body Problem (CR3BP) to train deep-learning architectures that\ncapture key orbital characteristics, and we set up physical evaluation metrics\nfor the generated trajectories. Through this investigation, we seek to enhance\nthe understanding of how Generative AI can improve space mission planning and\nastrodynamics research, leading to novel, data-driven approaches in the field.\n","authors":["Alvaro Francisco Gil","Walther Litteri","Victor Rodriguez-Fernandez","David Camacho","Massimiliano Vasile"],"pdf_url":"https://arxiv.org/pdf/2408.03691v1.pdf","comment":"SPAICE Conference 2024 (7 pages)"},{"id":"http://arxiv.org/abs/2408.03685v1","updated":"2024-08-07T10:53:07Z","published":"2024-08-07T10:53:07Z","title":"RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks","summary":"  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN.\n","authors":["Shengren Hou","Shuyi Gao","Weijie Xia","Edgar Mauricio Salazar Duque","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2408.03685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03669v1","updated":"2024-08-07T10:24:59Z","published":"2024-08-07T10:24:59Z","title":"Beyond Over-smoothing: Uncovering the Trainability Challenges in Deep\n  Graph Neural Networks","summary":"  The drastic performance degradation of Graph Neural Networks (GNNs) as the\ndepth of the graph propagation layers exceeds 8-10 is widely attributed to a\nphenomenon of Over-smoothing. Although recent research suggests that\nOver-smoothing may not be the dominant reason for such a performance\ndegradation, they have not provided rigorous analysis from a theoretical view,\nwhich warrants further investigation. In this paper, we systematically analyze\nthe real dominant problem in deep GNNs and identify the issues that these GNNs\ntowards addressing Over-smoothing essentially work on via empirical experiments\nand theoretical gradient analysis. We theoretically prove that the difficult\ntraining problem of deep MLPs is actually the main challenge, and various\nexisting methods that supposedly tackle Over-smoothing actually improve the\ntrainability of MLPs, which is the main reason for their performance gains. Our\nfurther investigation into trainability issues reveals that properly\nconstrained smaller upper bounds of gradient flow notably enhance the\ntrainability of GNNs. Experimental results on diverse datasets demonstrate\nconsistency between our theoretical findings and empirical evidence. Our\nanalysis provides new insights in constructing deep graph models.\n","authors":["Jie Peng","Runlin Lei","Zhewei Wei"],"pdf_url":"https://arxiv.org/pdf/2408.03669v1.pdf","comment":"CIKM2024"},{"id":"http://arxiv.org/abs/2403.04202v5","updated":"2024-08-07T10:10:34Z","published":"2024-03-07T04:12:24Z","title":"Dynamics of Moral Behavior in Heterogeneous Populations of Learning\n  Agents","summary":"  Growing concerns about safety and alignment of AI systems highlight the\nimportance of embedding moral capabilities in artificial agents: a promising\nsolution is the use of learning from experience, i.e., Reinforcement Learning.\nIn multi-agent (social) environments, complex population-level phenomena may\nemerge from interactions between individual learning agents. Many of the\nexisting studies rely on simulated social dilemma environments to study the\ninteractions of independent learning agents; however, they tend to ignore the\nmoral heterogeneity that is likely to be present in societies of agents in\npractice. For example, at different points in time a single learning agent may\nface opponents who are consequentialist (i.e., focused on maximizing outcomes\nover time), norm-based (i.e., conforming to specific norms), or virtue-based\n(i.e., considering a combination of different virtues). The extent to which\nagents' co-development may be impacted by such moral heterogeneity in\npopulations is not well understood. In this paper, we present a study of the\nlearning dynamics of morally heterogeneous populations interacting in a social\ndilemma setting. Using an Iterated Prisoner's Dilemma environment with a\npartner selection mechanism, we investigate the extent to which the prevalence\nof diverse moral agents in populations affects individual agents' learning\nbehaviors and emergent population-level outcomes. We observe several types of\nnon-trivial interactions between pro-social and anti-social agents, and find\nthat certain types of moral agents are able to steer selfish agents towards\nmore cooperative behavior.\n","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"pdf_url":"https://arxiv.org/pdf/2403.04202v5.pdf","comment":"Accepted at AIES 2024 (7th AAAI/ACM Conference on AI, Ethics, and\n  Society - San Jose, CA, USA)"},{"id":"http://arxiv.org/abs/2408.03664v1","updated":"2024-08-07T10:06:04Z","published":"2024-08-07T10:06:04Z","title":"AI-Driven approach for sustainable extraction of earth's subsurface\n  renewable energy while minimizing seismic activity","summary":"  Deep Geothermal Energy, Carbon Capture and Storage, and Hydrogen Storage hold\nconsiderable promise for meeting the energy sector's large-scale requirements\nand reducing CO$_2$ emissions. However, the injection of fluids into the\nEarth's crust, essential for these activities, can induce or trigger\nearthquakes. In this paper, we highlight a new approach based on Reinforcement\nLearning for the control of human-induced seismicity in the highly complex\nenvironment of an underground reservoir. This complex system poses significant\nchallenges in the control design due to parameter uncertainties and unmodeled\ndynamics. We show that the reinforcement learning algorithm can interact\nefficiently with a robust controller, by choosing the controller parameters in\nreal-time, reducing human-induced seismicity and allowing the consideration of\nfurther production objectives, \\textit{e.g.}, minimal control power.\nSimulations are presented for a simplified underground reservoir under various\nenergy demand scenarios, demonstrating the reliability and effectiveness of the\nproposed control-reinforcement learning approach.\n","authors":["Diego Gutierrez-Oribio","Alexandros Stathas","Ioannis Stefanou"],"pdf_url":"https://arxiv.org/pdf/2408.03664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18613v2","updated":"2024-08-07T09:46:49Z","published":"2024-03-27T14:28:44Z","title":"Scalable Lipschitz Estimation for CNNs","summary":"  Estimating the Lipschitz constant of deep neural networks is of growing\ninterest as it is useful for informing on generalisability and adversarial\nrobustness. Convolutional neural networks (CNNs) in particular, underpin much\nof the recent success in computer vision related applications. However,\nalthough existing methods for estimating the Lipschitz constant can be tight,\nthey have limited scalability when applied to CNNs. To tackle this, we propose\na novel method to accelerate Lipschitz constant estimation for CNNs. The core\nidea is to divide a large convolutional block via a joint layer and width-wise\npartition, into a collection of smaller blocks. We prove an upper-bound on the\nLipschitz constant of the larger block in terms of the Lipschitz constants of\nthe smaller blocks. Through varying the partition factor, the resulting method\ncan be adjusted to prioritise either accuracy or scalability and permits\nparallelisation. We demonstrate an enhanced scalability and comparable accuracy\nto existing baselines through a range of experiments.\n","authors":["Yusuf Sulehman","Tingting Mu"],"pdf_url":"https://arxiv.org/pdf/2403.18613v2.pdf","comment":"An inconsistency between the input of the flattened convolutional\n  block and the flattened, partitioned input impacts the validity of the\n  proposed Lipschitz bound"},{"id":"http://arxiv.org/abs/2408.03655v1","updated":"2024-08-07T09:45:24Z","published":"2024-08-07T09:45:24Z","title":"Consumer Transactions Simulation through Generative Adversarial Networks","summary":"  In the rapidly evolving domain of large-scale retail data systems,\nenvisioning and simulating future consumer transactions has become a crucial\narea of interest. It offers significant potential to fortify demand forecasting\nand fine-tune inventory management. This paper presents an innovative\napplication of Generative Adversarial Networks (GANs) to generate synthetic\nretail transaction data, specifically focusing on a novel system architecture\nthat combines consumer behavior modeling with stock-keeping unit (SKU)\navailability constraints to address real-world assortment optimization\nchallenges. We diverge from conventional methodologies by integrating SKU data\ninto our GAN architecture and using more sophisticated embedding methods (e.g.,\nhyper-graphs). This design choice enables our system to generate not only\nsimulated consumer purchase behaviors but also reflects the dynamic interplay\nbetween consumer behavior and SKU availability -- an aspect often overlooked,\namong others, because of data scarcity in legacy retail simulation models. Our\nGAN model generates transactions under stock constraints, pioneering a\nresourceful experimental system with practical implications for real-world\nretail operation and strategy. Preliminary results demonstrate enhanced realism\nin simulated transactions measured by comparing generated items with real ones\nusing methods employed earlier in related studies. This underscores the\npotential for more accurate predictive modeling.\n","authors":["Sergiy Tkachuk","Szymon ukasik","Anna Wrblewska"],"pdf_url":"https://arxiv.org/pdf/2408.03655v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.03652v1","updated":"2024-08-07T09:34:55Z","published":"2024-08-07T09:34:55Z","title":"mucAI at WojoodNER 2024: Arabic Named Entity Recognition with Nearest\n  Neighbor Search","summary":"  Named Entity Recognition (NER) is a task in Natural Language Processing (NLP)\nthat aims to identify and classify entities in text into predefined categories.\nHowever, when applied to Arabic data, NER encounters unique challenges stemming\nfrom the language's rich morphological inflections, absence of capitalization\ncues, and spelling variants, where a single word can comprise multiple\nmorphemes. In this paper, we introduce Arabic KNN-NER, our submission to the\nWojood NER Shared Task 2024 (ArabicNLP 2024). We have participated in the\nshared sub-task 1 Flat NER. In this shared sub-task, we tackle fine-grained\nflat-entity recognition for Arabic text, where we identify a single main entity\nand possibly zero or multiple sub-entities for each word. Arabic KNN-NER\naugments the probability distribution of a fine-tuned model with another label\nprobability distribution derived from performing a KNN search over the cached\ntraining data. Our submission achieved 91% on the test set on the WojoodFine\ndataset, placing Arabic KNN-NER on top of the leaderboard for the shared task.\n","authors":["Ahmed Abdou","Tasneem Mohsen"],"pdf_url":"https://arxiv.org/pdf/2408.03652v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.03703v2","updated":"2024-08-07T09:18:40Z","published":"2022-08-07T12:02:48Z","title":"Granger Causality using Neural Networks","summary":"  Dependence between nodes in a network is an important concept that pervades\nmany areas including finance, politics, sociology, genomics and the brain\nsciences. One way to characterize dependence between components of a\nmultivariate time series data is via Granger Causality (GC). Standard\ntraditional approaches to GC estimation / inference commonly assume linear\ndynamics, however such simplification does not hold in many real-world\napplications where signals are inherently non-linear. In such cases, imposing\nlinear models such as vector autoregressive (VAR) models can lead to\nmis-characterization of true Granger Causal interactions. To overcome this\nlimitation, Tank et al (IEEE Transactions on Pattern Analysis and Machine\nLearning, 2022) proposed a solution that uses neural networks with sparse\nregularization penalties. The regularization encourages learnable weights to be\nsparse, which enables inference on GC. This paper overcomes the limitations of\ncurrent methods by leveraging advances in machine learning and deep learning\nwhich have been demonstrated to learn hidden patterns in the data. We propose\nnovel classes of models that can handle underlying non-linearity in a\ncomputationally efficient manner, simultaneously providing GC and lag order\nselection. Firstly, we present the Learned Kernel VAR (LeKVAR) model that\nlearns kernel parameterized by a shared neural net followed by penalization on\nlearnable weights to discover GC structure. Secondly, we show one can directly\ndecouple lags and individual time series importance via decoupled penalties.\nThis is important as we want to select the lag order during the process of GC\nestimation. This decoupling acts as a filtering and can be extended to any DL\nmodel including Multi-Layer Perceptrons (MLP), Recurrent Neural Networks (RNN),\nLong Short Term Memory Networks (LSTM), Transformers etc, for simultaneous GC\nestimation and lag selection.\n","authors":["Malik Shahid Sultan","Samuel Horvath","Hernando Ombao"],"pdf_url":"https://arxiv.org/pdf/2208.03703v2.pdf","comment":"To be Submitted to a Journal work Presented at JSM. arXiv admin note:\n  text overlap with arXiv:1802.05842 by other authors"},{"id":"http://arxiv.org/abs/2407.19265v2","updated":"2024-08-07T09:16:12Z","published":"2024-07-27T14:16:25Z","title":"Towards Robust Few-shot Class Incremental Learning in Audio\n  Classification using Contrastive Representation","summary":"  In machine learning applications, gradual data ingress is common, especially\nin audio processing where incremental learning is vital for real-time\nanalytics. Few-shot class-incremental learning addresses challenges arising\nfrom limited incoming data. Existing methods often integrate additional\ntrainable components or rely on a fixed embedding extractor post-training on\nbase sessions to mitigate concerns related to catastrophic forgetting and the\ndangers of model overfitting. However, using cross-entropy loss alone during\nbase session training is suboptimal for audio data. To address this, we propose\nincorporating supervised contrastive learning to refine the representation\nspace, enhancing discriminative power and leading to better generalization\nsince it facilitates seamless integration of incremental classes, upon arrival.\nExperimental results on NSynth and LibriSpeech datasets with 100 classes, as\nwell as ESC dataset with 50 and 10 classes, demonstrate state-of-the-art\nperformance.\n","authors":["Riyansha Singh","Parinita Nema","Vinod K Kurmi"],"pdf_url":"https://arxiv.org/pdf/2407.19265v2.pdf","comment":"INTERSPEECH 2024 accepted"},{"id":"http://arxiv.org/abs/2408.03287v2","updated":"2024-08-07T09:07:01Z","published":"2024-08-06T16:35:25Z","title":"Malicious Internet Entity Detection Using Local Graph Inference","summary":"  Detection of malicious behavior in a large network is a challenging problem\nfor machine learning in computer security, since it requires a model with high\nexpressive power and scalable inference. Existing solutions struggle to achieve\nthis feat -- current cybersec-tailored approaches are still limited in\nexpressivity, and methods successful in other domains do not scale well for\nlarge volumes of data, rendering frequent retraining impossible. This work\nproposes a new perspective for learning from graph data that is modeling\nnetwork entity interactions as a large heterogeneous graph. High expressivity\nof the method is achieved with neural network architecture HMILnet that\nnaturally models this type of data and provides theoretical guarantees. The\nscalability is achieved by pursuing local graph inference, i.e., classifying\nindividual vertices and their neighborhood as independent samples. Our\nexperiments exhibit improvement over the state-of-the-art Probabilistic Threat\nPropagation (PTP) algorithm, show a further threefold accuracy improvement when\nadditional data is used, which is not possible with the PTP algorithm, and\ndemonstrate the generalization capabilities of the method to new, previously\nunseen entities.\n","authors":["Simon Mandlik","Tomas Pevny","Vaclav Smidl","Lukas Bajer"],"pdf_url":"https://arxiv.org/pdf/2408.03287v2.pdf","comment":"A preprint. Full publication:\n  https://ieeexplore.ieee.org/document/10418120"},{"id":"http://arxiv.org/abs/2403.14973v2","updated":"2024-08-07T08:53:25Z","published":"2024-03-22T06:04:11Z","title":"Pose-Aware Self-Supervised Learning with Viewpoint Trajectory\n  Regularization","summary":"  Learning visual features from unlabeled images has proven successful for\nsemantic categorization, often by mapping different $views$ of the same object\nto the same feature to achieve recognition invariance. However, visual\nrecognition involves not only identifying $what$ an object is but also\nunderstanding $how$ it is presented. For example, seeing a car from the side\nversus head-on is crucial for deciding whether to stay put or jump out of the\nway. While unsupervised feature learning for downstream viewpoint reasoning is\nimportant, it remains under-explored, partly due to the lack of a standardized\nevaluation method and benchmarks.\n  We introduce a new dataset of adjacent image triplets obtained from a\nviewpoint trajectory, without any semantic or pose labels. We benchmark both\nsemantic classification and pose estimation accuracies on the same visual\nfeature. Additionally, we propose a viewpoint trajectory regularization loss\nfor learning features from unlabeled image triplets. Our experiments\ndemonstrate that this approach helps develop a visual representation that\nencodes object identity and organizes objects by their poses, retaining\nsemantic classification accuracy while achieving emergent global pose awareness\nand better generalization to novel objects. Our dataset and code are available\nat http://pwang.pw/trajSSL/.\n","authors":["Jiayun Wang","Yubei Chen","Stella X. Yu"],"pdf_url":"https://arxiv.org/pdf/2403.14973v2.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03636v1","updated":"2024-08-07T08:51:10Z","published":"2024-08-07T08:51:10Z","title":"Time is Not Enough: Time-Frequency based Explanation for Time-Series\n  Black-Box Models","summary":"  Despite the massive attention given to time-series explanations due to their\nextensive applications, a notable limitation in existing approaches is their\nprimary reliance on the time-domain. This overlooks the inherent characteristic\nof time-series data containing both time and frequency features. In this work,\nwe present Spectral eXplanation (SpectralX), an XAI framework that provides\ntime-frequency explanations for time-series black-box classifiers. This easily\nadaptable framework enables users to \"plug-in\" various perturbation-based XAI\nmethods for any pre-trained time-series classification models to assess their\nimpact on the explanation quality without having to modify the framework\narchitecture. Additionally, we introduce Feature Importance Approximations\n(FIA), a new perturbation-based XAI method. These methods consist of feature\ninsertion, deletion, and combination techniques to enhance computational\nefficiency and class-specific explanations in time-series classification tasks.\nWe conduct extensive experiments in the generated synthetic dataset and various\nUCR Time-Series datasets to first compare the explanation performance of FIA\nand other existing perturbation-based XAI methods in both time-domain and\ntime-frequency domain, and then show the superiority of our FIA in the\ntime-frequency domain with the SpectralX framework. Finally, we conduct a user\nstudy to confirm the practicality of our FIA in SpectralX framework for\nclass-specific time-frequency based time-series explanations. The source code\nis available in https://github.com/gustmd0121/Time_is_not_Enough\n","authors":["Hyunseung Chung","Sumin Jo","Yeonsu Kwon","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03636v1.pdf","comment":"Accepted to CIKM 2024 (10 pages, 4 figures, 6 tables)"},{"id":"http://arxiv.org/abs/2408.03626v1","updated":"2024-08-07T08:37:23Z","published":"2024-08-07T08:37:23Z","title":"On the choice of the non-trainable internal weights in random feature\n  maps","summary":"  The computationally cheap machine learning architecture of random feature\nmaps can be viewed as a single-layer feedforward network in which the weights\nof the hidden layer are random but fixed and only the outer weights are learned\nvia linear regression. The internal weights are typically chosen from a\nprescribed distribution. The choice of the internal weights significantly\nimpacts the accuracy of random feature maps. We address here the task of how to\nbest select the internal weights. In particular, we consider the forecasting\nproblem whereby random feature maps are used to learn a one-step propagator map\nfor a dynamical system. We provide a computationally cheap hit-and-run\nalgorithm to select good internal weights which lead to good forecasting skill.\nWe show that the number of good features is the main factor controlling the\nforecasting skill of random feature maps and acts as an effective feature\ndimension. Lastly, we compare random feature maps with single-layer feedforward\nneural networks in which the internal weights are now learned using gradient\ndescent. We find that random feature maps have superior forecasting\ncapabilities whilst having several orders of magnitude lower computational\ncost.\n","authors":["Pinak Mandal","Georg A. Gottwald"],"pdf_url":"https://arxiv.org/pdf/2408.03626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.01185v5","updated":"2024-08-07T08:31:05Z","published":"2023-12-02T17:24:17Z","title":"A ripple in time: a discontinuity in American history","summary":"  In this technical note we suggest a novel approach to discover temporal\n(related and unrelated to language dilation) and personality (authorship\nattribution) in historical datasets. We exemplify our approach on the State of\nthe Union speeches given by the past 42 US presidents: this dataset is known\nfor its relatively small amount of data, and high variability of the amount and\nstyle of texts. Nevertheless we manage to achieve about 95\\% accuracy on the\nauthorship attribution task, and pin down the date of writing to a single\npresidential term.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2312.01185v5.pdf","comment":"6 pages, 8 figures; GitHub repository\n  (https://github.com/sashakolpakov/ripple_in_time); restructured manuscript"},{"id":"http://arxiv.org/abs/2408.03619v1","updated":"2024-08-07T08:23:42Z","published":"2024-08-07T08:23:42Z","title":"Making Robust Generalizers Less Rigid with Soft Ascent-Descent","summary":"  While the traditional formulation of machine learning tasks is in terms of\nperformance on average, in practice we are often interested in how well a\ntrained model performs on rare or difficult data points at test time. To\nachieve more robust and balanced generalization, methods applying\nsharpness-aware minimization to a subset of worst-case examples have proven\nsuccessful for image classification tasks, but only using deep neural networks\nin a scenario where the most difficult points are also the least common. In\nthis work, we show how such a strategy can dramatically break down under more\ndiverse models, and as a more robust alternative, instead of typical sharpness\nwe propose and evaluate a training criterion which penalizes poor loss\nconcentration, which can be easily combined with loss transformations such as\nCVaR or DRO that control tail emphasis.\n","authors":["Matthew J. Holland","Toma Hamada"],"pdf_url":"https://arxiv.org/pdf/2408.03619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14573v2","updated":"2024-08-07T08:22:35Z","published":"2024-07-21T06:27:45Z","title":"Trading Devil Final: Backdoor attack via Stock market and Bayesian\n  Optimization","summary":"  Since the advent of generative artificial intelligence, every company and\nresearcher has been rushing to develop their own generative models, whether\ncommercial or not. Given the large number of users of these powerful new tools,\nthere is currently no intrinsically verifiable way to explain from the ground\nup what happens when LLMs (large language models) learn. For example, those\nbased on automatic speech recognition systems, which have to rely on huge and\nastronomical amounts of data collected from all over the web to produce fast\nand efficient results, In this article, we develop a backdoor attack called\nMarketBackFinal 2.0, based on acoustic data poisoning, MarketBackFinal 2.0 is\nmainly based on modern stock market models. In order to show the possible\nvulnerabilities of speech-based transformers that may rely on LLMs.\n","authors":["Orson Mengara"],"pdf_url":"https://arxiv.org/pdf/2407.14573v2.pdf","comment":"END :jumps-Diffusion and stock market: Better quantify uncertainty in\n  financial simulations"},{"id":"http://arxiv.org/abs/2304.09914v4","updated":"2024-08-07T08:20:43Z","published":"2023-04-19T18:32:49Z","title":"The Face of Populism: Examining Differences in Facial Emotional\n  Expressions of Political Leaders Using Machine Learning","summary":"  Populist rhetoric employed on online media is characterized as deeply\nimpassioned and often imbued with strong emotions. The aim of this paper is to\nempirically investigate the differences in affective nonverbal communication of\npolitical leaders. We use a deep-learning approach to process a sample of 220\nYouTube videos of political leaders from 15 different countries, analyze their\nfacial expressions of emotion and then examine differences in average emotion\nscores representing the relative presence of 6 emotional states (anger,\ndisgust, fear, happiness, sadness, and surprise) and a neutral expression for\neach frame of the YouTube video. Based on a sample of manually coded images, we\nfind that this deep-learning approach has 53-60\\% agreement with human labels.\nWe observe statistically significant differences in the average score of\nnegative emotions between groups of leaders with varying degrees of populist\nrhetoric.\n","authors":["Sara Major","Aleksandar Tomaevi"],"pdf_url":"https://arxiv.org/pdf/2304.09914v4.pdf","comment":"Version 4.0: Annotation study added, supplementary information\n  extended"},{"id":"http://arxiv.org/abs/2408.03618v1","updated":"2024-08-07T08:19:44Z","published":"2024-08-07T08:19:44Z","title":"A Logical Fallacy-Informed Framework for Argument Generation","summary":"  Despite the remarkable performance of Large Language Models (LLMs), they\nstill struggle with generating logically sound arguments, resulting in\npotential risks such as spreading misinformation. An important factor\ncontributing to LLMs' suboptimal performance in generating coherent arguments\nis their oversight of logical fallacies. To address this issue, we introduce\nFIPO, a fallacy-informed framework that leverages preference optimization\nmethods to steer LLMs toward logically sound arguments. FIPO includes a\nclassification loss, to capture the fine-grained information on fallacy\ncategories. Our results on argumentation datasets show that our method reduces\nthe fallacy errors by up to 17.5%. Furthermore, our human evaluation results\nindicate that the quality of the generated arguments by our method\nsignificantly outperforms the fine-tuned baselines, as well as prior preference\noptimization methods, such as DPO. These findings highlight the importance of\nensuring models are aware of logical fallacies for effective argument\ngeneration.\n","authors":["Luca Mouchel","Debjit Paul","Shaobo Cui","Robert West","Antoine Bosselut","Boi Faltings"],"pdf_url":"https://arxiv.org/pdf/2408.03618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03617v1","updated":"2024-08-07T08:18:51Z","published":"2024-08-07T08:18:51Z","title":"Is Child-Directed Speech Effective Training Data for Language Models?","summary":"  While high-performing language models are typically trained on hundreds of\nbillions of words, human children become fluent language users with a much\nsmaller amount of data. What are the features of the data they receive, and how\ndo these features support language modeling objectives? To investigate this\nquestion, we train GPT-2 models on 29M words of English-language child-directed\nspeech and a new matched, synthetic dataset (TinyDialogues), comparing to a\nheterogeneous blend of datasets from the BabyLM challenge. We evaluate both the\nsyntactic and semantic knowledge of these models using developmentally-inspired\nevaluations. Through pretraining experiments, we test whether the global\ndevelopmental ordering or the local discourse ordering of children's training\ndata support high performance relative to other datasets. The local properties\nof the data affect model results, but somewhat surprisingly, global properties\ndo not. Further, child language input is not uniquely valuable for training\nlanguage models. These findings support the hypothesis that, rather than\nproceeding from better data, children's learning is instead substantially more\nefficient than current language modeling techniques.\n","authors":["Steven Y. Feng","Noah D. Goodman","Michael C. Frank"],"pdf_url":"https://arxiv.org/pdf/2408.03617v1.pdf","comment":"Preprint. Code and data will be released soon"},{"id":"http://arxiv.org/abs/2404.15311v2","updated":"2024-08-07T08:14:56Z","published":"2024-04-02T17:01:51Z","title":"Fusing Pretrained ViTs with TCNet for Enhanced EEG Regression","summary":"  The task of Electroencephalogram (EEG) analysis is paramount to the\ndevelopment of Brain-Computer Interfaces (BCIs). However, to reach the goal of\ndeveloping robust, useful BCIs depends heavily on the speed and the accuracy at\nwhich BCIs can understand neural dynamics. In response to that goal, this paper\ndetails the integration of pre-trained Vision Transformers (ViTs) with Temporal\nConvolutional Networks (TCNet) to enhance the precision of EEG regression. The\ncore of this approach lies in harnessing the sequential data processing\nstrengths of ViTs along with the superior feature extraction capabilities of\nTCNet, to significantly improve EEG analysis accuracy. In addition, we analyze\nthe importance of how to construct optimal patches for the attention mechanism\nto analyze, balancing both speed and accuracy tradeoffs. Our results showcase a\nsubstantial improvement in regression accuracy, as evidenced by the reduction\nof Root Mean Square Error (RMSE) from 55.4 to 51.8 on EEGEyeNet's Absolute\nPosition Task, outperforming existing state-of-the-art models. Without\nsacrificing performance, we increase the speed of this model by an order of\nmagnitude (up to 4.32x faster). This breakthrough not only sets a new benchmark\nin EEG regression analysis but also opens new avenues for future research in\nthe integration of transformer architectures with specialized feature\nextraction methods for diverse EEG datasets.\n","authors":["Eric Modesitt","Haicheng Yin","Williams Huang Wang","Brian Lu"],"pdf_url":"https://arxiv.org/pdf/2404.15311v2.pdf","comment":"Accepted HCI International 2024"},{"id":"http://arxiv.org/abs/2408.02835v2","updated":"2024-08-07T08:09:02Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Ddalo Sanz-Hernndez","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v2.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.03612v1","updated":"2024-08-07T08:08:08Z","published":"2024-08-07T08:08:08Z","title":"JARViS: Detecting Actions in Video Using Unified Actor-Scene Context\n  Relation Modeling","summary":"  Video action detection (VAD) is a formidable vision task that involves the\nlocalization and classification of actions within the spatial and temporal\ndimensions of a video clip. Among the myriad VAD architectures, two-stage VAD\nmethods utilize a pre-trained person detector to extract the region of interest\nfeatures, subsequently employing these features for action detection. However,\nthe performance of two-stage VAD methods has been limited as they depend solely\non localized actor features to infer action semantics. In this study, we\npropose a new two-stage VAD framework called Joint Actor-scene context Relation\nmodeling based on Visual Semantics (JARViS), which effectively consolidates\ncross-modal action semantics distributed globally across spatial and temporal\ndimensions using Transformer attention. JARViS employs a person detector to\nproduce densely sampled actor features from a keyframe. Concurrently, it uses a\nvideo backbone to create spatio-temporal scene features from a video clip.\nFinally, the fine-grained interactions between actors and scenes are modeled\nthrough a Unified Action-Scene Context Transformer to directly output the final\nset of actions in parallel. Our experimental results demonstrate that JARViS\noutperforms existing methods by significant margins and achieves\nstate-of-the-art performance on three popular VAD datasets, including AVA,\nUCF101-24, and JHMDB51-21.\n","authors":["Seok Hwan Lee","Taein Son","Soo Won Seo","Jisong Kim","Jun Won Choi"],"pdf_url":"https://arxiv.org/pdf/2408.03612v1.pdf","comment":"31 pages, 10 figures"},{"id":"http://arxiv.org/abs/2404.08271v3","updated":"2024-08-07T08:00:43Z","published":"2024-04-12T06:50:32Z","title":"Transfer Learning Study of Motion Transformer-based Trajectory\n  Predictions","summary":"  Trajectory planning in autonomous driving is highly dependent on predicting\nthe emergent behavior of other road users. Learning-based methods are currently\nshowing impressive results in simulation-based challenges, with\ntransformer-based architectures technologically leading the way. Ultimately,\nhowever, predictions are needed in the real world. In addition to the shifts\nfrom simulation to the real world, many vehicle- and country-specific shifts,\ni.e. differences in sensor systems, fusion and perception algorithms as well as\ntraffic rules and laws, are on the agenda. Since models that can cover all\nsystem setups and design domains at once are not yet foreseeable, model\nadaptation plays a central role. Therefore, a simulation-based study on\ntransfer learning techniques is conducted on basis of a transformer-based\nmodel. Furthermore, the study aims to provide insights into possible trade-offs\nbetween computational time and performance to support effective transfers into\nthe real world.\n","authors":["Lars Ullrich","Alex McMaster","Knut Graichen"],"pdf_url":"https://arxiv.org/pdf/2404.08271v3.pdf","comment":"Published in 2024 IEEE Intelligent Vehicles Symposium (IV), Jeju\n  Shinhwa World, Jeju Island, Korea, June 2-5, 2024"},{"id":"http://arxiv.org/abs/2408.03608v1","updated":"2024-08-07T07:54:19Z","published":"2024-08-07T07:54:19Z","title":"InPer: Whole-Process Domain Generalization via Causal Intervention and\n  Perturbation","summary":"  Despite the considerable advancements achieved by deep neural networks, their\nperformance tends to degenerate when the test environment diverges from the\ntraining ones. Domain generalization (DG) solves this issue by learning\nrepresentations independent of domain-related information, thus facilitating\nextrapolation to unseen environments. Existing approaches typically focus on\nformulating tailored training objectives to extract shared features from the\nsource data. However, the disjointed training and testing procedures may\ncompromise robustness, particularly in the face of unforeseen variations during\ndeployment. In this paper, we propose a novel and holistic framework based on\ncausality, named InPer, designed to enhance model generalization by\nincorporating causal intervention during training and causal perturbation\nduring testing. Specifically, during the training phase, we employ\nentropy-based causal intervention (EnIn) to refine the selection of causal\nvariables. To identify samples with anti-interference causal variables from the\ntarget domain, we propose a novel metric, homeostatic score, through causal\nperturbation (HoPer) to construct a prototype classifier in test time.\nExperimental results across multiple cross-domain tasks confirm the efficacy of\nInPer.\n","authors":["Luyao Tang","Yuxuan Yuan","Chaoqi Chen","Xinghao Ding","Yue Huang"],"pdf_url":"https://arxiv.org/pdf/2408.03608v1.pdf","comment":"Accepted by BMVC2024"},{"id":"http://arxiv.org/abs/2407.18990v2","updated":"2024-08-07T07:46:39Z","published":"2024-07-25T12:07:55Z","title":"Stay Tuned: An Empirical Study of the Impact of Hyperparameters on LLM\n  Tuning in Real-World Applications","summary":"  Fine-tuning Large Language Models (LLMs) is an effective method to enhance\ntheir performance on downstream tasks. However, choosing the appropriate\nsetting of tuning hyperparameters (HPs) is a labor-intensive and\ncomputationally expensive process. Here, we provide recommended HP\nconfigurations for practical use-cases that represent a better starting point\nfor practitioners, when considering two SOTA LLMs and two commonly used tuning\nmethods. We describe Coverage-based Search (CBS), a process for ranking HP\nconfigurations based on an offline extensive grid search, such that the top\nranked configurations collectively provide a practical robust recommendation\nfor a wide range of datasets and domains. We focus our experiments on\nLlama-3-8B and Mistral-7B, as well as full fine-tuning and LoRa, conducting a\ntotal of > 10,000 tuning experiments. Our results suggest that, in general,\nLlama-3-8B and LoRA should be preferred, when possible. Moreover, we show that\nfor both models and tuning methods, exploring only a few HP configurations, as\nrecommended by our analysis, can provide excellent results in practice, making\nthis work a valuable resource for practitioners.\n","authors":["Alon Halfon","Shai Gretz","Ofir Arviv","Artem Spector","Orith Toledo-Ronen","Yoav Katz","Liat Ein-Dor","Michal Shmueli-Scheuer","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2407.18990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03603v1","updated":"2024-08-07T07:46:08Z","published":"2024-08-07T07:46:08Z","title":"EnJa: Ensemble Jailbreak on Large Language Models","summary":"  As Large Language Models (LLMs) are increasingly being deployed in\nsafety-critical applications, their vulnerability to potential jailbreaks --\nmalicious prompts that can disable the safety mechanism of LLMs -- has\nattracted growing research attention. While alignment methods have been\nproposed to protect LLMs from jailbreaks, many have found that aligned LLMs can\nstill be jailbroken by carefully crafted malicious prompts, producing content\nthat violates policy regulations. Existing jailbreak attacks on LLMs can be\ncategorized into prompt-level methods which make up stories/logic to circumvent\nsafety alignment and token-level attack methods which leverage gradient methods\nto find adversarial tokens. In this work, we introduce the concept of Ensemble\nJailbreak and explore methods that can integrate prompt-level and token-level\njailbreak into a more powerful hybrid jailbreak attack. Specifically, we\npropose a novel EnJa attack to hide harmful instructions using prompt-level\njailbreak, boost the attack success rate using a gradient-based attack, and\nconnect the two types of jailbreak attacks via a template-based connector. We\nevaluate the effectiveness of EnJa on several aligned models and show that it\nachieves a state-of-the-art attack success rate with fewer queries and is much\nstronger than any individual jailbreak.\n","authors":["Jiahao Zhang","Zilong Wang","Ruofan Wang","Xingjun Ma","Yu-Gang Jiang"],"pdf_url":"https://arxiv.org/pdf/2408.03603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03599v1","updated":"2024-08-07T07:36:49Z","published":"2024-08-07T07:36:49Z","title":"Activations Through Extensions: A Framework To Boost Performance Of\n  Neural Networks","summary":"  Activation functions are non-linearities in neural networks that allow them\nto learn complex mapping between inputs and outputs. Typical choices for\nactivation functions are ReLU, Tanh, Sigmoid etc., where the choice generally\ndepends on the application domain. In this work, we propose a\nframework/strategy that unifies several works on activation functions and\ntheoretically explains the performance benefits of these works. We also propose\nnovel techniques that originate from the framework and allow us to obtain\n``extensions'' (i.e. special generalizations of a given neural network) of\nneural networks through operations on activation functions. We theoretically\nand empirically show that ``extensions'' of neural networks have performance\nbenefits compared to vanilla neural networks with insignificant space and time\ncomplexity costs on standard test functions. We also show the benefits of\nneural network ``extensions'' in the time-series domain on real-world datasets.\n","authors":["Chandramouli Kamanchi","Sumatra Mukherjee","Kameshwaran Sampath","Pankaj Dayama","Arindam Jati","Vijay Ekambaram","Dzung Phan"],"pdf_url":"https://arxiv.org/pdf/2408.03599v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17640v2","updated":"2024-08-07T07:29:39Z","published":"2024-05-27T20:24:03Z","title":"Probabilistically Plausible Counterfactual Explanations with Normalizing\n  Flows","summary":"  We present PPCEF, a novel method for generating probabilistically plausible\ncounterfactual explanations (CFs). PPCEF advances beyond existing methods by\ncombining a probabilistic formulation that leverages the data distribution with\nthe optimization of plausibility within a unified framework. Compared to\nreference approaches, our method enforces plausibility by directly optimizing\nthe explicit density function without assuming a particular family of\nparametrized distributions. This ensures CFs are not only valid (i.e., achieve\nclass change) but also align with the underlying data's probability density.\nFor that purpose, our approach leverages normalizing flows as powerful density\nestimators to capture the complex high-dimensional data distribution.\nFurthermore, we introduce a novel loss that balances the trade-off between\nachieving class change and maintaining closeness to the original instance while\nalso incorporating a probabilistic plausibility term. PPCEF's unconstrained\nformulation allows for efficient gradient-based optimization with batch\nprocessing, leading to orders of magnitude faster computation compared to prior\nmethods. Moreover, the unconstrained formulation of PPCEF allows for the\nseamless integration of future constraints tailored to specific counterfactual\nproperties. Finally, extensive evaluations demonstrate PPCEF's superiority in\ngenerating high-quality, probabilistically plausible counterfactual\nexplanations in high-dimensional tabular settings. This makes PPCEF a powerful\ntool for not only interpreting complex machine learning models but also for\nimproving fairness, accountability, and trust in AI systems.\n","authors":["Patryk Wielopolski","Oleksii Furman","Jerzy Stefanowski","Maciej Ziba"],"pdf_url":"https://arxiv.org/pdf/2405.17640v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09032v2","updated":"2024-08-07T07:20:23Z","published":"2024-03-14T01:51:35Z","title":"CodeUltraFeedback: An LLM-as-a-Judge Dataset for Aligning Large Language\n  Models to Coding Preferences","summary":"  Evaluating the alignment of large language models (LLMs) with user-defined\ncoding preferences is a challenging endeavour that requires a deep assessment\nof LLMs' outputs. Existing methods and benchmarks rely primarily on automated\nmetrics and static analysis tools, which often fail to capture the nuances of\nuser instructions and LLM outputs. To address this gap, we propose using the\nLLM-as-a-Judge methodology to evaluate the alignment of LLMs with coding\npreferences. Based on this approach, we present CodeUltraFeedback, a\ncomprehensive dataset designed to facilitate the evaluation and improvement of\nLLM alignment. CodeUltraFeedback consists of 10,000 coding instructions, each\nannotated with four responses generated from a diverse pool of 14 LLMs. These\nresponses are ranked based on five distinct coding preferences using GPT-3.5 as\na judge, providing both numerical scores and detailed textual feedback. Our\nanalysis of CodeUltraFeedback reveals that responses from GPT-3.5 and GPT-4 are\ngenerally preferred over those from open-weight LLMs, highlighting significant\ndifferences in alignment between closed and open-weight models. In turn, we\nexplore the usage of CodeUltraFeedback as feedback data to fine-tune and align\nCodeLlama-7B-Instruct using supervised fine-tuning (SFT) and reinforcement\nlearning from AI feedback (RLAIF) with direct preference optimization (DPO).\nThe resulting aligned CodeLlama-7B-Instruct model outperforms larger LLMs in\nterms of alignment with coding preferences and shows improved functional\ncorrectness on the HumanEval+ benchmark compared to the original instruct\nmodel. Therefore, our contributions bridge the gap in preference tuning of LLMs\nfor code and set the stage for further advancements in model alignment and\nRLAIF in automated software engineering.\n","authors":["Martin Weyssow","Aton Kamanda","Houari Sahraoui"],"pdf_url":"https://arxiv.org/pdf/2403.09032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03591v1","updated":"2024-08-07T07:09:14Z","published":"2024-08-07T07:09:14Z","title":"Focal Depth Estimation: A Calibration-Free, Subject- and Daytime\n  Invariant Approach","summary":"  In an era where personalized technology is increasingly intertwined with\ndaily life, traditional eye-tracking systems and autofocal glasses face a\nsignificant challenge: the need for frequent, user-specific calibration, which\nimpedes their practicality. This study introduces a groundbreaking\ncalibration-free method for estimating focal depth, leveraging machine learning\ntechniques to analyze eye movement features within short sequences. Our\napproach, distinguished by its innovative use of LSTM networks and\ndomain-specific feature engineering, achieves a mean absolute error (MAE) of\nless than 10 cm, setting a new focal depth estimation accuracy standard. This\nadvancement promises to enhance the usability of autofocal glasses and pave the\nway for their seamless integration into extended reality environments, marking\na significant leap forward in personalized visual technology.\n","authors":["Benedikt W. Hosp","Bjrn Severitt","Rajat Agarwala","Evgenia Rusak","Yannick Sauer","Siegfried Wahl"],"pdf_url":"https://arxiv.org/pdf/2408.03591v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03590v1","updated":"2024-08-07T07:09:06Z","published":"2024-08-07T07:09:06Z","title":"Sensitivity analysis using the Metamodel of Optimal Prognosis","summary":"  In real case applications within the virtual prototyping process, it is not\nalways possible to reduce the complexity of the physical models and to obtain\nnumerical models which can be solved quickly. Usually, every single numerical\nsimulation takes hours or even days. Although the progresses in numerical\nmethods and high performance computing, in such cases, it is not possible to\nexplore various model configurations, hence efficient surrogate models are\nrequired. Generally the available meta-model techniques show several advantages\nand disadvantages depending on the investigated problem. In this paper we\npresent an automatic approach for the selection of the optimal suitable\nmeta-model for the actual problem. Together with an automatic reduction of the\nvariable space using advanced filter techniques an efficient approximation is\nenabled also for high dimensional problems. This filter techniques enable a\nreduction of the high dimensional variable space to a much smaller subspace\nwhere meta-model-based sensitivity analyses are carried out to assess the\ninfluence of important variables and to identify the optimal subspace with\ncorresponding surrogate model which enables the most accurate probabilistic\nanalysis. For this purpose we investigate variance-based and moment-free\nsensitivity measures in combination with advanced meta-models as moving least\nsquares and kriging.\n","authors":["Thomas Most","Johannes Will"],"pdf_url":"https://arxiv.org/pdf/2408.03590v1.pdf","comment":"presented at 8th Optimization and Stochastic Days, Weimar, Germany,\n  24-25 November, 2011"},{"id":"http://arxiv.org/abs/2408.03588v1","updated":"2024-08-07T07:04:29Z","published":"2024-08-07T07:04:29Z","title":"Facing the Music: Tackling Singing Voice Separation in Cinematic Audio\n  Source Separation","summary":"  Cinematic audio source separation (CASS) is a fairly new subtask of audio\nsource separation. A typical setup of CASS is a three-stem problem, with the\naim of separating the mixture into the dialogue stem (DX), music stem (MX), and\neffects stem (FX). In practice, however, several edge cases exist as some sound\nsources do not fit neatly in either of these three stems, necessitating the use\nof additional auxiliary stems in production. One very common edge case is the\nsinging voice in film audio, which may belong in either the DX or MX, depending\nheavily on the cinematic context. In this work, we demonstrate a very\nstraightforward extension of the dedicated-decoder Bandit and query-based\nsingle-decoder Banquet models to a four-stem problem, treating non-musical\ndialogue, instrumental music, singing voice, and effects as separate stems.\nInterestingly, the query-based Banquet model outperformed the dedicated-decoder\nBandit model. We hypothesized that this is due to a better feature alignment at\nthe bottleneck as enforced by the band-agnostic FiLM layer. Dataset and model\nimplementation will be made available at\nhttps://github.com/kwatcharasupat/source-separation-landing.\n","authors":["Karn N. Watcharasupat","Chih-Wei Wu","Iroro Orife"],"pdf_url":"https://arxiv.org/pdf/2408.03588v1.pdf","comment":"Submitted to the Late-Breaking Demo Session of the 25th International\n  Society for Music Information Retrieval (ISMIR) Conference, 2024"},{"id":"http://arxiv.org/abs/2405.15081v3","updated":"2024-08-07T07:03:11Z","published":"2024-05-23T22:07:54Z","title":"Distributed Harmonization: Federated Clustered Batch Effect Adjustment\n  and Generalization","summary":"  Independent and identically distributed (i.i.d.) data is essential to many\ndata analysis and modeling techniques. In the medical domain, collecting data\nfrom multiple sites or institutions is a common strategy that guarantees\nsufficient clinical diversity, determined by the decentralized nature of\nmedical data. However, data from various sites are easily biased by the local\nenvironment or facilities, thereby violating the i.i.d. rule. A common strategy\nis to harmonize the site bias while retaining important biological information.\nThe ComBat is among the most popular harmonization approaches and has recently\nbeen extended to handle distributed sites. However, when faced with situations\ninvolving newly joined sites in training or evaluating data from unknown/unseen\nsites, ComBat lacks compatibility and requires retraining with data from all\nthe sites. The retraining leads to significant computational and logistic\noverhead that is usually prohibitive. In this work, we develop a novel Cluster\nComBat harmonization algorithm, which leverages cluster patterns of the data in\ndifferent sites and greatly advances the usability of ComBat harmonization. We\nuse extensive simulation and real medical imaging data from ADNI to demonstrate\nthe superiority of the proposed approach. Our codes are provided in\nhttps://github.com/illidanlab/distributed-cluster-harmonization.\n","authors":["Bao Hoang","Yijiang Pang","Siqi Liang","Liang Zhan","Paul Thompson","Jiayu Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.15081v3.pdf","comment":"11 pages, 7 figures, accepted to KDD2024-ADS"},{"id":"http://arxiv.org/abs/2407.21282v2","updated":"2024-08-07T07:00:13Z","published":"2024-07-31T02:12:05Z","title":"FedBChain: A Blockchain-enabled Federated Learning Framework for\n  Improving DeepConvLSTM with Comparative Strategy Insights","summary":"  Recent research in the field of Human Activity Recognition has shown that an\nimprovement in prediction performance can be achieved by reducing the number of\nLSTM layers. However, this kind of enhancement is only significant on\nmonolithic architectures, and when it runs on large-scale distributed training,\ndata security and privacy issues will be reconsidered, and its prediction\nperformance is unknown. In this paper, we introduce a novel framework:\nFedBChain, which integrates the federated learning paradigm based on a modified\nDeepConvLSTM architecture with a single LSTM layer. This framework performs\ncomparative tests of prediction performance on three different real-world\ndatasets based on three different hidden layer units (128, 256, and 512)\ncombined with five different federated learning strategies, respectively. The\nresults show that our architecture has significant improvements in Precision,\nRecall and F1-score compared to the centralized training approach on all\ndatasets with all hidden layer units for all strategies: FedAvg strategy\nimproves on average by 4.54%, FedProx improves on average by 4.57%,\nFedTrimmedAvg improves on average by 4.35%, Krum improves by 4.18% on average,\nand FedAvgM improves by 4.46% on average. Based on our results, it can be seen\nthat FedBChain not only improves in performance, but also guarantees the\nsecurity and privacy of user data compared to centralized training methods\nduring the training process. The code for our experiments is publicly available\n(https://github.com/Glen909/FedBChain).\n","authors":["Gaoxuan Li","Chern Hong Lim","Qiyao Ma","Xinyu Tang","Hwa Hui Tew","Fan Ding","Xuewen Luo"],"pdf_url":"https://arxiv.org/pdf/2407.21282v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03585v1","updated":"2024-08-07T06:44:47Z","published":"2024-08-07T06:44:47Z","title":"Hierarchical Neural Constructive Solver for Real-world TSP Scenarios","summary":"  Existing neural constructive solvers for routing problems have predominantly\nemployed transformer architectures, conceptualizing the route construction as a\nset-to-sequence learning task. However, their efficacy has primarily been\ndemonstrated on entirely random problem instances that inadequately capture\nreal-world scenarios. In this paper, we introduce realistic Traveling Salesman\nProblem (TSP) scenarios relevant to industrial settings and derive the\nfollowing insights: (1) The optimal next node (or city) to visit often lies\nwithin proximity to the current node, suggesting the potential benefits of\nbiasing choices based on current locations. (2) Effectively solving the TSP\nrequires robust tracking of unvisited nodes and warrants succinct grouping\nstrategies. Building upon these insights, we propose integrating a learnable\nchoice layer inspired by Hypernetworks to prioritize choices based on the\ncurrent location, and a learnable approximate clustering algorithm inspired by\nthe Expectation-Maximization algorithm to facilitate grouping the unvisited\ncities. Together, these two contributions form a hierarchical approach towards\nsolving the realistic TSP by considering both immediate local neighbourhoods\nand learning an intermediate set of node representations. Our hierarchical\napproach yields superior performance compared to both classical and recent\ntransformer models, showcasing the efficacy of the key designs.\n","authors":["Yong Liang Goh","Zhiguang Cao","Yining Ma","Yanfei Dong","Mohammed Haroon Dupty","Wee Sun Lee"],"pdf_url":"https://arxiv.org/pdf/2408.03585v1.pdf","comment":"Accepted to KDD 2024"},{"id":"http://arxiv.org/abs/2408.03574v1","updated":"2024-08-07T06:26:04Z","published":"2024-08-07T06:26:04Z","title":"Teach CLIP to Develop a Number Sense for Ordinal Regression","summary":"  Ordinal regression is a fundamental problem within the field of computer\nvision, with customised well-trained models on specific tasks. While\npre-trained vision-language models (VLMs) have exhibited impressive performance\non various vision tasks, their potential for ordinal regression has received\nless exploration. In this study, we first investigate CLIP's potential for\nordinal regression, from which we expect the model could generalise to\ndifferent ordinal regression tasks and scenarios. Unfortunately, vanilla CLIP\nfails on this task, since current VLMs have a well-documented limitation of\nencapsulating compositional concepts such as number sense. We propose a simple\nyet effective method called NumCLIP to improve the quantitative understanding\nof VLMs. We disassemble the exact image to number-specific text matching\nproblem into coarse classification and fine prediction stages. We discretize\nand phrase each numerical bin with common language concept to better leverage\nthe available pre-trained alignment in CLIP. To consider the inherent\ncontinuous property of ordinal regression, we propose a novel fine-grained\ncross-modal ranking-based regularisation loss specifically designed to keep\nboth semantic and ordinal alignment in CLIP's feature space. Experimental\nresults on three general ordinal regression tasks demonstrate the effectiveness\nof NumCLIP, with 10% and 3.83% accuracy improvement on historical image dating\nand image aesthetics assessment task, respectively. Code is publicly available\nat https://github.com/xmed-lab/NumCLIP.\n","authors":["Yao Du","Qiang Zhai","Weihang Dai","Xiaomeng Li"],"pdf_url":"https://arxiv.org/pdf/2408.03574v1.pdf","comment":"Accepted by ECCV 2024"},{"id":"http://arxiv.org/abs/2408.03572v1","updated":"2024-08-07T06:16:17Z","published":"2024-08-07T06:16:17Z","title":"2D-OOB: Attributing Data Contribution through Joint Valuation Framework","summary":"  Data valuation has emerged as a powerful framework to quantify the\ncontribution of each datum to the training of a particular machine learning\nmodel. However, it is crucial to recognize that the quality of various cells\nwithin a single data point can vary greatly in practice. For example, even in\nthe case of an abnormal data point, not all cells are necessarily noisy. The\nsingle scalar valuation assigned by existing methods blurs the distinction\nbetween noisy and clean cells of a data point, thereby compromising the\ninterpretability of the valuation. In this paper, we propose 2D-OOB, an\nout-of-bag estimation framework for jointly determining helpful (or\ndetrimental) samples, as well as the particular cells that drive them. Our\ncomprehensive experiments demonstrate that 2D-OOB achieves state-of-the-art\nperformance across multiple use cases, while being exponentially faster. 2D-OOB\nexcels in detecting and rectifying fine-grained outliers at the cell level, as\nwell as localizing backdoor triggers in data poisoning attacks.\n","authors":["Yifan Sun","Jingyan Shen","Yongchan Kwon"],"pdf_url":"https://arxiv.org/pdf/2408.03572v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03569v1","updated":"2024-08-07T06:11:37Z","published":"2024-08-07T06:11:37Z","title":"Maximum a Posteriori Estimation for Linear Structural Dynamics Models\n  Using Bayesian Optimization with Rational Polynomial Chaos Expansions","summary":"  Bayesian analysis enables combining prior knowledge with measurement data to\nlearn model parameters. Commonly, one resorts to computing the maximum a\nposteriori (MAP) estimate, when only a point estimate of the parameters is of\ninterest. We apply MAP estimation in the context of structural dynamic models,\nwhere the system response can be described by the frequency response function.\nTo alleviate high computational demands from repeated expensive model calls, we\nutilize a rational polynomial chaos expansion (RPCE) surrogate model that\nexpresses the system frequency response as a rational of two polynomials with\ncomplex coefficients. We propose an extension to an existing sparse Bayesian\nlearning approach for RPCE based on Laplace's approximation for the posterior\ndistribution of the denominator coefficients. Furthermore, we introduce a\nBayesian optimization approach, which allows to adaptively enrich the\nexperimental design throughout the optimization process of MAP estimation.\nThereby, we utilize the expected improvement acquisition function as a means to\nidentify sample points in the input space that are possibly associated with\nlarge objective function values. The acquisition function is estimated through\nMonte Carlo sampling based on the posterior distribution of the expansion\ncoefficients identified in the sparse Bayesian learning process. By combining\nthe sparsity-inducing learning procedure with the sequential experimental\ndesign, we effectively reduce the number of model evaluations in the MAP\nestimation problem. We demonstrate the applicability of the presented methods\non the parameter updating problem of an algebraic two-degree-of-freedom system\nand the finite element model of a cross-laminated timber plate.\n","authors":["Felix Schneider","Iason Papaioannou","Bruno Sudret","Gerhard Mller"],"pdf_url":"https://arxiv.org/pdf/2408.03569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03568v1","updated":"2024-08-07T06:11:25Z","published":"2024-08-07T06:11:25Z","title":"A comparative study of generative adversarial networks for image\n  recognition algorithms based on deep learning and traditional methods","summary":"  In this paper, an image recognition algorithm based on the combination of\ndeep learning and generative adversarial network (GAN) is studied, and compared\nwith traditional image recognition methods. The purpose of this study is to\nevaluate the advantages and application prospects of deep learning technology,\nespecially GAN, in the field of image recognition. Firstly, this paper reviews\nthe basic principles and techniques of traditional image recognition methods,\nincluding the classical algorithms based on feature extraction such as SIFT,\nHOG and their combination with support vector machine (SVM), random forest, and\nother classifiers. Then, the working principle, network structure, and unique\nadvantages of GAN in image generation and recognition are introduced. In order\nto verify the effectiveness of GAN in image recognition, a series of\nexperiments are designed and carried out using multiple public image data sets\nfor training and testing. The experimental results show that compared with\ntraditional methods, GAN has excellent performance in processing complex\nimages, recognition accuracy, and anti-noise ability. Specifically, Gans are\nbetter able to capture high-dimensional features and details of images,\nsignificantly improving recognition performance. In addition, Gans shows unique\nadvantages in dealing with image noise, partial missing information, and\ngenerating high-quality images.\n","authors":["Yihao Zhong","Yijing Wei","Yingbin Liang","Xiqing Liu","Rongwei Ji","Yiru Cang"],"pdf_url":"https://arxiv.org/pdf/2408.03568v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11755v3","updated":"2024-08-07T06:05:42Z","published":"2024-03-18T13:03:24Z","title":"Meta-Prompting for Automating Zero-shot Visual Recognition with LLMs","summary":"  Prompt ensembling of Large Language Model (LLM) generated category-specific\nprompts has emerged as an effective method to enhance zero-shot recognition\nability of Vision-Language Models (VLMs). To obtain these category-specific\nprompts, the present methods rely on hand-crafting the prompts to the LLMs for\ngenerating VLM prompts for the downstream tasks. However, this requires\nmanually composing these task-specific prompts and still, they might not cover\nthe diverse set of visual concepts and task-specific styles associated with the\ncategories of interest. To effectively take humans out of the loop and\ncompletely automate the prompt generation process for zero-shot recognition, we\npropose Meta-Prompting for Visual Recognition (MPVR). Taking as input only\nminimal information about the target task, in the form of its short natural\nlanguage description, and a list of associated class labels, MPVR automatically\nproduces a diverse set of category-specific prompts resulting in a strong\nzero-shot classifier. MPVR generalizes effectively across various popular\nzero-shot image recognition benchmarks belonging to widely different domains\nwhen tested with multiple LLMs and VLMs. For example, MPVR obtains a zero-shot\nrecognition improvement over CLIP by up to 19.8% and 18.2% (5.0% and 4.5% on\naverage over 20 datasets) leveraging GPT and Mixtral LLMs, respectively\n","authors":["M. Jehanzeb Mirza","Leonid Karlinsky","Wei Lin","Sivan Doveh","Jakub Micorek","Mateusz Kozinski","Hilde Kuehne","Horst Possegger"],"pdf_url":"https://arxiv.org/pdf/2403.11755v3.pdf","comment":"ECCV Camera Ready. Code & Data:\n  https://jmiemirza.github.io/Meta-Prompting/"},{"id":"http://arxiv.org/abs/2408.03029v2","updated":"2024-08-07T05:59:46Z","published":"2024-08-06T08:22:16Z","title":"Highly Efficient Self-Adaptive Reward Shaping for Reinforcement Learning","summary":"  Reward shaping addresses the challenge of sparse rewards in reinforcement\nlearning by constructing denser and more informative reward signals. To achieve\nself-adaptive and highly efficient reward shaping, we propose a novel method\nthat incorporates success rates derived from historical experiences into shaped\nrewards. Our approach utilizes success rates sampled from Beta distributions,\nwhich dynamically evolve from uncertain to reliable values as more data is\ncollected. Initially, the self-adaptive success rates exhibit more randomness\nto encourage exploration. Over time, they become more certain to enhance\nexploitation, thus achieving a better balance between exploration and\nexploitation. We employ Kernel Density Estimation (KDE) combined with Random\nFourier Features (RFF) to derive the Beta distributions, resulting in a\ncomputationally efficient implementation in high-dimensional continuous state\nspaces. This method provides a non-parametric and learning-free approach. The\nproposed method is evaluated on a wide range of continuous control tasks with\nsparse and delayed rewards, demonstrating significant improvements in sample\nefficiency and convergence stability compared to relevant baselines.\n","authors":["Haozhe Ma","Zhengding Luo","Thanh Vinh Vo","Kuankuan Sima","Tze-Yun Leong"],"pdf_url":"https://arxiv.org/pdf/2408.03029v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03561v1","updated":"2024-08-07T05:50:17Z","published":"2024-08-07T05:50:17Z","title":"MPC-Minimized Secure LLM Inference","summary":"  Many inference services based on large language models (LLMs) pose a privacy\nconcern, either revealing user prompts to the service or the proprietary\nweights to the user. Secure inference offers a solution to this problem through\nsecure multi-party computation (MPC), however, it is still impractical for\nmodern LLM workload due to the large overhead imposed by MPC. To address this\noverhead, we propose Marill, a framework that adapts LLM fine-tuning to\nminimize MPC usage during secure inference. Marill introduces high-level\narchitectural changes during fine-tuning that significantly reduce the number\nof expensive operations needed within MPC during inference, by removing some\nand relocating others outside MPC without compromising security. As a result,\nMarill-generated models are more efficient across all secure inference\nprotocols and our approach complements MPC-friendly approximations for such\noperations. Compared to standard fine-tuning, Marill results in 3.6-11.3x\nbetter runtime and 2.4-6.9x better communication during secure inference across\nvarious MPC settings, while typically preserving over 90% performance across\ndownstream tasks.\n","authors":["Deevashwer Rathee","Dacheng Li","Ion Stoica","Hao Zhang","Raluca Popa"],"pdf_url":"https://arxiv.org/pdf/2408.03561v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03560v1","updated":"2024-08-07T05:48:05Z","published":"2024-08-07T05:48:05Z","title":"In2Core: Leveraging Influence Functions for Coreset Selection in\n  Instruction Finetuning of Large Language Models","summary":"  Despite advancements, fine-tuning Large Language Models (LLMs) remains costly\ndue to the extensive parameter count and substantial data requirements for\nmodel generalization. Accessibility to computing resources remains a barrier\nfor the open-source community. To address this challenge, we propose the\nIn2Core algorithm, which selects a coreset by analyzing the correlation between\ntraining and evaluation samples with a trained model. Notably, we assess the\nmodel's internal gradients to estimate this relationship, aiming to rank the\ncontribution of each training point. To enhance efficiency, we propose an\noptimization to compute influence functions with a reduced number of layers\nwhile achieving similar accuracy. By applying our algorithm to instruction\nfine-tuning data of LLMs, we can achieve similar performance with just 50% of\nthe training data. Meantime, using influence functions to analyze model\ncoverage to certain testing samples could provide a reliable and interpretable\nsignal on the training set's coverage of those test points.\n","authors":["Ayrton San Joaquin","Bin Wang","Zhengyuan Liu","Nicholas Asher","Brian Lim","Philippe Muller","Nancy Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03554v1","updated":"2024-08-07T05:30:10Z","published":"2024-08-07T05:30:10Z","title":"Empirical Analysis of Large Vision-Language Models against Goal\n  Hijacking via Visual Prompt Injection","summary":"  We explore visual prompt injection (VPI) that maliciously exploits the\nability of large vision-language models (LVLMs) to follow instructions drawn\nonto the input image. We propose a new VPI method, \"goal hijacking via visual\nprompt injection\" (GHVPI), that swaps the execution task of LVLMs from an\noriginal task to an alternative task designated by an attacker. The\nquantitative analysis indicates that GPT-4V is vulnerable to the GHVPI and\ndemonstrates a notable attack success rate of 15.8%, which is an unignorable\nsecurity risk. Our analysis also shows that successful GHVPI requires high\ncharacter recognition capability and instruction-following ability in LVLMs.\n","authors":["Subaru Kimura","Ryota Tanaka","Shumpei Miyawaki","Jun Suzuki","Keisuke Sakaguchi"],"pdf_url":"https://arxiv.org/pdf/2408.03554v1.pdf","comment":"8 pages, 6 figures, Accepted to NAACL 2024 SRW"},{"id":"http://arxiv.org/abs/2106.11760v5","updated":"2024-08-07T05:28:30Z","published":"2021-06-19T06:25:10Z","title":"Fingerprinting Image-to-Image Generative Adversarial Networks","summary":"  Generative Adversarial Networks (GANs) have been widely used in various\napplication scenarios. Since the production of a commercial GAN requires\nsubstantial computational and human resources, the copyright protection of GANs\nis urgently needed. This paper presents a novel fingerprinting scheme for the\nIntellectual Property (IP) protection of image-to-image GANs based on a trusted\nthird party. We break through the stealthiness and robustness bottlenecks\nsuffered by previous fingerprinting methods for classification models being\nnaively transferred to GANs. Specifically, we innovatively construct a\ncomposite deep learning model from the target GAN and a classifier. Then we\ngenerate fingerprint samples from this composite model, and embed them in the\nclassifier for effective ownership verification. This scheme inspires some\nconcrete methodologies to practically protect the modern image-to-image\ntranslation GANs. Theoretical analysis proves that these methods can satisfy\ndifferent security requirements necessary for IP protection. We also conduct\nextensive experiments to show that our solutions outperform existing\nstrategies.\n","authors":["Guanlin Li","Guowen Xu","Han Qiu","Shangwei Guo","Run Wang","Jiwei Li","Tianwei Zhang","Rongxing Lu"],"pdf_url":"https://arxiv.org/pdf/2106.11760v5.pdf","comment":"Accepted by EuroS&P 2024"},{"id":"http://arxiv.org/abs/2302.01089v4","updated":"2024-08-07T05:22:57Z","published":"2023-02-02T13:22:18Z","title":"Curriculum Learning for ab initio Deep Learned Refractive Optics","summary":"  Deep optical optimization has recently emerged as a new paradigm for\ndesigning computational imaging systems using only the output image as the\nobjective. However, it has been limited to either simple optical systems\nconsisting of a single element such as a diffractive optical element (DOE) or\nmetalens, or the fine-tuning of compound lenses from good initial designs. Here\nwe present a DeepLens design method based on curriculum learning, which is able\nto learn optical designs of compound lenses ab initio from randomly initialized\nsurfaces without human intervention, therefore overcoming the need for a good\ninitial design. We demonstrate the effectiveness of our approach by fully\nautomatically designing both classical imaging lenses and a large field-of-view\nextended depth-of-field computational lens in a cellphone-style form factor,\nwith highly aspheric surfaces and a short back focal length.\n","authors":["Xinge Yang","Qiang Fu","Wolfgang Heidrich"],"pdf_url":"https://arxiv.org/pdf/2302.01089v4.pdf","comment":"Automatically design computational lenses from scratch with\n  differentiable ray tracing"},{"id":"http://arxiv.org/abs/2408.02050v2","updated":"2024-08-07T05:15:53Z","published":"2024-08-04T14:57:44Z","title":"Recovering the state and dynamics of autonomous system with partial\n  states solution using neural networks","summary":"  In this paper we explore the performance of deep hidden physics model (M.\nRaissi 2018) for autonomous systems. These systems are described by set of\nordinary differential equations which do not explicitly depend on time. Such\nsystems can be found in nature and have applications in modeling chemical\nconcentrations, population dynamics, n-body problems in physics etc. In this\nwork we consider dynamics of states, which explain how the states will evolve\nare unknown to us. We approximate state and dynamics both using neural\nnetworks. We have considered examples of 2D linear/nonlinear and Lorenz\nsystems. We observe that even without knowing all the states information, we\ncan estimate dynamics of certain states whose state information are known.\n","authors":["Vijay Kag"],"pdf_url":"https://arxiv.org/pdf/2408.02050v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11652v5","updated":"2024-08-07T05:14:24Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v5.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2304.06237v3","updated":"2024-08-07T04:59:21Z","published":"2023-04-13T03:20:45Z","title":"Deep learning based ECG segmentation for delineation of diverse\n  arrhythmias","summary":"  Accurate delineation of key waveforms in an ECG is a critical step in\nextracting relevant features to support the diagnosis and treatment of heart\nconditions. Although deep learning based methods using segmentation models to\nlocate P, QRS, and T waves have shown promising results, their ability to\nhandle arrhythmias has not been studied in any detail. In this paper we\ninvestigate the effect of arrhythmias on delineation quality and develop\nstrategies to improve performance in such cases. We introduce a U-Net-like\nsegmentation model for ECG delineation with a particular focus on diverse\narrhythmias. This is followed by a post-processing algorithm which removes\nnoise and automatically determines the boundaries of P, QRS, and T waves. Our\nmodel has been trained on a diverse dataset and evaluated against the LUDB and\nQTDB datasets to show strong performance, with F1-scores exceeding 99% for QRS\nand T waves, and over 97% for P waves in the LUDB dataset. Furthermore, we\nassess various models across a wide array of arrhythmias and observe that\nmodels with a strong performance on standard benchmarks may still perform\npoorly on arrhythmias that are underrepresented in these benchmarks, such as\ntachycardias. We propose solutions to address this discrepancy.\n","authors":["Chankyu Joung","Mijin Kim","Taejin Paik","Seong-Ho Kong","Seung-Young Oh","Won Kyeong Jeon","Jae-hu Jeon","Joong-Sik Hong","Wan-Joong Kim","Woong Kook","Myung-Jin Cha","Otto van Koert"],"pdf_url":"https://arxiv.org/pdf/2304.06237v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03539v1","updated":"2024-08-07T04:35:38Z","published":"2024-08-07T04:35:38Z","title":"Deep Reinforcement Learning for Robotics: A Survey of Real-World\n  Successes","summary":"  Reinforcement learning (RL), particularly its combination with deep neural\nnetworks referred to as deep RL (DRL), has shown tremendous promise across a\nwide range of applications, suggesting its potential for enabling the\ndevelopment of sophisticated robotic behaviors. Robotics problems, however,\npose fundamental difficulties for the application of RL, stemming from the\ncomplexity and cost of interacting with the physical world. This article\nprovides a modern survey of DRL for robotics, with a particular focus on\nevaluating the real-world successes achieved with DRL in realizing several key\nrobotic competencies. Our analysis aims to identify the key factors underlying\nthose exciting successes, reveal underexplored areas, and provide an overall\ncharacterization of the status of DRL in robotics. We highlight several\nimportant avenues for future work, emphasizing the need for stable and\nsample-efficient real-world RL paradigms, holistic approaches for discovering\nand integrating various competencies to tackle complex long-horizon, open-world\ntasks, and principled development and evaluation procedures. This survey is\ndesigned to offer insights for both RL practitioners and roboticists toward\nharnessing RL's power to create generally capable real-world robotic systems.\n","authors":["Chen Tang","Ben Abbatematteo","Jiaheng Hu","Rohan Chandra","Roberto Martn-Martn","Peter Stone"],"pdf_url":"https://arxiv.org/pdf/2408.03539v1.pdf","comment":"The first three authors contributed equally. Accepted to Annual\n  Review of Control, Robotics, and Autonomous Systems"},{"id":"http://arxiv.org/abs/2408.03526v1","updated":"2024-08-07T03:37:25Z","published":"2024-08-07T03:37:25Z","title":"Minimum Enclosing Ball Synthetic Minority Oversampling Technique from a\n  Geometric Perspective","summary":"  Class imbalance refers to the significant difference in the number of samples\nfrom different classes within a dataset, making it challenging to identify\nminority class samples correctly. This issue is prevalent in real-world\nclassification tasks, such as software defect prediction, medical diagnosis,\nand fraud detection. The synthetic minority oversampling technique (SMOTE) is\nwidely used to address class imbalance issue, which is based on interpolation\nbetween randomly selected minority class samples and their neighbors. However,\ntraditional SMOTE and most of its variants only interpolate between existing\nsamples, which may be affected by noise samples in some cases and synthesize\nsamples that lack diversity. To overcome these shortcomings, this paper\nproposes the Minimum Enclosing Ball SMOTE (MEB-SMOTE) method from a geometry\nperspective. Specifically, MEB is innovatively introduced into the oversampling\nmethod to construct a representative point. Then, high-quality samples are\nsynthesized by interpolation between this representative point and the existing\nsamples. The rationale behind constructing a representative point is discussed,\ndemonstrating that the center of MEB is more suitable as the representative\npoint. To exhibit the superiority of MEB-SMOTE, experiments are conducted on 15\nreal-world imbalanced datasets. The results indicate that MEB-SMOTE can\neffectively improve the classification performance on imbalanced datasets.\n","authors":["Yi-Yang Shangguan","Shi-Shun Chen","Xiao-Yang Li"],"pdf_url":"https://arxiv.org/pdf/2408.03526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04360v2","updated":"2024-08-07T03:36:51Z","published":"2024-04-05T19:14:14Z","title":"Prompt Public Large Language Models to Synthesize Data for Private\n  On-device Applications","summary":"  Pre-training on public data is an effective method to improve the performance\nfor federated learning (FL) with differential privacy (DP). This paper\ninvestigates how large language models (LLMs) trained on public data can\nimprove the quality of pre-training data for the on-device language models\ntrained with DP and FL. We carefully design LLM prompts to filter and transform\nexisting public data, and generate new data to resemble the real user data\ndistribution. The model pre-trained on our synthetic dataset achieves relative\nimprovement of 19.0% and 22.8% in next word prediction accuracy compared to the\nbaseline model pre-trained on a standard public dataset, when evaluated over\nthe real user data in Gboard (Google Keyboard, a production mobile keyboard\napplication). Furthermore, our method achieves evaluation accuracy better than\nor comparable to the baseline during the DP FL fine-tuning over millions of\nmobile devices, and our final model outperforms the baseline in production A/B\ntesting. Our experiments demonstrate the strengths of LLMs in synthesizing data\nclose to the private distribution even without accessing the private data, and\nalso suggest future research directions to further reduce the distribution gap.\n","authors":["Shanshan Wu","Zheng Xu","Yanxiang Zhang","Yuanbo Zhang","Daniel Ramage"],"pdf_url":"https://arxiv.org/pdf/2404.04360v2.pdf","comment":"COLM 2024"},{"id":"http://arxiv.org/abs/2305.18403v5","updated":"2024-08-07T03:30:30Z","published":"2023-05-28T15:15:48Z","title":"LoRAPrune: Structured Pruning Meets Low-Rank Parameter-Efficient\n  Fine-Tuning","summary":"  Large Language Models (LLMs), such as LLaMA and T5, have shown exceptional\nperformance across various tasks through fine-tuning. Although low-rank\nadaption (LoRA) has emerged to cheaply fine-tune these LLMs on downstream\ntasks, their deployment is still hindered by the vast model scale and\ncomputational costs. Post-training model pruning offers a way to compress LLMs.\nHowever, the current pruning methods designed for LLMs are not compatible with\nLoRA. This is due to their utilization of unstructured pruning on LLMs,\nimpeding the merging of LoRA weights, or their dependence on the gradients of\npre-trained weights to guide pruning, which can impose significant memory\noverhead. To this end, we propose LoRAPrune, a new framework that delivers an\naccurate structured pruned model in a highly memory-efficient manner.\nSpecifically, we first design a LoRA-guided pruning criterion, which uses the\nweights and gradients of LoRA, rather than the gradients of pre-trained weights\nfor importance estimation. We subsequently integrate this criterion into an\niterative pruning process, effectively removing redundant channels and heads.\nExtensive experimental results demonstrate the superior performance of our\nLoRAPrune over existing approaches on the LLaMA series models. At a 50\\%\ncompression rate, LoRAPrune demonstrates superior performance over LLM-Pruner,\nachieving a reduction in perplexity by 4.81 on WikiText2 and 3.46 on PTB, while\nalso decreasing memory usage by 52.6%. Besides, LoRAPrune also matches\nsemi-structural pruning across multiple LLMs, proving its wide applicability.\nThe code is available at https://github.com/aim-uofa/LoRAPrune.\n","authors":["Mingyang Zhang","Hao Chen","Chunhua Shen","Zhen Yang","Linlin Ou","Xinyi Yu","Bohan Zhuang"],"pdf_url":"https://arxiv.org/pdf/2305.18403v5.pdf","comment":"accepted by acl 2024 findings"},{"id":"http://arxiv.org/abs/2408.03516v1","updated":"2024-08-07T02:54:43Z","published":"2024-08-07T02:54:43Z","title":"Leveraging LLMs for Enhanced Open-Vocabulary 3D Scene Understanding in\n  Autonomous Driving","summary":"  This paper introduces a novel method for open-vocabulary 3D scene\nunderstanding in autonomous driving by combining Language Embedded 3D Gaussians\nwith Large Language Models (LLMs) for enhanced inference. We propose utilizing\nLLMs to generate contextually relevant canonical phrases for segmentation and\nscene interpretation. Our method leverages the contextual and semantic\ncapabilities of LLMs to produce a set of canonical phrases, which are then\ncompared with the language features embedded in the 3D Gaussians. This\nLLM-guided approach significantly improves zero-shot scene understanding and\ndetection of objects of interest, even in the most challenging or unfamiliar\nenvironments. Experimental results on the WayveScenes101 dataset demonstrate\nthat our approach surpasses state-of-the-art methods in terms of accuracy and\nflexibility for open-vocabulary object detection and segmentation. This work\nrepresents a significant advancement towards more intelligent, context-aware\nautonomous driving systems, effectively bridging 3D scene representation with\nhigh-level semantic understanding.\n","authors":["Amirhosein Chahe","Lifeng Zhou"],"pdf_url":"https://arxiv.org/pdf/2408.03516v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16087v4","updated":"2024-08-07T02:36:19Z","published":"2024-06-23T12:02:17Z","title":"Imperative Learning: A Self-supervised Neural-Symbolic Learning\n  Framework for Robot Autonomy","summary":"  Data-driven methods such as reinforcement and imitation learning have\nachieved remarkable success in robot autonomy. However, their data-centric\nnature still hinders them from generalizing well to ever-changing environments.\nMoreover, collecting large datasets for robotic tasks is often impractical and\nexpensive. To overcome these challenges, we introduce a new self-supervised\nneural-symbolic (NeSy) computational framework, imperative learning (IL), for\nrobot autonomy, leveraging the generalization abilities of symbolic reasoning.\nThe framework of IL consists of three primary components: a neural module, a\nreasoning engine, and a memory system. We formulate IL as a special bilevel\noptimization (BLO), which enables reciprocal learning over the three modules.\nThis overcomes the label-intensive obstacles associated with data-driven\napproaches and takes advantage of symbolic reasoning concerning logical\nreasoning, physical principles, geometric analysis, etc. We discuss several\noptimization techniques for IL and verify their effectiveness in five distinct\nrobot autonomy tasks including path planning, rule induction, optimal control,\nvisual odometry, and multi-robot routing. Through various experiments, we show\nthat IL can significantly enhance robot autonomy capabilities and we anticipate\nthat it will catalyze further research across diverse domains.\n","authors":["Chen Wang","Kaiyi Ji","Junyi Geng","Zhongqiang Ren","Taimeng Fu","Fan Yang","Yifan Guo","Haonan He","Xiangyu Chen","Zitong Zhan","Qiwei Du","Shaoshu Su","Bowen Li","Yuheng Qiu","Yi Du","Qihang Li","Yifan Yang","Xiao Lin","Zhipeng Zhao"],"pdf_url":"https://arxiv.org/pdf/2406.16087v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03508v1","updated":"2024-08-07T02:19:17Z","published":"2024-08-07T02:19:17Z","title":"Unsupervised, Self-driving Multi-Step Growth of InAs/GaAs Quantum Dots\n  Heterostructures Guided by Machine Learning","summary":"  The semiconductor industry has prioritized automating repetitive tasks by\nclosed-loop, autonomous experimentation which enables accelerated optimization\nof complex multi-step processes. The emergence of machine learning (ML) has\nushered in automated process with minimal human intervention. In this work, we\ndevelop SemiEpi, a self-driving automation platform capable of executing\nmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ\nmonitoring, and on-the-fly feedback control. By integrating standard hardware,\nhomemade software, curve fitting, and multiple ML models, SemiEpi operates\nautonomously, eliminating the need for extensive expertise in MBE processes to\nachieve optimal outcomes. The platform actively learns from previous\nexperimental results, identifying favorable conditions and proposing new\nexperiments to achieve the desired results. We standardize and optimize growth\nfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of\nML-guided multi-step growth. A temperature calibration was implemented to get\nthe initial growth condition, and fine control of the process was executed\nusing ML. Leveraging RHEED movies acquired during the growth, SemiEpi\nsuccessfully identified and optimized a novel route for multi-step\nheterostructure growth. This work demonstrates the capabilities of closed-loop,\nML-guided systems in addressing challenges in multi-step growth for any device.\nOur method is critical to achieve repeatable materials growth using\ncommercially scalable tools. Our strategy facilitates the development of a\nhardware-independent process and enhancing process repeatability and stability,\neven without exhaustive knowledge of growth parameters.\n","authors":["Chao Shen","Wenkang Zhan","Hongyu Sun","Kaiyao Xin","Bo Xu","Zhanguo Wang","Chao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.03508v1.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2407.03089v3","updated":"2024-08-07T02:06:13Z","published":"2024-07-03T13:26:31Z","title":"Spatio-Temporal Adaptive Diffusion Models for EEG Super-Resolution in\n  Epilepsy Diagnosis","summary":"  Electroencephalogram (EEG) technology, particularly high-density EEG (HD EEG)\ndevices, is widely used in fields such as neuroscience. HD EEG devices improve\nthe spatial resolution of EEG by placing more electrodes on the scalp, meeting\nthe requirements of clinical diagnostic applications such as epilepsy focus\nlocalization. However, this technique faces challenges such as high acquisition\ncosts and limited usage scenarios. In this paper, spatio-temporal adaptive\ndiffusion models (STADMs) are proposed to pioneer the use of diffusion models\nfor achieving spatial SR reconstruction from low-resolution (LR, 64 channels or\nfewer) EEG to high-resolution (HR, 256 channels) EEG. Specifically, a\nspatio-temporal condition module is designed to extract the spatio-temporal\nfeatures of LR EEG, which then serve as conditional inputs to guide the reverse\ndenoising process of diffusion models. Additionally, a multi-scale Transformer\ndenoising module is constructed to leverage multi-scale convolution blocks and\ncross-attention-based diffusion Transformer blocks for conditional guidance to\ngenerate subject-adaptive SR EEG. Experimental results demonstrate that the\nproposed method effectively enhances the spatial resolution of LR EEG and\nquantitatively outperforms existing methods. Furthermore, STADMs demonstrate\ntheir value by applying synthetic SR EEG to classification and source\nlocalization tasks of epilepsy patients, indicating their potential to\nsignificantly improve the spatial resolution of LR EEG.\n","authors":["Tong Zhou","Shuqiang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.03089v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.09274v4","updated":"2024-08-07T01:46:11Z","published":"2024-01-17T15:25:50Z","title":"Avoiding strict saddle points of nonconvex regularized problems","summary":"  In this paper, we consider a class of non-convex and non-smooth sparse\noptimization problems, which encompass most existing nonconvex\nsparsity-inducing terms. We show the second-order optimality conditions only\ndepend on the nonzeros of the stationary points. We propose two damped\niterative reweighted algorithms including the iteratively reweighted $\\ell_1$\nalgorithm (DIRL$_1$) and the iteratively reweighted $\\ell_2$ (DIRL$_2$)\nalgorithm, to solve these problems. For DIRL$_1$, we show the reweighted\n$\\ell_1$ subproblem has support identification property so that DIRL$_1$\nlocally reverts to a gradient descent algorithm around a stationary point. For\nDIRL$_2$, we show the solution map of the reweighted $\\ell_2$ subproblem is\ndifferentiable and Lipschitz continuous everywhere. Therefore, the map of\nDIRL$_1$ and DIRL$_2$ and their inverse are Lipschitz continuous, and the\nstrict saddle points are their unstable fixed points. By applying the stable\nmanifold theorem, these algorithms are shown to converge only to local\nminimizers with randomly initialization when the strictly saddle point property\nis assumed.\n","authors":["Luwei Bai","Yaohua Hu","Hao Wang","Xiaoqi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.09274v4.pdf","comment":"34 pages,4 figures"},{"id":"http://arxiv.org/abs/2408.03497v1","updated":"2024-08-07T01:37:10Z","published":"2024-08-07T01:37:10Z","title":"Advanced User Credit Risk Prediction Model using LightGBM, XGBoost and\n  Tabnet with SMOTEENN","summary":"  Bank credit risk is a significant challenge in modern financial transactions,\nand the ability to identify qualified credit card holders among a large number\nof applicants is crucial for the profitability of a bank'sbank's credit card\nbusiness. In the past, screening applicants'applicants' conditions often\nrequired a significant amount of manual labor, which was time-consuming and\nlabor-intensive. Although the accuracy and reliability of previously used ML\nmodels have been continuously improving, the pursuit of more reliable and\npowerful AI intelligent models is undoubtedly the unremitting pursuit by major\nbanks in the financial industry. In this study, we used a dataset of over\n40,000 records provided by a commercial bank as the research object. We\ncompared various dimensionality reduction techniques such as PCA and T-SNE for\npreprocessing high-dimensional datasets and performed in-depth adaptation and\ntuning of distributed models such as LightGBM and XGBoost, as well as deep\nmodels like Tabnet. After a series of research and processing, we obtained\nexcellent research results by combining SMOTEENN with these techniques. The\nexperiments demonstrated that LightGBM combined with PCA and SMOTEENN\ntechniques can assist banks in accurately predicting potential high-quality\ncustomers, showing relatively outstanding performance compared to other models.\n","authors":["Chang Yu","Yixin Jin","Qianwen Xing","Ye Zhang","Shaobo Guo","Shuchen Meng"],"pdf_url":"https://arxiv.org/pdf/2408.03497v1.pdf","comment":"8 pagess on IEEE ICPICS"},{"id":"http://arxiv.org/abs/2407.02702v3","updated":"2024-08-07T01:35:58Z","published":"2024-07-02T22:51:01Z","title":"Practical Guide for Causal Pathways and Sub-group Disparity Analysis","summary":"  In this study, we introduce the application of causal disparity analysis to\nunveil intricate relationships and causal pathways between sensitive attributes\nand the targeted outcomes within real-world observational data. Our methodology\ninvolves employing causal decomposition analysis to quantify and examine the\ncausal interplay between sensitive attributes and outcomes. We also emphasize\nthe significance of integrating heterogeneity assessment in causal disparity\nanalysis to gain deeper insights into the impact of sensitive attributes within\nspecific sub-groups on outcomes. Our two-step investigation focuses on datasets\nwhere race serves as the sensitive attribute. The results on two datasets\nindicate the benefit of leveraging causal analysis and heterogeneity assessment\nnot only for quantifying biases in the data but also for disentangling their\ninfluences on outcomes. We demonstrate that the sub-groups identified by our\napproach to be affected the most by disparities are the ones with the largest\nML classification errors. We also show that grouping the data only based on a\nsensitive attribute is not enough, and through these analyses, we can find\nsub-groups that are directly affected by disparities. We hope that our findings\nwill encourage the adoption of such methodologies in future ethical AI\npractices and bias audits, fostering a more equitable and fair technological\nlandscape.\n","authors":["Farnaz Kohankhaki","Shaina Raza","Oluwanifemi Bamgbose","Deval Pandya","Elham Dolatabadi"],"pdf_url":"https://arxiv.org/pdf/2407.02702v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03490v1","updated":"2024-08-07T01:01:35Z","published":"2024-08-07T01:01:35Z","title":"Simultaneous and Meshfree Topology Optimization with Physics-informed\n  Gaussian Processes","summary":"  Topology optimization (TO) provides a principled mathematical approach for\noptimizing the performance of a structure by designing its material spatial\ndistribution in a pre-defined domain and subject to a set of constraints. The\nmajority of existing TO approaches leverage numerical solvers for design\nevaluations during the optimization and hence have a nested nature and rely on\ndiscretizing the design variables. Contrary to these approaches, herein we\ndevelop a new class of TO methods based on the framework of Gaussian processes\n(GPs) whose mean functions are parameterized via deep neural networks.\nSpecifically, we place GP priors on all design and state variables to represent\nthem via parameterized continuous functions. These GPs share a deep neural\nnetwork as their mean function but have as many independent kernels as there\nare state and design variables. We estimate all the parameters of our model in\na single for loop that optimizes a penalized version of the performance metric\nwhere the penalty terms correspond to the state equations and design\nconstraints. Attractive features of our approach include $(1)$ having a\nbuilt-in continuation nature since the performance metric is optimized at the\nsame time that the state equations are solved, and $(2)$ being\ndiscretization-invariant and accommodating complex domains and topologies. To\ntest our method against conventional TO approaches implemented in commercial\nsoftware, we evaluate it on four problems involving the minimization of\ndissipated power in Stokes flow. The results indicate that our approach does\nnot need filtering techniques, has consistent computational costs, and is\nhighly robust against random initializations and problem setup.\n","authors":["Amin Yousefpour","Shirin Hosseinmardi","Carlos Mora","Ramin Bostanabad"],"pdf_url":"https://arxiv.org/pdf/2408.03490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03281v2","updated":"2024-08-07T01:00:55Z","published":"2024-08-06T16:28:30Z","title":"StructEval: Deepen and Broaden Large Language Model Assessment via\n  Structured Evaluation","summary":"  Evaluation is the baton for the development of large language models. Current\nevaluations typically employ a single-item assessment paradigm for each atomic\ntest objective, which struggles to discern whether a model genuinely possesses\nthe required capabilities or merely memorizes/guesses the answers to specific\nquestions. To this end, we propose a novel evaluation framework referred to as\nStructEval. Starting from an atomic test objective, StructEval deepens and\nbroadens the evaluation by conducting a structured assessment across multiple\ncognitive levels and critical concepts, and therefore offers a comprehensive,\nrobust and consistent evaluation for LLMs. Experiments on three widely-used\nbenchmarks demonstrate that StructEval serves as a reliable tool for resisting\nthe risk of data contamination and reducing the interference of potential\nbiases, thereby providing more reliable and consistent conclusions regarding\nmodel capabilities. Our framework also sheds light on the design of future\nprincipled and trustworthy LLM evaluation protocols.\n","authors":["Boxi Cao","Mengjie Ren","Hongyu Lin","Xianpei Han","Feng Zhang","Junfeng Zhan","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2408.03281v2.pdf","comment":"ACL 2024;Benchmark at https://github.com/c-box/StructEval\n  ;Leaderboard at https://huggingface.co/spaces/Bowieee/StructEval_leaderboard"},{"id":"http://arxiv.org/abs/2408.04131v1","updated":"2024-08-07T23:41:09Z","published":"2024-08-07T23:41:09Z","title":"Heterogeneous Graph Sequence Neural Networks for Dynamic Traffic\n  Assignment","summary":"  Traffic assignment and traffic flow prediction provide critical insights for\nurban planning, traffic management, and the development of intelligent\ntransportation systems. An efficient model for calculating traffic flows over\nthe entire transportation network could provide a more detailed and realistic\nunderstanding of traffic dynamics. However, existing traffic prediction\napproaches, such as those utilizing graph neural networks, are typically\nlimited to locations where sensors are deployed and cannot predict traffic\nflows beyond sensor locations. To alleviate this limitation, inspired by\nfundamental relationship that exists between link flows and the\norigin-destination (OD) travel demands, we proposed the Heterogeneous\nSpatio-Temporal Graph Sequence Network (HSTGSN). HSTGSN exploits dependency\nbetween origin and destination nodes, even when it is long-range, and learns\nimplicit vehicle route choices under different origin-destination demands. This\nmodel is based on a heterogeneous graph which consists of road links, OD links\n(virtual links connecting origins and destinations) and a spatio-temporal graph\nencoder-decoder that captures the spatio-temporal relationship between OD\ndemands and flow distribution. We will show how the graph encoder-decoder is\nable to recover the incomplete information in the OD demand, by using node\nembedding from the graph decoder to predict the temporal changes in flow\ndistribution. Using extensive experimental studies on real-world networks with\ncomplete/incomplete OD demands, we demonstrate that our method can not only\ncapture the implicit spatio-temporal relationship between link traffic flows\nand OD demands but also achieve accurate prediction performance and\ngeneralization capability.\n","authors":["Tong Liu","Hadi Meidani"],"pdf_url":"https://arxiv.org/pdf/2408.04131v1.pdf","comment":"9 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04129v1","updated":"2024-08-07T23:30:53Z","published":"2024-08-07T23:30:53Z","title":"Out-of-Core Dimensionality Reduction for Large Data via Out-of-Sample\n  Extensions","summary":"  Dimensionality reduction (DR) is a well-established approach for the\nvisualization of high-dimensional data sets. While DR methods are often applied\nto typical DR benchmark data sets in the literature, they might suffer from\nhigh runtime complexity and memory requirements, making them unsuitable for\nlarge data visualization especially in environments outside of high-performance\ncomputing. To perform DR on large data sets, we propose the use of\nout-of-sample extensions. Such extensions allow inserting new data into\nexisting projections, which we leverage to iteratively project data into a\nreference projection that consists only of a small manageable subset. This\nprocess makes it possible to perform DR out-of-core on large data, which would\notherwise not be possible due to memory and runtime limitations. For metric\nmultidimensional scaling (MDS), we contribute an implementation with\nout-of-sample projection capability since typical software libraries do not\nsupport it. We provide an evaluation of the projection quality of five common\nDR algorithms (MDS, PCA, t-SNE, UMAP, and autoencoders) using quality metrics\nfrom the literature and analyze the trade-off between the size of the reference\nset and projection quality. The runtime behavior of the algorithms is also\nquantified with respect to reference set size, out-of-sample batch size, and\ndimensionality of the data sets. Furthermore, we compare the out-of-sample\napproach to other recently introduced DR methods, such as PaCMAP and TriMAP,\nwhich claim to handle larger data sets than traditional approaches. To showcase\nthe usefulness of DR on this large scale, we contribute a use case where we\nanalyze ensembles of streamlines amounting to one billion projected instances.\n","authors":["Luca Reichmann","David Hgele","Daniel Weiskopf"],"pdf_url":"https://arxiv.org/pdf/2408.04129v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04125v1","updated":"2024-08-07T23:22:58Z","published":"2024-08-07T23:22:58Z","title":"Exploring RAG-based Vulnerability Augmentation with LLMs","summary":"  Detecting vulnerabilities is a crucial task for maintaining the integrity,\navailability, and security of software systems. Utilizing DL-based models for\nvulnerability detection has become commonplace in recent years. However, such\ndeep learning-based vulnerability detectors (DLVD) suffer from a shortage of\nsizable datasets to train effectively. Data augmentation can potentially\nalleviate the shortage of data, but augmenting vulnerable code is challenging\nand requires designing a generative solution that maintains vulnerability.\nHence, the work on generating vulnerable code samples has been limited and\nprevious works have only focused on generating samples that contain single\nstatements or specific types of vulnerabilities. Lately, large language models\n(LLMs) are being used for solving various code generation and comprehension\ntasks and have shown inspiring results, especially when fused with retrieval\naugmented generation (RAG). In this study, we explore three different\nstrategies to augment vulnerabilities both single and multi-statement\nvulnerabilities, with LLMs, namely Mutation, Injection, and Extension. We\nconducted an extensive evaluation of our proposed approach on three\nvulnerability datasets and three DLVD models, using two LLMs. Our results show\nthat our injection-based clustering-enhanced RAG method beats the baseline\nsetting (NoAug), Vulgen, and VGX (two SOTA methods), and Random Oversampling\n(ROS) by 30.80\\%, 27.48\\%, 27.93\\%, and 15.41\\% in f1-score with 5K generated\nvulnerable samples on average, and 53.84\\%, 54.10\\%, 69.90\\%, and 40.93\\% with\n15K generated vulnerable samples. Our approach demonstrates its feasibility for\nlarge-scale data augmentation by generating 1K samples at as cheap as US$ 1.88.\n","authors":["Seyed Shayan Daneshvar","Yu Nong","Xu Yang","Shaowei Wang","Haipeng Cai"],"pdf_url":"https://arxiv.org/pdf/2408.04125v1.pdf","comment":"13 pages, 6 figures, 5 tables, 3 prompt templates, 1 algorithm"},{"id":"http://arxiv.org/abs/2408.04122v1","updated":"2024-08-07T23:15:21Z","published":"2024-08-07T23:15:21Z","title":"Overcoming Brittleness in Pareto-Optimal Learning-Augmented Algorithms","summary":"  The study of online algorithms with machine-learned predictions has gained\nconsiderable prominence in recent years. One of the common objectives in the\ndesign and analysis of such algorithms is to attain (Pareto) optimal tradeoffs\nbetween the consistency of the algorithm, i.e., its performance assuming\nperfect predictions, and its robustness, i.e., the performance of the algorithm\nunder adversarial predictions. In this work, we demonstrate that this\noptimization criterion can be extremely brittle, in that the performance of\nPareto-optimal algorithms may degrade dramatically even in the presence of\nimperceptive prediction error. To remedy this drawback, we propose a new\nframework in which the smoothness in the performance of the algorithm is\nenforced by means of a user-specified profile. This allows us to regulate the\nperformance of the algorithm as a function of the prediction error, while\nsimultaneously maintaining the analytical notion of consistency/robustness\ntradeoffs, adapted to the profile setting. We apply this new approach to a\nwell-studied online problem, namely the one-way trading problem. For this\nproblem, we further address another limitation of the state-of-the-art\nPareto-optimal algorithms, namely the fact that they are tailored to\nworst-case, and extremely pessimistic inputs. We propose a new Pareto-optimal\nalgorithm that leverages any deviation from the worst-case input to its\nbenefit, and introduce a new metric that allows us to compare any two\nPareto-optimal algorithms via a dominance relation.\n","authors":["Spyros Angelopoulos","Christoph Drr","Alex Elenter","Yanni Lefki"],"pdf_url":"https://arxiv.org/pdf/2408.04122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.01536v2","updated":"2024-08-07T22:46:04Z","published":"2024-04-02T00:02:00Z","title":"Laying Anchors: Semantically Priming Numerals in Language Modeling","summary":"  Off-the-shelf pre-trained language models have become the de facto standard\nin NLP pipelines for a multitude of downstream tasks. However, the inability of\nthese models to properly encode numerals limits their performance on tasks\nrequiring numeric comprehension. We introduce strategies to semantically prime\nnumerals in any corpus by generating anchors governed by the distribution of\nnumerals in said corpus, thereby enabling mathematically grounded\nrepresentations of these numeral tokens. We establish the superiority of our\nproposed techniques through evaluation on a range of numeracy tasks for both\nin-domain (seen) and out-domain (unseen) numerals. Further, we expand our\nempirical evaluations to numerals ranging from 1 to 10 billion, a significantly\nbroader range compared to previous studies of the same nature, and we\ndemonstrate significant improvements in the mathematical grounding of our\nlearned embeddings.\n","authors":["Mandar Sharma","Rutuja Murlidhar Taware","Pravesh Koirala","Nikhil Muralidhar","Naren Ramakrishnan"],"pdf_url":"https://arxiv.org/pdf/2404.01536v2.pdf","comment":"Accepted to the findings of NAACL 2024"},{"id":"http://arxiv.org/abs/2311.03366v4","updated":"2024-08-07T22:43:10Z","published":"2023-10-16T22:20:31Z","title":"Functional Overlap Reranking for Neural Code Generation","summary":"  Code Large Language Models (CodeLLMs) have ushered in a new era in code\ngeneration advancements. However, selecting the best code solutions from all\npossible CodeLLM outputs remains a challenge. Previous methods often overlooked\nthe intricate functional similarities and interactions between solution\nclusters. We introduce SRank, a novel reranking strategy for selecting the best\nsolutions from code generation, focusing on modeling the relationships between\nclusters of solutions. By quantifying the functional overlap between solution\nclusters, our approach provides a better ranking strategy for code solutions.\nEmpirical results show that our method achieves remarkable results on the\npass@1 score. For instance, on the Human-Eval benchmark, we achieve 69.66% in\npass@1 with Codex002, 75.31% with WizardCoder, 53.99% with StarCoder, and\n60.55% with CodeGen, surpassing state-of-the-art code generation reranking\nmethods such as CodeT and Coder-Reviewer on the same CodeLLM by a significant\nmargin (approximately 6.1% improvement on average). Even in scenarios with a\nlimited number of sampled solutions and test cases, our approach demonstrates\nrobustness and superiority, marking a new benchmark in code generation\nreranking. Our implementation can be found at\nhttps://github.com/FSoft-AI4Code/SRank-CodeRanker.\n","authors":["Hung Quoc To","Minh Huynh Nguyen","Nghi D. Q. Bui"],"pdf_url":"https://arxiv.org/pdf/2311.03366v4.pdf","comment":"ACL 2024, Long Findings"},{"id":"http://arxiv.org/abs/2408.04116v1","updated":"2024-08-07T22:40:05Z","published":"2024-08-07T22:40:05Z","title":"Combining Neural Architecture Search and Automatic Code Optimization: A\n  Survey","summary":"  Deep Learning models have experienced exponential growth in complexity and\nresource demands in recent years. Accelerating these models for efficient\nexecution on resource-constrained devices has become more crucial than ever.\nTwo notable techniques employed to achieve this goal are Hardware-aware Neural\nArchitecture Search (HW-NAS) and Automatic Code Optimization (ACO). HW-NAS\nautomatically designs accurate yet hardware-friendly neural networks, while ACO\ninvolves searching for the best compiler optimizations to apply on neural\nnetworks for efficient mapping and inference on the target hardware. This\nsurvey explores recent works that combine these two techniques within a single\nframework. We present the fundamental principles of both domains and\ndemonstrate their sub-optimality when performed independently. We then\ninvestigate their integration into a joint optimization process that we call\nHardware Aware-Neural Architecture and Compiler Optimizations co-Search\n(NACOS).\n","authors":["Inas Bachiri","Hadjer Benmeziane","Smail Niar","Riyadh Baghdadi","Hamza Ouarnoughi","Abdelkrime Aries"],"pdf_url":"https://arxiv.org/pdf/2408.04116v1.pdf","comment":"version 0, 13 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.04114v1","updated":"2024-08-07T22:32:19Z","published":"2024-08-07T22:32:19Z","title":"Zero-shot Factual Consistency Evaluation Across Domains","summary":"  This work addresses the challenge of factual consistency in text generation\nsystems. We unify the tasks of Natural Language Inference, Summarization\nEvaluation, Factuality Verification and Factual Consistency Evaluation to train\nmodels capable of evaluating the factual consistency of source-target pairs\nacross diverse domains. We rigorously evaluate these against eight baselines on\na comprehensive benchmark suite comprising 22 datasets that span various tasks,\ndomains, and document lengths. Results demonstrate that our method achieves\nstate-of-the-art performance on this heterogeneous benchmark while addressing\nefficiency concerns and attaining cross-domain generalization.\n","authors":["Raunak Agarwal"],"pdf_url":"https://arxiv.org/pdf/2408.04114v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04113v1","updated":"2024-08-07T22:30:43Z","published":"2024-08-07T22:30:43Z","title":"UpLIF: An Updatable Self-Tuning Learned Index Framework","summary":"  The emergence of learned indexes has caused a paradigm shift in our\nperception of indexing by considering indexes as predictive models that\nestimate keys' positions within a data set, resulting in notable improvements\nin key search efficiency and index size reduction; however, a significant\nchallenge inherent in learned index modeling is its constrained support for\nupdate operations, necessitated by the requirement for a fixed distribution of\nrecords. Previous studies have proposed various approaches to address this\nissue with the drawback of high overhead due to multiple model retraining. In\nthis paper, we present UpLIF, an adaptive self-tuning learned index that\nadjusts the model to accommodate incoming updates, predicts the distribution of\nupdates for performance improvement, and optimizes its index structure using\nreinforcement learning. We also introduce the concept of balanced model\nadjustment, which determines the model's inherent properties (i.e. bias and\nvariance), enabling the integration of these factors into the existing index\nmodel without the need for retraining with new data. Our comprehensive\nexperiments show that the system surpasses state-of-the-art indexing solutions\n(both traditional and ML-based), achieving an increase in throughput of up to\n3.12 times with 1000 times less memory usage.\n","authors":["Alireza Heidari","Amirhossein Ahmadi","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04113v1.pdf","comment":"20 pages, ACM IDEAS 2024"},{"id":"http://arxiv.org/abs/2408.04107v1","updated":"2024-08-07T22:10:26Z","published":"2024-08-07T22:10:26Z","title":"Zero-Delay QKV Compression for Mitigating KV Cache and Network\n  Bottlenecks in LLM Inference","summary":"  In large-language models, memory constraints in the key-value cache (KVC)\npose a challenge during inference, especially with long prompts. In this work,\nwe observed that compressing KV values is more effective than compressing the\nmodel regarding accuracy and job completion time (JCT). However, quantizing KV\nvalues and dropping less-important tokens incur significant runtime\ncomputational time overhead, delaying JCT. These methods also cannot reduce\ncomputation time or high network communication time overhead in\nsequence-parallelism (SP) frameworks for long prompts. To tackle these issues,\nbased on our insightful observations from experimental analysis, we propose\nZeroC, a Zero-delay QKV Compression system that eliminates time overhead and\neven reduces computation and communication time of the model operations. ZeroC\ninnovatively embeds compression and decompression operations within model\noperations and adaptively determines compression ratios at a hybrid layer-token\nlevel. Further, it enables a communication-efficient SP inference framework.\nTrace-driven experiments demonstrate that ZeroC achieves up to 80% lower\naverage JCT, 35% lower average perplexity, and 2.8x higher throughput with the\nsame latency compared to state-of-the-art compression methods. ZeroC also\nreduces the average JCT of current LLM serving systems by up to 91% with the\nconstraint of 0.1 perplexity increase. We open-sourced the code.\n","authors":["Zeyu Zhang","Haiying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04868v3","updated":"2024-08-07T22:00:18Z","published":"2024-06-07T12:07:16Z","title":"Perturb-and-Project: Differentially Private Similarities and Marginals","summary":"  We revisit the input perturbations framework for differential privacy where\nnoise is added to the input $A\\in \\mathcal{S}$ and the result is then projected\nback to the space of admissible datasets $\\mathcal{S}$. Through this framework,\nwe first design novel efficient algorithms to privately release pair-wise\ncosine similarities. Second, we derive a novel algorithm to compute $k$-way\nmarginal queries over $n$ features. Prior work could achieve comparable\nguarantees only for $k$ even. Furthermore, we extend our results to $t$-sparse\ndatasets, where our efficient algorithms yields novel, stronger guarantees\nwhenever $t\\le n^{5/6}/\\log n\\,.$ Finally, we provide a theoretical perspective\non why \\textit{fast} input perturbation algorithms works well in practice. The\nkey technical ingredients behind our results are tight sum-of-squares\ncertificates upper bounding the Gaussian complexity of sets of solutions.\n","authors":["Vincent Cohen-Addad","Tommaso d'Orsi","Alessandro Epasto","Vahab Mirrokni","Peilin Zhong"],"pdf_url":"https://arxiv.org/pdf/2406.04868v3.pdf","comment":"21 ppages, ICML 2024"},{"id":"http://arxiv.org/abs/2404.10859v2","updated":"2024-08-07T21:57:20Z","published":"2024-04-16T19:17:23Z","title":"Forcing Diffuse Distributions out of Language Models","summary":"  Despite being trained specifically to follow user instructions, today's\ninstructiontuned language models perform poorly when instructed to produce\nrandom outputs. For example, when prompted to pick a number uniformly between\none and ten Llama-2-13B-chat disproportionately favors the number five, and\nwhen tasked with picking a first name at random, Mistral-7B-Instruct chooses\nAvery 40 times more often than we would expect based on the U.S. population.\nWhen these language models are used for real-world tasks where diversity of\noutputs is crucial, such as language model assisted dataset construction, their\ninability to produce diffuse distributions over valid choices is a major\nhurdle. In this work, we propose a fine-tuning method that encourages language\nmodels to output distributions that are diffuse over valid outcomes. The\nmethods we introduce generalize across a variety of tasks and distributions and\nmake large language models practical for synthetic dataset generation with\nlittle human intervention.\n","authors":["Yiming Zhang","Avi Schwarzschild","Nicholas Carlini","Zico Kolter","Daphne Ippolito"],"pdf_url":"https://arxiv.org/pdf/2404.10859v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01391v2","updated":"2024-08-07T21:55:08Z","published":"2024-08-02T17:01:36Z","title":"FT K-means: A High-Performance K-means on GPU with Fault Tolerance","summary":"  K-means is a widely used algorithm in clustering, however, its efficiency is\nprimarily constrained by the computational cost of distance computing. Existing\nimplementations suffer from suboptimal utilization of computational units and\nlack resilience against soft errors. To address these challenges, we introduce\nFT K-means, a high-performance GPU-accelerated implementation of K-means with\nonline fault tolerance. We first present a stepwise optimization strategy that\nachieves competitive performance compared to NVIDIA's cuML library. We further\nimprove FT K-means with a template-based code generation framework that\nsupports different data types and adapts to different input shapes. A novel\nwarp-level tensor-core error correction scheme is proposed to address the\nfailure of existing fault tolerance methods due to memory asynchronization\nduring copy operations. Our experimental evaluations on NVIDIA T4 GPU and A100\nGPU demonstrate that FT K-means without fault tolerance outperforms cuML's\nK-means implementation, showing a performance increase of 10\\%-300\\% in\nscenarios involving irregular data shapes. Moreover, the fault tolerance\nfeature of FT K-means introduces only an overhead of 11\\%, maintaining robust\nperformance even with tens of errors injected per second.\n","authors":["Shixun Wu","Yitong Ding","Yujia Zhai","Jinyang Liu","Jiajun Huang","Zizhe Jian","Huangliang Dai","Sheng Di","Bryan M. Wong","Zizhong Chen","Franck Cappello"],"pdf_url":"https://arxiv.org/pdf/2408.01391v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04104v1","updated":"2024-08-07T21:45:01Z","published":"2024-08-07T21:45:01Z","title":"Hardware-Assisted Virtualization of Neural Processing Units for Cloud\n  Platforms","summary":"  Cloud platforms today have been deploying hardware accelerators like neural\nprocessing units (NPUs) for powering machine learning (ML) inference services.\nTo maximize the resource utilization while ensuring reasonable quality of\nservice, a natural approach is to virtualize NPUs for efficient resource\nsharing for multi-tenant ML services. However, virtualizing NPUs for modern\ncloud platforms is not easy. This is not only due to the lack of system\nabstraction support for NPU hardware, but also due to the lack of architectural\nand ISA support for enabling fine-grained dynamic operator scheduling for\nvirtualized NPUs.\n  We present TCloud, a holistic NPU virtualization framework. We investigate\nvirtualization techniques for NPUs across the entire software and hardware\nstack. TCloud consists of (1) a flexible NPU abstraction called vNPU, which\nenables fine-grained virtualization of the heterogeneous compute units in a\nphysical NPU (pNPU); (2) a vNPU resource allocator that enables pay-as-you-go\ncomputing model and flexible vNPU-to-pNPU mappings for improved resource\nutilization and cost-effectiveness; (3) an ISA extension of modern NPU\narchitecture for facilitating fine-grained tensor operator scheduling for\nmultiple vNPUs. We implement TCloud based on a production-level NPU simulator.\nOur experiments show that TCloud improves the throughput of ML inference\nservices by up to 1.4$\\times$ and reduces the tail latency by up to\n4.6$\\times$, while improving the NPU utilization by 1.2$\\times$ on average,\ncompared to state-of-the-art NPU sharing approaches.\n","authors":["Yuqi Xue","Yiqi Liu","Lifeng Nai","Jian Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04104v1.pdf","comment":"Accepted to MICRO'24"},{"id":"http://arxiv.org/abs/2408.04093v1","updated":"2024-08-07T21:16:55Z","published":"2024-08-07T21:16:55Z","title":"Tree Attention: Topology-aware Decoding for Long-Context Attention on\n  GPU clusters","summary":"  Self-attention is the core mathematical operation of modern transformer\narchitectures and is also a significant computational bottleneck due to its\nquadratic complexity in the sequence length. In this work, we derive the scalar\nenergy function whose gradient computes the self-attention block, thus\nelucidating the theoretical underpinnings of self-attention, providing a\nBayesian interpretation of the operation and linking it closely with\nenergy-based models such as Hopfield Networks. Moreover, due to this\nformulation, we discover that we can use efficient and optimized\nautomatic-differentiation techniques to derive a highly efficient Tree\nAttention algorithm to compute the gradient of the energy and hence\nself-attention. Our formulation reveals that the reduction across the sequence\naxis can be efficiently computed in parallel through a tree reduction. Our\nalgorithm, for parallelizing attention computation across multiple GPUs,\nenables cross-device decoding to be performed asymptotically faster (up to 8x\nfaster) than alternative approaches such as Ring Attention, while also\nrequiring significantly less communication volume and incurring 2x less peak\nmemory. Our code is publicly available here:\n\\url{https://github.com/Zyphra/tree_attention}\n","authors":["Vasudev Shyam","Jonathan Pilault","Emily Shepperd","Quentin Anthony","Beren Millidge"],"pdf_url":"https://arxiv.org/pdf/2408.04093v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.11651v4","updated":"2024-08-07T21:14:03Z","published":"2023-09-20T21:32:58Z","title":"Drift Control of High-Dimensional RBM: A Computational Method Based on\n  Neural Networks","summary":"  Motivated by applications in queueing theory, we consider a stochastic\ncontrol problem whose state space is the $d$-dimensional positive orthant. The\ncontrolled process $Z$ evolves as a reflected Brownian motion whose covariance\nmatrix is exogenously specified, as are its directions of reflection from the\northant's boundary surfaces. A system manager chooses a drift vector\n$\\theta(t)$ at each time $t$ based on the history of $Z$, and the cost rate at\ntime $t$ depends on both $Z(t)$ and $\\theta(t)$. In our initial problem\nformulation, the objective is to minimize expected discounted cost over an\ninfinite planning horizon, after which we treat the corresponding ergodic\ncontrol problem. Extending earlier work by Han et al. (Proceedings of the\nNational Academy of Sciences, 2018, 8505-8510), we develop and illustrate a\nsimulation-based computational method that relies heavily on deep neural\nnetwork technology. For test problems studied thus far, our method is accurate\nto within a fraction of one percent, and is computationally feasible in\ndimensions up to at least $d=30$.\n","authors":["Baris Ata","J. Michael Harrison","Nian Si"],"pdf_url":"https://arxiv.org/pdf/2309.11651v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14631v2","updated":"2024-08-07T21:10:30Z","published":"2024-07-19T19:07:53Z","title":"Two new feature selection methods based on learn-heuristic techniques\n  for breast cancer prediction: A comprehensive analysis","summary":"  Breast cancer is not preventable because of its unknown causes. However, its\nearly diagnosis increases patients' recovery chances. Machine learning (ML) can\nbe utilized to improve treatment outcomes in healthcare operations while\ndiminishing costs and time. In this research, we suggest two novel feature\nselection (FS) methods based upon an imperialist competitive algorithm (ICA)\nand a bat algorithm (BA) and their combination with ML algorithms. This study\naims to enhance diagnostic models' efficiency and present a comprehensive\nanalysis to help clinical physicians make much more precise and reliable\ndecisions than before. K-nearest neighbors, support vector machine, decision\ntree, Naive Bayes, AdaBoost, linear discriminant analysis, random forest,\nlogistic regression, and artificial neural network are some of the methods\nemployed. This paper applied a distinctive integration of evaluation measures\nand ML algorithms using the wrapper feature selection based on ICA (WFSIC) and\nBA (WFSB) separately. We compared two proposed approaches for the performance\nof the classifiers. Also, we compared our best diagnostic model with previous\nworks reported in the literature survey. Experimentations were performed on the\nWisconsin diagnostic breast cancer dataset. Results reveal that the proposed\nframework that uses the BA with an accuracy of 99.12\\%, surpasses the framework\nusing the ICA and most previous works. Additionally, the RF classifier in the\napproach of FS based on BA emerges as the best model and outperforms others\nregarding its criteria. Besides, the results illustrate the role of our\ntechniques in reducing the dataset dimensions up to 90\\% and increasing the\nperformance of diagnostic models by over 99\\%. Moreover, the result\ndemonstrates that there are more critical features than the optimum dataset\nobtained by proposed FS approaches that have been selected by most ML models.\n","authors":["Kamyab Karimi","Ali Ghodratnama","Reza Tavakkoli-Moghaddam"],"pdf_url":"https://arxiv.org/pdf/2407.14631v2.pdf","comment":"36 pages, 3 figures, 12 tables"},{"id":"http://arxiv.org/abs/2203.08248v2","updated":"2024-08-07T20:39:29Z","published":"2022-03-15T20:50:26Z","title":"Non-Linear Reinforcement Learning in Large Action Spaces: Structural\n  Conditions and Sample-efficiency of Posterior Sampling","summary":"  Provably sample-efficient Reinforcement Learning (RL) with rich observations\nand function approximation has witnessed tremendous recent progress,\nparticularly when the underlying function approximators are linear. In this\nlinear regime, computationally and statistically efficient methods exist where\nthe potentially infinite state and action spaces can be captured through a\nknown feature embedding, with the sample complexity scaling with the\n(intrinsic) dimension of these features. When the action space is finite,\nsignificantly more sophisticated results allow non-linear function\napproximation under appropriate structural constraints on the underlying RL\nproblem, permitting for instance, the learning of good features instead of\nassuming access to them. In this work, we present the first result for\nnon-linear function approximation which holds for general action spaces under a\nlinear embeddability condition, which generalizes all linear and finite action\nsettings. We design a novel optimistic posterior sampling strategy, TS^3 for\nsuch problems, and show worst case sample complexity guarantees that scale with\na rank parameter of the RL problem, the linear embedding dimension introduced\nin this work and standard measures of the function class complexity.\n","authors":["Alekh Agarwal","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2203.08248v2.pdf","comment":"Fixes an error in the earlier version in the proof of Proposition 13"},{"id":"http://arxiv.org/abs/2406.10407v2","updated":"2024-08-07T20:10:14Z","published":"2024-06-14T20:31:22Z","title":"Suboptimality bounds for trace-bounded SDPs enable a faster and scalable\n  low-rank SDP solver SDPLR+","summary":"  Semidefinite programs (SDPs) and their solvers are powerful tools with many\napplications in machine learning and data science. Designing scalable SDP\nsolvers is challenging because by standard the positive semidefinite decision\nvariable is an $n \\times n$ dense matrix, even though the input is often $n\n\\times n$ sparse matrices. However, the information in the solution may not\ncorrespond to a full-rank dense matrix as shown by Barvinok and Pataki. Two\ndecades ago, Burer and Monteiro developed an SDP solver $\\texttt{SDPLR}$ that\noptimizes over a low-rank factorization instead of the full matrix. This\ngreatly decreases the storage cost and works well for many problems. The\noriginal solver $\\texttt{SDPLR}$ tracks only the primal infeasibility of the\nsolution, limiting the technique's flexibility to produce moderate accuracy\nsolutions. We use a suboptimality bound for trace-bounded SDP problems that\nenables us to track the progress better and perform early termination. We then\ndevelop $\\texttt{SDPLR+}$, which starts the optimization with an extremely\nlow-rank factorization and dynamically updates the rank based on the primal\ninfeasibility and suboptimality. This further speeds up the computation and\nsaves the storage cost. Numerical experiments on Max Cut, Minimum Bisection,\nCut Norm, and Lov\\'{a}sz Theta problems with many recent memory-efficient\nscalable SDP solvers demonstrate its scalability up to problems with\nmillion-by-million decision variables and it is often the fastest solver to a\nmoderate accuracy of $10^{-2}$.\n","authors":["Yufan Huang","David F. Gleich"],"pdf_url":"https://arxiv.org/pdf/2406.10407v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2408.04065v1","updated":"2024-08-07T20:07:25Z","published":"2024-08-07T20:07:25Z","title":"Do Sharpness-based Optimizers Improve Generalization in Medical Image\n  Analysis?","summary":"  Effective clinical deployment of deep learning models in healthcare demands\nhigh generalization performance to ensure accurate diagnosis and treatment\nplanning. In recent years, significant research has focused on improving the\ngeneralization of deep learning models by regularizing the sharpness of the\nloss landscape. Among the optimization approaches that explicitly minimize\nsharpness, Sharpness-Aware Minimization (SAM) has shown potential in enhancing\ngeneralization performance on general domain image datasets. This success has\nled to the development of several advanced sharpness-based algorithms aimed at\naddressing the limitations of SAM, such as Adaptive SAM, surrogate-Gap SAM,\nWeighted SAM, and Curvature Regularized SAM. These sharpness-based optimizers\nhave shown improvements in model generalization compared to conventional\nstochastic gradient descent optimizers and their variants on general domain\nimage datasets, but they have not been thoroughly evaluated on medical images.\nThis work provides a review of recent sharpness-based methods for improving the\ngeneralization of deep learning networks and evaluates the methods performance\non medical breast ultrasound images. Our findings indicate that the initial SAM\nmethod successfully enhances the generalization of various deep learning\nmodels. While Adaptive SAM improves generalization of convolutional neural\nnetworks, it fails to do so for vision transformers. Other sharpness-based\noptimizers, however, do not demonstrate consistent results. The results reveal\nthat, contrary to findings in the non-medical domain, SAM is the only\nrecommended sharpness-based optimizer that consistently improves generalization\nin medical image analysis, and further research is necessary to refine the\nvariants of SAM to enhance generalization performance in this field\n","authors":["Mohamed Hassan","Aleksander Vakanski","Min Xian"],"pdf_url":"https://arxiv.org/pdf/2408.04065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02058v2","updated":"2024-08-07T19:45:05Z","published":"2024-04-02T15:57:32Z","title":"Generalizable, Fast, and Accurate DeepQSPR with fastprop","summary":"  Quantitative Structure Property Relationship studies aim to define a mapping\nbetween molecular structure and arbitrary quantities of interest. This was\nhistorically accomplished via the development of descriptors which requires\nsignificant domain expertise and struggles to generalize. Thus the field has\nmorphed into Molecular Property Prediction and been given over to learned\nrepresentations which are highly generalizable. The paper introduces fastprop,\na DeepQSPR framework which uses a cogent set of molecular level descriptors to\nmeet and exceed the performance of learned representations on diverse datasets\nin dramatically less time. fastprop is freely available on github at\ngithub.com/JacksonBurns/fastprop.\n","authors":["Jackson Burns","William Green"],"pdf_url":"https://arxiv.org/pdf/2404.02058v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04057v1","updated":"2024-08-07T19:39:37Z","published":"2024-08-07T19:39:37Z","title":"PowerPM: Foundation Model for Power Systems","summary":"  The emergence of abundant electricity time series (ETS) data provides ample\nopportunities for various applications in the power systems, including\ndemand-side management, grid stability, and consumer behavior analysis. Deep\nlearning models have advanced ETS modeling by effectively capturing sequence\ndependence. Nevertheless, learning a generic representation of ETS data for\nvarious applications remains challenging due to the inherently complex\nhierarchical structure of ETS data. Moreover, ETS data exhibits intricate\ntemporal dependencies and is suscepti ble to the influence of exogenous\nvariables. Furthermore, different instances exhibit diverse electricity\nconsumption behavior. In this paper, we propose a foundation model PowerPM to\nmodel ETS data, providing a large-scale, off-the-shelf model for power systems.\nPowerPM consists of a temporal encoder and a hierarchical encoder. The temporal\nencoder captures both temporal dependencies in ETS data, considering exogenous\nvariables. The hierarchical encoder models the correlation between hierarchy.\nFurthermore, PowerPM leverages a novel self-supervised pretraining framework\nconsisting of masked ETS modeling and dual-view contrastive learning, which\nenable PowerPM to capture temporal dependency within ETS windows and aware the\ndiscrepancy across ETS windows, providing two different perspectives to learn\ngeneric representation. Our experiments involve five real world scenario\ndatasets, comprising private and public data. Through pre-training on massive\nETS data, PowerPM achieves SOTA performance on diverse downstream tasks within\nthe private dataset. Impressively, when transferred to the public datasets,\nPowerPM maintains its superiority, showcasing its remarkable generalization\nability across various tasks and domains. Moreover, ablation studies, few-shot\nexperiments provide additional evidence of the effectiveness of our model.\n","authors":["Shihao Tu","Yupeng Zhang","Jing Zhang","Yang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04057v1.pdf","comment":"23 pages, 5 figures, 8 tables"},{"id":"http://arxiv.org/abs/2408.04055v1","updated":"2024-08-07T19:34:42Z","published":"2024-08-07T19:34:42Z","title":"Machine Learning-Based Reward-Driven Tuning of Scanning Probe\n  Microscopy: Towards Fully Automated Microscopy","summary":"  Since the dawn of scanning probe microscopy (SPM), tapping or intermittent\ncontact mode has been one of the most widely used imaging modes. Manual\noptimization of tapping mode not only takes a lot of instrument and operator\ntime, but also often leads to frequent probe and sample damage, poor image\nquality and reproducibility issues for new types of samples or inexperienced\nusers. Despite wide use, optimization of tapping mode imaging is an extremely\nhard problem, ill-suited to either classical control methods or machine\nlearning. Here we introduce a reward-driven workflow to automate the\noptimization of SPM in the tapping mode. The reward function is defined based\non multiple channels with physical and empirical knowledge of good scans\nencoded, representing a sample-agnostic measure of image quality and imitating\nthe decision-making logic employed by human operators. This automated workflow\ngives optimal scanning parameters for different probes and samples and gives\nhigh-quality SPM images consistently in the attractive mode. This study\nbroadens the application and accessibility of SPM and opens the door for fully\nautomated SPM.\n","authors":["Yu Liu","Roger Proksch","Jason Bemis","Utkarsh Pratiush","Astita Dubey","Mahshid Ahmadi","Reece Emery","Philip D. Rack","Yu-Chen Liu","Jan-Chi Yang","Sergei V. Kalinin"],"pdf_url":"https://arxiv.org/pdf/2408.04055v1.pdf","comment":"20 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.09611v2","updated":"2024-08-07T19:27:53Z","published":"2024-02-14T22:57:03Z","title":"Towards Privacy-Aware Sign Language Translation at Scale","summary":"  A major impediment to the advancement of sign language translation (SLT) is\ndata scarcity. Much of the sign language data currently available on the web\ncannot be used for training supervised models due to the lack of aligned\ncaptions. Furthermore, scaling SLT using large-scale web-scraped datasets bears\nprivacy risks due to the presence of biometric information, which the\nresponsible development of SLT technologies should account for. In this work,\nwe propose a two-stage framework for privacy-aware SLT at scale that addresses\nboth of these issues. We introduce SSVP-SLT, which leverages self-supervised\nvideo pretraining on anonymized and unannotated videos, followed by supervised\nSLT finetuning on a curated parallel dataset. SSVP-SLT achieves\nstate-of-the-art finetuned and zero-shot gloss-free SLT performance on the\nHow2Sign dataset, outperforming the strongest respective baselines by over 3\nBLEU-4. Based on controlled experiments, we further discuss the advantages and\nlimitations of self-supervised pretraining and anonymization via facial\nobfuscation for SLT.\n","authors":["Phillip Rust","Bowen Shi","Skyler Wang","Necati Cihan Camgz","Jean Maillard"],"pdf_url":"https://arxiv.org/pdf/2402.09611v2.pdf","comment":"ACL 2024"},{"id":"http://arxiv.org/abs/2408.04053v1","updated":"2024-08-07T19:24:02Z","published":"2024-08-07T19:24:02Z","title":"Deep Generative Models for Subgraph Prediction","summary":"  Graph Neural Networks (GNNs) are important across different domains, such as\nsocial network analysis and recommendation systems, due to their ability to\nmodel complex relational data. This paper introduces subgraph queries as a new\ntask for deep graph learning. Unlike traditional graph prediction tasks that\nfocus on individual components like link prediction or node classification,\nsubgraph queries jointly predict the components of a target subgraph based on\nevidence that is represented by an observed subgraph. For instance, a subgraph\nquery can predict a set of target links and/or node labels. To answer subgraph\nqueries, we utilize a probabilistic deep Graph Generative Model. Specifically,\nwe inductively train a Variational Graph Auto-Encoder (VGAE) model, augmented\nto represent a joint distribution over links, node features and labels.\nBayesian optimization is used to tune a weighting for the relative importance\nof links, node features and labels in a specific domain. We describe a\ndeterministic and a sampling-based inference method for estimating subgraph\nprobabilities from the VGAE generative graph distribution, without retraining,\nin zero-shot fashion. For evaluation, we apply the inference methods on a range\nof subgraph queries on six benchmark datasets. We find that inference from a\nmodel achieves superior predictive performance, surpassing independent\nprediction baselines with improvements in AUC scores ranging from 0.06 to 0.2\npoints, depending on the dataset.\n","authors":["Erfaneh Mahmoudzadeh","Parmis Naddaf","Kiarash Zahirnia","Oliver Schulte"],"pdf_url":"https://arxiv.org/pdf/2408.04053v1.pdf","comment":"accepted at ECAI 2024"},{"id":"http://arxiv.org/abs/2307.07176v3","updated":"2024-08-07T19:08:36Z","published":"2023-07-14T06:00:08Z","title":"SafeDreamer: Safe Reinforcement Learning with World Models","summary":"  The deployment of Reinforcement Learning (RL) in real-world applications is\nconstrained by its failure to satisfy safety criteria. Existing Safe\nReinforcement Learning (SafeRL) methods, which rely on cost functions to\nenforce safety, often fail to achieve zero-cost performance in complex\nscenarios, especially vision-only tasks. These limitations are primarily due to\nmodel inaccuracies and inadequate sample efficiency. The integration of the\nworld model has proven effective in mitigating these shortcomings. In this\nwork, we introduce SafeDreamer, a novel algorithm incorporating\nLagrangian-based methods into world model planning processes within the\nsuperior Dreamer framework. Our method achieves nearly zero-cost performance on\nvarious tasks, spanning low-dimensional and vision-only input, within the\nSafety-Gymnasium benchmark, showcasing its efficacy in balancing performance\nand safety in RL tasks. Further details can be found in the code repository:\n\\url{https://github.com/PKU-Alignment/SafeDreamer}.\n","authors":["Weidong Huang","Jiaming Ji","Chunhe Xia","Borong Zhang","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2307.07176v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2408.04046v1","updated":"2024-08-07T18:55:58Z","published":"2024-08-07T18:55:58Z","title":"Learning Rate-Free Reinforcement Learning: A Case for Model Selection\n  with Non-Stationary Objectives","summary":"  The performance of reinforcement learning (RL) algorithms is sensitive to the\nchoice of hyperparameters, with the learning rate being particularly\ninfluential. RL algorithms fail to reach convergence or demand an extensive\nnumber of samples when the learning rate is not optimally set. In this work, we\nshow that model selection can help to improve the failure modes of RL that are\ndue to suboptimal choices of learning rate. We present a model selection\nframework for Learning Rate-Free Reinforcement Learning that employs model\nselection methods to select the optimal learning rate on the fly. This approach\nof adaptive learning rate tuning neither depends on the underlying RL algorithm\nnor the optimizer and solely uses the reward feedback to select the learning\nrate; hence, the framework can input any RL algorithm and produce a learning\nrate-free version of it. We conduct experiments for policy optimization methods\nand evaluate various model selection strategies within our framework. Our\nresults indicate that data-driven model selection algorithms are better\nalternatives to standard bandit algorithms when the optimal choice of\nhyperparameter is time-dependent and non-stationary.\n","authors":["Aida Afshar","Aldo Pacchiano"],"pdf_url":"https://arxiv.org/pdf/2408.04046v1.pdf","comment":"RLC 2024 Workshop on Failure Modes of Sequential Decision-Making in\n  Practice"},{"id":"http://arxiv.org/abs/2408.04042v1","updated":"2024-08-07T18:47:58Z","published":"2024-08-07T18:47:58Z","title":"Scaling Law of Sim2Real Transfer Learning in Expanding Computational\n  Materials Databases for Real-World Predictions","summary":"  To address the challenge of limited experimental materials data, extensive\nphysical property databases are being developed based on high-throughput\ncomputational experiments, such as molecular dynamics simulations. Previous\nstudies have shown that fine-tuning a predictor pretrained on a computational\ndatabase to a real system can result in models with outstanding generalization\ncapabilities compared to learning from scratch. This study demonstrates the\nscaling law of simulation-to-real (Sim2Real) transfer learning for several\nmachine learning tasks in materials science. Case studies of three prediction\ntasks for polymers and inorganic materials reveal that the prediction error on\nreal systems decreases according to a power-law as the size of the\ncomputational data increases. Observing the scaling behavior offers various\ninsights for database development, such as determining the sample size\nnecessary to achieve a desired performance, identifying equivalent sample sizes\nfor physical and computational experiments, and guiding the design of data\nproduction protocols for downstream real-world tasks.\n","authors":["Shunya Minami","Yoshihiro Hayashi","Stephen Wu","Kenji Fukumizu","Hiroki Sugisawa","Masashi Ishii","Isao Kuwajima","Kazuya Shiratori","Ryo Yoshida"],"pdf_url":"https://arxiv.org/pdf/2408.04042v1.pdf","comment":"22 pages, 6 figures"},{"id":"http://arxiv.org/abs/2402.07812v2","updated":"2024-08-07T18:27:59Z","published":"2024-02-12T17:17:50Z","title":"Retrieval Augmented Thought Process for Private Data Handling in\n  Healthcare","summary":"  Large Language Models (LLMs) have demonstrated the strong potential to assist\nboth clinicians and the general public with their extensive medical knowledge.\nHowever, their application in healthcare is constrained due to concerns about\nthe privacy of data used in training, which prevents the integration of private\nand personal information because of security and ethical issues. Moreover, if\ntheir capabilities can be enhanced with information retrieval to access\nup-to-date knowledge, the current integration of LLMs with Information\nretrieval lacks robustness to imperfect retrieval, which can hinder their\neffectiveness and even reduce overall performance. In this work, we address\nthis challenge by introducing the Retrieval-Augmented Thought Process (RATP).\nGiven access to external knowledge, RATP formulates the thought generation of\nLLMs as a multiple-step decision process. To optimise such a thought process,\nRATP leverages Monte-Carlo Tree Search and learns a proxy reward function that\npermits cost-efficient inference. On a private dataset of electronic medical\nrecords, deliberately excluded from any LLM training set, RATP achieves 35%\nadditional accuracy compared to in-context retrieval-augmented generation for\nthe question-answering task.\n","authors":["Thomas Pouplin","Hao Sun","Samuel Holt","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2402.07812v2.pdf","comment":"17 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.04026v1","updated":"2024-08-07T18:19:18Z","published":"2024-08-07T18:19:18Z","title":"Multimodal Gender Fairness in Depression Prediction: Insights on Data\n  from the USA & China","summary":"  Social agents and robots are increasingly being used in wellbeing settings.\nHowever, a key challenge is that these agents and robots typically rely on\nmachine learning (ML) algorithms to detect and analyse an individual's mental\nwellbeing. The problem of bias and fairness in ML algorithms is becoming an\nincreasingly greater source of concern. In concurrence, existing literature has\nalso indicated that mental health conditions can manifest differently across\ngenders and cultures. We hypothesise that the representation of features\n(acoustic, textual, and visual) and their inter-modal relations would vary\namong subjects from different cultures and genders, thus impacting the\nperformance and fairness of various ML models. We present the very first\nevaluation of multimodal gender fairness in depression manifestation by\nundertaking a study on two different datasets from the USA and China. We\nundertake thorough statistical and ML experimentation and repeat the\nexperiments for several different algorithms to ensure that the results are not\nalgorithm-dependent. Our findings indicate that though there are differences\nbetween both datasets, it is not conclusive whether this is due to the\ndifference in depression manifestation as hypothesised or other external\nfactors such as differences in data collection methodology. Our findings\nfurther motivate a call for a more consistent and culturally aware data\ncollection process in order to address the problem of ML bias in depression\ndetection and to promote the development of fairer agents and robots for\nwellbeing.\n","authors":["Joseph Cameron","Jiaee Cheong","Micol Spitale","Hatice Gunes"],"pdf_url":"https://arxiv.org/pdf/2408.04026v1.pdf","comment":"9 Pages, 7 Tables. To be published and indexed in the IEEE Xplore\n  Digital Library under the ACII 2024 Workshop Proceedings"},{"id":"http://arxiv.org/abs/2403.00952v2","updated":"2024-08-07T18:14:12Z","published":"2024-03-01T20:03:44Z","title":"MediSwift: Efficient Sparse Pre-trained Biomedical Language Models","summary":"  Large language models (LLMs) are typically trained on general source data for\nvarious domains, but a recent surge in domain-specific LLMs has shown their\npotential to outperform general-purpose models in domain-specific tasks (e.g.,\nbiomedicine). Although domain-specific pre-training enhances efficiency and\nleads to smaller models, the computational costs of training these LLMs remain\nhigh, posing budgeting challenges. We introduce MediSwift, a suite of\nbiomedical LMs that leverage sparse pre-training on domain-specific biomedical\ntext data. By inducing up to 75% weight sparsity during the pre-training phase,\nMediSwift achieves a 2-2.5x reduction in training FLOPs. Notably, all sparse\npre-training was performed on the Cerebras CS-2 system, which is specifically\ndesigned to realize the acceleration benefits from unstructured weight\nsparsity, thereby significantly enhancing the efficiency of the MediSwift\nmodels. Through subsequent dense fine-tuning and strategic soft prompting,\nMediSwift models outperform existing LLMs up to 7B parameters on biomedical\ntasks, setting new benchmarks w.r.t efficiency-accuracy on tasks such as\nPubMedQA. Our results show that sparse pre-training, along with dense\nfine-tuning and soft prompting, offers an effective method for creating\nhigh-performing, computationally efficient models in specialized domains.\n","authors":["Vithursan Thangarasa","Mahmoud Salem","Shreyas Saxena","Kevin Leong","Joel Hestness","Sean Lie"],"pdf_url":"https://arxiv.org/pdf/2403.00952v2.pdf","comment":"14 pages, 2 Figures, 5 Tables (Main Paper) + 3 pages (Supplementary\n  Material). Published at ACL 2024"},{"id":"http://arxiv.org/abs/2405.15682v3","updated":"2024-08-07T17:44:58Z","published":"2024-05-24T16:20:46Z","title":"The Road Less Scheduled","summary":"  Existing learning rate schedules that do not require specification of the\noptimization stopping step T are greatly out-performed by learning rate\nschedules that depend on T. We propose an approach that avoids the need for\nthis stopping time by eschewing the use of schedules entirely, while exhibiting\nstate-of-the-art performance compared to schedules across a wide family of\nproblems ranging from convex problems to large-scale deep learning problems.\nOur Schedule-Free approach introduces no additional hyper-parameters over\nstandard optimizers with momentum. Our method is a direct consequence of a new\ntheory we develop that unifies scheduling and iterate averaging. An open source\nimplementation of our method is available\n(https://github.com/facebookresearch/schedule_free).\n","authors":["Aaron Defazio","Xingyu Alice Yang","Harsh Mehta","Konstantin Mishchenko","Ahmed Khaled","Ashok Cutkosky"],"pdf_url":"https://arxiv.org/pdf/2405.15682v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2401.11764v2","updated":"2024-08-07T15:31:39Z","published":"2024-01-22T08:59:09Z","title":"Identity-Driven Multimedia Forgery Detection via Reference Assistance","summary":"  Recent advancements in \"deepfake\" techniques have paved the way for\ngenerating various media forgeries. In response to the potential hazards of\nthese media forgeries, many researchers engage in exploring detection methods,\nincreasing the demand for high-quality media forgery datasets. Despite this,\nexisting datasets have certain limitations. Firstly, most datasets focus on\nmanipulating visual modality and usually lack diversity, as only a few forgery\napproaches are considered. Secondly, the quality of media is often inadequate\nin clarity and naturalness. Meanwhile, the size of the dataset is also limited.\nThirdly, it is commonly observed that real-world forgeries are motivated by\nidentity, yet the identity information of the individuals portrayed in these\nforgeries within existing datasets remains under-explored. For detection,\nidentity information could be an essential clue to boost performance. Moreover,\nofficial media concerning relevant identities on the Internet can serve as\nprior knowledge, aiding both the audience and forgery detectors in determining\nthe true identity. Therefore, we propose an identity-driven multimedia forgery\ndataset, IDForge, which contains 249,138 video shots sourced from 324 wild\nvideos of 54 celebrities collected from the Internet. The fake video shots\ninvolve 9 types of manipulation across visual, audio, and textual modalities.\nAdditionally, IDForge provides extra 214,438 real video shots as a reference\nset for the 54 celebrities. Correspondingly, we propose the Reference-assisted\nMultimodal Forgery Detection Network (R-MFDN), aiming at the detection of\ndeepfake videos. Through extensive experiments on the proposed dataset, we\ndemonstrate the effectiveness of R-MFDN on the multimedia detection task.\n","authors":["Junhao Xu","Jingjing Chen","Xue Song","Feng Han","Haijun Shan","Yugang Jiang"],"pdf_url":"https://arxiv.org/pdf/2401.11764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.00763v2","updated":"2024-08-07T15:10:15Z","published":"2024-01-01T14:06:55Z","title":"New Job, New Gender? Measuring the Social Bias in Image Generation\n  Models","summary":"  Image generation models can generate or edit images from a given text. Recent\nadvancements in image generation technology, exemplified by DALL-E and\nMidjourney, have been groundbreaking. These advanced models, despite their\nimpressive capabilities, are often trained on massive Internet datasets, making\nthem susceptible to generating content that perpetuates social stereotypes and\nbiases, which can lead to severe consequences. Prior research on assessing bias\nwithin image generation models suffers from several shortcomings, including\nlimited accuracy, reliance on extensive human labor, and lack of comprehensive\nanalysis. In this paper, we propose BiasPainter, a novel evaluation framework\nthat can accurately, automatically and comprehensively trigger social bias in\nimage generation models. BiasPainter uses a diverse range of seed images of\nindividuals and prompts the image generation models to edit these images using\ngender, race, and age-neutral queries. These queries span 62 professions, 39\nactivities, 57 types of objects, and 70 personality traits. The framework then\ncompares the edited images to the original seed images, focusing on the\nsignificant changes related to gender, race, and age. BiasPainter adopts a key\ninsight that these characteristics should not be modified when subjected to\nneutral prompts. Built upon this design, BiasPainter can trigger the social\nbias and evaluate the fairness of image generation models. We use BiasPainter\nto evaluate six widely-used image generation models, such as stable diffusion\nand Midjourney. Experimental results show that BiasPainter can successfully\ntrigger social bias in image generation models. According to our human\nevaluation, BiasPainter can achieve 90.8% accuracy on automatic bias detection,\nwhich is significantly higher than the results reported in previous work.\n","authors":["Wenxuan Wang","Haonan Bai","Jen-tse Huang","Yuxuan Wan","Youliang Yuan","Haoyi Qiu","Nanyun Peng","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2401.00763v2.pdf","comment":"ACM MM 2024 Oral"},{"id":"http://arxiv.org/abs/2408.01669v2","updated":"2024-08-07T12:06:18Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v2.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.03650v1","updated":"2024-08-07T09:25:17Z","published":"2024-08-07T09:25:17Z","title":"Towards Multimodal Emotional Support Conversation Systems","summary":"  The integration of conversational artificial intelligence (AI) into mental\nhealth care promises a new horizon for therapist-client interactions, aiming to\nclosely emulate the depth and nuance of human conversations. Despite the\npotential, the current landscape of conversational AI is markedly limited by\nits reliance on single-modal data, constraining the systems' ability to\nempathize and provide effective emotional support. This limitation stems from a\npaucity of resources that encapsulate the multimodal nature of human\ncommunication essential for therapeutic counseling. To address this gap, we\nintroduce the Multimodal Emotional Support Conversation (MESC) dataset, a\nfirst-of-its-kind resource enriched with comprehensive annotations across text,\naudio, and video modalities. This dataset captures the intricate interplay of\nuser emotions, system strategies, system emotion, and system responses, setting\na new precedent in the field. Leveraging the MESC dataset, we propose a general\nSequential Multimodal Emotional Support framework (SMES) grounded in\nTherapeutic Skills Theory. Tailored for multimodal dialogue systems, the SMES\nframework incorporates an LLM-based reasoning model that sequentially generates\nuser emotion recognition, system strategy prediction, system emotion\nprediction, and response generation. Our rigorous evaluations demonstrate that\nthis framework significantly enhances the capability of AI systems to mimic\ntherapist behaviors with heightened empathy and strategic responsiveness. By\nintegrating multimodal data in this innovative manner, we bridge the critical\ngap between emotion recognition and emotional support, marking a significant\nadvancement in conversational AI for mental health support.\n","authors":["Yuqi Chu","Lizi Liao","Zhiyuan Zhou","Chong-Wah Ngo","Richang Hong"],"pdf_url":"https://arxiv.org/pdf/2408.03650v1.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.03648v1","updated":"2024-08-07T09:23:01Z","published":"2024-08-07T09:23:01Z","title":"HiQuE: Hierarchical Question Embedding Network for Multimodal Depression\n  Detection","summary":"  The utilization of automated depression detection significantly enhances\nearly intervention for individuals experiencing depression. Despite numerous\nproposals on automated depression detection using recorded clinical interview\nvideos, limited attention has been paid to considering the hierarchical\nstructure of the interview questions. In clinical interviews for diagnosing\ndepression, clinicians use a structured questionnaire that includes routine\nbaseline questions and follow-up questions to assess the interviewee's\ncondition. This paper introduces HiQuE (Hierarchical Question Embedding\nnetwork), a novel depression detection framework that leverages the\nhierarchical relationship between primary and follow-up questions in clinical\ninterviews. HiQuE can effectively capture the importance of each question in\ndiagnosing depression by learning mutual information across multiple\nmodalities. We conduct extensive experiments on the widely-used clinical\ninterview data, DAIC-WOZ, where our model outperforms other state-of-the-art\nmultimodal depression detection models and emotion recognition models,\nshowcasing its clinical utility in depression detection.\n","authors":["Juho Jung","Chaewon Kang","Jeewoo Yoon","Seungbae Kim","Jinyoung Han"],"pdf_url":"https://arxiv.org/pdf/2408.03648v1.pdf","comment":"11 pages, 6 figures, Proceedings of the 33rd ACM International\n  Conference on Information and Knowledge Management (CIKM '24)"},{"id":"http://arxiv.org/abs/2408.03637v1","updated":"2024-08-07T08:52:21Z","published":"2024-08-07T08:52:21Z","title":"TALE: Training-free Cross-domain Image Composition via Adaptive Latent\n  Manipulation and Energy-guided Optimization","summary":"  We present TALE, a novel training-free framework harnessing the generative\ncapabilities of text-to-image diffusion models to address the cross-domain\nimage composition task that focuses on flawlessly incorporating user-specified\nobjects into a designated visual contexts regardless of domain disparity.\nPrevious methods often involve either training auxiliary networks or finetuning\ndiffusion models on customized datasets, which are expensive and may undermine\nthe robust textual and visual priors of pre-trained diffusion models. Some\nrecent works attempt to break the barrier by proposing training-free\nworkarounds that rely on manipulating attention maps to tame the denoising\nprocess implicitly. However, composing via attention maps does not necessarily\nyield desired compositional outcomes. These approaches could only retain some\nsemantic information and usually fall short in preserving identity\ncharacteristics of input objects or exhibit limited background-object style\nadaptation in generated images. In contrast, TALE is a novel method that\noperates directly on latent space to provide explicit and effective guidance\nfor the composition process to resolve these problems. Specifically, we equip\nTALE with two mechanisms dubbed Adaptive Latent Manipulation and Energy-guided\nLatent Optimization. The former formulates noisy latents conducive to\ninitiating and steering the composition process by directly leveraging\nbackground and foreground latents at corresponding timesteps, and the latter\nexploits designated energy functions to further optimize intermediate latents\nconforming to specific conditions that complement the former to generate\ndesired final results. Our experiments demonstrate that TALE surpasses prior\nbaselines and attains state-of-the-art performance in image-guided composition\nacross various photorealistic and artistic domains.\n","authors":["Kien T. Pham","Jingye Chen","Qifeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.03637v1.pdf","comment":"The 32nd ACM Multimedia Conference (MM '24)"},{"id":"http://arxiv.org/abs/2408.03632v1","updated":"2024-08-07T08:43:58Z","published":"2024-08-07T08:43:58Z","title":"Concept Conductor: Orchestrating Multiple Personalized Concepts in\n  Text-to-Image Synthesis","summary":"  The customization of text-to-image models has seen significant advancements,\nyet generating multiple personalized concepts remains a challenging task.\nCurrent methods struggle with attribute leakage and layout confusion when\nhandling multiple concepts, leading to reduced concept fidelity and semantic\nconsistency. In this work, we introduce a novel training-free framework,\nConcept Conductor, designed to ensure visual fidelity and correct layout in\nmulti-concept customization. Concept Conductor isolates the sampling processes\nof multiple custom models to prevent attribute leakage between different\nconcepts and corrects erroneous layouts through self-attention-based spatial\nguidance. Additionally, we present a concept injection technique that employs\nshape-aware masks to specify the generation area for each concept. This\ntechnique injects the structure and appearance of personalized concepts through\nfeature fusion in the attention layers, ensuring harmony in the final image.\nExtensive qualitative and quantitative experiments demonstrate that Concept\nConductor can consistently generate composite images with accurate layouts\nwhile preserving the visual details of each concept. Compared to existing\nbaselines, Concept Conductor shows significant performance improvements. Our\nmethod supports the combination of any number of concepts and maintains high\nfidelity even when dealing with visually similar concepts. The code and models\nare available at https://github.com/Nihukat/Concept-Conductor.\n","authors":["Zebin Yao","Fangxiang Feng","Ruifan Li","Xiaojie Wang"],"pdf_url":"https://arxiv.org/pdf/2408.03632v1.pdf","comment":"Github Page: https://github.com/Nihukat/Concept-Conductor"},{"id":"http://arxiv.org/abs/2408.04013v1","updated":"2024-08-07T18:02:53Z","published":"2024-08-07T18:02:53Z","title":"MetaDragonBoat: Exploring Paddling Techniques of Virtual Dragon Boating\n  in a Metaverse Campus","summary":"  The preservation of cultural heritage, as mandated by the United Nations\nSustainable Development Goals (SDGs), is integral to sustainable urban\ndevelopment. This paper focuses on the Dragon Boat Festival, a prominent event\nin Chinese cultural heritage, and proposes leveraging Virtual Reality (VR), to\nenhance its preservation and accessibility. Traditionally, participation in the\nfestival's dragon boat races was limited to elite athletes, excluding broader\ndemographics. Our proposed solution, named MetaDragonBoat, enables virtual\nparticipation in dragon boat racing, offering immersive experiences that\nreplicate physical exertion through a cultural journey. Thus, we build a\ndigital twin of a university campus located in a region with a rich dragon boat\nracing tradition. Coupled with three paddling techniques that are enabled by\neither commercial controllers or physical paddle controllers with haptic\nfeedback, diversified users can engage in realistic rowing experiences. Our\nresults demonstrate that by integrating resistance into the paddle controls,\nusers could simulate the physical effort of dragon boat racing, promoting a\ndeeper understanding and appreciation of this cultural heritage.\n","authors":["Wei He","Xiang Li","Shengtian Xu","Yuzheng Chen","Chan-In Sio","Ge Lin Kan","Lik-Hang Lee"],"pdf_url":"https://arxiv.org/pdf/2408.04013v1.pdf","comment":"10 pages, accepted at ACM MM 2024"}]},"2024-08-08T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2408.04632v1","updated":"2024-08-08T17:59:46Z","published":"2024-08-08T17:59:46Z","title":"Arctic-TILT. Business Document Understanding at Sub-Billion Scale","summary":"  The vast portion of workloads employing LLMs involves answering questions\ngrounded on PDF or scan content. We introduce the Arctic-TILT achieving\naccuracy on par with models 1000$\\times$ its size on these use cases. It can be\nfine-tuned and deployed on a single 24GB GPU, lowering operational costs while\nprocessing Visually Rich Documents with up to 400k tokens. The model\nestablishes state-of-the-art results on seven diverse Document Understanding\nbenchmarks, as well as provides reliable confidence scores and quick inference,\nwhich are essential for processing files in large-scale or time-sensitive\nenterprise environments.\n","authors":["ukasz Borchmann","Micha Pietruszka","Wojciech Jakowski","Dawid Jurkiewicz","Piotr Halama","Pawe Jziak","ukasz Garncarek","Pawe Liskowski","Karolina Szyndler","Andrzej Gretkowski","Julita Otusek","Gabriela Nowakowska","Artur Zawocki","ukasz Duhr","Pawe Dyda","Micha Turski"],"pdf_url":"https://arxiv.org/pdf/2408.04632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04628v1","updated":"2024-08-08T17:58:06Z","published":"2024-08-08T17:58:06Z","title":"LogogramNLP: Comparing Visual and Textual Representations of Ancient\n  Logographic Writing Systems for NLP","summary":"  Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.\n","authors":["Danlu Chen","Freda Shi","Aditi Agarwal","Jacobo Myerston","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2408.04628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04619v1","updated":"2024-08-08T17:49:07Z","published":"2024-08-08T17:49:07Z","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","summary":"  Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.\n","authors":["Aeree Cho","Grace C. Kim","Alexander Karpekov","Alec Helbling","Zijie J. Wang","Seongmin Lee","Benjamin Hoover","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2408.04619v1.pdf","comment":"To be presented at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.04614v1","updated":"2024-08-08T17:42:32Z","published":"2024-08-08T17:42:32Z","title":"Better Alignment with Instruction Back-and-Forth Translation","summary":"  We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.\n","authors":["Thao Nguyen","Jeffrey Li","Sewoong Oh","Ludwig Schmidt","Jason Weston","Luke Zettlemoyer","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.04614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18418v2","updated":"2024-08-08T17:39:47Z","published":"2024-07-25T22:31:50Z","title":"Know Your Limits: A Survey of Abstention in Large Language Models","summary":"  Abstention, the refusal of large language models (LLMs) to provide an answer,\nis increasingly recognized for its potential to mitigate hallucinations and\nenhance safety in LLM systems. In this survey, we introduce a framework to\nexamine abstention from three perspectives: the query, the model, and human\nvalues. We organize the literature on abstention methods, benchmarks, and\nevaluation metrics using this framework, and discuss merits and limitations of\nprior work. We further identify and motivate areas for future work, centered\naround whether abstention can be achieved as a meta-capability that transcends\nspecific tasks or domains, while still providing opportunities to optimize\nabstention abilities based on context.\n","authors":["Bingbing Wen","Jihan Yao","Shangbin Feng","Chenjun Xu","Yulia Tsvetkov","Bill Howe","Lucy Lu Wang"],"pdf_url":"https://arxiv.org/pdf/2407.18418v2.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2408.04596v1","updated":"2024-08-08T17:14:12Z","published":"2024-08-08T17:14:12Z","title":"Code-switching in text and speech reveals information-theoretic audience\n  design","summary":"  In this work, we use language modeling to investigate the factors that\ninfluence code-switching. Code-switching occurs when a speaker alternates\nbetween one language variety (the primary language) and another (the secondary\nlanguage), and is widely observed in multilingual contexts. Recent work has\nshown that code-switching is often correlated with areas of high information\nload in the primary language, but it is unclear whether high primary language\nload only makes the secondary language relatively easier to produce at\ncode-switching points (speaker-driven code-switching), or whether\ncode-switching is additionally used by speakers to signal the need for greater\nattention on the part of listeners (audience-driven code-switching). In this\npaper, we use bilingual Chinese-English online forum posts and transcripts of\nspontaneous Chinese-English speech to replicate prior findings that high\nprimary language (Chinese) information load is correlated with switches to the\nsecondary language (English). We then demonstrate that the information load of\nthe English productions is even higher than that of meaning equivalent Chinese\nalternatives, and these are therefore not easier to produce, providing evidence\nof audience-driven influences in code-switching at the level of the\ncommunication channel, not just at the sociolinguistic level, in both writing\nand speech.\n","authors":["Debasmita Bhattacharya","Marten van Schijndel"],"pdf_url":"https://arxiv.org/pdf/2408.04596v1.pdf","comment":"Submitted to Journal of Memory and Language on 7 June 2024"},{"id":"http://arxiv.org/abs/2408.02666v2","updated":"2024-08-08T17:09:58Z","published":"2024-08-05T17:57:02Z","title":"Self-Taught Evaluators","summary":"  Model-based evaluation is at the heart of successful model development -- as\na reward model for training, and as a replacement for human evaluation. To\ntrain such evaluators, the standard approach is to collect a large amount of\nhuman preference judgments over model responses, which is costly and the data\nbecomes stale as models improve. In this work, we present an approach that aims\nto im-prove evaluators without human annotations, using synthetic training data\nonly. Starting from unlabeled instructions, our iterative self-improvement\nscheme generates contrasting model outputs and trains an LLM-as-a-Judge to\nproduce reasoning traces and final judgments, repeating this training at each\nnew iteration using the improved predictions. Without any labeled preference\ndata, our Self-Taught Evaluator can improve a strong LLM (Llama3-70B-Instruct)\nfrom 75.4 to 88.3 (88.7 with majority vote) on RewardBench. This outperforms\ncommonly used LLM judges such as GPT-4 and matches the performance of the\ntop-performing reward models trained with labeled examples.\n","authors":["Tianlu Wang","Ilia Kulikov","Olga Golovneva","Ping Yu","Weizhe Yuan","Jane Dwivedi-Yu","Richard Yuanzhe Pang","Maryam Fazel-Zarandi","Jason Weston","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.02666v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04585v1","updated":"2024-08-08T16:54:40Z","published":"2024-08-08T16:54:40Z","title":"Towards Resilient and Efficient LLMs: A Comparative Study of Efficiency,\n  Performance, and Adversarial Robustness","summary":"  With the increasing demand for practical applications of Large Language\nModels (LLMs), many attention-efficient models have been developed to balance\nperformance and computational cost. However, the adversarial robustness of\nthese models remains under-explored. In this work, we design a framework to\ninvestigate the trade-off between efficiency, performance, and adversarial\nrobustness of LLMs by comparing three prominent models with varying levels of\ncomplexity and efficiency -- Transformer++, Gated Linear Attention (GLA)\nTransformer, and MatMul-Free LM -- utilizing the GLUE and AdvGLUE datasets. The\nAdvGLUE dataset extends the GLUE dataset with adversarial samples designed to\nchallenge model robustness. Our results show that while the GLA Transformer and\nMatMul-Free LM achieve slightly lower accuracy on GLUE tasks, they demonstrate\nhigher efficiency and either superior or comparative robustness on AdvGLUE\ntasks compared to Transformer++ across different attack levels. These findings\nhighlight the potential of simplified architectures to achieve a compelling\nbalance between efficiency, performance, and adversarial robustness, offering\nvaluable insights for applications where resource constraints and resilience to\nadversarial attacks are critical.\n","authors":["Xiaojing Fan","Chunliang Tao"],"pdf_url":"https://arxiv.org/pdf/2408.04585v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04575v1","updated":"2024-08-08T16:36:24Z","published":"2024-08-08T16:36:24Z","title":"SCENE: Evaluating Explainable AI Techniques Using Soft Counterfactuals","summary":"  Explainable Artificial Intelligence (XAI) is essential for enhancing the\ntransparency and accountability of AI models, especially in natural language\nprocessing (NLP) tasks. This paper introduces SCENE (Soft Counterfactual\nEvaluation for Natural language Explainability), a novel evaluation method that\nleverages large language models (LLMs) to generate Soft Counterfactual\nexplanations in a zero-shot manner. By focusing on token-based substitutions,\nSCENE creates contextually appropriate and seman-tically meaningful Soft\nCounterfactuals without extensive fine-tuning. SCENE adopts Validitysoft and\nCsoft metrics to evaluate the effectiveness of model-agnostic XAI methods in\ntext classification tasks. Applied to CNN, RNN, and BERT architectures, SCENE\nprovides valuable insights into the strengths and limitations of various XAI\ntechniques.\n","authors":["Haoran Zheng","Utku Pamuksuz"],"pdf_url":"https://arxiv.org/pdf/2408.04575v1.pdf","comment":"10 pages, 5 tables"},{"id":"http://arxiv.org/abs/2408.00161v2","updated":"2024-08-08T16:31:05Z","published":"2024-07-31T21:12:21Z","title":"Automatic Generation of Behavioral Test Cases For Natural Language\n  Processing Using Clustering and Prompting","summary":"  Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.\n","authors":["Ying Li","Rahul Singh","Tarun Joshi","Agus Sudjianto"],"pdf_url":"https://arxiv.org/pdf/2408.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04568v1","updated":"2024-08-08T16:28:22Z","published":"2024-08-08T16:28:22Z","title":"Learning Fine-Grained Grounded Citations for Attributed Large Language\n  Models","summary":"  Despite the impressive performance on information-seeking tasks, large\nlanguage models (LLMs) still struggle with hallucinations. Attributed LLMs,\nwhich augment generated text with in-line citations, have shown potential in\nmitigating hallucinations and improving verifiability. However, current\napproaches suffer from suboptimal citation quality due to their reliance on\nin-context learning. Furthermore, the practice of citing only coarse document\nidentifiers makes it challenging for users to perform fine-grained\nverification. In this work, we introduce FRONT, a training framework designed\nto teach LLMs to generate Fine-Grained Grounded Citations. By grounding model\noutputs in fine-grained supporting quotes, these quotes guide the generation of\ngrounded and consistent responses, not only improving citation quality but also\nfacilitating fine-grained verification. Experiments on the ALCE benchmark\ndemonstrate the efficacy of FRONT in generating superior grounded responses and\nhighly supportive citations. With LLaMA-2-7B, the framework significantly\noutperforms all the baselines, achieving an average of 14.21% improvement in\ncitation quality across all datasets, even surpassing ChatGPT.\n","authors":["Lei Huang","Xiaocheng Feng","Weitao Ma","Yuxuan Gu","Weihong Zhong","Xiachong Feng","Weijiang Yu","Weihua Peng","Duyu Tang","Dandan Tu","Bing Qin"],"pdf_url":"https://arxiv.org/pdf/2408.04568v1.pdf","comment":"Accepted by ACL 2024 Findings"},{"id":"http://arxiv.org/abs/2408.04560v1","updated":"2024-08-08T16:18:39Z","published":"2024-08-08T16:18:39Z","title":"Conversational Prompt Engineering","summary":"  Prompts are how humans communicate with LLMs. Informative prompts are\nessential for guiding LLMs to produce the desired output. However, prompt\nengineering is often tedious and time-consuming, requiring significant\nexpertise, limiting its widespread use. We propose Conversational Prompt\nEngineering (CPE), a user-friendly tool that helps users create personalized\nprompts for their specific tasks. CPE uses a chat model to briefly interact\nwith users, helping them articulate their output preferences and integrating\nthese into the prompt. The process includes two main stages: first, the model\nuses user-provided unlabeled data to generate data-driven questions and utilize\nuser responses to shape the initial instruction. Then, the model shares the\noutputs generated by the instruction and uses user feedback to further refine\nthe instruction and the outputs. The final result is a few-shot prompt, where\nthe outputs approved by the user serve as few-shot examples. A user study on\nsummarization tasks demonstrates the value of CPE in creating personalized,\nhigh-performing prompts. The results suggest that the zero-shot prompt obtained\nis comparable to its - much longer - few-shot counterpart, indicating\nsignificant savings in scenarios involving repetitive tasks with large text\nvolumes.\n","authors":["Liat Ein-Dor","Orith Toledo-Ronen","Artem Spector","Shai Gretz","Lena Dankin","Alon Halfon","Yoav Katz","Noam Slonim"],"pdf_url":"https://arxiv.org/pdf/2408.04560v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04556v1","updated":"2024-08-08T16:13:26Z","published":"2024-08-08T16:13:26Z","title":"Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of\n  Large Language Models","summary":"  Large language models (LLMs) have exhibited remarkable proficiency across a\ndiverse array of natural language processing (NLP) tasks. However, adapting\nLLMs to downstream applications typically necessitates computationally\nintensive and memory-demanding fine-tuning procedures. To mitigate these\nburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a\npromising approach to tailor LLMs with minimal computational overhead. While\nPEFT methods offer substantial advantages, they do not fully address the\npervasive issue of bias propagation from pre-training data. In this work, we\nintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method\ndesigned to counteract bias inheritance. BA-LoRA incorporates three distinct\nregularization terms: (1) consistency regularizer, (2) diversity regularizer,\nand (3) singular vector decomposition regularizer. These regularizers\ncollectively aim to improve the generative models' consistency, diversity, and\ngeneralization capabilities during the fine-tuning process. Through extensive\nexperiments on a variety of natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,\nMistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of\nLoRA and its state-of-the-art variants. Moreover, our method effectively\nmitigates the deleterious effects of pre-training bias, leading to more\nreliable and robust model outputs. The code is available at\nhttps://github.com/cyp-jlu-ai/BA-LoRA.\n","authors":["Yupeng Chang","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04556v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2408.04554v1","updated":"2024-08-08T16:09:40Z","published":"2024-08-08T16:09:40Z","title":"Moly: A Corpus-based Approach to Language Contact in Colonial France","summary":"  Whether or not several Creole languages which developed during the early\nmodern period can be considered genetic descendants of European languages has\nbeen the subject of intense debate. This is in large part due to the absence of\nevidence of intermediate forms. This work introduces a new open corpus, the\nMoly\\'e corpus, which combines stereotypical representations of three kinds of\nlanguage variation in Europe with early attestations of French-based Creole\nlanguages across a period of 400 years. It is intended to facilitate future\nresearch on the continuity between contact situations in Europe and Creolophone\n(former) colonies.\n","authors":["Rasul Dent","Juliette Jans","Thibault Clrice","Pedro Ortiz Suarez","Benot Sagot"],"pdf_url":"https://arxiv.org/pdf/2408.04554v1.pdf","comment":"8 main pages and 3 pages of references"},{"id":"http://arxiv.org/abs/2406.01561v3","updated":"2024-08-08T16:00:01Z","published":"2024-06-03T17:44:11Z","title":"Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation","summary":"  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n","authors":["Mingyuan Zhou","Zhendong Wang","Huangjie Zheng","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.01561v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/mingyuanzhou/SiD-LSG"},{"id":"http://arxiv.org/abs/2311.09805v2","updated":"2024-08-08T15:56:27Z","published":"2023-11-16T11:30:53Z","title":"DocMath-Eval: Evaluating Math Reasoning Capabilities of LLMs in\n  Understanding Long and Specialized Documents","summary":"  Recent LLMs have demonstrated remarkable performance in solving exam-like\nmath word problems. However, the degree to which these numerical reasoning\nskills are effective in real-world scenarios, particularly in expert domains,\nis still largely unexplored. This paper introduces DocMath-Eval, a\ncomprehensive benchmark specifically designed to evaluate the numerical\nreasoning capabilities of LLMs in the context of understanding and analyzing\nspecialized documents containing both text and tables. We evaluate a wide\nspectrum of 48 LLMs with Chain-of-Thought and Program-of-Thought prompting\nmethods, aiming to comprehensively assess the capabilities and limitations of\nexisting LLMs in DocMath-Eval. We found that even the current best-performing\nsystem (i.e., GPT-4o) still significantly lags behind human experts in solving\ncomplex numerical reasoning problems grounded in long contexts. We believe that\nDocMath-Eval can serve as a valuable benchmark for evaluating LLMs'\ncapabilities in solving challenging numerical reasoning problems within expert\ndomains.\n","authors":["Yilun Zhao","Yitao Long","Hongjun Liu","Ryo Kamoi","Linyong Nan","Lyuhao Chen","Yixin Liu","Xiangru Tang","Rui Zhang","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09805v2.pdf","comment":"ACL 2024 Oral. arXiv admin note: text overlap with arXiv:2311.09797"},{"id":"http://arxiv.org/abs/2408.04540v1","updated":"2024-08-08T15:49:01Z","published":"2024-08-08T15:49:01Z","title":"MemeMind at ArAIEval Shared Task: Spotting Persuasive Spans in Arabic\n  Text with Persuasion Techniques Identification","summary":"  This paper focuses on detecting propagandistic spans and persuasion\ntechniques in Arabic text from tweets and news paragraphs. Each entry in the\ndataset contains a text sample and corresponding labels that indicate the start\nand end positions of propaganda techniques within the text. Tokens falling\nwithin a labeled span were assigned \"B\" (Begin) or \"I\" (Inside), \"O\",\ncorresponding to the specific propaganda technique. Using attention masks, we\ncreated uniform lengths for each span and assigned BIO tags to each token based\non the provided labels. Then, we used AraBERT-base pre-trained model for Arabic\ntext tokenization and embeddings with a token classification layer to identify\npropaganda techniques. Our training process involves a two-phase fine-tuning\napproach. First, we train only the classification layer for a few epochs,\nfollowed by full model fine-tuning, updating all parameters. This methodology\nallows the model to adapt to the specific characteristics of the propaganda\ndetection task while leveraging the knowledge captured by the pre-trained\nAraBERT model. Our approach achieved an F1 score of 0.2774, securing the 3rd\nposition in the leaderboard of Task 1.\n","authors":["Md Rafiul Biswas","Zubair Shah","Wajdi Zaghouani"],"pdf_url":"https://arxiv.org/pdf/2408.04540v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.09797v2","updated":"2024-08-08T15:45:11Z","published":"2023-11-16T11:22:08Z","title":"FinanceMath: Knowledge-Intensive Math Reasoning in Finance Domains","summary":"  We introduce FinanceMath, a novel benchmark designed to evaluate LLMs'\ncapabilities in solving knowledge-intensive math reasoning problems. Compared\nto prior works, this study features three core advancements. First, FinanceMath\nincludes 1,200 problems with a hybrid of textual and tabular content. These\nproblems require college-level knowledge in the finance domain for effective\nresolution. Second, we provide expert-annotated, detailed solution references\nin Python program format, ensuring a high-quality benchmark for LLM assessment.\nWe also construct a finance-domain knowledge bank and investigate various\nknowledge integration strategies. Finally, we evaluate a wide spectrum of 44\nLLMs with both Chain-of-Thought and Program-of-Thought prompting methods. Our\nexperimental results reveal that the current best-performing system (i.e.,\nGPT-4o) achieves only 60.9% accuracy using CoT prompting, leaving substantial\nroom for improvement. Moreover, while augmenting LLMs with external knowledge\ncan improve model performance (e.g., from 47.5% to 54.5% for Gemini-1.5-Pro),\ntheir accuracy remains significantly lower than the estimated human expert\nperformance of 92%. We believe that FinanceMath can advance future research in\nthe area of domain-specific knowledge retrieval and integration, particularly\nwithin the context of solving reasoning-intensive tasks.\n","authors":["Yilun Zhao","Hongjun Liu","Yitao Long","Rui Zhang","Chen Zhao","Arman Cohan"],"pdf_url":"https://arxiv.org/pdf/2311.09797v2.pdf","comment":"ACL 2024 Oral"},{"id":"http://arxiv.org/abs/2404.01288v2","updated":"2024-08-08T15:44:43Z","published":"2024-04-01T17:56:30Z","title":"Large Language Models are Capable of Offering Cognitive Reappraisal, if\n  Guided","summary":"  Large language models (LLMs) have offered new opportunities for emotional\nsupport, and recent work has shown that they can produce empathic responses to\npeople in distress. However, long-term mental well-being requires emotional\nself-regulation, where a one-time empathic response falls short. This work\ntakes a first step by engaging with cognitive reappraisals, a strategy from\npsychology practitioners that uses language to targetedly change negative\nappraisals that an individual makes of the situation; such appraisals is known\nto sit at the root of human emotional experience. We hypothesize that\npsychologically grounded principles could enable such advanced psychology\ncapabilities in LLMs, and design RESORT which consists of a series of\nreappraisal constitutions across multiple dimensions that can be used as LLM\ninstructions. We conduct a first-of-its-kind expert evaluation (by clinical\npsychologists with M.S. or Ph.D. degrees) of an LLM's zero-shot ability to\ngenerate cognitive reappraisal responses to medium-length social media messages\nasking for support. This fine-grained evaluation showed that even LLMs at the\n7B scale guided by RESORT are capable of generating empathic responses that can\nhelp users reappraise their situations.\n","authors":["Hongli Zhan","Allen Zheng","Yoon Kyung Lee","Jina Suh","Junyi Jessy Li","Desmond C. Ong"],"pdf_url":"https://arxiv.org/pdf/2404.01288v2.pdf","comment":"Accepted to COLM 2024"},{"id":"http://arxiv.org/abs/2407.21024v2","updated":"2024-08-08T15:32:43Z","published":"2024-07-13T14:23:57Z","title":"An Autonomous GIS Agent Framework for Geospatial Data Retrieval","summary":"  Powered by the emerging large language models (LLMs), autonomous geographic\ninformation systems (GIS) agents have the potential to accomplish spatial\nanalyses and cartographic tasks. However, a research gap exists to support\nfully autonomous GIS agents: how to enable agents to discover and download the\nnecessary data for geospatial analyses. This study proposes an autonomous GIS\nagent framework capable of retrieving required geospatial data by generating,\nexecuting, and debugging programs. The framework utilizes the LLM as the\ndecision-maker, selects the appropriate data source (s) from a pre-defined\nsource list, and fetches the data from the chosen source. Each data source has\na handbook that records the metadata and technical details for data retrieval.\nThe proposed framework is designed in a plug-and-play style to ensure\nflexibility and extensibility. Human users or autonomous data scrawlers can add\nnew data sources by adding new handbooks. We developed a prototype agent based\non the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a\nPython program. Experiment results demonstrate its capability of retrieving\ndata from various sources including OpenStreetMap, administrative boundaries\nand demographic data from the US Census Bureau, satellite basemaps from ESRI\nWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,\nweather data from a commercial provider, the COVID-19 cases from the NYTimes\nGitHub. Our study is among the first attempts to develop an autonomous\ngeospatial data retrieval agent.\n","authors":["Huan Ning","Zhenlong Li","Temitope Akinboyewa","M. Naser Lessani"],"pdf_url":"https://arxiv.org/pdf/2407.21024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04522v1","updated":"2024-08-08T15:24:03Z","published":"2024-08-08T15:24:03Z","title":"Compromesso! Italian Many-Shot Jailbreaks Undermine the Safety of Large\n  Language Models","summary":"  As diverse linguistic communities and users adopt large language models\n(LLMs), assessing their safety across languages becomes critical. Despite\nongoing efforts to make LLMs safe, they can still be made to behave unsafely\nwith jailbreaking, a technique in which models are prompted to act outside\ntheir operational guidelines. Research on LLM safety and jailbreaking, however,\nhas so far mostly focused on English, limiting our understanding of LLM safety\nin other languages. We contribute towards closing this gap by investigating the\neffectiveness of many-shot jailbreaking, where models are prompted with unsafe\ndemonstrations to induce unsafe behaviour, in Italian. To enable our analysis,\nwe create a new dataset of unsafe Italian question-answer pairs. With this\ndataset, we identify clear safety vulnerabilities in four families of\nopen-weight LLMs. We find that the models exhibit unsafe behaviors even when\nprompted with few unsafe demonstrations, and -- more alarmingly -- that this\ntendency rapidly escalates with more demonstrations.\n","authors":["Fabio Pernisi","Dirk Hovy","Paul Rttger"],"pdf_url":"https://arxiv.org/pdf/2408.04522v1.pdf","comment":"Accepted at ACL 2024 (Student Research Workshop)"},{"id":"http://arxiv.org/abs/2408.04519v1","updated":"2024-08-08T15:20:39Z","published":"2024-08-08T15:20:39Z","title":"Articulatory Configurations across Genders and Periods in French Radio\n  and TV archives","summary":"  This paper studies changes in articulatory configurations across genders and\nperiods using an inversion from acoustic to articulatory parameters. From a\ndiachronic corpus based on French media archives spanning 60 years from 1955 to\n2015, automatic transcription and forced alignment allowed extracting the\ncentral frame of each vowel. More than one million frames were obtained from\nover a thousand speakers across gender and age categories. Their formants were\nused from these vocalic frames to fit the parameters of Maeda's articulatory\nmodel. Evaluations of the quality of these processes are provided. We focus\nhere on two parameters of Maeda's model linked to total vocal tract length: the\nrelative position of the larynx (higher for females) and the lips protrusion\n(more protruded for males). Implications for voice quality across genders are\ndiscussed. The effect across periods seems gender independent; thus, the\nassertion that females lowered their pitch with time is not supported.\n","authors":["Benjamin Elie","David Doukhan","Rmi Uro","Lucas Ondel-Yang","Albert Rilliard","Simon Devauchelle"],"pdf_url":"https://arxiv.org/pdf/2408.04519v1.pdf","comment":"accepted to InterSpeech 2024, Kos Island, Greece keywords : acoustic\n  to articulatory inversion, diachrony, gender, French, media"},{"id":"http://arxiv.org/abs/2408.04472v1","updated":"2024-08-08T14:02:45Z","published":"2024-08-08T14:02:45Z","title":"Can LLMs Beat Humans in Debating? A Dynamic Multi-agent Framework for\n  Competitive Debate","summary":"  Competitive debate is a comprehensive and complex computational argumentation\ntask. Large Language Models (LLMs) encounter hallucinations and lack\ncompetitiveness in this task. To address these challenges, we introduce Agent\nfor Debate (Agent4Debate), a dynamic, multi-agent framework based on LLMs\ndesigned to enhance their capabilities in competitive debate. Drawing\ninspiration from human behavior in debate preparation and execution,\nAgent4Debate employs a collaborative architecture where four specialized agents\n(Searcher, Analyzer, Writer, and Reviewer) dynamically interact and cooperate.\nThese agents work throughout the debate process, covering multiple stages from\ninitial research and argument formulation to rebuttal and summary. To\ncomprehensively evaluate framework performance, we construct the Chinese Debate\nArena, comprising 66 carefully selected Chinese debate motions. We recruite ten\nexperienced human debaters and collect records of 200 debates involving\nAgent4Debate, baseline models, and humans. The evaluation employs the Debatrix\nautomatic scoring system and professional human reviewers based on the\nestablished Debatrix-Elo and Human-Elo ranking. Experimental results indicate\nthat the state-of-the-art Agent4Debate exhibits capabilities comparable to\nthose of humans. Furthermore, ablation studies demonstrate the effectiveness of\neach component in the agent structure.\n","authors":["Yiqun Zhang","Xiaocui Yang","Shi Feng","Daling Wang","Yifei Zhang","Kaisong Song"],"pdf_url":"https://arxiv.org/pdf/2408.04472v1.pdf","comment":"9 pages, 3 figures"},{"id":"http://arxiv.org/abs/2408.04463v1","updated":"2024-08-08T13:45:23Z","published":"2024-08-08T13:45:23Z","title":"Crowd Intelligence for Early Misinformation Prediction on Social Media","summary":"  Misinformation spreads rapidly on social media, causing serious damage by\ninfluencing public opinion, promoting dangerous behavior, or eroding trust in\nreliable sources. It spreads too fast for traditional fact-checking, stressing\nthe need for predictive methods. We introduce CROWDSHIELD, a crowd\nintelligence-based method for early misinformation prediction. We hypothesize\nthat the crowd's reactions to misinformation reveal its accuracy. Furthermore,\nwe hinge upon exaggerated assertions/claims and replies with particular\npositions/stances on the source post within a conversation thread. We employ\nQ-learning to capture the two dimensions -- stances and claims. We utilize deep\nQ-learning due to its proficiency in navigating complex decision spaces and\neffectively learning network properties. Additionally, we use a\ntransformer-based encoder to develop a comprehensive understanding of both\ncontent and context. This multifaceted approach helps ensure the model pays\nattention to user interaction and stays anchored in the communication's\ncontent. We propose MIST, a manually annotated misinformation detection Twitter\ncorpus comprising nearly 200 conversation threads with more than 14K replies.\nIn experiments, CROWDSHIELD outperformed ten baseline systems, achieving an\nimprovement of ~4% macro-F1 score. We conduct an ablation study and error\nanalysis to validate our proposed model's performance. The source code and\ndataset are available at https://github.com/LCS2-IIITD/CrowdShield.git.\n","authors":["Megha Sundriyal","Harshit Choudhary","Tanmoy Chakraborty","Md Shad Akhtar"],"pdf_url":"https://arxiv.org/pdf/2408.04463v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2403.13000v2","updated":"2024-08-08T13:33:12Z","published":"2024-03-12T16:25:38Z","title":"Duwak: Dual Watermarks in Large Language Models","summary":"  As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.\n","authors":["Chaoyi Zhu","Jeroen Galjaard","Pin-Yu Chen","Lydia Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.05530v4","updated":"2024-08-08T13:25:56Z","published":"2024-03-08T18:54:20Z","title":"Gemini 1.5: Unlocking multimodal understanding across millions of tokens\n  of context","summary":"  In this report, we introduce the Gemini 1.5 family of models, representing\nthe next generation of highly compute-efficient multimodal models capable of\nrecalling and reasoning over fine-grained information from millions of tokens\nof context, including multiple long documents and hours of video and audio. The\nfamily includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds\nthe February version on the great majority of capabilities and benchmarks; (2)\nGemini 1.5 Flash, a more lightweight variant designed for efficiency with\nminimal regression in quality. Gemini 1.5 models achieve near-perfect recall on\nlong-context retrieval tasks across modalities, improve the state-of-the-art in\nlong-document QA, long-video QA and long-context ASR, and match or surpass\nGemini 1.0 Ultra's state-of-the-art performance across a broad set of\nbenchmarks. Studying the limits of Gemini 1.5's long-context ability, we find\ncontinued improvement in next-token prediction and near-perfect retrieval\n(>99%) up to at least 10M tokens, a generational leap over existing models such\nas Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world\nuse cases, such as Gemini 1.5 collaborating with professionals on completing\ntheir tasks achieving 26 to 75% time savings across 10 different job\ncategories, as well as surprising new capabilities of large language models at\nthe frontier; when given a grammar manual for Kalamang, a language with fewer\nthan 200 speakers worldwide, the model learns to translate English to Kalamang\nat a similar level to a person who learned from the same content.\n","authors":[" Gemini Team","Petko Georgiev","Ving Ian Lei","Ryan Burnell","Libin Bai","Anmol Gulati","Garrett Tanzer","Damien Vincent","Zhufeng Pan","Shibo Wang","Soroosh Mariooryad","Yifan Ding","Xinyang Geng","Fred Alcober","Roy Frostig","Mark Omernick","Lexi Walker","Cosmin Paduraru","Christina Sorokin","Andrea Tacchetti","Colin Gaffney","Samira Daruki","Olcan Sercinoglu","Zach Gleicher","Juliette Love","Paul Voigtlaender","Rohan Jain","Gabriela Surita","Kareem Mohamed","Rory Blevins","Junwhan Ahn","Tao Zhu","Kornraphop Kawintiranon","Orhan Firat","Yiming Gu","Yujing Zhang","Matthew Rahtz","Manaal Faruqui","Natalie Clay","Justin Gilmer","JD Co-Reyes","Ivo Penchev","Rui Zhu","Nobuyuki Morioka","Kevin Hui","Krishna Haridasan","Victor Campos","Mahdis Mahdieh","Mandy Guo","Samer Hassan","Kevin Kilgour","Arpi Vezer","Heng-Tze Cheng","Raoul de Liedekerke","Siddharth Goyal","Paul Barham","DJ Strouse","Seb Noury","Jonas Adler","Mukund Sundararajan","Sharad Vikram","Dmitry Lepikhin","Michela Paganini","Xavier Garcia","Fan Yang","Dasha Valter","Maja Trebacz","Kiran Vodrahalli","Chulayuth Asawaroengchai","Roman Ring","Norbert Kalb","Livio Baldini Soares","Siddhartha Brahma","David Steiner","Tianhe Yu","Fabian Mentzer","Antoine He","Lucas Gonzalez","Bibo Xu","Raphael Lopez Kaufman","Laurent El Shafey","Junhyuk Oh","Tom Hennigan","George van den Driessche","Seth Odoom","Mario Lucic","Becca Roelofs","Sid Lall","Amit Marathe","Betty Chan","Santiago Ontanon","Luheng He","Denis Teplyashin","Jonathan Lai","Phil Crone","Bogdan Damoc","Lewis Ho","Sebastian Riedel","Karel Lenc","Chih-Kuan Yeh","Aakanksha Chowdhery","Yang Xu","Mehran Kazemi","Ehsan Amid","Anastasia Petrushkina","Kevin Swersky","Ali Khodaei","Gowoon Chen","Chris Larkin","Mario Pinto","Geng Yan","Adria Puigdomenech Badia","Piyush Patil","Steven Hansen","Dave Orr","Sebastien M. R. Arnold","Jordan Grimstad","Andrew Dai","Sholto Douglas","Rishika Sinha","Vikas Yadav","Xi Chen","Elena Gribovskaya","Jacob Austin","Jeffrey Zhao","Kaushal Patel","Paul Komarek","Sophia Austin","Sebastian Borgeaud","Linda Friso","Abhimanyu Goyal","Ben Caine","Kris Cao","Da-Woon Chung","Matthew Lamm","Gabe Barth-Maron","Thais Kagohara","Kate Olszewska","Mia Chen","Kaushik Shivakumar","Rishabh Agarwal","Harshal Godhia","Ravi Rajwar","Javier Snaider","Xerxes Dotiwalla","Yuan Liu","Aditya Barua","Victor Ungureanu","Yuan Zhang","Bat-Orgil Batsaikhan","Mateo Wirth","James Qin","Ivo Danihelka","Tulsee Doshi","Martin Chadwick","Jilin Chen","Sanil Jain","Quoc Le","Arjun Kar","Madhu Gurumurthy","Cheng Li","Ruoxin Sang","Fangyu Liu","Lampros Lamprou","Rich Munoz","Nathan Lintz","Harsh Mehta","Heidi Howard","Malcolm Reynolds","Lora Aroyo","Quan Wang","Lorenzo Blanco","Albin Cassirer","Jordan Griffith","Dipanjan Das","Stephan Lee","Jakub Sygnowski","Zach Fisher","James Besley","Richard Powell","Zafarali Ahmed","Dominik Paulus","David Reitter","Zalan Borsos","Rishabh Joshi","Aedan Pope","Steven Hand","Vittorio Selo","Vihan Jain","Nikhil Sethi","Megha Goel","Takaki Makino","Rhys May","Zhen Yang","Johan Schalkwyk","Christina Butterfield","Anja Hauth","Alex Goldin","Will Hawkins","Evan Senter","Sergey Brin","Oliver Woodman","Marvin Ritter","Eric Noland","Minh Giang","Vijay Bolina","Lisa Lee","Tim Blyth","Ian Mackinnon","Machel Reid","Obaid Sarvana","David Silver","Alexander Chen","Lily Wang","Loren Maggiore","Oscar Chang","Nithya Attaluri","Gregory Thornton","Chung-Cheng Chiu","Oskar Bunyan","Nir Levine","Timothy Chung","Evgenii Eltyshev","Xiance Si","Timothy Lillicrap","Demetra Brady","Vaibhav Aggarwal","Boxi Wu","Yuanzhong Xu","Ross McIlroy","Kartikeya Badola","Paramjit Sandhu","Erica Moreira","Wojciech Stokowiec","Ross Hemsley","Dong Li","Alex Tudor","Pranav Shyam","Elahe Rahimtoroghi","Salem Haykal","Pablo Sprechmann","Xiang Zhou","Diana Mincu","Yujia Li","Ravi Addanki","Kalpesh Krishna","Xiao Wu","Alexandre Frechette","Matan Eyal","Allan Dafoe","Dave Lacey","Jay Whang","Thi Avrahami","Ye Zhang","Emanuel Taropa","Hanzhao Lin","Daniel Toyama","Eliza Rutherford","Motoki Sano","HyunJeong Choe","Alex Tomala","Chalence Safranek-Shrader","Nora Kassner","Mantas Pajarskas","Matt Harvey","Sean Sechrist","Meire Fortunato","Christina Lyu","Gamaleldin Elsayed","Chenkai Kuang","James Lottes","Eric Chu","Chao Jia","Chih-Wei Chen","Peter Humphreys","Kate Baumli","Connie Tao","Rajkumar Samuel","Cicero Nogueira dos Santos","Anders Andreassen","Nemanja Rakievi","Dominik Grewe","Aviral Kumar","Stephanie Winkler","Jonathan Caton","Andrew Brock","Sid Dalmia","Hannah Sheahan","Iain Barr","Yingjie Miao","Paul Natsev","Jacob Devlin","Feryal Behbahani","Flavien Prost","Yanhua Sun","Artiom Myaskovsky","Thanumalayan Sankaranarayana Pillai","Dan Hurt","Angeliki Lazaridou","Xi Xiong","Ce Zheng","Fabio Pardo","Xiaowei Li","Dan Horgan","Joe Stanton","Moran Ambar","Fei Xia","Alejandro Lince","Mingqiu Wang","Basil Mustafa","Albert Webson","Hyo Lee","Rohan Anil","Martin Wicke","Timothy Dozat","Abhishek Sinha","Enrique Piqueras","Elahe Dabir","Shyam Upadhyay","Anudhyan Boral","Lisa Anne Hendricks","Corey Fry","Josip Djolonga","Yi Su","Jake Walker","Jane Labanowski","Ronny Huang","Vedant Misra","Jeremy Chen","RJ Skerry-Ryan","Avi Singh","Shruti Rijhwani","Dian Yu","Alex Castro-Ros","Beer Changpinyo","Romina Datta","Sumit Bagri","Arnar Mar Hrafnkelsson","Marcello Maggioni","Daniel Zheng","Yury Sulsky","Shaobo Hou","Tom Le Paine","Antoine Yang","Jason Riesa","Dominika Rogozinska","Dror Marcus","Dalia El Badawy","Qiao Zhang","Luyu Wang","Helen Miller","Jeremy Greer","Lars Lowe Sjos","Azade Nova","Heiga Zen","Rahma Chaabouni","Mihaela Rosca","Jiepu Jiang","Charlie Chen","Ruibo Liu","Tara Sainath","Maxim Krikun","Alex Polozov","Jean-Baptiste Lespiau","Josh Newlan","Zeyncep Cankara","Soo Kwak","Yunhan Xu","Phil Chen","Andy Coenen","Clemens Meyer","Katerina Tsihlas","Ada Ma","Juraj Gottweis","Jinwei Xing","Chenjie Gu","Jin Miao","Christian Frank","Zeynep Cankara","Sanjay Ganapathy","Ishita Dasgupta","Steph Hughes-Fitt","Heng Chen","David Reid","Keran Rong","Hongmin Fan","Joost van Amersfoort","Vincent Zhuang","Aaron Cohen","Shixiang Shane Gu","Anhad Mohananey","Anastasija Ilic","Taylor Tobin","John Wieting","Anna Bortsova","Phoebe Thacker","Emma Wang","Emily Caveness","Justin Chiu","Eren Sezener","Alex Kaskasoli","Steven Baker","Katie Millican","Mohamed Elhawaty","Kostas Aisopos","Carl Lebsack","Nathan Byrd","Hanjun Dai","Wenhao Jia","Matthew Wiethoff","Elnaz Davoodi","Albert Weston","Lakshman Yagati","Arun Ahuja","Isabel Gao","Golan Pundak","Susan Zhang","Michael Azzam","Khe Chai Sim","Sergi Caelles","James Keeling","Abhanshu Sharma","Andy Swing","YaGuang Li","Chenxi Liu","Carrie Grimes Bostock","Yamini Bansal","Zachary Nado","Ankesh Anand","Josh Lipschultz","Abhijit Karmarkar","Lev Proleev","Abe Ittycheriah","Soheil Hassas Yeganeh","George Polovets","Aleksandra Faust","Jiao Sun","Alban Rrustemi","Pen Li","Rakesh Shivanna","Jeremiah Liu","Chris Welty","Federico Lebron","Anirudh Baddepudi","Sebastian Krause","Emilio Parisotto","Radu Soricut","Zheng Xu","Dawn Bloxwich","Melvin Johnson","Behnam Neyshabur","Justin Mao-Jones","Renshen Wang","Vinay Ramasesh","Zaheer Abbas","Arthur Guez","Constant Segal","Duc Dung Nguyen","James Svensson","Le Hou","Sarah York","Kieran Milan","Sophie Bridgers","Wiktor Gworek","Marco Tagliasacchi","James Lee-Thorp","Michael Chang","Alexey Guseynov","Ale Jakse Hartman","Michael Kwong","Ruizhe Zhao","Sheleem Kashem","Elizabeth Cole","Antoine Miech","Richard Tanburn","Mary Phuong","Filip Pavetic","Sebastien Cevey","Ramona Comanescu","Richard Ives","Sherry Yang","Cosmo Du","Bo Li","Zizhao Zhang","Mariko Iinuma","Clara Huiyi Hu","Aurko Roy","Shaan Bijwadia","Zhenkai Zhu","Danilo Martins","Rachel Saputro","Anita Gergely","Steven Zheng","Dawei Jia","Ioannis Antonoglou","Adam Sadovsky","Shane Gu","Yingying Bi","Alek Andreev","Sina Samangooei","Mina Khan","Tomas Kocisky","Angelos Filos","Chintu Kumar","Colton Bishop","Adams Yu","Sarah Hodkinson","Sid Mittal","Premal Shah","Alexandre Moufarek","Yong Cheng","Adam Bloniarz","Jaehoon Lee","Pedram Pejman","Paul Michel","Stephen Spencer","Vladimir Feinberg","Xuehan Xiong","Nikolay Savinov","Charlotte Smith","Siamak Shakeri","Dustin Tran","Mary Chesus","Bernd Bohnet","George Tucker","Tamara von Glehn","Carrie Muir","Yiran Mao","Hideto Kazawa","Ambrose Slone","Kedar Soparkar","Disha Shrivastava","James Cobon-Kerr","Michael Sharman","Jay Pavagadhi","Carlos Araya","Karolis Misiunas","Nimesh Ghelani","Michael Laskin","David Barker","Qiujia Li","Anton Briukhov","Neil Houlsby","Mia Glaese","Balaji Lakshminarayanan","Nathan Schucher","Yunhao Tang","Eli Collins","Hyeontaek Lim","Fangxiaoyu Feng","Adria Recasens","Guangda Lai","Alberto Magni","Nicola De Cao","Aditya Siddhant","Zoe Ashwood","Jordi Orbay","Mostafa Dehghani","Jenny Brennan","Yifan He","Kelvin Xu","Yang Gao","Carl Saroufim","James Molloy","Xinyi Wu","Seb Arnold","Solomon Chang","Julian Schrittwieser","Elena Buchatskaya","Soroush Radpour","Martin Polacek","Skye Giordano","Ankur Bapna","Simon Tokumine","Vincent Hellendoorn","Thibault Sottiaux","Sarah Cogan","Aliaksei Severyn","Mohammad Saleh","Shantanu Thakoor","Laurent Shefey","Siyuan Qiao","Meenu Gaba","Shuo-yiin Chang","Craig Swanson","Biao Zhang","Benjamin Lee","Paul Kishan Rubenstein","Gan Song","Tom Kwiatkowski","Anna Koop","Ajay Kannan","David Kao","Parker Schuh","Axel Stjerngren","Golnaz Ghiasi","Gena Gibson","Luke Vilnis","Ye Yuan","Felipe Tiengo Ferreira","Aishwarya Kamath","Ted Klimenko","Ken Franko","Kefan Xiao","Indro Bhattacharya","Miteyan Patel","Rui Wang","Alex Morris","Robin Strudel","Vivek Sharma","Peter Choy","Sayed Hadi Hashemi","Jessica Landon","Mara Finkelstein","Priya Jhakra","Justin Frye","Megan Barnes","Matthew Mauger","Dennis Daun","Khuslen Baatarsukh","Matthew Tung","Wael Farhan","Henryk Michalewski","Fabio Viola","Felix de Chaumont Quitry","Charline Le Lan","Tom Hudson","Qingze Wang","Felix Fischer","Ivy Zheng","Elspeth White","Anca Dragan","Jean-baptiste Alayrac","Eric Ni","Alexander Pritzel","Adam Iwanicki","Michael Isard","Anna Bulanova","Lukas Zilka","Ethan Dyer","Devendra Sachan","Srivatsan Srinivasan","Hannah Muckenhirn","Honglong Cai","Amol Mandhane","Mukarram Tariq","Jack W. Rae","Gary Wang","Kareem Ayoub","Nicholas FitzGerald","Yao Zhao","Woohyun Han","Chris Alberti","Dan Garrette","Kashyap Krishnakumar","Mai Gimenez","Anselm Levskaya","Daniel Sohn","Josip Matak","Inaki Iturrate","Michael B. Chang","Jackie Xiang","Yuan Cao","Nishant Ranka","Geoff Brown","Adrian Hutter","Vahab Mirrokni","Nanxin Chen","Kaisheng Yao","Zoltan Egyed","Francois Galilee","Tyler Liechty","Praveen Kallakuri","Evan Palmer","Sanjay Ghemawat","Jasmine Liu","David Tao","Chloe Thornton","Tim Green","Mimi Jasarevic","Sharon Lin","Victor Cotruta","Yi-Xuan Tan","Noah Fiedel","Hongkun Yu","Ed Chi","Alexander Neitz","Jens Heitkaemper","Anu Sinha","Denny Zhou","Yi Sun","Charbel Kaed","Brice Hulse","Swaroop Mishra","Maria Georgaki","Sneha Kudugunta","Clement Farabet","Izhak Shafran","Daniel Vlasic","Anton Tsitsulin","Rajagopal Ananthanarayanan","Alen Carin","Guolong Su","Pei Sun","Shashank V","Gabriel Carvajal","Josef Broder","Iulia Comsa","Alena Repina","William Wong","Warren Weilun Chen","Peter Hawkins","Egor Filonov","Lucia Loher","Christoph Hirnschall","Weiyi Wang","Jingchen Ye","Andrea Burns","Hardie Cate","Diana Gage Wright","Federico Piccinini","Lei Zhang","Chu-Cheng Lin","Ionel Gog","Yana Kulizhskaya","Ashwin Sreevatsa","Shuang Song","Luis C. Cobo","Anand Iyer","Chetan Tekur","Guillermo Garrido","Zhuyun Xiao","Rupert Kemp","Huaixiu Steven Zheng","Hui Li","Ananth Agarwal","Christel Ngani","Kati Goshvadi","Rebeca Santamaria-Fernandez","Wojciech Fica","Xinyun Chen","Chris Gorgolewski","Sean Sun","Roopal Garg","Xinyu Ye","S. M. Ali Eslami","Nan Hua","Jon Simon","Pratik Joshi","Yelin Kim","Ian Tenney","Sahitya Potluri","Lam Nguyen Thiet","Quan Yuan","Florian Luisier","Alexandra Chronopoulou","Salvatore Scellato","Praveen Srinivasan","Minmin Chen","Vinod Koverkathu","Valentin Dalibard","Yaming Xu","Brennan Saeta","Keith Anderson","Thibault Sellam","Nick Fernando","Fantine Huot","Junehyuk Jung","Mani Varadarajan","Michael Quinn","Amit Raul","Maigo Le","Ruslan Habalov","Jon Clark","Komal Jalan","Kalesha Bullard","Achintya Singhal","Thang Luong","Boyu Wang","Sujeevan Rajayogam","Julian Eisenschlos","Johnson Jia","Daniel Finchelstein","Alex Yakubovich","Daniel Balle","Michael Fink","Sameer Agarwal","Jing Li","Dj Dvijotham","Shalini Pal","Kai Kang","Jaclyn Konzelmann","Jennifer Beattie","Olivier Dousse","Diane Wu","Remi Crocker","Chen Elkind","Siddhartha Reddy Jonnalagadda","Jong Lee","Dan Holtmann-Rice","Krystal Kallarackal","Rosanne Liu","Denis Vnukov","Neera Vats","Luca Invernizzi","Mohsen Jafari","Huanjie Zhou","Lilly Taylor","Jennifer Prendki","Marcus Wu","Tom Eccles","Tianqi Liu","Kavya Kopparapu","Francoise Beaufays","Christof Angermueller","Andreea Marzoca","Shourya Sarcar","Hilal Dib","Jeff Stanway","Frank Perbet","Nejc Trdin","Rachel Sterneck","Andrey Khorlin","Dinghua Li","Xihui Wu","Sonam Goenka","David Madras","Sasha Goldshtein","Willi Gierke","Tong Zhou","Yaxin Liu","Yannie Liang","Anais White","Yunjie Li","Shreya Singh","Sanaz Bahargam","Mark Epstein","Sujoy Basu","Li Lao","Adnan Ozturel","Carl Crous","Alex Zhai","Han Lu","Zora Tung","Neeraj Gaur","Alanna Walton","Lucas Dixon","Ming Zhang","Amir Globerson","Grant Uy","Andrew Bolt","Olivia Wiles","Milad Nasr","Ilia Shumailov","Marco Selvi","Francesco Piccinno","Ricardo Aguilar","Sara McCarthy","Misha Khalman","Mrinal Shukla","Vlado Galic","John Carpenter","Kevin Villela","Haibin Zhang","Harry Richardson","James Martens","Matko Bosnjak","Shreyas Rammohan Belle","Jeff Seibert","Mahmoud Alnahlawi","Brian McWilliams","Sankalp Singh","Annie Louis","Wen Ding","Dan Popovici","Lenin Simicich","Laura Knight","Pulkit Mehta","Nishesh Gupta","Chongyang Shi","Saaber Fatehi","Jovana Mitrovic","Alex Grills","Joseph Pagadora","Dessie Petrova","Danielle Eisenbud","Zhishuai Zhang","Damion Yates","Bhavishya Mittal","Nilesh Tripuraneni","Yannis Assael","Thomas Brovelli","Prateek Jain","Mihajlo Velimirovic","Canfer Akbulut","Jiaqi Mu","Wolfgang Macherey","Ravin Kumar","Jun Xu","Haroon Qureshi","Gheorghe Comanici","Jeremy Wiesner","Zhitao Gong","Anton Ruddock","Matthias Bauer","Nick Felt","Anirudh GP","Anurag Arnab","Dustin Zelle","Jonas Rothfuss","Bill Rosgen","Ashish Shenoy","Bryan Seybold","Xinjian Li","Jayaram Mudigonda","Goker Erdogan","Jiawei Xia","Jiri Simsa","Andrea Michi","Yi Yao","Christopher Yew","Steven Kan","Isaac Caswell","Carey Radebaugh","Andre Elisseeff","Pedro Valenzuela","Kay McKinney","Kim Paterson","Albert Cui","Eri Latorre-Chimoto","Solomon Kim","William Zeng","Ken Durden","Priya Ponnapalli","Tiberiu Sosea","Christopher A. Choquette-Choo","James Manyika","Brona Robenek","Harsha Vashisht","Sebastien Pereira","Hoi Lam","Marko Velic","Denese Owusu-Afriyie","Katherine Lee","Tolga Bolukbasi","Alicia Parrish","Shawn Lu","Jane Park","Balaji Venkatraman","Alice Talbert","Lambert Rosique","Yuchung Cheng","Andrei Sozanschi","Adam Paszke","Praveen Kumar","Jessica Austin","Lu Li","Khalid Salama","Wooyeol Kim","Nandita Dukkipati","Anthony Baryshnikov","Christos Kaplanis","XiangHai Sheng","Yuri Chervonyi","Caglar Unlu","Diego de Las Casas","Harry Askham","Kathryn Tunyasuvunakool","Felix Gimeno","Siim Poder","Chester Kwak","Matt Miecnikowski","Vahab Mirrokni","Alek Dimitriev","Aaron Parisi","Dangyi Liu","Tomy Tsai","Toby Shevlane","Christina Kouridi","Drew Garmon","Adrian Goedeckemeyer","Adam R. Brown","Anitha Vijayakumar","Ali Elqursh","Sadegh Jazayeri","Jin Huang","Sara Mc Carthy","Jay Hoover","Lucy Kim","Sandeep Kumar","Wei Chen","Courtney Biles","Garrett Bingham","Evan Rosen","Lisa Wang","Qijun Tan","David Engel","Francesco Pongetti","Dario de Cesare","Dongseong Hwang","Lily Yu","Jennifer Pullman","Srini Narayanan","Kyle Levin","Siddharth Gopal","Megan Li","Asaf Aharoni","Trieu Trinh","Jessica Lo","Norman Casagrande","Roopali Vij","Loic Matthey","Bramandia Ramadhana","Austin Matthews","CJ Carey","Matthew Johnson","Kremena Goranova","Rohin Shah","Shereen Ashraf","Kingshuk Dasgupta","Rasmus Larsen","Yicheng Wang","Manish Reddy Vuyyuru","Chong Jiang","Joana Ijazi","Kazuki Osawa","Celine Smith","Ramya Sree Boppana","Taylan Bilal","Yuma Koizumi","Ying Xu","Yasemin Altun","Nir Shabat","Ben Bariach","Alex Korchemniy","Kiam Choo","Olaf Ronneberger","Chimezie Iwuanyanwu","Shubin Zhao","David Soergel","Cho-Jui Hsieh","Irene Cai","Shariq Iqbal","Martin Sundermeyer","Zhe Chen","Elie Bursztein","Chaitanya Malaviya","Fadi Biadsy","Prakash Shroff","Inderjit Dhillon","Tejasi Latkar","Chris Dyer","Hannah Forbes","Massimo Nicosia","Vitaly Nikolaev","Somer Greene","Marin Georgiev","Pidong Wang","Nina Martin","Hanie Sedghi","John Zhang","Praseem Banzal","Doug Fritz","Vikram Rao","Xuezhi Wang","Jiageng Zhang","Viorica Patraucean","Dayou Du","Igor Mordatch","Ivan Jurin","Lewis Liu","Ayush Dubey","Abhi Mohan","Janek Nowakowski","Vlad-Doru Ion","Nan Wei","Reiko Tojo","Maria Abi Raad","Drew A. Hudson","Vaishakh Keshava","Shubham Agrawal","Kevin Ramirez","Zhichun Wu","Hoang Nguyen","Ji Liu","Madhavi Sewak","Bryce Petrini","DongHyun Choi","Ivan Philips","Ziyue Wang","Ioana Bica","Ankush Garg","Jarek Wilkiewicz","Priyanka Agrawal","Xiaowei Li","Danhao Guo","Emily Xue","Naseer Shaik","Andrew Leach","Sadh MNM Khan","Julia Wiesinger","Sammy Jerome","Abhishek Chakladar","Alek Wenjiao Wang","Tina Ornduff","Folake Abu","Alireza Ghaffarkhah","Marcus Wainwright","Mario Cortes","Frederick Liu","Joshua Maynez","Andreas Terzis","Pouya Samangouei","Riham Mansour","Tomasz Kpa","Franois-Xavier Aubet","Anton Algymr","Dan Banica","Agoston Weisz","Andras Orban","Alexandre Senges","Ewa Andrejczuk","Mark Geller","Niccolo Dal Santo","Valentin Anklin","Majd Al Merey","Martin Baeuml","Trevor Strohman","Junwen Bai","Slav Petrov","Yonghui Wu","Demis Hassabis","Koray Kavukcuoglu","Jeffrey Dean","Oriol Vinyals"],"pdf_url":"https://arxiv.org/pdf/2403.05530v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.02699v2","updated":"2024-08-08T13:10:50Z","published":"2024-04-03T12:57:19Z","title":"Scalable Model Editing via Customized Expert Networks","summary":"  Addressing the issues of hallucinations and outdated knowledge in large\nlanguage models is critical for their reliable application. Model Editing\npresents a promising avenue for mitigating these challenges in a cost-effective\nmanner. However, existing methods often suffer from unsatisfactory\ngeneralization and unintended effects on non-edited samples. To overcome these\nlimitations, we introduce a novel approach: Scalable Model Editing via\nCustomized Expert Networks (SCEN), which is a two-stage continuous training\nparadigm. Specifically, in the first stage, we train lightweight expert\nnetworks individually for each piece of knowledge that needs to be updated.\nSubsequently, we train a corresponding indexing neuron for each expert to\ncontrol the activation state of that expert. We conducted a series of\nexperiments on the ZsRE and Hallucination benchmarks by tuning the advanced\nopen-source LLM, Llama2, achieving state-of-the-art results compared to current\nmainstream methods. Our code is available at\nhttps://github.com/TAL-auroraX/SCEN.\n","authors":["Zihan Yao","Yu He","Tianyu Qi","Ming Li"],"pdf_url":"https://arxiv.org/pdf/2404.02699v2.pdf","comment":"Accepted by COLM2024"},{"id":"http://arxiv.org/abs/2406.08223v2","updated":"2024-08-08T13:07:21Z","published":"2024-06-12T13:52:38Z","title":"Research Trends for the Interplay between Large Language Models and\n  Knowledge Graphs","summary":"  This survey investigates the synergistic relationship between Large Language\nModels (LLMs) and Knowledge Graphs (KGs), which is crucial for advancing AI's\ncapabilities in understanding, reasoning, and language processing. It aims to\naddress gaps in current research by exploring areas such as KG Question\nAnswering, ontology generation, KG validation, and the enhancement of KG\naccuracy and consistency through LLMs. The paper further examines the roles of\nLLMs in generating descriptive texts and natural language queries for KGs.\nThrough a structured analysis that includes categorizing LLM-KG interactions,\nexamining methodologies, and investigating collaborative uses and potential\nbiases, this study seeks to provide new insights into the combined potential of\nLLMs and KGs. It highlights the importance of their interaction for improving\nAI applications and outlines future research directions.\n","authors":["Hanieh Khorashadizadeh","Fatima Zahra Amara","Morteza Ezzabady","Frdric Ieng","Sanju Tiwari","Nandana Mihindukulasooriya","Jinghua Groppe","Soror Sahri","Farah Benamara","Sven Groppe"],"pdf_url":"https://arxiv.org/pdf/2406.08223v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04427v1","updated":"2024-08-08T12:53:26Z","published":"2024-08-08T12:53:26Z","title":"AcrosticSleuth: Probabilistic Identification and Ranking of Acrostics in\n  Multilingual Corpora","summary":"  For centuries, writers have hidden messages in their texts as acrostics,\nwhere initial letters of consecutive lines or paragraphs form meaningful words\nor phrases. Scholars searching for acrostics manually can only focus on a few\nauthors at a time and often favor qualitative arguments in discussing\nintentionally. We aim to put the study of acrostics on firmer statistical\nfooting by presenting AcrosticSleuth, a first-of-its-kind tool that\nautomatically identifies acrostics and ranks them by the probability that the\nsequence of characters does not occur by chance (and therefore may have been\ninserted intentionally). Acrostics are rare, so we formalize the problem as a\nbinary classification task in the presence of extreme class imbalance. To\nevaluate AcrosticSleuth, we present the Acrostic Identification Dataset\n(AcrostID), a collection of acrostics from the WikiSource online database.\nDespite the class imbalance, AcrosticSleuth achieves F1 scores of 0.39, 0.59,\nand 0.66 on French, English, and Russian subdomains of WikiSource,\nrespectively. We further demonstrate that AcrosticSleuth can identify\npreviously unknown high-profile instances of wordplay, such as the acrostic\nspelling ARSPOETICA (``art of poetry\") by Italian Humanist Albertino Mussato\nand English philosopher Thomas Hobbes' signature in the opening paragraphs of\nThe Elements of Law.\n","authors":["Aleksandr Fedchin","Isabel Cooperman","Pramit Chaudhuri","Joseph P. Dexter"],"pdf_url":"https://arxiv.org/pdf/2408.04427v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04420v1","updated":"2024-08-08T12:47:10Z","published":"2024-08-08T12:47:10Z","title":"Recognizing Emotion Regulation Strategies from Human Behavior with Large\n  Language Models","summary":"  Human emotions are often not expressed directly, but regulated according to\ninternal processes and social display rules. For affective computing systems,\nan understanding of how users regulate their emotions can be highly useful, for\nexample to provide feedback in job interview training, or in psychotherapeutic\nscenarios. However, at present no method to automatically classify different\nemotion regulation strategies in a cross-user scenario exists. At the same\ntime, recent studies showed that instruction-tuned Large Language Models (LLMs)\ncan reach impressive performance across a variety of affect recognition tasks\nsuch as categorical emotion recognition or sentiment analysis. While these\nresults are promising, it remains unclear to what extent the representational\npower of LLMs can be utilized in the more subtle task of classifying users'\ninternal emotion regulation strategy. To close this gap, we make use of the\nrecently introduced \\textsc{Deep} corpus for modeling the social display of the\nemotion shame, where each point in time is annotated with one of seven\ndifferent emotion regulation classes. We fine-tune Llama2-7B as well as the\nrecently introduced Gemma model using Low-rank Optimization on prompts\ngenerated from different sources of information on the \\textsc{Deep} corpus.\nThese include verbal and nonverbal behavior, person factors, as well as the\nresults of an in-depth interview after the interaction. Our results show, that\na fine-tuned Llama2-7B LLM is able to classify the utilized emotion regulation\nstrategy with high accuracy (0.84) without needing access to data from\npost-interaction interviews. This represents a significant improvement over\nprevious approaches based on Bayesian Networks and highlights the importance of\nmodeling verbal behavior in emotion regulation.\n","authors":["Philipp Mller","Alexander Heimerl","Sayed Muddashir Hossain","Lea Siegel","Jan Alexandersson","Patrick Gebhard","Elisabeth Andr","Tanja Schneeberger"],"pdf_url":"https://arxiv.org/pdf/2408.04420v1.pdf","comment":"Accepted to ACII'24"},{"id":"http://arxiv.org/abs/2408.04414v1","updated":"2024-08-08T12:42:43Z","published":"2024-08-08T12:42:43Z","title":"Enhancing Robustness of Retrieval-Augmented Language Models with\n  In-Context Learning","summary":"  Retrieval-Augmented Language Models (RALMs) have significantly improved\nperformance in open-domain question answering (QA) by leveraging external\nknowledge. However, RALMs still struggle with unanswerable queries, where the\nretrieved contexts do not contain the correct answer, and with conflicting\ninformation, where different sources provide contradictory answers due to\nimperfect retrieval. This study introduces an in-context learning-based\napproach to enhance the reasoning capabilities of RALMs, making them more\nrobust in imperfect retrieval scenarios. Our method incorporates Machine\nReading Comprehension (MRC) demonstrations, referred to as cases, to boost the\nmodel's capabilities to identify unanswerabilities and conflicts among the\nretrieved contexts. Experiments on two open-domain QA datasets show that our\napproach increases accuracy in identifying unanswerable and conflicting\nscenarios without requiring additional fine-tuning. This work demonstrates that\nin-context learning can effectively enhance the robustness of RALMs in\nopen-domain QA tasks.\n","authors":["Seong-Il Park","Seung-Woo Choi","Na-Hyun Kim","Jay-Yoon Lee"],"pdf_url":"https://arxiv.org/pdf/2408.04414v1.pdf","comment":"10 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.03354v2","updated":"2024-08-08T12:31:12Z","published":"2024-08-06T09:15:25Z","title":"The Use of Large Language Models (LLM) for Cyber Threat Intelligence\n  (CTI) in Cybercrime Forums","summary":"  Large language models (LLMs) can be used to analyze cyber threat intelligence\n(CTI) data from cybercrime forums, which contain extensive information and key\ndiscussions about emerging cyber threats. However, to date, the level of\naccuracy and efficiency of LLMs for such critical tasks has yet to be\nthoroughly evaluated. Hence, this study assesses the accuracy of an LLM system\nbuilt on the OpenAI GPT-3.5-turbo model [7] to extract CTI information. To do\nso, a random sample of 500 daily conversations from three cybercrime forums,\nXSS, Exploit_in, and RAMP, was extracted, and the LLM system was instructed to\nsummarize the conversations and code 10 key CTI variables, such as whether a\nlarge organization and/or a critical infrastructure is being targeted. Then,\ntwo coders reviewed each conversation and evaluated whether the information\nextracted by the LLM was accurate. The LLM system performed strikingly well,\nwith an average accuracy score of 98%. Various ways to enhance the model were\nuncovered, such as the need to help the LLM distinguish between stories and\npast events, as well as being careful with verb tenses in prompts.\nNevertheless, the results of this study highlight the efficiency and relevance\nof using LLMs for cyber threat intelligence.\n","authors":["Vanessa Clairoux-Trepanier","Isa-May Beauchamp","Estelle Ruellan","Masarah Paquet-Clouston","Serge-Olivier Paquette","Eric Clay"],"pdf_url":"https://arxiv.org/pdf/2408.03354v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04403v1","updated":"2024-08-08T12:10:50Z","published":"2024-08-08T12:10:50Z","title":"Exploring Reasoning Biases in Large Language Models Through Syllogism:\n  Insights from the NeuBAROCO Dataset","summary":"  This paper explores the question of how accurately current large language\nmodels can perform logical reasoning in natural language, with an emphasis on\nwhether these models exhibit reasoning biases similar to humans. Specifically,\nour study focuses on syllogistic reasoning, a form of deductive reasoning\nextensively studied in cognitive science as a natural form of human reasoning.\nWe present a syllogism dataset called NeuBAROCO, which consists of syllogistic\nreasoning problems in English and Japanese. This dataset was originally\ndesigned for psychological experiments to assess human reasoning capabilities\nusing various forms of syllogisms. Our experiments with leading large language\nmodels indicate that these models exhibit reasoning biases similar to humans,\nalong with other error tendencies. Notably, there is significant room for\nimprovement in reasoning problems where the relationship between premises and\nhypotheses is neither entailment nor contradiction. We also present\nexperimental results and in-depth analysis using a new Chain-of-Thought\nprompting method, which asks LLMs to translate syllogisms into abstract logical\nexpressions and then explain their reasoning process. Our analysis using this\nmethod suggests that the primary limitations of LLMs lie in the reasoning\nprocess itself rather than the interpretation of syllogisms.\n","authors":["Kentaro Ozeki","Risako Ando","Takanobu Morishita","Hirohiko Abe","Koji Mineshima","Mitsuhiro Okada"],"pdf_url":"https://arxiv.org/pdf/2408.04403v1.pdf","comment":"To appear in Findings of the Association for Computational\n  Linguistics: ACL 2024"},{"id":"http://arxiv.org/abs/2408.04394v1","updated":"2024-08-08T11:56:57Z","published":"2024-08-08T11:56:57Z","title":"Automated Educational Question Generation at Different Bloom's Skill\n  Levels using Large Language Models: Strategies and Evaluation","summary":"  Developing questions that are pedagogically sound, relevant, and promote\nlearning is a challenging and time-consuming task for educators. Modern-day\nlarge language models (LLMs) generate high-quality content across multiple\ndomains, potentially helping educators to develop high-quality questions.\nAutomated educational question generation (AEQG) is important in scaling online\neducation catering to a diverse student population. Past attempts at AEQG have\nshown limited abilities to generate questions at higher cognitive levels. In\nthis study, we examine the ability of five state-of-the-art LLMs of different\nsizes to generate diverse and high-quality questions of different cognitive\nlevels, as defined by Bloom's taxonomy. We use advanced prompting techniques\nwith varying complexity for AEQG. We conducted expert and LLM-based evaluations\nto assess the linguistic and pedagogical relevance and quality of the\nquestions. Our findings suggest that LLms can generate relevant and\nhigh-quality educational questions of different cognitive levels when prompted\nwith adequate information, although there is a significant variance in the\nperformance of the five LLms considered. We also show that automated evaluation\nis not on par with human evaluation.\n","authors":["Nicy Scaria","Suma Dharani Chenna","Deepak Subramani"],"pdf_url":"https://arxiv.org/pdf/2408.04394v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04392v1","updated":"2024-08-08T11:51:45Z","published":"2024-08-08T11:51:45Z","title":"Open-domain Implicit Format Control for Large Language Model Generation","summary":"  Controlling the format of outputs generated by large language models (LLMs)\nis a critical functionality in various applications. Current methods typically\nemploy constrained decoding with rule-based automata or fine-tuning with\nmanually crafted format instructions, both of which struggle with open-domain\nformat requirements. To address this limitation, we introduce a novel framework\nfor controlled generation in LLMs, leveraging user-provided, one-shot QA pairs.\nThis study investigates LLMs' capabilities to follow open-domain, one-shot\nconstraints and replicate the format of the example answers. We observe that\nthis is a non-trivial problem for current LLMs. We also develop a dataset\ncollection methodology for supervised fine-tuning that enhances the open-domain\nformat control of LLMs without degrading output quality, as well as a benchmark\non which we evaluate both the helpfulness and format correctness of LLM\noutputs. The resulting datasets, named OIFC-SFT, along with the related code,\nwill be made publicly available at https://github.com/cofe-ai/OIFC.\n","authors":["Yiqun Yao","Wenjia Ma","Xuezhi Fang","Xin Jiang","Xiang Li","Xuying Meng","Peng Han","Jing Li","Aixin Sun","Yequan Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04392v1.pdf","comment":"6 pages"},{"id":"http://arxiv.org/abs/2408.04378v1","updated":"2024-08-08T11:29:43Z","published":"2024-08-08T11:29:43Z","title":"Overview of the NLPCC 2024 Shared Task on Chinese Metaphor Generation","summary":"  This paper presents the results of the shared task on Chinese metaphor\ngeneration, hosted at the 13th CCF Conference on Natural Language Processing\nand Chinese Computing (NLPCC 2024). The goal of this shared task is to generate\nChinese metaphors using machine learning techniques and effectively identifying\nbasic components of metaphorical sentences. It is divided into two subtasks: 1)\nMetaphor Generation, which involves creating a metaphor from a provided tuple\nconsisting of TENOR, GROUND, and VEHICLE. The goal here is to synthesize a\nmetaphor that connects the subject (i.e. TENOR) with the object (i.e. VEHICLE),\nguided by the concept of the GROUND. 2) Metaphor Components Identification,\nwhich extracts the most fitting TENORs, GROUNDs, and VEHICLEs from a\nmetaphorical sentence. This component requires the identification of the most\nfitting metaphor elements that correspond to the specified grounds. In addition\nto overall results, we report on the setup and insights from the metaphor\ngeneration shared task, which attracted a total of 4 participating teams across\nboth subtasks.\n","authors":["Xingwei Qu","Ge Zhang","Siwei Wu","Yizhi Li","Chenghua Lin"],"pdf_url":"https://arxiv.org/pdf/2408.04378v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01139v3","updated":"2024-08-08T11:12:31Z","published":"2024-05-02T09:55:19Z","title":"It Couldn't Help But Overhear: On the Limits of Modelling\n  Meta-Communicative Grounding Acts with Supervised Learning","summary":"  Active participation in a conversation is key to building common ground,\nsince understanding is jointly tailored by producers and recipients.\nOverhearers are deprived of the privilege of performing grounding acts and can\nonly conjecture about intended meanings. Still, data generation and annotation,\nmodelling, training and evaluation of NLP dialogue models place reliance on the\noverhearing paradigm. How much of the underlying grounding processes are\nthereby forfeited? As we show, there is evidence pointing to the impossibility\nof properly modelling human meta-communicative acts with data-driven learning\nmodels. In this paper, we discuss this issue and provide a preliminary analysis\non the variability of human decisions for requesting clarification. Most\nimportantly, we wish to bring this topic back to the community's table,\nencouraging discussion on the consequences of having models designed to only\n\"listen in\".\n","authors":["Brielen Madureira","David Schlangen"],"pdf_url":"https://arxiv.org/pdf/2405.01139v3.pdf","comment":"Accepted to SIGdial 2024"},{"id":"http://arxiv.org/abs/2408.04369v1","updated":"2024-08-08T10:58:33Z","published":"2024-08-08T10:58:33Z","title":"Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings:\n  An Indian Perspective","summary":"  In the internet era, almost every business entity is trying to have its\ndigital footprint in digital media and other social media platforms. For these\nentities, word of mouse is also very important. Particularly, this is quite\ncrucial for the hospitality sector dealing with hotels, restaurants etc.\nConsumers do read other consumers reviews before making final decisions. This\nis where it becomes very important to understand which aspects are affecting\nmost in the minds of the consumers while giving their ratings. The current\nstudy focuses on the consumer reviews of Indian hotels to extract aspects\nimportant for final ratings. The study involves gathering data using web\nscraping methods, analyzing the texts using Latent Dirichlet Allocation for\ntopic extraction and sentiment analysis for aspect-specific sentiment mapping.\nFinally, it incorporates Random Forest to understand the importance of the\naspects in predicting the final rating of a user.\n","authors":["Subhasis Dasgupta","Soumya Roy","Jaydip Sen"],"pdf_url":"https://arxiv.org/pdf/2408.04369v1.pdf","comment":"This is the pre-print of the paper that was accepted for oral\n  presentation and publication in the proceedings of IEEE ICCCNT 2024 which was\n  organized as IIT Mandi, India from June 24 to 28, 2024. The paper is 5 pages\n  long and it contains 4 figures and 6 tables. The is not the final version of\n  the paper"},{"id":"http://arxiv.org/abs/2408.04363v1","updated":"2024-08-08T10:51:16Z","published":"2024-08-08T10:51:16Z","title":"Simulating Articulatory Trajectories with Phonological Feature\n  Interpolation","summary":"  As a first step towards a complete computational model of speech learning\ninvolving perception-production loops, we investigate the forward mapping\nbetween pseudo-motor commands and articulatory trajectories. Two phonological\nfeature sets, based respectively on generative and articulatory phonology, are\nused to encode a phonetic target sequence. Different interpolation techniques\nare compared to generate smooth trajectories in these feature spaces, with a\npotential optimisation of the target value and timing to capture\nco-articulation effects. We report the Pearson correlation between a linear\nprojection of the generated trajectories and articulatory data derived from a\nmulti-speaker dataset of electromagnetic articulography (EMA) recordings. A\ncorrelation of 0.67 is obtained with an extended feature set based on\ngenerative phonology and a linear interpolation technique. We discuss the\nimplications of our results for our understanding of the dynamics of biological\nmotion.\n","authors":["Angelo Ortiz Tandazo","Thomas Schatz","Thomas Hueber","Emmanuel Dupoux"],"pdf_url":"https://arxiv.org/pdf/2408.04363v1.pdf","comment":"accepted at Interspeech 2024"},{"id":"http://arxiv.org/abs/2408.04331v1","updated":"2024-08-08T09:31:24Z","published":"2024-08-08T09:31:24Z","title":"Enhancing Journalism with AI: A Study of Contextualized Image Captioning\n  for News Articles using LLMs and LMMs","summary":"  Large language models (LLMs) and large multimodal models (LMMs) have\nsignificantly impacted the AI community, industry, and various economic\nsectors. In journalism, integrating AI poses unique challenges and\nopportunities, particularly in enhancing the quality and efficiency of news\nreporting. This study explores how LLMs and LMMs can assist journalistic\npractice by generating contextualised captions for images accompanying news\narticles. We conducted experiments using the GoodNews dataset to evaluate the\nability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of\ncontext: entire news articles, or extracted named entities. In addition, we\ncompared their performance to a two-stage pipeline composed of a captioning\nmodel (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs\n(GPT-4 or LLaMA). We assess a diversity of models, and we find that while the\nchoice of contextualisation model is a significant factor for the two-stage\npipelines, this is not the case in the LMMs, where smaller, open-source models\nperform well compared to proprietary, GPT-powered ones. Additionally, we found\nthat controlling the amount of provided context enhances performance. These\nresults highlight the limitations of a fully automated approach and underscore\nthe necessity for an interactive, human-in-the-loop strategy.\n","authors":["Aliki Anagnostopoulou","Thiago Gouvea","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2408.04331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04325v1","updated":"2024-08-08T09:08:27Z","published":"2024-08-08T09:08:27Z","title":"HydraFormer: One Encoder For All Subsampling Rates","summary":"  In automatic speech recognition, subsampling is essential for tackling\ndiverse scenarios. However, the inadequacy of a single subsampling rate to\naddress various real-world situations often necessitates training and deploying\nmultiple models, consequently increasing associated costs. To address this\nissue, we propose HydraFormer, comprising HydraSub, a Conformer-based encoder,\nand a BiTransformer-based decoder. HydraSub encompasses multiple branches, each\nrepresenting a distinct subsampling rate, allowing for the flexible selection\nof any branch during inference based on the specific use case. HydraFormer can\nefficiently manage different subsampling rates, significantly reducing training\nand deployment expenses. Experiments on AISHELL-1 and LibriSpeech datasets\nreveal that HydraFormer effectively adapts to various subsampling rates and\nlanguages while maintaining high recognition performance. Additionally,\nHydraFormer showcases exceptional stability, sustaining consistent performance\nunder various initialization conditions, and exhibits robust transferability by\nlearning from pretrained single subsampling rate automatic speech recognition\nmodels\\footnote{Model code and scripts:\nhttps://github.com/HydraFormer/hydraformer}.\n","authors":["Yaoxun Xu","Xingchen Song","Zhiyong Wu","Di Wu","Zhendong Peng","Binbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04325v1.pdf","comment":"accepted by ICME 2024"},{"id":"http://arxiv.org/abs/2404.16407v2","updated":"2024-08-08T09:01:00Z","published":"2024-04-25T08:34:21Z","title":"U2++ MoE: Scaling 4.7x parameters with minimal impact on RTF","summary":"  Scale has opened new frontiers in natural language processing, but at a high\ncost. In response, by learning to only activate a subset of parameters in\ntraining and inference, Mixture-of-Experts (MoE) have been proposed as an\nenergy efficient path to even larger and more capable language models and this\nshift towards a new generation of foundation models is gaining momentum,\nparticularly within the field of Automatic Speech Recognition (ASR). Recent\nworks that incorporating MoE into ASR models have complex designs such as\nrouting frames via supplementary embedding network, improving multilingual\nability for the experts, and utilizing dedicated auxiliary losses for either\nexpert load balancing or specific language handling. We found that delicate\ndesigns are not necessary, while an embarrassingly simple substitution of MoE\nlayers for all Feed-Forward Network (FFN) layers is competent for the ASR task.\nTo be more specific, we benchmark our proposed model on a large scale\ninner-source dataset (160k hours), the results show that we can scale our\nbaseline Conformer (Dense-225M) to its MoE counterparts (MoE-1B) and achieve\nDense-1B level Word Error Rate (WER) while maintaining a Dense-225M level Real\nTime Factor (RTF). Furthermore, by applying Unified 2-pass framework with\nbidirectional attention decoders (U2++), we achieve the streaming and\nnon-streaming decoding modes in a single MoE based model, which we call U2++\nMoE. We hope that our study can facilitate the research on scaling speech\nfoundation models without sacrificing deployment efficiency.\n","authors":["Xingchen Song","Di Wu","Binbin Zhang","Dinghao Zhou","Zhendong Peng","Bo Dang","Fuping Pan","Chao Yang"],"pdf_url":"https://arxiv.org/pdf/2404.16407v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.18349v3","updated":"2024-08-08T08:57:23Z","published":"2024-03-27T08:39:56Z","title":"Rejection Improves Reliability: Training LLMs to Refuse Unknown\n  Questions Using RL from Knowledge Feedback","summary":"  Large Language Models (LLMs) often generate erroneous outputs, known as\nhallucinations, due to their limitations in discerning questions beyond their\nknowledge scope. While addressing hallucination has been a focal point in\nresearch, previous efforts primarily concentrate on enhancing correctness\nwithout giving due consideration to the significance of rejection mechanisms.\nIn this paper, we conduct a comprehensive examination of the role of rejection,\nintroducing the notion of model reliability along with corresponding metrics.\nThese metrics measure the model's ability to provide accurate responses while\nadeptly rejecting questions exceeding its knowledge boundaries, thereby\nminimizing hallucinations. To improve the inherent reliability of LLMs, we\npresent a novel alignment framework called Reinforcement Learning from\nKnowledge Feedback (RLKF). RLKF leverages knowledge feedback to dynamically\ndetermine the model's knowledge boundary and trains a reliable reward model to\nencourage the refusal of out-of-knowledge questions. Experimental results on\nmathematical questions affirm the substantial efficacy of RLKF in significantly\nenhancing LLM reliability.\n","authors":["Hongshen Xu","Zichen Zhu","Situo Zhang","Da Ma","Shuai Fan","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2403.18349v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04303v1","updated":"2024-08-08T08:37:28Z","published":"2024-08-08T08:37:28Z","title":"Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language\n  Adaptation of LLMs for Low-Resource NLP","summary":"  The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.\n","authors":["Franois Remy","Pieter Delobelle","Hayastan Avetisyan","Alfiya Khabibullina","Miryam de Lhoneux","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2408.04303v1.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2408.04293v1","updated":"2024-08-08T08:13:25Z","published":"2024-08-08T08:13:25Z","title":"Are Social Sentiments Inherent in LLMs? An Empirical Study on Extraction\n  of Inter-demographic Sentiments","summary":"  Large language models (LLMs) are supposed to acquire unconscious human\nknowledge and feelings, such as social common sense and biases, by training\nmodels from large amounts of text. However, it is not clear how much the\nsentiments of specific social groups can be captured in various LLMs. In this\nstudy, we focus on social groups defined in terms of nationality, religion, and\nrace/ethnicity, and validate the extent to which sentiments between social\ngroups can be captured in and extracted from LLMs. Specifically, we input\nquestions regarding sentiments from one group to another into LLMs, apply\nsentiment analysis to the responses, and compare the results with social\nsurveys. The validation results using five representative LLMs showed higher\ncorrelations with relatively small p-values for nationalities and religions,\nwhose number of data points were relatively large. This result indicates that\nthe LLM responses including the inter-group sentiments align well with actual\nsocial survey results.\n","authors":["Kunitomo Tanaka","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2408.04293v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04289v1","updated":"2024-08-08T08:00:45Z","published":"2024-08-08T08:00:45Z","title":"EMTeC: A Corpus of Eye Movements on Machine-Generated Texts","summary":"  The Eye Movements on Machine-Generated Texts Corpus (EMTeC) is a naturalistic\neye-movements-while-reading corpus of 107 native English speakers reading\nmachine-generated texts. The texts are generated by three large language models\nusing five different decoding strategies, and they fall into six different text\ntype categories. EMTeC entails the eye movement data at all stages of\npre-processing, i.e., the raw coordinate data sampled at 2000 Hz, the fixation\nsequences, and the reading measures. It further provides both the original and\na corrected version of the fixation sequences, accounting for vertical\ncalibration drift. Moreover, the corpus includes the language models' internals\nthat underlie the generation of the stimulus texts: the transition scores, the\nattention scores, and the hidden states. The stimuli are annotated for a range\nof linguistic features both at text and at word level. We anticipate EMTeC to\nbe utilized for a variety of use cases such as, but not restricted to, the\ninvestigation of reading behavior on machine-generated text and the impact of\ndifferent decoding strategies; reading behavior on different text types; the\ndevelopment of new pre-processing, data filtering, and drift correction\nalgorithms; the cognitive interpretability and enhancement of language models;\nand the assessment of the predictive power of surprisal and entropy for human\nreading times. The data at all stages of pre-processing, the model internals,\nand the code to reproduce the stimulus generation, data pre-processing and\nanalyses can be accessed via https://github.com/DiLi-Lab/EMTeC/.\n","authors":["Lena Sophia Bolliger","Patrick Haller","Isabelle Caroline Rose Cretton","David Robert Reich","Tannon Kew","Lena Ann Jger"],"pdf_url":"https://arxiv.org/pdf/2408.04289v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04284v1","updated":"2024-08-08T07:43:17Z","published":"2024-08-08T07:43:17Z","title":"LLM-DetectAIve: a Tool for Fine-Grained Machine-Generated Text Detection","summary":"  The widespread accessibility of large language models (LLMs) to the general\npublic has significantly amplified the dissemination of machine-generated texts\n(MGTs). Advancements in prompt manipulation have exacerbated the difficulty in\ndiscerning the origin of a text (human-authored vs machinegenerated). This\nraises concerns regarding the potential misuse of MGTs, particularly within\neducational and academic domains. In this paper, we present\n$\\textbf{LLM-DetectAIve}$ -- a system designed for fine-grained MGT detection.\nIt is able to classify texts into four categories: human-written,\nmachine-generated, machine-written machine-humanized, and human-written\nmachine-polished. Contrary to previous MGT detectors that perform binary\nclassification, introducing two additional categories in LLM-DetectiAIve offers\ninsights into the varying degrees of LLM intervention during the text creation.\nThis might be useful in some domains like education, where any LLM intervention\nis usually prohibited. Experiments show that LLM-DetectAIve can effectively\nidentify the authorship of textual content, proving its usefulness in enhancing\nintegrity in education, academia, and other domains. LLM-DetectAIve is publicly\naccessible at https://huggingface.co/spaces/raj-tomar001/MGT-New. The video\ndescribing our system is available at https://youtu.be/E8eT_bE7k8c.\n","authors":["Mervat Abassy","Kareem Elozeiri","Alexander Aziz","Minh Ngoc Ta","Raj Vardhan Tomar","Bimarsha Adhikari","Saad El Dine Ahmed","Yuxia Wang","Osama Mohammed Afzal","Zhuohan Xie","Jonibek Mansurov","Ekaterina Artemova","Vladislav Mikhailov","Rui Xing","Jiahui Geng","Hasan Iqbal","Zain Muhammad Mujahid","Tarek Mahmoud","Akim Tsvigun","Alham Fikri Aji","Artem Shelmanov","Nizar Habash","Iryna Gurevych","Preslav Nakov"],"pdf_url":"https://arxiv.org/pdf/2408.04284v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04278v1","updated":"2024-08-08T07:37:26Z","published":"2024-08-08T07:37:26Z","title":"LaDiMo: Layer-wise Distillation Inspired MoEfier","summary":"  The advent of large language models has revolutionized natural language\nprocessing, but their increasing complexity has led to substantial training\ncosts, resource demands, and environmental impacts. In response, sparse\nMixture-of-Experts (MoE) models have emerged as a promising alternative to\ndense models. Since training MoE models from scratch can be prohibitively\nexpensive, recent studies have explored leveraging knowledge from pre-trained\nnon-MoE models. However, existing approaches have limitations, such as\nrequiring significant hardware resources and data. We propose a novel\nalgorithm, LaDiMo, which efficiently converts a Transformer-based non-MoE model\ninto a MoE model with minimal additional training cost. LaDiMo consists of two\nstages: layer-wise expert construction and routing policy decision. By\nharnessing the concept of Knowledge Distillation, we compress the model and\nrapidly recover its performance. Furthermore, we develop an adaptive router\nthat optimizes inference efficiency by profiling the distribution of routing\nweights and determining a layer-wise policy that balances accuracy and latency.\nWe demonstrate the effectiveness of our method by converting the LLaMA2-7B\nmodel to a MoE model using only 100K tokens, reducing activated parameters by\nover 20% while keeping accuracy. Our approach offers a flexible and efficient\nsolution for building and deploying MoE models.\n","authors":["Sungyoon Kim","Youngjun Kim","Kihyo Moon","Minsung Jang"],"pdf_url":"https://arxiv.org/pdf/2408.04278v1.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2406.07430v2","updated":"2024-08-08T07:34:50Z","published":"2024-06-11T16:34:02Z","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","summary":"  Out-of-context news is a common type of misinformation on online media\nplatforms. This involves posting a caption, alongside a mismatched news image.\nExisting out-of-context news detection models only consider the scenario where\npre-labeled data is available for each domain, failing to address the\nout-of-context news detection on unlabeled domains (e.g. news topics or\nagencies). In this work, we therefore focus on domain adaptive out-of-context\nnews detection. In order to effectively adapt the detection model to unlabeled\nnews topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation\nwith Test-Time Adaptation) which applies contrastive learning and maximum mean\ndiscrepancy (MMD) to learn domain-invariant features. In addition, we leverage\ntest-time target domain statistics to further assist domain adaptation.\nExperimental results show that our approach outperforms baselines in most\ndomain adaptation settings on two public datasets, by as much as 2.93% in F1\nand 2.08% in accuracy.\n","authors":["Yimeng Gu","Mengqi Zhang","Ignacio Castro","Shu Wu","Gareth Tyson"],"pdf_url":"https://arxiv.org/pdf/2406.07430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13988v6","updated":"2024-08-08T07:32:14Z","published":"2023-03-24T13:24:41Z","title":"Machine Psychology","summary":"  Large language models (LLMs) show increasingly advanced emergent capabilities\nand are being incorporated across various societal domains. Understanding their\nbehavior and reasoning abilities therefore holds significant importance. We\nargue that a fruitful direction for research is engaging LLMs in behavioral\nexperiments inspired by psychology that have traditionally been aimed at\nunderstanding human cognition and behavior. In this article, we highlight and\nsummarize theoretical perspectives, experimental paradigms, and computational\nanalysis techniques that this approach brings to the table. It paves the way\nfor a \"machine psychology\" for generative artificial intelligence (AI) that\ngoes beyond performance benchmarks and focuses instead on computational\ninsights that move us toward a better understanding and discovery of emergent\nabilities and behavioral patterns in LLMs. We review existing work taking this\napproach, synthesize best practices, and highlight promising future directions.\nWe also highlight the important caveats of applying methodologies designed for\nunderstanding humans to machines. We posit that leveraging tools from\nexperimental psychology to study AI will become increasingly valuable as models\nevolve to be more powerful, opaque, multi-modal, and integrated into complex\nreal-world settings.\n","authors":["Thilo Hagendorff","Ishita Dasgupta","Marcel Binz","Stephanie C. Y. Chan","Andrew Lampinen","Jane X. Wang","Zeynep Akata","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2303.13988v6.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06204v2","updated":"2024-08-08T07:13:37Z","published":"2024-06-26T16:34:33Z","title":"A Survey on Mixture of Experts","summary":"  Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge developments in MoE\nresearch, we have established a resource repository accessible at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.\n","authors":["Weilin Cai","Juyong Jiang","Fan Wang","Jing Tang","Sunghun Kim","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2407.06204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04270v1","updated":"2024-08-08T07:12:46Z","published":"2024-08-08T07:12:46Z","title":"Analysis of Argument Structure Constructions in the Large Language Model\n  BERT","summary":"  This study investigates how BERT processes and represents Argument Structure\nConstructions (ASCs), extending previous LSTM analyses. Using a dataset of 2000\nsentences across four ASC types (transitive, ditransitive, caused-motion,\nresultative), we analyzed BERT's token embeddings across 12 layers.\nVisualizations with MDS and t-SNE and clustering quantified by Generalized\nDiscrimination Value (GDV) were used. Feedforward classifiers (probes)\npredicted construction categories from embeddings. CLS token embeddings\nclustered best in layers 2-4, decreased in intermediate layers, and slightly\nincreased in final layers. DET and SUBJ embeddings showed consistent clustering\nin intermediate layers, VERB embeddings increased in clustering from layer 1 to\n12, and OBJ embeddings peaked in layer 10. Probe accuracies indicated low\nconstruction information in layer 1, with over 90 percent accuracy from layer 2\nonward, revealing latent construction information beyond GDV clustering. Fisher\nDiscriminant Ratio (FDR) analysis of attention weights showed OBJ tokens were\ncrucial for differentiating ASCs, followed by VERB and DET tokens. SUBJ, CLS,\nand SEP tokens had insignificant FDR scores. This study highlights BERT's\nlayered processing of linguistic constructions and its differences from LSTMs.\nFuture research will compare these findings with neuroimaging data to\nunderstand the neural correlates of ASC processing. This research underscores\nneural language models' potential to mirror linguistic processing in the human\nbrain, offering insights into the computational and neural mechanisms\nunderlying language understanding.\n","authors":["Pegah Ramezani","Achim Schilling","Patrick Krauss"],"pdf_url":"https://arxiv.org/pdf/2408.04270v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2408.03062"},{"id":"http://arxiv.org/abs/2402.15809v2","updated":"2024-08-08T07:05:46Z","published":"2024-02-24T13:13:04Z","title":"Empowering Large Language Model Agents through Action Learning","summary":"  Large Language Model (LLM) Agents have recently garnered increasing interest\nyet they are limited in their ability to learn from trial and error, a key\nelement of intelligent behavior. In this work, we argue that the capacity to\nlearn new actions from experience is fundamental to the advancement of learning\nin LLM agents. While humans naturally expand their action spaces and develop\nskills through experiential learning, LLM agents typically operate within fixed\naction spaces, limiting their potential for growth. To address these\nchallenges, our study explores open-action learning for language agents. We\nintroduce a framework LearnAct with an iterative learning strategy to create\nand improve actions in the form of Python functions. In each iteration, LLM\nrevises and updates the currently available actions based on the errors\nidentified in unsuccessful training tasks, thereby enhancing action\neffectiveness. Our experimental evaluations across Robotic Planning and\nAlfworld environments reveal that after learning on a few training task\ninstances, our approach to open-action learning markedly improves agent\nperformance for the type of task (by 32 percent in AlfWorld compared to\nReAct+Reflexion, for instance) highlighting the importance of experiential\naction learning in the development of more intelligent LLM agents.\n","authors":["Haiteng Zhao","Chang Ma","Guoyin Wang","Jing Su","Lingpeng Kong","Jingjing Xu","Zhi-Hong Deng","Hongxia Yang"],"pdf_url":"https://arxiv.org/pdf/2402.15809v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2408.04259v1","updated":"2024-08-08T06:57:49Z","published":"2024-08-08T06:57:49Z","title":"EfficientRAG: Efficient Retriever for Multi-Hop Question Answering","summary":"  Retrieval-augmented generation (RAG) methods encounter difficulties when\naddressing complex questions like multi-hop queries. While iterative retrieval\nmethods improve performance by gathering additional information, current\napproaches often rely on multiple calls of large language models (LLMs). In\nthis paper, we introduce EfficientRAG, an efficient retriever for multi-hop\nquestion answering. EfficientRAG iteratively generates new queries without the\nneed for LLM calls at each iteration and filters out irrelevant information.\nExperimental results demonstrate that EfficientRAG surpasses existing RAG\nmethods on three open-domain multi-hop question-answering datasets.\n","authors":["Ziyuan Zhuang","Zhiyang Zhang","Sitao Cheng","Fangkai Yang","Jia Liu","Shujian Huang","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04259v1.pdf","comment":"20 pages, 4 figures"},{"id":"http://arxiv.org/abs/2403.19913v2","updated":"2024-08-08T06:38:31Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v2.pdf","comment":"COLM 2024 camera-ready"},{"id":"http://arxiv.org/abs/2310.17876v3","updated":"2024-08-08T06:32:00Z","published":"2023-10-27T03:32:17Z","title":"TarGEN: Targeted Data Generation with Large Language Models","summary":"  The rapid advancement of large language models (LLMs) has sparked interest in\ndata synthesis techniques, aiming to generate diverse and high-quality\nsynthetic datasets. However, these synthetic datasets often suffer from a lack\nof diversity and added noise. In this paper, we present TarGEN, a multi-step\nprompting strategy for generating high-quality synthetic datasets utilizing a\nLLM. An advantage of TarGEN is its seedless nature; it does not require\nspecific task instances, broadening its applicability beyond task replication.\nWe augment TarGEN with a method known as self-correction empowering LLMs to\nrectify inaccurately labeled instances during dataset creation, ensuring\nreliable labels. To assess our technique's effectiveness, we emulate 8 tasks\nfrom the SuperGLUE benchmark and finetune various language models, including\nencoder-only, encoder-decoder, and decoder-only models on both synthetic and\noriginal training sets. Evaluation on the original test set reveals that models\ntrained on datasets generated by TarGEN perform approximately 1-2% points\nbetter than those trained on original datasets (82.84% via syn. vs. 81.12% on\nog. using Flan-T5). When incorporating instruction tuning, the performance\nincreases to 84.54% on synthetic data vs. 81.49% on original data by Flan-T5. A\ncomprehensive analysis of the synthetic dataset compared to the original\ndataset reveals that the synthetic dataset demonstrates similar or higher\nlevels of dataset complexity and diversity. Furthermore, the synthetic dataset\ndisplays a bias level that aligns closely with the original dataset. Finally,\nwhen pre-finetuned on our synthetic SuperGLUE dataset, T5-3B yields impressive\nresults on the OpenLLM leaderboard, surpassing the model trained on the\nSelf-Instruct dataset by 4.14% points. We hope that TarGEN can be helpful for\nquality data generation and reducing the human efforts to create complex\nbenchmarks.\n","authors":["Himanshu Gupta","Kevin Scaria","Ujjwala Anantheswaran","Shreyas Verma","Mihir Parmar","Saurabh Arjun Sawant","Chitta Baral","Swaroop Mishra"],"pdf_url":"https://arxiv.org/pdf/2310.17876v3.pdf","comment":"COLM 2024, 35 pages"},{"id":"http://arxiv.org/abs/2407.09020v3","updated":"2024-08-08T06:19:10Z","published":"2024-07-12T06:22:45Z","title":"3M-Health: Multimodal Multi-Teacher Knowledge Distillation for Mental\n  Health Detection","summary":"  The significance of mental health classification is paramount in contemporary\nsociety, where digital platforms serve as crucial sources for monitoring\nindividuals' well-being. However, existing social media mental health datasets\nprimarily consist of text-only samples, potentially limiting the efficacy of\nmodels trained on such data. Recognising that humans utilise cross-modal\ninformation to comprehend complex situations or issues, we present a novel\napproach to address the limitations of current methodologies. In this work, we\nintroduce a Multimodal and Multi-Teacher Knowledge Distillation model for\nMental Health Classification, leveraging insights from cross-modal human\nunderstanding. Unlike conventional approaches that often rely on simple\nconcatenation to integrate diverse features, our model addresses the challenge\nof appropriately representing inputs of varying natures (e.g., texts and\nsounds). To mitigate the computational complexity associated with integrating\nall features into a single model, we employ a multimodal and multi-teacher\narchitecture. By distributing the learning process across multiple teachers,\neach specialising in a particular feature extraction aspect, we enhance the\noverall mental health classification performance. Through experimental\nvalidation, we demonstrate the efficacy of our model in achieving improved\nperformance.\n","authors":["Rina Carines Cabral","Siwen Luo","Josiah Poon","Soyeon Caren Han"],"pdf_url":"https://arxiv.org/pdf/2407.09020v3.pdf","comment":"Accepted at CIKM 2024; Code will be made available at\n  https://github.com/adlnlp/3mhealth"},{"id":"http://arxiv.org/abs/2408.04246v1","updated":"2024-08-08T06:18:24Z","published":"2024-08-08T06:18:24Z","title":"Explicating the Implicit: Argument Detection Beyond Sentence Boundaries","summary":"  Detecting semantic arguments of a predicate word has been conventionally\nmodeled as a sentence-level task. The typical reader, however, perfectly\ninterprets predicate-argument relations in a much wider context than just the\nsentence where the predicate was evoked. In this work, we reformulate the\nproblem of argument detection through textual entailment to capture semantic\nrelations across sentence boundaries. We propose a method that tests whether\nsome semantic relation can be inferred from a full passage by first encoding it\ninto a simple and standalone proposition and then testing for entailment\nagainst the passage. Our method does not require direct supervision, which is\ngenerally absent due to dataset scarcity, but instead builds on existing NLI\nand sentence-level SRL resources. Such a method can potentially explicate\npragmatically understood relations into a set of explicit sentences. We\ndemonstrate it on a recent document-level benchmark, outperforming some\nsupervised methods and contemporary language models.\n","authors":["Paul Roit","Aviv Slobodkin","Eran Hirsch","Arie Cattan","Ayal Klein","Valentina Pyatkin","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2408.04246v1.pdf","comment":"9 pages, ACL 2024"},{"id":"http://arxiv.org/abs/2407.12393v4","updated":"2024-08-08T06:08:54Z","published":"2024-07-17T08:13:22Z","title":"PersLLM: A Personified Training Approach for Large Language Models","summary":"  Large language models exhibit aspects of human-level intelligence that\ncatalyze their application as human-like agents in domains such as social\nsimulations, human-machine interactions, and collaborative multi-agent systems.\nHowever, the absence of distinct personalities, such as displaying ingratiating\nbehaviors, inconsistent opinions, and uniform response patterns, diminish LLMs\nutility in practical applications. Addressing this, the development of\npersonality traits in LLMs emerges as a crucial area of research to unlock\ntheir latent potential. Existing methods to personify LLMs generally involve\nstrategies like employing stylized training data for instruction tuning or\nusing prompt engineering to simulate different personalities. These methods\nonly capture superficial linguistic styles instead of the core of personalities\nand are therefore not stable. In this study, we propose PersLLM, integrating\npsychology-grounded principles of personality: social practice, consistency,\nand dynamic development, into a comprehensive training methodology. We\nincorporate personality traits directly into the model parameters, enhancing\nthe model's resistance to induction, promoting consistency, and supporting the\ndynamic evolution of personality. Single-agent evaluation validates our\nmethod's superiority, as it produces responses more aligned with reference\npersonalities compared to other approaches. Case studies for multi-agent\ncommunication highlight its benefits in enhancing opinion consistency within\nindividual agents and fostering collaborative creativity among multiple agents\nin dialogue contexts, potentially benefiting human simulation and multi-agent\ncooperation. Additionally, human-agent interaction evaluations indicate that\nour personified models significantly enhance interactive experiences,\nunderscoring the practical implications of our research.\n","authors":["Zheni Zeng","Jiayi Chen","Huimin Chen","Yukun Yan","Yuxuan Chen","Zhenghao Liu","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2407.12393v4.pdf","comment":"10 pages for main text, 5 figures"},{"id":"http://arxiv.org/abs/2408.04237v1","updated":"2024-08-08T05:53:39Z","published":"2024-08-08T05:53:39Z","title":"Learning to Rewrite: Generalized LLM-Generated Text Detection","summary":"  Large language models (LLMs) can be abused at scale to create non-factual\ncontent and spread disinformation. Detecting LLM-generated content is essential\nto mitigate these risks, but current classifiers often fail to generalize in\nopen-world contexts. Prior work shows that LLMs tend to rewrite LLM-generated\ncontent less frequently, which can be used for detection and naturally\ngeneralizes to unforeseen data. However, we find that the rewriting edit\ndistance between human and LLM content can be indistinguishable across domains,\nleading to detection failures. We propose training an LLM to rewrite input\ntext, producing minimal edits for LLM-generated content and more edits for\nhuman-written text, deriving a distinguishable and generalizable edit distance\ndifference across different domains. Experiments on text from 21 independent\ndomains and three popular LLMs (e.g., GPT-4o, Gemini, and Llama-3) show that\nour classifier outperforms the state-of-the-art zero-shot classifier by up to\n20.6% on AUROC score and the rewriting classifier by 9.2% on F1 score. Our work\nsuggests that LLM can effectively detect machine-generated text if they are\ntrained properly.\n","authors":["Wei Hao","Ran Li","Weiliang Zhao","Junfeng Yang","Chengzhi Mao"],"pdf_url":"https://arxiv.org/pdf/2408.04237v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04226v1","updated":"2024-08-08T05:28:34Z","published":"2024-08-08T05:28:34Z","title":"Evaluating Language Model Math Reasoning via Grounding in Educational\n  Curricula","summary":"  Our work presents a novel angle for evaluating language models' (LMs)\nmathematical abilities, by investigating whether they can discern skills and\nconcepts enabled by math content. We contribute two datasets: one consisting of\n385 fine-grained descriptions of K-12 math skills and concepts, or standards,\nfrom Achieve the Core (ATC), and another of 9.9K problems labeled with these\nstandards (MathFish). Working with experienced teachers, we find that LMs\nstruggle to tag and verify standards linked to problems, and instead predict\nlabels that are close to ground truth, but differ in subtle ways. We also show\nthat LMs often generate problems that do not fully align with standards\ndescribed in prompts. Finally, we categorize problems in GSM8k using math\nstandards, allowing us to better understand why some problems are more\ndifficult to solve for models than others.\n","authors":["Li Lucy","Tal August","Rose E. Wang","Luca Soldaini","Courtney Allison","Kyle Lo"],"pdf_url":"https://arxiv.org/pdf/2408.04226v1.pdf","comment":"30 pages, 23 figures"},{"id":"http://arxiv.org/abs/2408.04220v1","updated":"2024-08-08T05:06:22Z","published":"2024-08-08T05:06:22Z","title":"Diffusion Guided Language Modeling","summary":"  Current language models demonstrate remarkable proficiency in text\ngeneration. However, for many applications it is desirable to control\nattributes, such as sentiment, or toxicity, of the generated language --\nideally tailored towards each specific use case and target audience. For\nauto-regressive language models, existing guidance methods are prone to\ndecoding errors that cascade during generation and degrade performance. In\ncontrast, text diffusion models can easily be guided with, for example, a\nsimple linear sentiment classifier -- however they do suffer from significantly\nhigher perplexity than auto-regressive alternatives. In this paper we use a\nguided diffusion model to produce a latent proposal that steers an\nauto-regressive language model to generate text with desired properties. Our\nmodel inherits the unmatched fluency of the auto-regressive approach and the\nplug-and-play flexibility of diffusion. We show that it outperforms previous\nplug-and-play guidance methods across a wide range of benchmark data sets.\nFurther, controlling a new attribute in our framework is reduced to training a\nsingle logistic regression classifier.\n","authors":["Justin Lovelace","Varsha Kishore","Yiwei Chen","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2408.04220v1.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2408.04217v1","updated":"2024-08-08T04:57:36Z","published":"2024-08-08T04:57:36Z","title":"Simplifying Translations for Children: Iterative Simplification\n  Considering Age of Acquisition with LLMs","summary":"  In recent years, neural machine translation (NMT) has been widely used in\neveryday life. However, the current NMT lacks a mechanism to adjust the\ndifficulty level of translations to match the user's language level.\nAdditionally, due to the bias in the training data for NMT, translations of\nsimple source sentences are often produced with complex words. In particular,\nthis could pose a problem for children, who may not be able to understand the\nmeaning of the translations correctly. In this study, we propose a method that\nreplaces words with high Age of Acquisitions (AoA) in translations with simpler\nwords to match the translations to the user's level. We achieve this by using\nlarge language models (LLMs), providing a triple of a source sentence, a\ntranslation, and a target word to be replaced. We create a benchmark dataset\nusing back-translation on Simple English Wikipedia. The experimental results\nobtained from the dataset show that our method effectively replaces high-AoA\nwords with lower-AoA words and, moreover, can iteratively replace most of the\nhigh-AoA words while still maintaining high BLEU and COMET scores.\n","authors":["Masashi Oshika","Makoto Morishita","Tsutomu Hirao","Ryohei Sasano","Koichi Takeda"],"pdf_url":"https://arxiv.org/pdf/2408.04217v1.pdf","comment":"Findings of ACL 2024"},{"id":"http://arxiv.org/abs/2408.04216v1","updated":"2024-08-08T04:52:10Z","published":"2024-08-08T04:52:10Z","title":"Attention Mechanism and Context Modeling System for Text Mining Machine\n  Translation","summary":"  This paper advances a novel architectural schema anchored upon the\nTransformer paradigm and innovatively amalgamates the K-means categorization\nalgorithm to augment the contextual apprehension capabilities of the schema.\nThe transformer model performs well in machine translation tasks due to its\nparallel computing power and multi-head attention mechanism. However, it may\nencounter contextual ambiguity or ignore local features when dealing with\nhighly complex language structures. To circumvent this constraint, this\nexposition incorporates the K-Means algorithm, which is used to stratify the\nlexis and idioms of the input textual matter, thereby facilitating superior\nidentification and preservation of the local structure and contextual\nintelligence of the language. The advantage of this combination is that K-Means\ncan automatically discover the topic or concept regions in the text, which may\nbe directly related to translation quality. Consequently, the schema contrived\nherein enlists K-Means as a preparatory phase antecedent to the Transformer and\nrecalibrates the multi-head attention weights to assist in the discrimination\nof lexis and idioms bearing analogous semantics or functionalities. This\nensures the schema accords heightened regard to the contextual intelligence\nembodied by these clusters during the training phase, rather than merely\nfocusing on locational intelligence.\n","authors":["Shi Bo","Yuwei Zhang","Junming Huang","Sitong Liu","Zexi Chen","Zizheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.04216v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06373v4","updated":"2024-08-08T04:47:20Z","published":"2024-05-10T10:19:14Z","title":"LLM Discussion: Enhancing the Creativity of Large Language Models via\n  Discussion Framework and Role-Play","summary":"  Large language models (LLMs) have shown exceptional proficiency in natural\nlanguage processing but often fall short of generating creative and original\nresponses to open-ended questions. To enhance LLM creativity, our key insight\nis to emulate the human process of inducing collective creativity through\nengaging discussions with participants from diverse backgrounds and\nperspectives. To this end, we propose LLM Discussion, a three-phase discussion\nframework that facilitates vigorous and diverging idea exchanges and ensures\nconvergence to creative answers. Moreover, we adopt a role-playing technique by\nassigning distinct roles to LLMs to combat the homogeneity of LLMs. We evaluate\nthe efficacy of the proposed framework with the Alternative Uses Test,\nSimilarities Test, Instances Test, and Scientific Creativity Test through both\nLLM evaluation and human study. The results show that our proposed framework\noutperforms single-LLM approaches and existing multi-LLM frameworks across\nvarious creativity metrics. The code is available at\nhttps://github.com/lawraa/LLM-Discussion.\n","authors":["Li-Chun Lu","Shou-Jen Chen","Tsung-Min Pai","Chan-Hung Yu","Hung-yi Lee","Shao-Hua Sun"],"pdf_url":"https://arxiv.org/pdf/2405.06373v4.pdf","comment":"40 pages, 9 figures, COLM 2024"},{"id":"http://arxiv.org/abs/2408.03541v2","updated":"2024-08-08T04:35:23Z","published":"2024-08-07T04:38:38Z","title":"EXAONE 3.0 7.8B Instruction Tuned Language Model","summary":"  We introduce EXAONE 3.0 instruction-tuned language model, the first open\nmodel in the family of Large Language Models (LLMs) developed by LG AI\nResearch. Among different model sizes, we publicly release the 7.8B\ninstruction-tuned model to promote open research and innovations. Through\nextensive evaluations across a wide range of public and in-house benchmarks,\nEXAONE 3.0 demonstrates highly competitive real-world performance with\ninstruction-following capability against other state-of-the-art open models of\nsimilar size. Our comparative analysis shows that EXAONE 3.0 excels\nparticularly in Korean, while achieving compelling performance across general\ntasks and complex reasoning. With its strong real-world effectiveness and\nbilingual proficiency, we hope that EXAONE keeps contributing to advancements\nin Expert AI. Our EXAONE 3.0 instruction-tuned model is available at\nhttps://huggingface.co/LGAI-EXAONE/EXAONE-3.0-7.8B-Instruct\n","authors":["LG AI Research"," :","Soyoung An","Kyunghoon Bae","Eunbi Choi","Stanley Jungkyu Choi","Yemuk Choi","Seokhee Hong","Yeonjung Hong","Junwon Hwang","Hyojin Jeon","Gerrard Jeongwon Jo","Hyunjik Jo","Jiyeon Jung","Yountae Jung","Euisoon Kim","Hyosang Kim","Joonkee Kim","Seonghwan Kim","Soyeon Kim","Sunkyoung Kim","Yireun Kim","Youchul Kim","Edward Hwayoung Lee","Haeju Lee","Honglak Lee","Jinsik Lee","Kyungmin Lee","Moontae Lee","Seungjun Lee","Woohyung Lim","Sangha Park","Sooyoun Park","Yongmin Park","Boseong Seo","Sihoon Yang","Heuiyeen Yeen","Kyungjae Yoo","Hyeongu Yun"],"pdf_url":"https://arxiv.org/pdf/2408.03541v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04211v1","updated":"2024-08-08T04:31:29Z","published":"2024-08-08T04:31:29Z","title":"MMREC: LLM Based Multi-Modal Recommender System","summary":"  The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations.\n","authors":["Jiahao Tian","Jinman Zhao","Zhenkai Wang","Zhicheng Ding"],"pdf_url":"https://arxiv.org/pdf/2408.04211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07887v5","updated":"2024-08-08T03:49:59Z","published":"2023-12-13T04:14:22Z","title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models","summary":"  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.\n","authors":["Junhao Zheng","Shengjie Qiu","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07887v5.pdf","comment":"ACL 2024 main conference (Oral)"},{"id":"http://arxiv.org/abs/2308.12539v3","updated":"2024-08-08T03:20:17Z","published":"2023-08-24T03:53:55Z","title":"CALM : A Multi-task Benchmark for Comprehensive Assessment of Language\n  Model Bias","summary":"  As language models (LMs) become increasingly powerful and widely used, it is\nimportant to quantify them for sociodemographic bias with potential for harm.\nPrior measures of bias are sensitive to perturbations in the templates designed\nto compare performance across social groups, due to factors such as low\ndiversity or limited number of templates. Also, most previous work considers\nonly one NLP task. We introduce Comprehensive Assessment of Language Models\n(CALM) for robust measurement of two types of universally relevant\nsociodemographic bias, gender and race. CALM integrates sixteen datasets for\nquestion-answering, sentiment analysis and natural language inference. Examples\nfrom each dataset are filtered to produce 224 templates with high diversity\n(e.g., length, vocabulary). We assemble 50 highly frequent person names for\neach of seven distinct demographic groups to generate 78,400 prompts covering\nthe three NLP tasks. Our empirical evaluation shows that CALM bias scores are\nmore robust and far less sensitive than previous bias measurements to\nperturbations in the templates, such as synonym substitution, or to random\nsubset selection of templates. We apply CALM to 20 large language models, and\nfind that for 2 language model series, larger parameter models tend to be more\nbiased than smaller ones. The T0 series is the least biased model families, of\nthe 20 LLMs investigated here. The code is available at\nhttps://github.com/vipulgupta1011/CALM.\n","authors":["Vipul Gupta","Pranav Narayanan Venkit","Hugo Laurenon","Shomir Wilson","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2308.12539v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.06555v3","updated":"2024-08-08T03:20:05Z","published":"2023-11-11T12:05:01Z","title":"LLMs Learn Task Heuristics from Demonstrations: A Heuristic-Driven\n  Prompting Strategy for Document-Level Event Argument Extraction","summary":"  In this study, we investigate in-context learning (ICL) in document-level\nevent argument extraction (EAE) to alleviate the dependency on large-scale\nlabeled data for this task. We introduce the Heuristic-Driven Link-of-Analogy\n(HD-LoA) prompting to address the challenge of example selection and to develop\na prompting strategy tailored for EAE. Specifically, we hypothesize and\nvalidate that LLMs learn task-specific heuristics from demonstrations via ICL.\nBuilding upon this hypothesis, we introduce an explicit heuristic-driven\ndemonstration construction approach, which transforms the haphazard example\nselection process into a methodical method that emphasizes task heuristics.\nAdditionally, inspired by the analogical reasoning of human, we propose the\nlink-of-analogy prompting, which enables LLMs to process new situations by\ndrawing analogies to known situations, enhancing their performance on unseen\nclasses beyond limited ICL examples. Experiments show that our method\noutperforms existing prompting methods and few-shot supervised learning methods\non document-level EAE datasets. Additionally, the HD-LoA prompting shows\neffectiveness in diverse tasks like sentiment analysis and natural language\ninference, demonstrating its broad adaptability.\n","authors":["Hanzhang Zhou","Junlang Qian","Zijian Feng","Hui Lu","Zixiao Zhu","Kezhi Mao"],"pdf_url":"https://arxiv.org/pdf/2311.06555v3.pdf","comment":"Accepted to ACL 2024"},{"id":"http://arxiv.org/abs/2406.12389v2","updated":"2024-08-08T03:07:58Z","published":"2024-06-18T08:26:33Z","title":"EMO-KNOW: A Large Scale Dataset on Emotion and Emotion-cause","summary":"  Emotion-Cause analysis has attracted the attention of researchers in recent\nyears. However, most existing datasets are limited in size and number of\nemotion categories. They often focus on extracting parts of the document that\ncontain the emotion cause and fail to provide more abstractive, generalizable\nroot cause. To bridge this gap, we introduce a large-scale dataset of emotion\ncauses, derived from 9.8 million cleaned tweets over 15 years. We describe our\ncuration process, which includes a comprehensive pipeline for data gathering,\ncleaning, labeling, and validation, ensuring the dataset's reliability and\nrichness. We extract emotion labels and provide abstractive summarization of\nthe events causing emotions. The final dataset comprises over 700,000 tweets\nwith corresponding emotion-cause pairs spanning 48 emotion classes, validated\nby human evaluators. The novelty of our dataset stems from its broad spectrum\nof emotion classes and the abstractive emotion cause that facilitates the\ndevelopment of an emotion-cause knowledge graph for nuanced reasoning. Our\ndataset will enable the design of emotion-aware systems that account for the\ndiverse emotional responses of different people for the same event.\n","authors":["Mia Huong Nguyen","Yasith Samaradivakara","Prasanth Sasikumar","Chitralekha Gupta","Suranga Nanayakkara"],"pdf_url":"https://arxiv.org/pdf/2406.12389v2.pdf","comment":"Accepted to Findings of EMNLP 2023"},{"id":"http://arxiv.org/abs/2406.07822v2","updated":"2024-08-08T02:36:04Z","published":"2024-06-12T02:43:19Z","title":"Tell Me What's Next: Textual Foresight for Generic UI Representations","summary":"  Mobile app user interfaces (UIs) are rich with action, text, structure, and\nimage content that can be utilized to learn generic UI representations for\ntasks like automating user commands, summarizing content, and evaluating the\naccessibility of user interfaces. Prior work has learned strong visual\nrepresentations with local or global captioning losses, but fails to retain\nboth granularities. To combat this, we propose Textual Foresight, a novel\npretraining objective for learning UI screen representations. Textual Foresight\ngenerates global text descriptions of future UI states given a current UI and\nlocal action taken. Our approach requires joint reasoning over elements and\nentire screens, resulting in improved UI features: on generation tasks, UI\nagents trained with Textual Foresight outperform state-of-the-art by 2% with\n28x fewer images. We train with our newly constructed mobile app dataset,\nOpenApp, which results in the first public dataset for app UI representation\nlearning. OpenApp enables new baselines, and we find Textual Foresight improves\naverage task performance over them by 5.7% while having access to 2x less data.\n","authors":["Andrea Burns","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2406.07822v2.pdf","comment":"Accepted to ACL 2024 Findings. Data and code to be released at\n  https://github.com/aburns4/textualforesight"},{"id":"http://arxiv.org/abs/2408.04174v1","updated":"2024-08-08T02:36:04Z","published":"2024-08-08T02:36:04Z","title":"wav2graph: A Framework for Supervised Learning Knowledge Graph from\n  Speech","summary":"  Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.\n","authors":["Khai Le-Duc","Quy-Anh Dang","Tan-Hanh Pham","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2408.04174v1.pdf","comment":"Preprint, 32 pages"},{"id":"http://arxiv.org/abs/2408.04167v1","updated":"2024-08-08T02:28:32Z","published":"2024-08-08T02:28:32Z","title":"mbrs: A Library for Minimum Bayes Risk Decoding","summary":"  Minimum Bayes risk (MBR) decoding is a decision rule of text generation tasks\nthat outperforms conventional maximum a posterior (MAP) decoding using beam\nsearch by selecting high-quality outputs based on a utility function rather\nthan those with high-probability. Typically, it finds the most suitable\nhypothesis from the set of hypotheses under the sampled pseudo-references. mbrs\nis a library of MBR decoding, which can flexibly combine various metrics,\nalternative expectation estimations, and algorithmic variants. It is designed\nwith a focus on speed measurement and calling count of code blocks,\ntransparency, reproducibility, and extensibility, which are essential for\nresearchers and developers. We published our mbrs as an MIT-licensed\nopen-source project, and the code is available on GitHub.\n  GitHub: https://github.com/naist-nlp/mbrs\n","authors":["Hiroyuki Deguchi","Yusuke Sakai","Hidetaka Kamigaito","Taro Watanabe"],"pdf_url":"https://arxiv.org/pdf/2408.04167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00981v2","updated":"2024-08-08T02:15:53Z","published":"2024-08-02T02:31:54Z","title":"Cross-domain Named Entity Recognition via Graph Matching","summary":"  Cross-domain NER is a practical yet challenging problem since the data\nscarcity in the real-world scenario. A common practice is first to learn a NER\nmodel in a rich-resource general domain and then adapt the model to specific\ndomains. Due to the mismatch problem between entity types across domains, the\nwide knowledge in the general domain can not effectively transfer to the target\ndomain NER model. To this end, we model the label relationship as a probability\ndistribution and construct label graphs in both source and target label spaces.\nTo enhance the contextual representation with label structures, we fuse the\nlabel graph into the word embedding output by BERT. By representing label\nrelationships as graphs, we formulate cross-domain NER as a graph matching\nproblem. Furthermore, the proposed method has good applicability with\npre-training methods and is potentially capable of other cross-domain\nprediction tasks. Empirical results on four datasets show that our method\noutperforms a series of transfer learning, multi-task learning, and few-shot\nlearning methods.\n","authors":["Junhao Zheng","Haibin Chen","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2408.00981v2.pdf","comment":"Findings of ACL; available at Findings 2022\n  https://aclanthology.org/2022.findings-acl.210/; Improve presentation"},{"id":"http://arxiv.org/abs/2408.04162v1","updated":"2024-08-08T02:07:25Z","published":"2024-08-08T02:07:25Z","title":"Semantics or spelling? Probing contextual word embeddings with\n  orthographic noise","summary":"  Pretrained language model (PLM) hidden states are frequently employed as\ncontextual word embeddings (CWE): high-dimensional representations that encode\nsemantic information given linguistic context. Across many areas of\ncomputational linguistics research, similarity between CWEs is interpreted as\nsemantic similarity. However, it remains unclear exactly what information is\nencoded in PLM hidden states. We investigate this practice by probing PLM\nrepresentations using minimal orthographic noise. We expect that if CWEs\nprimarily encode semantic information, a single character swap in the input\nword will not drastically affect the resulting representation,given sufficient\nlinguistic context. Surprisingly, we find that CWEs generated by popular PLMs\nare highly sensitive to noise in input data, and that this sensitivity is\nrelated to subword tokenization: the fewer tokens used to represent a word at\ninput, the more sensitive its corresponding CWE. This suggests that CWEs\ncapture information unrelated to word-level meaning and can be manipulated\nthrough trivial modifications of input data. We conclude that these PLM-derived\nCWEs may not be reliable semantic proxies, and that caution is warranted when\ninterpreting representational similarity\n","authors":["Jacob A. Matthews","John R. Starr","Marten van Schijndel"],"pdf_url":"https://arxiv.org/pdf/2408.04162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.08817v2","updated":"2024-08-08T01:46:19Z","published":"2023-11-15T09:38:53Z","title":"MAP's not dead yet: Uncovering true language model modes by conditioning\n  away degeneracy","summary":"  It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior\nwork has attributed this behavior to either a fundamental and unavoidable\ninadequacy of modes in probabilistic models or weaknesses in language modeling.\nContrastingly, we argue that degenerate modes can even occur in the absence of\nany modeling error, due to contamination of the training data. Specifically, we\nargue that mixing even a tiny amount of low-entropy noise with a population\ntext distribution can cause the data distribution's mode to become degenerate.\nWe therefore propose to apply MAP decoding to the model's true conditional\ndistribution where the conditioning variable explicitly avoids specific\ndegenerate behavior. Using exact search, we empirically verify that the\nlength-conditional modes of machine translation models and language models are\nindeed more fluent and topical than their unconditional modes. For the first\ntime, we also share many examples of exact modal sequences from these models,\nand from several variants of the LLaMA-7B model. Notably, we observe that\nvarious kinds of degenerate modes persist, even at the scale of LLaMA-7B.\nAlthough we cannot tractably address these degeneracies with exact search, we\nperform a classifier-based approximate search on LLaMA-7B, a model which was\nnot trained for instruction following, and find that we are able to elicit\nreasonable outputs without any finetuning.\n","authors":["Davis Yoshida","Kartik Goyal","Kevin Gimpel"],"pdf_url":"https://arxiv.org/pdf/2311.08817v2.pdf","comment":"52 pages, 5 figures, ACL version"},{"id":"http://arxiv.org/abs/2408.00992v3","updated":"2024-08-08T01:23:11Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hours","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03675v2","updated":"2024-08-08T01:20:13Z","published":"2024-08-07T10:31:07Z","title":"NACL: A General and Effective KV Cache Eviction Framework for LLMs at\n  Inference Time","summary":"  Large Language Models (LLMs) have ignited an innovative surge of AI\napplications, marking a new era of exciting possibilities equipped with\nextended context windows. However, hosting these models is cost-prohibitive\nmainly due to the extensive memory consumption of KV Cache involving\nlong-context modeling. Despite several works proposing to evict unnecessary\ntokens from the KV Cache, most of them rely on the biased local statistics of\naccumulated attention scores and report performance using unconvincing metric\nlike perplexity on inadequate short-text evaluation. In this paper, we propose\nNACL, a general framework for long-context KV cache eviction that achieves more\noptimal and efficient eviction in a single operation during the encoding phase.\nDue to NACL's efficiency, we combine more accurate attention score statistics\nin PROXY TOKENS EVICTION with the diversified random eviction strategy of\nRANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance\nthe robustness in maintaining pivotal tokens for long-context modeling tasks.\nNotably, our method significantly improves the performance on short- and\nlong-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50%\nwith over 95% performance maintenance. The code is available at\nhttps://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.\n","authors":["Yilong Chen","Guoxia Wang","Junyuan Shang","Shiyao Cui","Zhenyu Zhang","Tingwen Liu","Shuohuan Wang","Yu Sun","Dianhai Yu","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2408.03675v2.pdf","comment":"Accepted by ACL 2024 (main conference, long paper)"},{"id":"http://arxiv.org/abs/2408.03630v2","updated":"2024-08-08T01:19:46Z","published":"2024-08-07T08:43:18Z","title":"PAGED: A Benchmark for Procedural Graphs Extraction from Documents","summary":"  Automatic extraction of procedural graphs from documents creates a low-cost\nway for users to easily understand a complex procedure by skimming visual\ngraphs. Despite the progress in recent studies, it remains unanswered: whether\nthe existing studies have well solved this task (Q1) and whether the emerging\nlarge language models (LLMs) can bring new opportunities to this task (Q2). To\nthis end, we propose a new benchmark PAGED, equipped with a large high-quality\ndataset and standard evaluations. It investigates five state-of-the-art\nbaselines, revealing that they fail to extract optimal procedural graphs well\nbecause of their heavy reliance on hand-written rules and limited available\ndata. We further involve three advanced LLMs in PAGED and enhance them with a\nnovel self-refine strategy. The results point out the advantages of LLMs in\nidentifying textual elements and their gaps in building logical structures. We\nhope PAGED can serve as a major landmark for automatic procedural graph\nextraction and the investigations in PAGED can offer insights into the research\non logic reasoning among non-sequential elements.\n","authors":["Weihong Du","Wenrui Liao","Hongru Liang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2408.03630v2.pdf","comment":"Accepted to The 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2408.03633v2","updated":"2024-08-08T01:17:06Z","published":"2024-08-07T08:44:44Z","title":"CARE: A Clue-guided Assistant for CSRs to Read User Manuals","summary":"  It is time-saving to build a reading assistant for customer service\nrepresentations (CSRs) when reading user manuals, especially information-rich\nones. Current solutions don't fit the online custom service scenarios well due\nto the lack of attention to user questions and possible responses. Hence, we\npropose to develop a time-saving and careful reading assistant for CSRs, named\nCARE. It can help the CSRs quickly find proper responses from the user manuals\nvia explicit clue chains. Specifically, each of the clue chains is formed by\ninferring over the user manuals, starting from the question clue aligned with\nthe user question and ending at a possible response. To overcome the shortage\nof supervised data, we adopt the self-supervised strategy for model learning.\nThe offline experiment shows that CARE is efficient in automatically inferring\naccurate responses from the user manual. The online experiment further\ndemonstrates the superiority of CARE to reduce CSRs' reading burden and keep\nhigh service quality, in particular with >35% decrease in time spent and\nkeeping a >0.75 ICC score.\n","authors":["Weihong Du","Jia Liu","Zujie Wen","Dingnan Jin","Hongru Liang","Wenqiang Lei"],"pdf_url":"https://arxiv.org/pdf/2408.03633v2.pdf","comment":"Accepted to The 62nd Annual Meeting of the Association for\n  Computational Linguistics (ACL 2024)"},{"id":"http://arxiv.org/abs/2408.04140v1","updated":"2024-08-08T00:53:31Z","published":"2024-08-08T00:53:31Z","title":"UNLEARN Efficient Removal of Knowledge in Large Language Models","summary":"  Given the prevalence of large language models (LLMs) and the prohibitive cost\nof training these models from scratch, dynamically forgetting specific\nknowledge e.g., private or proprietary, without retraining the model has become\nan important capability. This paper proposes a novel method to achieve this\nobjective called UNLEARN. The approach builds upon subspace methods to identify\nand specifically target the removal of knowledge without adversely affecting\nother knowledge in the LLM. Results demonstrate 96% of targeted knowledge can\nbe forgotten while maintaining performance on other knowledge within 2.5% of\nthe original model, significantly outperforming the discriminatory abilities of\nthe previous state-of-the-art. A dual method called LEARN is also proposed for\ntargeted knowledge addition. Results show LEARN can match the fine-tuning\naccuracy of Low-Rank Adaptation (LoRA) without adversely affecting similar\ntasks.\n","authors":["Tyler Lizzo","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2408.04140v1.pdf","comment":"11 pages, 2 Figures"},{"id":"http://arxiv.org/abs/2408.04138v1","updated":"2024-08-08T00:35:39Z","published":"2024-08-08T00:35:39Z","title":"Enhancing Healthcare through Large Language Models: A Study on Medical\n  Question Answering","summary":"  In recent years, the application of Large Language Models (LLMs) in\nhealthcare has shown significant promise in improving the accessibility and\ndissemination of medical knowledge. This paper presents a detailed study of\nvarious LLMs trained on the MedQuAD medical question-answering dataset, with a\nfocus on identifying the most effective model for providing accurate medical\ninformation. Among the models tested, the Sentence-t5 combined with Mistral 7B\ndemonstrated superior performance, achieving a precision score of 0.762. This\nmodel's enhanced capabilities are attributed to its advanced pretraining\ntechniques, robust architecture, and effective prompt construction\nmethodologies. By leveraging these strengths, the Sentence-t5 + Mistral 7B\nmodel excels in understanding and generating precise medical answers. Our\nfindings highlight the potential of integrating sophisticated LLMs in medical\ncontexts to facilitate efficient and accurate medical knowledge retrieval, thus\nsignificantly enhancing patient education and support.\n","authors":["Haoran Yu","Chang Yu","Zihan Wang","Dongxian Zou","Hao Qin"],"pdf_url":"https://arxiv.org/pdf/2408.04138v1.pdf","comment":"received by IEEE ICPICS"},{"id":"http://arxiv.org/abs/2406.15786v4","updated":"2024-08-08T00:30:20Z","published":"2024-06-22T08:41:48Z","title":"What Matters in Transformers? Not All Attention is Needed","summary":"  Scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks. However, it also introduces\nredundant structures, posing challenges for real-world deployment. Despite some\nrecognition of redundancy in LLMs, the variability of redundancy across\ndifferent modules, such as MLP and Attention layers, is under-explored. In this\nwork, we investigate the varying redundancy across different modules within\nTransformers, including Blocks, MLP, and Attention layers, using a\nsimilarity-based metric. This metric operates on the premise that redundant\nstructures produce outputs highly similar to their inputs. Surprisingly, while\nattention layers are essential for transformers and distinguish them from other\nmainstream architectures, we found that a large proportion of attention layers\nexhibit excessively high similarity and can be safely pruned without degrading\nperformance, leading to reduced memory and computation costs. Additionally, we\nfurther propose a method that jointly drops Attention and MLP layers, achieving\nimproved performance and dropping ratios. Extensive experiments demonstrate the\neffectiveness of our methods, e.g., Llama-3-70B maintains comparable\nperformance even after pruning half of the attention layers. Our findings\nprovide valuable insights for future network architecture design. The code is\nreleased at: \\url{https://github.com/Shwai-He/LLM-Drop}.\n","authors":["Shwai He","Guoheng Sun","Zheyu Shen","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2406.15786v4.pdf","comment":"15 pages, 13 figures, 6 tables"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2408.04633v1","updated":"2024-08-08T17:59:58Z","published":"2024-08-08T17:59:58Z","title":"LiDAR-Event Stereo Fusion with Hallucinations","summary":"  Event stereo matching is an emerging technique to estimate depth from\nneuromorphic cameras; however, events are unlikely to trigger in the absence of\nmotion or the presence of large, untextured regions, making the correspondence\nproblem extremely challenging. Purposely, we propose integrating a stereo event\ncamera with a fixed-frequency active sensor -- e.g., a LiDAR -- collecting\nsparse depth measurements, overcoming the aforementioned limitations. Such\ndepth hints are used by hallucinating -- i.e., inserting fictitious events --\nthe stacks or raw input streams, compensating for the lack of information in\nthe absence of brightness changes. Our techniques are general, can be adapted\nto any structured representation to stack events and outperform\nstate-of-the-art fusion methods applied to event-based stereo.\n","authors":["Luca Bartolomei","Matteo Poggi","Andrea Conti","Stefano Mattoccia"],"pdf_url":"https://arxiv.org/pdf/2408.04633v1.pdf","comment":"ECCV 2024. Code: https://github.com/bartn8/eventvppstereo/ - Project\n  Page: https://eventvppstereo.github.io/"},{"id":"http://arxiv.org/abs/2408.04632v1","updated":"2024-08-08T17:59:46Z","published":"2024-08-08T17:59:46Z","title":"Arctic-TILT. Business Document Understanding at Sub-Billion Scale","summary":"  The vast portion of workloads employing LLMs involves answering questions\ngrounded on PDF or scan content. We introduce the Arctic-TILT achieving\naccuracy on par with models 1000$\\times$ its size on these use cases. It can be\nfine-tuned and deployed on a single 24GB GPU, lowering operational costs while\nprocessing Visually Rich Documents with up to 400k tokens. The model\nestablishes state-of-the-art results on seven diverse Document Understanding\nbenchmarks, as well as provides reliable confidence scores and quick inference,\nwhich are essential for processing files in large-scale or time-sensitive\nenterprise environments.\n","authors":["ukasz Borchmann","Micha Pietruszka","Wojciech Jakowski","Dawid Jurkiewicz","Piotr Halama","Pawe Jziak","ukasz Garncarek","Pawe Liskowski","Karolina Szyndler","Andrzej Gretkowski","Julita Otusek","Gabriela Nowakowska","Artur Zawocki","ukasz Duhr","Pawe Dyda","Micha Turski"],"pdf_url":"https://arxiv.org/pdf/2408.04632v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04631v1","updated":"2024-08-08T17:59:38Z","published":"2024-08-08T17:59:38Z","title":"Puppet-Master: Scaling Interactive Video Generation as a Motion Prior\n  for Part-Level Dynamics","summary":"  We present Puppet-Master, an interactive video generative model that can\nserve as a motion prior for part-level dynamics. At test time, given a single\nimage and a sparse set of motion trajectories (i.e., drags), Puppet-Master can\nsynthesize a video depicting realistic part-level motion faithful to the given\ndrag interactions. This is achieved by fine-tuning a large-scale pre-trained\nvideo diffusion model, for which we propose a new conditioning architecture to\ninject the dragging control effectively. More importantly, we introduce the\nall-to-first attention mechanism, a drop-in replacement for the widely adopted\nspatial attention modules, which significantly improves generation quality by\naddressing the appearance and background issues in existing models. Unlike\nother motion-conditioned video generators that are trained on in-the-wild\nvideos and mostly move an entire object, Puppet-Master is learned from\nObjaverse-Animation-HQ, a new dataset of curated part-level motion clips. We\npropose a strategy to automatically filter out sub-optimal animations and\naugment the synthetic renderings with meaningful motion trajectories.\nPuppet-Master generalizes well to real images across various categories and\noutperforms existing methods in a zero-shot manner on a real-world benchmark.\nSee our project page for more results: vgg-puppetmaster.github.io.\n","authors":["Ruining Li","Chuanxia Zheng","Christian Rupprecht","Andrea Vedaldi"],"pdf_url":"https://arxiv.org/pdf/2408.04631v1.pdf","comment":"Project page: https://vgg-puppetmaster.github.io/"},{"id":"http://arxiv.org/abs/2408.04628v1","updated":"2024-08-08T17:58:06Z","published":"2024-08-08T17:58:06Z","title":"LogogramNLP: Comparing Visual and Textual Representations of Ancient\n  Logographic Writing Systems for NLP","summary":"  Standard natural language processing (NLP) pipelines operate on symbolic\nrepresentations of language, which typically consist of sequences of discrete\ntokens. However, creating an analogous representation for ancient logographic\nwriting systems is an extremely labor intensive process that requires expert\nknowledge. At present, a large portion of logographic data persists in a purely\nvisual form due to the absence of transcription -- this issue poses a\nbottleneck for researchers seeking to apply NLP toolkits to study ancient\nlogographic languages: most of the relevant data are images of writing.\n  This paper investigates whether direct processing of visual representations\nof language offers a potential solution. We introduce LogogramNLP, the first\nbenchmark enabling NLP analysis of ancient logographic languages, featuring\nboth transcribed and visual datasets for four writing systems along with\nannotations for tasks like classification, translation, and parsing. Our\nexperiments compare systems that employ recent visual and text encoding\nstrategies as backbones. The results demonstrate that visual representations\noutperform textual representations for some investigated tasks, suggesting that\nvisual processing pipelines may unlock a large amount of cultural heritage data\nof logographic languages for NLP-based analyses.\n","authors":["Danlu Chen","Freda Shi","Aditi Agarwal","Jacobo Myerston","Taylor Berg-Kirkpatrick"],"pdf_url":"https://arxiv.org/pdf/2408.04628v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.07061v2","updated":"2024-08-08T17:52:16Z","published":"2024-01-13T12:32:29Z","title":"Dual-View Data Hallucination with Semantic Relation Guidance for\n  Few-Shot Image Recognition","summary":"  Learning to recognize novel concepts from just a few image samples is very\nchallenging as the learned model is easily overfitted on the few data and\nresults in poor generalizability. One promising but underexplored solution is\nto compensate the novel classes by generating plausible samples. However, most\nexisting works of this line exploit visual information only, rendering the\ngenerated data easy to be distracted by some challenging factors contained in\nthe few available samples. Being aware of the semantic information in the\ntextual modality that reflects human concepts, this work proposes a novel\nframework that exploits semantic relations to guide dual-view data\nhallucination for few-shot image recognition. The proposed framework enables\ngenerating more diverse and reasonable data samples for novel classes through\neffective information transfer from base classes. Specifically, an\ninstance-view data hallucination module hallucinates each sample of a novel\nclass to generate new data by employing local semantic correlated attention and\nglobal semantic feature fusion derived from base classes. Meanwhile, a\nprototype-view data hallucination module exploits semantic-aware measure to\nestimate the prototype of a novel class and the associated distribution from\nthe few samples, which thereby harvests the prototype as a more stable sample\nand enables resampling a large number of samples. We conduct extensive\nexperiments and comparisons with state-of-the-art methods on several popular\nfew-shot benchmarks to verify the effectiveness of the proposed framework.\n","authors":["Hefeng Wu","Guangzhi Ye","Ziyang Zhou","Ling Tian","Qing Wang","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2401.07061v2.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2408.04610v1","updated":"2024-08-08T17:28:32Z","published":"2024-08-08T17:28:32Z","title":"Quantifying the Impact of Population Shift Across Age and Sex for\n  Abdominal Organ Segmentation","summary":"  Deep learning-based medical image segmentation has seen tremendous progress\nover the last decade, but there is still relatively little transfer into\nclinical practice. One of the main barriers is the challenge of domain\ngeneralisation, which requires segmentation models to maintain high performance\nacross a wide distribution of image data. This challenge is amplified by the\nmany factors that contribute to the diverse appearance of medical images, such\nas acquisition conditions and patient characteristics. The impact of shifting\npatient characteristics such as age and sex on segmentation performance remains\nrelatively under-studied, especially for abdominal organs, despite that this is\ncrucial for ensuring the fairness of the segmentation model. We perform the\nfirst study to determine the impact of population shift with respect to age and\nsex on abdominal CT image segmentation, by leveraging two large public\ndatasets, and introduce a novel metric to quantify the impact. We find that\npopulation shift is a challenge similar in magnitude to cross-dataset shift for\nabdominal organ segmentation, and that the effect is asymmetric and\ndataset-dependent. We conclude that dataset diversity in terms of known patient\ncharacteristics is not necessarily equivalent to dataset diversity in terms of\nimage features. This implies that simple population matching to ensure good\ngeneralisation and fairness may be insufficient, and we recommend that fairness\nresearch should be directed towards better understanding and quantifying\nmedical image dataset diversity in terms of performance-relevant\ncharacteristics such as organ morphology.\n","authors":["Kate evora","Ben Glocker","Wenjia Bai"],"pdf_url":"https://arxiv.org/pdf/2408.04610v1.pdf","comment":"This paper has been accepted for publication by the MICCAI 2024\n  Fairness of AI in Medical Imaging (FAIMI) Workshop"},{"id":"http://arxiv.org/abs/2408.04606v1","updated":"2024-08-08T17:26:56Z","published":"2024-08-08T17:26:56Z","title":"Enhanced Prototypical Part Network (EPPNet) For Explainable Image\n  Classification Via Prototypes","summary":"  Explainable Artificial Intelligence (xAI) has the potential to enhance the\ntransparency and trust of AI-based systems. Although accurate predictions can\nbe made using Deep Neural Networks (DNNs), the process used to arrive at such\npredictions is usually hard to explain. In terms of perceptibly human-friendly\nrepresentations, such as word phrases in text or super-pixels in images,\nprototype-based explanations can justify a model's decision. In this work, we\nintroduce a DNN architecture for image classification, the Enhanced\nPrototypical Part Network (EPPNet), which achieves strong performance while\ndiscovering relevant prototypes that can be used to explain the classification\nresults. This is achieved by introducing a novel cluster loss that helps to\ndiscover more relevant human-understandable prototypes. We also introduce a\nfaithfulness score to evaluate the explainability of the results based on the\ndiscovered prototypes. Our score not only accounts for the relevance of the\nlearned prototypes but also the performance of a model. Our evaluations on the\nCUB-200-2011 dataset show that the EPPNet outperforms state-of-the-art\nxAI-based methods, in terms of both classification accuracy and explainability\n","authors":["Bhushan Atote","Victor Sanchez"],"pdf_url":"https://arxiv.org/pdf/2408.04606v1.pdf","comment":"Accepted at the International Conference on Image Processing (ICIP),\n  IEEE (2024), we will update the new version after published through IEEE"},{"id":"http://arxiv.org/abs/2408.04605v1","updated":"2024-08-08T17:24:54Z","published":"2024-08-08T17:24:54Z","title":"Fall Detection for Industrial Setups Using YOLOv8 Variants","summary":"  This paper presents the development of an industrial fall detection system\nutilizing YOLOv8 variants, enhanced by our proposed augmentation pipeline to\nincrease dataset variance and improve detection accuracy. Among the models\nevaluated, the YOLOv8m model, consisting of 25.9 million parameters and 79.1\nGFLOPs, demonstrated a respectable balance between computational efficiency and\ndetection performance, achieving a mean Average Precision (mAP) of 0.971 at 50%\nIntersection over Union (IoU) across both \"Fall Detected\" and \"Human in Motion\"\ncategories. Although the YOLOv8l and YOLOv8x models presented higher precision\nand recall, particularly in fall detection, their higher computational demands\nand model size make them less suitable for resource-constrained environments.\n","authors":["Gracile Astlin Pereira"],"pdf_url":"https://arxiv.org/pdf/2408.04605v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04604v1","updated":"2024-08-08T17:24:03Z","published":"2024-08-08T17:24:03Z","title":"Towards High-resolution 3D Anomaly Detection via Group-Level Feature\n  Contrastive Learning","summary":"  High-resolution point clouds~(HRPCD) anomaly detection~(AD) plays a critical\nrole in precision machining and high-end equipment manufacturing. Despite\nconsiderable 3D-AD methods that have been proposed recently, they still cannot\nmeet the requirements of the HRPCD-AD task. There are several challenges: i) It\nis difficult to directly capture HRPCD information due to large amounts of\npoints at the sample level; ii) The advanced transformer-based methods usually\nobtain anisotropic features, leading to degradation of the representation; iii)\nThe proportion of abnormal areas is very small, which makes it difficult to\ncharacterize. To address these challenges, we propose a novel group-level\nfeature-based network, called Group3AD, which has a significantly efficient\nrepresentation ability. First, we design an Intercluster Uniformity\nNetwork~(IUN) to present the mapping of different groups in the feature space\nas several clusters, and obtain a more uniform distribution between clusters\nrepresenting different parts of the point clouds in the feature space. Then, an\nIntracluster Alignment Network~(IAN) is designed to encourage groups within the\ncluster to be distributed tightly in the feature space. In addition, we propose\nan Adaptive Group-Center Selection~(AGCS) based on geometric information to\nimprove the pixel density of potential anomalous regions during inference. The\nexperimental results verify the effectiveness of our proposed Group3AD, which\nsurpasses Reg3D-AD by the margin of 5\\% in terms of object-level AUROC on\nReal3D-AD. We provide the code and supplementary information on our website:\nhttps://github.com/M-3LAB/Group3AD.\n","authors":["Hongze Zhu","Guoyang Xie","Chengbin Hou","Tao Dai","Can Gao","Jinbao Wang","Linlin Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04604v1.pdf","comment":"ACMMM24, 12 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04600v1","updated":"2024-08-08T17:20:08Z","published":"2024-08-08T17:20:08Z","title":"Improving Network Interpretability via Explanation Consistency\n  Evaluation","summary":"  While deep neural networks have achieved remarkable performance, they tend to\nlack transparency in prediction. The pursuit of greater interpretability in\nneural networks often results in a degradation of their original performance.\nSome works strive to improve both interpretability and performance, but they\nprimarily depend on meticulously imposed conditions. In this paper, we propose\na simple yet effective framework that acquires more explainable activation\nheatmaps and simultaneously increase the model performance, without the need\nfor any extra supervision. Specifically, our concise framework introduces a new\nmetric, i.e., explanation consistency, to reweight the training samples\nadaptively in model learning. The explanation consistency metric is utilized to\nmeasure the similarity between the model's visual explanations of the original\nsamples and those of semantic-preserved adversarial samples, whose background\nregions are perturbed by using image adversarial attack techniques. Our\nframework then promotes the model learning by paying closer attention to those\ntraining samples with a high difference in explanations (i.e., low explanation\nconsistency), for which the current model cannot provide robust\ninterpretations. Comprehensive experimental results on various benchmarks\ndemonstrate the superiority of our framework in multiple aspects, including\nhigher recognition accuracy, greater data debiasing capability, stronger\nnetwork robustness, and more precise localization ability on both regular\nnetworks and interpretable networks. We also provide extensive ablation studies\nand qualitative analyses to unveil the detailed contribution of each component.\n","authors":["Hefeng Wu","Hao Jiang","Keze Wang","Ziyi Tang","Xianghuan He","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2408.04600v1.pdf","comment":"To appear in IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2408.04594v1","updated":"2024-08-08T17:10:16Z","published":"2024-08-08T17:10:16Z","title":"Img-Diff: Contrastive Data Synthesis for Multimodal Large Language\n  Models","summary":"  High-performance Multimodal Large Language Models (MLLMs) rely heavily on\ndata quality. This study introduces a novel dataset named Img-Diff, designed to\nenhance fine-grained image recognition in MLLMs by leveraging insights from\ncontrastive learning and image difference captioning. By analyzing object\ndifferences between similar images, we challenge models to identify both\nmatching and distinct components. We utilize the Stable-Diffusion-XL model and\nadvanced image editing techniques to create pairs of similar images that\nhighlight object replacements. Our methodology includes a Difference Area\nGenerator for object differences identifying, followed by a Difference Captions\nGenerator for detailed difference descriptions. The result is a relatively\nsmall but high-quality dataset of \"object replacement\" samples. We use the the\nproposed dataset to fine-tune state-of-the-art (SOTA) MLLMs such as MGM-7B,\nyielding comprehensive improvements of performance scores over SOTA models that\ntrained with larger-scale datasets, in numerous image difference and Visual\nQuestion Answering tasks. For instance, our trained models notably surpass the\nSOTA models GPT-4V and Gemini on the MMVP benchmark. Besides, we investigate\nalternative methods for generating image difference data through \"object\nremoval\" and conduct thorough evaluation to confirm the dataset's diversity,\nquality, and robustness, presenting several insights on synthesis of such\ncontrastive dataset. To encourage further research and advance the field of\nmultimodal data synthesis and enhancement of MLLMs' fundamental capabilities\nfor image understanding, we release our codes and dataset at\nhttps://github.com/modelscope/data-juicer/tree/ImgDiff.\n","authors":["Qirui Jiao","Daoyuan Chen","Yilun Huang","Yaliang Li","Ying Shen"],"pdf_url":"https://arxiv.org/pdf/2408.04594v1.pdf","comment":"14 pages, 9 figures, 7 tables"},{"id":"http://arxiv.org/abs/2408.04593v1","updated":"2024-08-08T17:08:57Z","published":"2024-08-08T17:08:57Z","title":"SAM 2 in Robotic Surgery: An Empirical Evaluation for Robustness and\n  Generalization in Surgical Video Segmentation","summary":"  The recent Segment Anything Model (SAM) 2 has demonstrated remarkable\nfoundational competence in semantic segmentation, with its memory mechanism and\nmask decoder further addressing challenges in video tracking and object\nocclusion, thereby achieving superior results in interactive segmentation for\nboth images and videos. Building upon our previous empirical studies, we\nfurther explore the zero-shot segmentation performance of SAM 2 in\nrobot-assisted surgery based on prompts, alongside its robustness against\nreal-world corruption. For static images, we employ two forms of prompts:\n1-point and bounding box, while for video sequences, the 1-point prompt is\napplied to the initial frame. Through extensive experimentation on the MICCAI\nEndoVis 2017 and EndoVis 2018 benchmarks, SAM 2, when utilizing bounding box\nprompts, outperforms state-of-the-art (SOTA) methods in comparative\nevaluations. The results with point prompts also exhibit a substantial\nenhancement over SAM's capabilities, nearing or even surpassing existing\nunprompted SOTA methodologies. Besides, SAM 2 demonstrates improved inference\nspeed and less performance degradation against various image corruption.\nAlthough slightly unsatisfactory results remain in specific edges or regions,\nSAM 2's robust adaptability to 1-point prompts underscores its potential for\ndownstream surgical tasks with limited prompt requirements.\n","authors":["Jieming Yu","An Wang","Wenzhen Dong","Mengya Xu","Mobarakol Islam","Jie Wang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.04593v1.pdf","comment":"Empirical study. Previous work \"SAM Meets Robotic Surgery\" is\n  accessible at: arXiv:2308.07156"},{"id":"http://arxiv.org/abs/2408.04591v1","updated":"2024-08-08T17:04:06Z","published":"2024-08-08T17:04:06Z","title":"HiLo: A Learning Framework for Generalized Category Discovery Robust to\n  Domain Shifts","summary":"  Generalized Category Discovery (GCD) is a challenging task in which, given a\npartially labelled dataset, models must categorize all unlabelled instances,\nregardless of whether they come from labelled categories or from new ones. In\nthis paper, we challenge a remaining assumption in this task: that all images\nshare the same domain. Specifically, we introduce a new task and method to\nhandle GCD when the unlabelled data also contains images from different domains\nto the labelled set. Our proposed `HiLo' networks extract High-level semantic\nand Low-level domain features, before minimizing the mutual information between\nthe representations. Our intuition is that the clusterings based on domain\ninformation and semantic information should be independent. We further extend\nour method with a specialized domain augmentation tailored for the GCD task, as\nwell as a curriculum learning approach. Finally, we construct a benchmark from\ncorrupted fine-grained datasets as well as a large-scale evaluation on\nDomainNet with real-world domain shifts, reimplementing a number of GCD\nbaselines in this setting. We demonstrate that HiLo outperforms SoTA category\ndiscovery models by a large margin on all evaluations.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2408.04591v1.pdf","comment":"39 pages, 9 figures, 26 tables"},{"id":"http://arxiv.org/abs/2408.04586v1","updated":"2024-08-08T16:56:03Z","published":"2024-08-08T16:56:03Z","title":"Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond","summary":"  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n","authors":["Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2408.04586v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2024"},{"id":"http://arxiv.org/abs/2312.02647v2","updated":"2024-08-08T16:47:26Z","published":"2023-12-05T10:39:37Z","title":"TPA3D: Triplane Attention for Fast Text-to-3D Generation","summary":"  Due to the lack of large-scale text-3D correspondence data, recent text-to-3D\ngeneration works mainly rely on utilizing 2D diffusion models for synthesizing\n3D data. Since diffusion-based methods typically require significant\noptimization time for both training and inference, the use of GAN-based models\nwould still be desirable for fast 3D generation. In this work, we propose\nTriplane Attention for text-guided 3D generation (TPA3D), an end-to-end\ntrainable GAN-based deep learning model for fast text-to-3D generation. With\nonly 3D shape data and their rendered 2D images observed during training, our\nTPA3D is designed to retrieve detailed visual descriptions for synthesizing the\ncorresponding 3D mesh data. This is achieved by the proposed attention\nmechanisms on the extracted sentence and word-level text features. In our\nexperiments, we show that TPA3D generates high-quality 3D textured shapes\naligned with fine-grained descriptions, while impressive computation efficiency\ncan be observed.\n","authors":["Bin-Shih Wu","Hong-En Chen","Sheng-Yu Huang","Yu-Chiang Frank Wang"],"pdf_url":"https://arxiv.org/pdf/2312.02647v2.pdf","comment":"ECCV2024"},{"id":"http://arxiv.org/abs/2408.04579v1","updated":"2024-08-08T16:40:15Z","published":"2024-08-08T16:40:15Z","title":"SAM2-Adapter: Evaluating & Adapting Segment Anything 2 in Downstream\n  Tasks: Camouflage, Shadow, Medical Image Segmentation, and More","summary":"  The advent of large models, also known as foundation models, has\nsignificantly transformed the AI research landscape, with models like Segment\nAnything (SAM) achieving notable success in diverse image segmentation\nscenarios. Despite its advancements, SAM encountered limitations in handling\nsome complex low-level segmentation tasks like camouflaged object and medical\nimaging. In response, in 2023, we introduced SAM-Adapter, which demonstrated\nimproved performance on these challenging tasks. Now, with the release of\nSegment Anything 2 (SAM2), a successor with enhanced architecture and a larger\ntraining corpus, we reassess these challenges. This paper introduces\nSAM2-Adapter, the first adapter designed to overcome the persistent limitations\nobserved in SAM2 and achieve new state-of-the-art (SOTA) results in specific\ndownstream tasks including medical image segmentation, camouflaged (concealed)\nobject detection, and shadow detection. SAM2-Adapter builds on the\nSAM-Adapter's strengths, offering enhanced generalizability and composability\nfor diverse applications. We present extensive experimental results\ndemonstrating SAM2-Adapter's effectiveness. We show the potential and encourage\nthe research community to leverage the SAM2 model with our SAM2-Adapter for\nachieving superior segmentation outcomes. Code, pre-trained models, and data\nprocessing protocols are available at\nhttp://tianrun-chen.github.io/SAM-Adaptor/\n","authors":["Tianrun Chen","Ankang Lu","Lanyun Zhu","Chaotao Ding","Chunan Yu","Deyi Ji","Zejian Li","Lingyun Sun","Papa Mao","Ying Zang"],"pdf_url":"https://arxiv.org/pdf/2408.04579v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2304.09148"},{"id":"http://arxiv.org/abs/2408.04567v1","updated":"2024-08-08T16:27:37Z","published":"2024-08-08T16:27:37Z","title":"Sketch2Scene: Automatic Generation of Interactive 3D Game Scenes from\n  User's Casual Sketches","summary":"  3D Content Generation is at the heart of many computer graphics applications,\nincluding video gaming, film-making, virtual and augmented reality, etc. This\npaper proposes a novel deep-learning based approach for automatically\ngenerating interactive and playable 3D game scenes, all from the user's casual\nprompts such as a hand-drawn sketch. Sketch-based input offers a natural, and\nconvenient way to convey the user's design intention in the content creation\nprocess. To circumvent the data-deficient challenge in learning (i.e. the lack\nof large training data of 3D scenes), our method leverages a pre-trained 2D\ndenoising diffusion model to generate a 2D image of the scene as the conceptual\nguidance. In this process, we adopt the isometric projection mode to factor out\nunknown camera poses while obtaining the scene layout. From the generated\nisometric image, we use a pre-trained image understanding method to segment the\nimage into meaningful parts, such as off-ground objects, trees, and buildings,\nand extract the 2D scene layout. These segments and layouts are subsequently\nfed into a procedural content generation (PCG) engine, such as a 3D video game\nengine like Unity or Unreal, to create the 3D scene. The resulting 3D scene can\nbe seamlessly integrated into a game development environment and is readily\nplayable. Extensive tests demonstrate that our method can efficiently generate\nhigh-quality and interactive 3D game scenes with layouts that closely follow\nthe user's intention.\n","authors":["Yongzhi Xu","Yonhon Ng","Yifu Wang","Inkyu Sa","Yunfei Duan","Yang Li","Pan Ji","Hongdong Li"],"pdf_url":"https://arxiv.org/pdf/2408.04567v1.pdf","comment":"Project Page: https://xrvisionlabs.github.io/Sketch2Scene/"},{"id":"http://arxiv.org/abs/2307.02694v3","updated":"2024-08-08T16:24:52Z","published":"2023-07-05T23:53:55Z","title":"Loss Functions and Metrics in Deep Learning","summary":"  When training or evaluating deep learning models, two essential parts are\npicking the proper loss function and deciding on performance metrics. In this\npaper, we provide a comprehensive overview of the most common loss functions\nand metrics used across many different types of deep learning tasks, from\ngeneral tasks such as regression and classification to more specific tasks in\nComputer Vision and Natural Language Processing. We introduce the formula for\neach loss and metric, discuss their strengths and limitations, and describe how\nthese methods can be applied to various problems within deep learning. We hope\nthis work serves as a reference for researchers and practitioners in the field,\nhelping them make informed decisions when selecting the most appropriate loss\nfunction and performance metrics for their deep learning projects.\n","authors":["Juan Terven","Diana M. Cordova-Esparza","Alfonso Ramirez-Pedraza","Edgar A. Chavez-Urbiola","Julio A. Romero-Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2307.02694v3.pdf","comment":"76 pages, 4 figures, 13 tables, 127 equations"},{"id":"http://arxiv.org/abs/2407.14153v3","updated":"2024-08-08T16:20:02Z","published":"2024-07-19T09:32:30Z","title":"ESP-MedSAM: Efficient Self-Prompting SAM for Universal\n  Domain-Generalized Image Segmentation","summary":"  The universality of deep neural networks across different modalities and\ntheir generalization capabilities to unseen domains play an essential role in\nmedical image segmentation. The recent Segment Anything Model (SAM) has\ndemonstrated its potential in both settings. However, the huge computational\ncosts, demand for manual annotations as prompts and conflict-prone decoding\nprocess of SAM degrade its generalizability and applicability in clinical\nscenarios. To address these issues, we propose an efficient self-prompting SAM\nfor universal domain-generalized medical image segmentation, named ESP-MedSAM.\nSpecifically, we first devise the Multi-Modal Decoupled Knowledge Distillation\n(MMDKD) strategy to construct a lightweight semi-parameter sharing image\nencoder that produces discriminative visual features for diverse modalities.\nFurther, we introduce the Self-Patch Prompt Generator (SPPG) to automatically\ngenerate high-quality dense prompt embeddings for guiding segmentation\ndecoding. Finally, we design the Query-Decoupled Modality Decoder (QDMD) that\nleverages a one-to-one strategy to provide an independent decoding channel for\nevery modality. Extensive experiments indicate that ESP-MedSAM outperforms\nstate-of-the-arts in diverse medical imaging segmentation tasks, displaying\nsuperior modality universality and generalization capabilities. Especially,\nESP-MedSAM uses only 4.5\\% parameters compared to SAM-H. The source code is\navailable at https://github.com/xq141839/ESP-MedSAM.\n","authors":["Qing Xu","Jiaxuan Li","Xiangjian He","Ziyu Liu","Zhen Chen","Wenting Duan","Chenxin Li","Maggie M. He","Fiseha B. Tesema","Wooi P. Cheah","Yi Wang","Rong Qu","Jonathan M. Garibaldi"],"pdf_url":"https://arxiv.org/pdf/2407.14153v3.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2208.03561v2","updated":"2024-08-08T16:12:51Z","published":"2022-08-06T18:30:53Z","title":"Study of detecting behavioral signatures within DeepFake videos","summary":"  There is strong interest in the generation of synthetic video imagery of\npeople talking for various purposes, including entertainment, communication,\ntraining, and advertisement. With the development of deep fake generation\nmodels, synthetic video imagery will soon be visually indistinguishable to the\nnaked eye from a naturally capture video. In addition, many methods are\ncontinuing to improve to avoid more careful, forensic visual analysis. Some\ndeep fake videos are produced through the use of facial puppetry, which\ndirectly controls the head and face of the synthetic image through the\nmovements of the actor, allow the actor to 'puppet' the image of another. In\nthis paper, we address the question of whether one person's movements can be\ndistinguished from the original speaker by controlling the visual appearance of\nthe speaker but transferring the behavior signals from another source. We\nconduct a study by comparing synthetic imagery that: 1) originates from a\ndifferent person speaking a different utterance, 2) originates from the same\nperson speaking a different utterance, and 3) originates from a different\nperson speaking the same utterance. Our study shows that synthetic videos in\nall three cases are seen as less real and less engaging than the original\nsource video. Our results indicate that there could be a behavioral signature\nthat is detectable from a person's movements that is separate from their visual\nappearance, and that this behavioral signature could be used to distinguish a\ndeep fake from a properly captured video.\n","authors":["Qiaomu Miao","Sinhwa Kang","Stacy Marsella","Steve DiPaola","Chao Wang","Ari Shapiro"],"pdf_url":"https://arxiv.org/pdf/2208.03561v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2406.01561v3","updated":"2024-08-08T16:00:01Z","published":"2024-06-03T17:44:11Z","title":"Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation","summary":"  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n","authors":["Mingyuan Zhou","Zhendong Wang","Huangjie Zheng","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.01561v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/mingyuanzhou/SiD-LSG"},{"id":"http://arxiv.org/abs/2404.02282v3","updated":"2024-08-08T15:25:44Z","published":"2024-04-02T20:15:43Z","title":"Smooth Deep Saliency","summary":"  In this work, we investigate methods to reduce the noise in deep saliency\nmaps coming from convolutional downsampling. Those methods make the\ninvestigated models more interpretable for gradient-based saliency maps,\ncomputed in hidden layers. We evaluate the faithfulness of those methods using\ninsertion and deletion metrics, finding that saliency maps computed in hidden\nlayers perform better compared to both the input layer and GradCAM. We test our\napproach on different models trained for image classification on ImageNet1K,\nand models trained for tumor detection on Camelyon16 and in-house real-world\ndigital pathology scans of stained tissue samples. Our results show that the\ncheckerboard noise in the gradient gets reduced, resulting in smoother and\ntherefore easier to interpret saliency maps.\n","authors":["Rudolf Herdt","Maximilian Schmidt","Daniel Otero Baguer","Peter Maa"],"pdf_url":"https://arxiv.org/pdf/2404.02282v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04523v1","updated":"2024-08-08T15:24:07Z","published":"2024-08-08T15:24:07Z","title":"Depth Any Canopy: Leveraging Depth Foundation Models for Canopy Height\n  Estimation","summary":"  Estimating global tree canopy height is crucial for forest conservation and\nclimate change applications. However, capturing high-resolution ground truth\ncanopy height using LiDAR is expensive and not available globally. An efficient\nalternative is to train a canopy height estimator to operate on single-view\nremotely sensed imagery. The primary obstacle to this approach is that these\nmethods require significant training data to generalize well globally and\nacross uncommon edge cases. Recent monocular depth estimation foundation models\nhave show strong zero-shot performance even for complex scenes. In this paper\nwe leverage the representations learned by these models to transfer to the\nremote sensing domain for measuring canopy height. Our findings suggest that\nour proposed Depth Any Canopy, the result of fine-tuning the Depth Anything v2\nmodel for canopy height estimation, provides a performant and efficient\nsolution, surpassing the current state-of-the-art with superior or comparable\nperformance using only a fraction of the computational resources and\nparameters. Furthermore, our approach requires less than \\$1.30 in compute and\nresults in an estimated carbon footprint of 0.14 kgCO2. Code, experimental\nresults, and model checkpoints are openly available at\nhttps://github.com/DarthReca/depth-any-canopy.\n","authors":["Daniele Rege Cambrin","Isaac Corley","Paolo Garza"],"pdf_url":"https://arxiv.org/pdf/2408.04523v1.pdf","comment":"Accepted at ECCV 2024 CV4E Workshop"},{"id":"http://arxiv.org/abs/2408.04515v1","updated":"2024-08-08T15:15:48Z","published":"2024-08-08T15:15:48Z","title":"Saliency Detection in Educational Videos: Analyzing the Performance of\n  Current Models, Identifying Limitations and Advancement Directions","summary":"  Identifying the regions of a learning resource that a learner pays attention\nto is crucial for assessing the material's impact and improving its design and\nrelated support systems. Saliency detection in videos addresses the automatic\nrecognition of attention-drawing regions in single frames. In educational\nsettings, the recognition of pertinent regions in a video's visual stream can\nenhance content accessibility and information retrieval tasks such as video\nsegmentation, navigation, and summarization. Such advancements can pave the way\nfor the development of advanced AI-assisted technologies that support learning\nwith greater efficacy. However, this task becomes particularly challenging for\neducational videos due to the combination of unique characteristics such as\ntext, voice, illustrations, animations, and more. To the best of our knowledge,\nthere is currently no study that evaluates saliency detection approaches in\neducational videos. In this paper, we address this gap by evaluating four\nstate-of-the-art saliency detection approaches for educational videos. We\nreproduce the original studies and explore the replication capabilities for\ngeneral-purpose (non-educational) datasets. Then, we investigate the\ngeneralization capabilities of the models and evaluate their performance on\neducational videos. We conduct a comprehensive analysis to identify common\nfailure scenarios and possible areas of improvement. Our experimental results\nshow that educational videos remain a challenging context for generic video\nsaliency detection models.\n","authors":["Evelyn Navarrete","Ralph Ewerth","Anett Hoppe"],"pdf_url":"https://arxiv.org/pdf/2408.04515v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.16677v2","updated":"2024-08-08T15:02:13Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on quality measures at lower bitrates. We extensively\nevaluate transfer cost reduction by including the peculiarity of intermittently\navailable network connections in low earth orbit. Lastly, we test the\nfeasibility of our system for standardized nanosatellite form factors. We\ndemonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v2.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Revision 1"},{"id":"http://arxiv.org/abs/2408.04491v1","updated":"2024-08-08T14:41:32Z","published":"2024-08-08T14:41:32Z","title":"Towards Synergistic Deep Learning Models for Volumetric Cirrhotic Liver\n  Segmentation in MRIs","summary":"  Liver cirrhosis, a leading cause of global mortality, requires precise\nsegmentation of ROIs for effective disease monitoring and treatment planning.\nExisting segmentation models often fail to capture complex feature interactions\nand generalize across diverse datasets. To address these limitations, we\npropose a novel synergistic theory that leverages complementary latent spaces\nfor enhanced feature interaction modeling. Our proposed architecture,\nnnSynergyNet3D integrates continuous and discrete latent spaces for 3D volumes\nand features auto-configured training. This approach captures both fine-grained\nand coarse features, enabling effective modeling of intricate feature\ninteractions. We empirically validated nnSynergyNet3D on a private dataset of\n628 high-resolution T1 abdominal MRI scans from 339 patients. Our model\noutperformed the baseline nnUNet3D by approximately 2%. Additionally, zero-shot\ntesting on healthy liver CT scans from the public LiTS dataset demonstrated\nsuperior cross-modal generalization capabilities. These results highlight the\npotential of synergistic latent space models to improve segmentation accuracy\nand robustness, thereby enhancing clinical workflows by ensuring consistency\nacross CT and MRI modalities.\n","authors":["Vandan Gorade","Onkar Susladkar","Gorkem Durak","Elif Keles","Ertugrul Aktas","Timurhan Cebeci","Alpay Medetalibeyoglu","Daniela Ladner","Debesh Jha","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2408.04491v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.06657v3","updated":"2024-08-08T14:34:52Z","published":"2023-03-12T13:13:05Z","title":"Color Mismatches in Stereoscopic Video: Real-World Dataset and Deep\n  Correction Method","summary":"  Stereoscopic videos can contain color mismatches between the left and right\nviews due to minor variations in camera settings, lenses, and even object\nreflections captured from different positions. The presence of color mismatches\ncan lead to viewer discomfort and headaches. This problem can be solved by\ntransferring color between stereoscopic views, but traditional methods often\nlack quality, while neural-network-based methods can easily overfit on\nartificial data. The scarcity of stereoscopic videos with real-world color\nmismatches hinders the evaluation of different methods' performance. Therefore,\nwe filmed a video dataset, which includes both distorted frames with color\nmismatches and ground-truth data, using a beam-splitter. Our second\ncontribution is a deep multiscale neural network that solves the\ncolor-mismatch-correction task by leveraging stereo correspondences. The\nexperimental results demonstrate the effectiveness of the proposed method on a\nconventional dataset, but there remains room for improvement on challenging\nreal-world data.\n","authors":["Egor Chistov","Nikita Alutis","Dmitriy Vatolin"],"pdf_url":"https://arxiv.org/pdf/2303.06657v3.pdf","comment":"The code and datasets are at\n  https://github.com/egorchistov/color-transfer/"},{"id":"http://arxiv.org/abs/2407.15787v2","updated":"2024-08-08T14:33:12Z","published":"2024-07-22T16:47:29Z","title":"Unsupervised Mastoidectomy for Cochlear CT Mesh Reconstruction Using\n  Highly Noisy Data","summary":"  Cochlear Implant (CI) procedures involve inserting an array of electrodes\ninto the cochlea located inside the inner ear. Mastoidectomy is a surgical\nprocedure that uses a high-speed drill to remove part of the mastoid region of\nthe temporal bone, providing safe access to the cochlea through the middle and\ninner ear. We aim to develop an intraoperative navigation system that registers\nplans created using 3D preoperative Computerized Tomography (CT) volumes with\nthe 2D surgical microscope view. Herein, we propose a method to synthesize the\nmastoidectomy volume using only the preoperative CT scan, where the mastoid is\nintact. We introduce an unsupervised learning framework designed to synthesize\nmastoidectomy. For model training purposes, this method uses postoperative CT\nscans to avoid manual data cleaning or labeling, even when the region removed\nduring mastoidectomy is visible but affected by metal artifacts, low\nsignal-to-noise ratio, or electrode wiring. Our approach estimates\nmastoidectomy regions with a mean dice score of 70.0%. This approach represents\na major step forward for CI intraoperative navigation by predicting realistic\nmastoidectomy-removed regions in preoperative planning that can be used to\nregister the pre-surgery plan to intraoperative microscopy.\n","authors":["Yike Zhang","Dingjie Su","Eduardo Davalos","Jack H. Noble"],"pdf_url":"https://arxiv.org/pdf/2407.15787v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12904v3","updated":"2024-08-08T14:21:35Z","published":"2023-10-19T16:57:49Z","title":"Unsupervised Object Localization in the Era of Self-Supervised ViTs: A\n  Survey","summary":"  The recent enthusiasm for open-world vision systems show the high interest of\nthe community to perform perception tasks outside of the closed-vocabulary\nbenchmark setups which have been so popular until now. Being able to discover\nobjects in images/videos without knowing in advance what objects populate the\ndataset is an exciting prospect. But how to find objects without knowing\nanything about them? Recent works show that it is possible to perform\nclass-agnostic unsupervised object localization by exploiting self-supervised\npre-trained features. We propose here a survey of unsupervised object\nlocalization methods that discover objects in images without requiring any\nmanual annotation in the era of self-supervised ViTs. We gather links of\ndiscussed methods in the repository\nhttps://github.com/valeoai/Awesome-Unsupervised-Object-Localization.\n","authors":["Oriane Simoni","loi Zablocki","Spyros Gidaris","Gilles Puy","Patrick Prez"],"pdf_url":"https://arxiv.org/pdf/2310.12904v3.pdf","comment":"IJCV 2024"},{"id":"http://arxiv.org/abs/2408.04482v1","updated":"2024-08-08T14:19:11Z","published":"2024-08-08T14:19:11Z","title":"SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios","summary":"  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n","authors":["Sriram Mandalika","Athira Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.04482v1.pdf","comment":"17 pages, 7 figures. To appear in the proceedings of the 27th\n  International Conference on Pattern Recognition (ICPR), 01-05 December, 2024,\n  Kolkata, India"},{"id":"http://arxiv.org/abs/2408.04471v1","updated":"2024-08-08T14:01:12Z","published":"2024-08-08T14:01:12Z","title":"What could go wrong? Discovering and describing failure modes in\n  computer vision","summary":"  Deep learning models are effective, yet brittle. Even carefully trained,\ntheir behavior tends to be hard to predict when confronted with\nout-of-distribution samples. In this work, our goal is to propose a simple yet\neffective solution to predict and describe via natural language potential\nfailure modes of computer vision models. Given a pretrained model and a set of\nsamples, our aim is to find sentences that accurately describe the visual\nconditions in which the model underperforms. In order to study this important\ntopic and foster future research on it, we formalize the problem of\nLanguage-Based Error Explainability (LBEE) and propose a set of metrics to\nevaluate and compare different methods for this task. We propose solutions that\noperate in a joint vision-and-language embedding space, and can characterize\nthrough language descriptions model failures caused, e.g., by objects unseen\nduring training or adverse visual conditions. We experiment with different\ntasks, such as classification under the presence of dataset bias and semantic\nsegmentation in unseen environments, and show that the proposed methodology\nisolates nontrivial sentences associated with specific error causes. We hope\nour work will help practitioners better understand the behavior of models,\nincreasing their overall safety and interpretability.\n","authors":["Gabriela Csurka","Tyler L. Hayes","Diane Larlus","Riccardo Volpi"],"pdf_url":"https://arxiv.org/pdf/2408.04471v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07128v2","updated":"2024-08-08T13:58:51Z","published":"2023-12-12T10:04:11Z","title":"MS-Twins: Multi-Scale Deep Self-Attention Networks for Medical Image\n  Segmentation","summary":"  Chest X-ray is one of the most common radiological examination types for the\ndiagnosis of chest diseases. Nowadays, the automatic classification technology\nof radiological images has been widely used in clinical diagnosis and treatment\nplans. However, each disease has its own different response characteristic\nreceptive field region, which is the main challenge for chest disease\nclassification tasks. Besides, the imbalance of sample data categories further\nincreases the difficulty of tasks. To solve these problems, we propose a new\nmulti-label chest disease image classification scheme based on a multi-scale\nattention network. In this scheme, multi-scale information is iteratively fused\nto focus on regions with a high probability of disease, to effectively mine\nmore meaningful information from data, and the classification performance can\nbe improved only by image level annotation. We also designed a new loss\nfunction to improve the rationality of visual perception and the performance of\nmulti-label image classification by forcing the consistency of attention\nregions before and after image transformation. A comprehensive experiment was\ncarried out on the public Chest X-Ray14 and CheXpert datasets to achieve state\nof the art results, which verified the effectiveness of this method in chest\nX-ray image classification.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2312.07128v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.12539v3","updated":"2024-08-08T13:57:35Z","published":"2023-11-21T11:33:15Z","title":"GMISeg: General Medical Image Segmentation without Re-Training","summary":"  The online shopping behavior has the characteristics of rich granularity\ndimension and data sparsity and previous researches on user behavior prediction\ndid not seriously discuss feature selection and ensemble design. In this paper,\nwe proposed a SE-Stacking model based on information fusion and ensemble\nlearning for user purchase behavior prediction. After successfully utilizing\nthe ensemble feature selection method to screen purchase-related factors, we\nused the Stacking algorithm for user purchase behavior prediction. In our\nefforts to avoid the deviation of prediction results, we optimized the model by\nselecting ten different kinds of models as base learners and modifying relevant\nparameters specifically for them. The experiments conducted on a\npublicly-available dataset shows that the SE-Stacking model can achieve a\n98.40% F1-score, about 0.09% higher than the optimal base models. The\nSE-Stacking model not only has a good application in the prediction of user\npurchase behavior but also has practical value combining with the actual\ne-commerce scene. At the same time, it has important significance for academic\nresearch and the development of this field.\n","authors":["Jing Xu"],"pdf_url":"https://arxiv.org/pdf/2311.12539v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12487v2","updated":"2024-08-08T13:40:06Z","published":"2024-05-21T04:10:26Z","title":"3DSS-Mamba: 3D-Spectral-Spatial Mamba for Hyperspectral Image\n  Classification","summary":"  Hyperspectral image (HSI) classification constitutes the fundamental research\nin remote sensing fields. Convolutional Neural Networks (CNNs) and Transformers\nhave demonstrated impressive capability in capturing spectral-spatial\ncontextual dependencies. However, these architectures suffer from limited\nreceptive fields and quadratic computational complexity, respectively.\nFortunately, recent Mamba architectures built upon the State Space Model\nintegrate the advantages of long-range sequence modeling and linear\ncomputational efficiency, exhibiting substantial potential in low-dimensional\nscenarios. Motivated by this, we propose a novel 3D-Spectral-Spatial Mamba\n(3DSS-Mamba) framework for HSI classification, allowing for global\nspectral-spatial relationship modeling with greater computational efficiency.\nTechnically, a spectral-spatial token generation (SSTG) module is designed to\nconvert the HSI cube into a set of 3D spectral-spatial tokens. To overcome the\nlimitations of traditional Mamba, which is confined to modeling causal\nsequences and inadaptable to high-dimensional scenarios, a 3D-Spectral-Spatial\nSelective Scanning (3DSS) mechanism is introduced, which performs pixel-wise\nselective scanning on 3D hyperspectral tokens along the spectral and spatial\ndimensions. Five scanning routes are constructed to investigate the impact of\ndimension prioritization. The 3DSS scanning mechanism combined with\nconventional mapping operations forms the 3D-spectral-spatial mamba block\n(3DMB), enabling the extraction of global spectral-spatial semantic\nrepresentations. Experimental results and analysis demonstrate that the\nproposed method outperforms the state-of-the-art methods on HSI classification\nbenchmarks.\n","authors":["Yan He","Bing Tu","Bo Liu","Jun Li","Antonio Plaza"],"pdf_url":"https://arxiv.org/pdf/2405.12487v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.03166v4","updated":"2024-08-08T13:32:21Z","published":"2024-02-05T16:35:29Z","title":"RRWNet: Recursive Refinement Network for Effective Retinal Artery/Vein\n  Segmentation and Classification","summary":"  The caliber and configuration of retinal blood vessels serve as important\nbiomarkers for various diseases and medical conditions. A thorough analysis of\nthe retinal vasculature requires the segmentation of the blood vessels and\ntheir classification into arteries and veins, typically performed on color\nfundus images obtained by retinography. However, manually performing these\ntasks is labor-intensive and prone to human error. While several automated\nmethods have been proposed to address this task, the current state of art faces\nchallenges due to manifest classification errors affecting the topological\nconsistency of segmentation maps. In this work, we introduce RRWNet, a novel\nend-to-end deep learning framework that addresses this limitation. The\nframework consists of a fully convolutional neural network that recursively\nrefines semantic segmentation maps, correcting manifest classification errors\nand thus improving topological consistency. In particular, RRWNet is composed\nof two specialized subnetworks: a Base subnetwork that generates base\nsegmentation maps from the input images, and a Recursive Refinement subnetwork\nthat iteratively and recursively improves these maps. Evaluation on three\ndifferent public datasets demonstrates the state-of-the-art performance of the\nproposed method, yielding more topologically consistent segmentation maps with\nfewer manifest classification errors than existing approaches. In addition, the\nRecursive Refinement module within RRWNet proves effective in post-processing\nsegmentation maps from other methods, further demonstrating its potential. The\nmodel code, weights, and predictions will be publicly available at\nhttps://github.com/j-morano/rrwnet.\n","authors":["Jos Morano","Guilherme Aresta","Hrvoje Bogunovi"],"pdf_url":"https://arxiv.org/pdf/2402.03166v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04439v1","updated":"2024-08-08T13:10:03Z","published":"2024-08-08T13:10:03Z","title":"Deep Learning for identifying systolic complexes in SCG traces: a\n  cross-dataset analysis","summary":"  The seismocardiographic signal is a promising alternative to the traditional\nECG in the analysis of the cardiac activity. In particular, the systolic\ncomplex is known to be the most informative part of the seismocardiogram, thus\nrequiring further analysis. State-of-art solutions to detect the systolic\ncomplex are based on Deep Learning models, which have been proven effective in\npioneering studies. However, these solutions have only been tested in a\ncontrolled scenario considering only clean signals acquired from users\nmaintained still in supine position. On top of that, all these studies consider\ndata coming from a single dataset, ignoring the benefits and challenges related\nto a cross-dataset scenario. In this work, a cross-dataset experimental\nanalysis was performed considering also data from a real-world scenario. Our\nfindings prove the effectiveness of a deep learning solution, while showing the\nimportance of a personalization step to contrast the domain shift, namely a\nchange in data distribution between training and testing data. Finally, we\ndemonstrate the benefits of a multi-channels approach, leveraging the\ninformation extracted from both accelerometers and gyroscopes data.\n","authors":["Michele Craighero","Sarah Solbiati","Federica Mozzini","Enrico Caiani","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2408.04439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04426v1","updated":"2024-08-08T12:51:23Z","published":"2024-08-08T12:51:23Z","title":"A Review of 3D Reconstruction Techniques for Deformable Tissues in\n  Robotic Surgery","summary":"  As a crucial and intricate task in robotic minimally invasive surgery,\nreconstructing surgical scenes using stereo or monocular endoscopic video holds\nimmense potential for clinical applications. NeRF-based techniques have\nrecently garnered attention for the ability to reconstruct scenes implicitly.\nOn the other hand, Gaussian splatting-based 3D-GS represents scenes explicitly\nusing 3D Gaussians and projects them onto a 2D plane as a replacement for the\ncomplex volume rendering in NeRF. However, these methods face challenges\nregarding surgical scene reconstruction, such as slow inference, dynamic\nscenes, and surgical tool occlusion. This work explores and reviews\nstate-of-the-art (SOTA) approaches, discussing their innovations and\nimplementation principles. Furthermore, we replicate the models and conduct\ntesting and evaluation on two datasets. The test results demonstrate that with\nadvancements in these techniques, achieving real-time, high-quality\nreconstructions becomes feasible.\n","authors":["Mengya Xu","Ziqi Guo","An Wang","Long Bai","Hongliang Ren"],"pdf_url":"https://arxiv.org/pdf/2408.04426v1.pdf","comment":"To appear in MICCAI 2024 EARTH Workshop. Code availability:\n  https://github.com/Epsilon404/surgicalnerf"},{"id":"http://arxiv.org/abs/2406.18284v2","updated":"2024-08-08T12:18:30Z","published":"2024-06-26T12:09:59Z","title":"RealTalk: Real-time and Realistic Audio-driven Face Generation with 3D\n  Facial Prior-guided Identity Alignment Network","summary":"  Person-generic audio-driven face generation is a challenging task in computer\nvision. Previous methods have achieved remarkable progress in audio-visual\nsynchronization, but there is still a significant gap between current results\nand practical applications. The challenges are two-fold: 1) Preserving unique\nindividual traits for achieving high-precision lip synchronization. 2)\nGenerating high-quality facial renderings in real-time performance. In this\npaper, we propose a novel generalized audio-driven framework RealTalk, which\nconsists of an audio-to-expression transformer and a high-fidelity\nexpression-to-face renderer. In the first component, we consider both identity\nand intra-personal variation features related to speaking lip movements. By\nincorporating cross-modal attention on the enriched facial priors, we can\neffectively align lip movements with audio, thus attaining greater precision in\nexpression prediction. In the second component, we design a lightweight facial\nidentity alignment (FIA) module which includes a lip-shape control structure\nand a face texture reference structure. This novel design allows us to generate\nfine details in real-time, without depending on sophisticated and inefficient\nfeature alignment modules. Our experimental results, both quantitative and\nqualitative, on public datasets demonstrate the clear advantages of our method\nin terms of lip-speech synchronization and generation quality. Furthermore, our\nmethod is efficient and requires fewer computational resources, making it\nwell-suited to meet the needs of practical applications.\n","authors":["Xiaozhong Ji","Chuming Lin","Zhonggan Ding","Ying Tai","Junwei Zhu","Xiaobin Hu","Donghao Luo","Yanhao Ge","Chengjie Wang"],"pdf_url":"https://arxiv.org/pdf/2406.18284v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04407v1","updated":"2024-08-08T12:16:14Z","published":"2024-08-08T12:16:14Z","title":"Clutter Classification Using Deep Learning in Multiple Stages","summary":"  Path loss prediction for wireless communications is highly dependent on the\nlocal environment. Propagation models including clutter information have been\nshown to significantly increase model accuracy. This paper explores the\napplication of deep learning to satellite imagery to identify environmental\nclutter types automatically. Recognizing these clutter types has numerous uses,\nbut our main application is to use clutter information to enhance propagation\nprediction models. Knowing the type of obstruction (tree, building, and further\nclassifications) can improve the prediction accuracy of key propagation metrics\nsuch as path loss.\n","authors":["Ryan Dempsey","Jonathan Ethier"],"pdf_url":"https://arxiv.org/pdf/2408.04407v1.pdf","comment":"SoutheastCon 2024"},{"id":"http://arxiv.org/abs/2403.08214v2","updated":"2024-08-08T11:51:15Z","published":"2024-03-13T03:23:50Z","title":"P2LHAP:Wearable sensor-based human activity recognition, segmentation\n  and forecast through Patch-to-Label Seq2Seq Transformer","summary":"  Traditional deep learning methods struggle to simultaneously segment,\nrecognize, and forecast human activities from sensor data. This limits their\nusefulness in many fields such as healthcare and assisted living, where\nreal-time understanding of ongoing and upcoming activities is crucial. This\npaper introduces P2LHAP, a novel Patch-to-Label Seq2Seq framework that tackles\nall three tasks in a efficient single-task model. P2LHAP divides sensor data\nstreams into a sequence of \"patches\", served as input tokens, and outputs a\nsequence of patch-level activity labels including the predicted future\nactivities. A unique smoothing technique based on surrounding patch labels, is\nproposed to identify activity boundaries accurately. Additionally, P2LHAP\nlearns patch-level representation by sensor signal channel-independent\nTransformer encoders and decoders. All channels share embedding and Transformer\nweights across all sequences. Evaluated on three public datasets, P2LHAP\nsignificantly outperforms the state-of-the-art in all three tasks,\ndemonstrating its effectiveness and potential for real-world applications.\n","authors":["Shuangjian Li","Tao Zhu","Mingxing Nie","Huansheng Ning","Zhenyu Liu","Liming Chen"],"pdf_url":"https://arxiv.org/pdf/2403.08214v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.09630v2","updated":"2024-08-08T11:38:21Z","published":"2024-03-14T17:58:33Z","title":"GenAD: Generalized Predictive Model for Autonomous Driving","summary":"  In this paper, we introduce the first large-scale video prediction model in\nthe autonomous driving discipline. To eliminate the restriction of high-cost\ndata collection and empower the generalization ability of our model, we acquire\nmassive data from the web and pair it with diverse and high-quality text\ndescriptions. The resultant dataset accumulates over 2000 hours of driving\nvideos, spanning areas all over the world with diverse weather conditions and\ntraffic scenarios. Inheriting the merits from recent latent diffusion models,\nour model, dubbed GenAD, handles the challenging dynamics in driving scenes\nwith novel temporal reasoning blocks. We showcase that it can generalize to\nvarious unseen driving datasets in a zero-shot manner, surpassing general or\ndriving-specific video prediction counterparts. Furthermore, GenAD can be\nadapted into an action-conditioned prediction model or a motion planner,\nholding great potential for real-world driving applications.\n","authors":["Jiazhi Yang","Shenyuan Gao","Yihang Qiu","Li Chen","Tianyu Li","Bo Dai","Kashyap Chitta","Penghao Wu","Jia Zeng","Ping Luo","Jun Zhang","Andreas Geiger","Yu Qiao","Hongyang Li"],"pdf_url":"https://arxiv.org/pdf/2403.09630v2.pdf","comment":"CVPR 2024 Highlight Paper. Dataset:\n  https://github.com/OpenDriveLab/DriveAGI"},{"id":"http://arxiv.org/abs/2408.01669v3","updated":"2024-08-08T11:19:37Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v3.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2408.04367v1","updated":"2024-08-08T10:55:55Z","published":"2024-08-08T10:55:55Z","title":"MultiViPerFrOG: A Globally Optimized Multi-Viewpoint Perception\n  Framework for Camera Motion and Tissue Deformation","summary":"  Reconstructing the 3D shape of a deformable environment from the information\ncaptured by a moving depth camera is highly relevant to surgery. The underlying\nchallenge is the fact that simultaneously estimating camera motion and tissue\ndeformation in a fully deformable scene is an ill-posed problem, especially\nfrom a single arbitrarily moving viewpoint. Current solutions are often\norgan-specific and lack the robustness required to handle large deformations.\nHere we propose a multi-viewpoint global optimization framework that can\nflexibly integrate the output of low-level perception modules (data\nassociation, depth, and relative scene flow) with kinematic and scene-modeling\npriors to jointly estimate multiple camera motions and absolute scene flow. We\nuse simulated noisy data to show three practical examples that successfully\nconstrain the convergence to a unique solution. Overall, our method shows\nrobustness to combined noisy input measures and can process hundreds of points\nin a few milliseconds. MultiViPerFrOG builds a generalized learning-free\nscaffolding for spatio-temporal encoding that can unlock advanced surgical\nscene representations and will facilitate the development of the\ncomputer-assisted-surgery technologies of the future.\n","authors":["Guido Caccianiga","Julian Nubert","Cesar Cadena","Marco Hutter","Katherine J. Kuchenbecker"],"pdf_url":"https://arxiv.org/pdf/2408.04367v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04360v1","updated":"2024-08-08T10:47:02Z","published":"2024-08-08T10:47:02Z","title":"Detecting Car Speed using Object Detection and Depth Estimation: A Deep\n  Learning Framework","summary":"  Road accidents are quite common in almost every part of the world, and, in\nmajority, fatal accidents are attributed to over speeding of vehicles. The\ntendency to over speeding is usually tried to be controlled using check points\nat various parts of the road but not all traffic police have the device to\ncheck speed with existing speed estimating devices such as LIDAR based, or\nRadar based guns. The current project tries to address the issue of vehicle\nspeed estimation with handheld devices such as mobile phones or wearable\ncameras with network connection to estimate the speed using deep learning\nframeworks.\n","authors":["Subhasis Dasgupta","Arshi Naaz","Jayeeta Choudhury","Nancy Lahiri"],"pdf_url":"https://arxiv.org/pdf/2408.04360v1.pdf","comment":"This is the pre-print of the paper which was accepted for oral\n  presentation and publication in the proceedings of IEEE CONIT 2024, organized\n  at Pune from June 21 to 23, 2024. The paper is 6 pages long and it contains\n  11 figures and 1 table. This is not the final version of the paper"},{"id":"http://arxiv.org/abs/2408.04347v1","updated":"2024-08-08T10:16:02Z","published":"2024-08-08T10:16:02Z","title":"AggSS: An Aggregated Self-Supervised Approach for Class-Incremental\n  Learning","summary":"  This paper investigates the impact of self-supervised learning, specifically\nimage rotations, on various class-incremental learning paradigms. Here, each\nimage with a predefined rotation is considered as a new class for training. At\ninference, all image rotation predictions are aggregated for the final\nprediction, a strategy we term Aggregated Self-Supervision (AggSS). We observe\na shift in the deep neural network's attention towards intrinsic object\nfeatures as it learns through AggSS strategy. This learning approach\nsignificantly enhances class-incremental learning by promoting robust feature\nlearning. AggSS serves as a plug-and-play module that can be seamlessly\nincorporated into any class-incremental learning framework, leveraging its\npowerful feature learning capabilities to enhance performance across various\nclass-incremental learning approaches. Extensive experiments conducted on\nstandard incremental learning datasets CIFAR-100 and ImageNet-Subset\ndemonstrate the significant role of AggSS in improving performance within these\nparadigms.\n","authors":["Jayateja Kalla","Soma Biswas"],"pdf_url":"https://arxiv.org/pdf/2408.04347v1.pdf","comment":"Accepted in BMVC 2024"},{"id":"http://arxiv.org/abs/2407.06704v2","updated":"2024-08-08T09:41:40Z","published":"2024-07-09T09:31:15Z","title":"Self-supervised visual learning from interactions with objects","summary":"  Self-supervised learning (SSL) has revolutionized visual representation\nlearning, but has not achieved the robustness of human vision. A reason for\nthis could be that SSL does not leverage all the data available to humans\nduring learning. When learning about an object, humans often purposefully turn\nor move around objects and research suggests that these interactions can\nsubstantially enhance their learning. Here we explore whether such\nobject-related actions can boost SSL. For this, we extract the actions\nperformed to change from one ego-centric view of an object to another in four\nvideo datasets. We then introduce a new loss function to learn visual and\naction embeddings by aligning the performed action with the representations of\ntwo images extracted from the same clip. This permits the performed actions to\nstructure the latent visual representation. Our experiments show that our\nmethod consistently outperforms previous methods on downstream category\nrecognition. In our analysis, we find that the observed improvement is\nassociated with a better viewpoint-wise alignment of different objects from the\nsame category. Overall, our work demonstrates that embodied interactions with\nobjects can improve SSL of object categories.\n","authors":["Arthur Aubret","Cline Teulire","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2407.06704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.08995v2","updated":"2024-08-08T09:40:29Z","published":"2023-03-15T23:59:18Z","title":"Fast and Accurate Object Detection on Asymmetrical Receptive Field","summary":"  Object detection has been used in a wide range of industries. For example, in\nautonomous driving, the task of object detection is to accurately and\nefficiently identify and locate a large number of predefined classes of object\ninstances (vehicles, pedestrians, traffic signs, etc.) from videos of roads. In\nrobotics, the industry robot needs to recognize specific machine elements. In\nthe security field, the camera should accurately recognize each face of people.\nWith the wide application of deep learning, the accuracy and efficiency of\nobject detection have been greatly improved, but object detection based on deep\nlearning still faces challenges. Different applications of object detection\nhave different requirements, including highly accurate detection,\nmulti-category object detection, real-time detection, robustness to occlusions,\netc. To address the above challenges, based on extensive literature research,\nthis paper analyzes methods for improving and optimizing mainstream object\ndetection algorithms from the perspective of evolution of one-stage and\ntwo-stage object detection algorithms. Furthermore, this article proposes\nmethods for improving object detection accuracy from the perspective of\nchanging receptive fields. The new model is based on the original YOLOv5 (You\nLook Only Once) with some modifications. The structure of the head part of\nYOLOv5 is modified by adding asymmetrical pooling layers. As a result, the\naccuracy of the algorithm is improved while ensuring the speed. The\nperformances of the new model in this article are compared with original YOLOv5\nmodel and analyzed from several parameters. And the evaluation of the new model\nis presented in four situations. Moreover, the summary and outlooks are made on\nthe problems to be solved and the research directions in the future.\n","authors":["Tianhao Lin"],"pdf_url":"https://arxiv.org/pdf/2303.08995v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.20183v3","updated":"2024-08-08T09:40:10Z","published":"2024-03-29T13:57:46Z","title":"HARMamba: Efficient and Lightweight Wearable Sensor Human Activity\n  Recognition Based on Bidirectional Mamba","summary":"  Wearable sensor-based human activity recognition (HAR) is a critical research\ndomain in activity perception. However, achieving high efficiency and long\nsequence recognition remains a challenge. Despite the extensive investigation\nof temporal deep learning models, such as CNNs, RNNs, and transformers, their\nextensive parameters often pose significant computational and memory\nconstraints, rendering them less suitable for resource-constrained mobile\nhealth applications. This study introduces HARMamba, an innovative light-weight\nand versatile HAR architecture that combines selective bidirectional State\nSpaces Model and hardware-aware design. To optimize real-time resource\nconsumption in practical scenarios, HARMamba employs linear recursive\nmechanisms and parameter discretization, allowing it to selectively focus on\nrelevant input sequences while efficiently fusing scan and recompute\noperations. The model employs independent channels to process sensor data\nstreams, dividing each channel into patches and appending classification tokens\nto the end of the sequence. It utilizes position embedding to represent the\nsequence order. The patch sequence is subsequently processed by HARMamba Block,\nand the classification head finally outputs the activity category. The HARMamba\nBlock serves as the fundamental component of the HARMamba architecture,\nenabling the effective capture of more discriminative activity sequence\nfeatures. HARMamba outperforms contemporary state-of-the-art frameworks,\ndelivering comparable or better accuracy with significantly reducing\ncomputational and memory demands. It's effectiveness has been extensively\nvalidated on 4 publically available datasets namely PAMAP2, WISDM, UNIMIB SHAR\nand UCI. The F1 scores of HARMamba on the four datasets are 99.74%, 99.20%,\n88.23% and 97.01%, respectively.\n","authors":["Shuangjian Li","Tao Zhu","Furong Duan","Liming Chen","Huansheng Ning","Christopher Nugent","Yaping Wan"],"pdf_url":"https://arxiv.org/pdf/2403.20183v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04331v1","updated":"2024-08-08T09:31:24Z","published":"2024-08-08T09:31:24Z","title":"Enhancing Journalism with AI: A Study of Contextualized Image Captioning\n  for News Articles using LLMs and LMMs","summary":"  Large language models (LLMs) and large multimodal models (LMMs) have\nsignificantly impacted the AI community, industry, and various economic\nsectors. In journalism, integrating AI poses unique challenges and\nopportunities, particularly in enhancing the quality and efficiency of news\nreporting. This study explores how LLMs and LMMs can assist journalistic\npractice by generating contextualised captions for images accompanying news\narticles. We conducted experiments using the GoodNews dataset to evaluate the\nability of LMMs (BLIP-2, GPT-4v, or LLaVA) to incorporate one of two types of\ncontext: entire news articles, or extracted named entities. In addition, we\ncompared their performance to a two-stage pipeline composed of a captioning\nmodel (BLIP-2, OFA, or ViT-GPT2) with post-hoc contextualisation with LLMs\n(GPT-4 or LLaMA). We assess a diversity of models, and we find that while the\nchoice of contextualisation model is a significant factor for the two-stage\npipelines, this is not the case in the LMMs, where smaller, open-source models\nperform well compared to proprietary, GPT-powered ones. Additionally, we found\nthat controlling the amount of provided context enhances performance. These\nresults highlight the limitations of a fully automated approach and underscore\nthe necessity for an interactive, human-in-the-loop strategy.\n","authors":["Aliki Anagnostopoulou","Thiago Gouvea","Daniel Sonntag"],"pdf_url":"https://arxiv.org/pdf/2408.04331v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.08389v3","updated":"2024-08-08T09:28:22Z","published":"2023-05-15T07:12:19Z","title":"Edit As You Wish: Video Caption Editing with Multi-grained User Control","summary":"  Automatically narrating videos in natural language complying with user\nrequests, i.e. Controllable Video Captioning task, can help people manage\nmassive videos with desired intentions. However, existing works suffer from two\nshortcomings: 1) the control signal is single-grained which can not satisfy\ndiverse user intentions; 2) the video description is generated in a single\nround which can not be further edited to meet dynamic needs. In this paper, we\npropose a novel \\textbf{V}ideo \\textbf{C}aption \\textbf{E}diting \\textbf{(VCE)}\ntask to automatically revise an existing video description guided by\nmulti-grained user requests. Inspired by human writing-revision habits, we\ndesign the user command as a pivotal triplet \\{\\textit{operation, position,\nattribute}\\} to cover diverse user needs from coarse-grained to fine-grained.\nTo facilitate the VCE task, we \\textit{automatically} construct an open-domain\nbenchmark dataset named VATEX-EDIT and \\textit{manually} collect an e-commerce\ndataset called EMMAD-EDIT. We further propose a specialized small-scale model\n(i.e., OPA) compared with two generalist Large Multi-modal Models to perform an\nexhaustive analysis of the novel task. For evaluation, we adopt comprehensive\nmetrics considering caption fluency, command-caption consistency, and\nvideo-caption alignment. Experiments reveal the task challenges of fine-grained\nmulti-modal semantics understanding and processing. Our datasets, codes, and\nevaluation tools are available at https://github.com/yaolinli/VCE.\n","authors":["Linli Yao","Yuanmeng Zhang","Ziheng Wang","Xinglin Hou","Tiezheng Ge","Yuning Jiang","Xu Sun","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2305.08389v3.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.04326v1","updated":"2024-08-08T09:09:37Z","published":"2024-08-08T09:09:37Z","title":"Multi-Scale and Detail-Enhanced Segment Anything Model for Salient\n  Object Detection","summary":"  Salient Object Detection (SOD) aims to identify and segment the most\nprominent objects in images. Advanced SOD methods often utilize various\nConvolutional Neural Networks (CNN) or Transformers for deep feature\nextraction. However, these methods still deliver low performance and poor\ngeneralization in complex cases. Recently, Segment Anything Model (SAM) has\nbeen proposed as a visual fundamental model, which gives strong segmentation\nand generalization capabilities. Nonetheless, SAM requires accurate prompts of\ntarget objects, which are unavailable in SOD. Additionally, SAM lacks the\nutilization of multi-scale and multi-level information, as well as the\nincorporation of fine-grained details. To address these shortcomings, we\npropose a Multi-scale and Detail-enhanced SAM (MDSAM) for SOD. Specifically, we\nfirst introduce a Lightweight Multi-Scale Adapter (LMSA), which allows SAM to\nlearn multi-scale information with very few trainable parameters. Then, we\npropose a Multi-Level Fusion Module (MLFM) to comprehensively utilize the\nmulti-level information from the SAM's encoder. Finally, we propose a Detail\nEnhancement Module (DEM) to incorporate SAM with fine-grained details.\nExperimental results demonstrate the superior performance of our model on\nmultiple SOD datasets and its strong generalization on other segmentation\ntasks. The source code is released at https://github.com/BellyBeauty/MDSAM.\n","authors":["Shixuan Gao","Pingping Zhang","Tianyu Yan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.04326v1.pdf","comment":"This work is accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2408.04318v1","updated":"2024-08-08T08:52:29Z","published":"2024-08-08T08:52:29Z","title":"Deep Transfer Learning for Kidney Cancer Diagnosis","summary":"  Many incurable diseases prevalent across global societies stem from various\ninfluences, including lifestyle choices, economic conditions, social factors,\nand genetics. Research predominantly focuses on these diseases due to their\nwidespread nature, aiming to decrease mortality, enhance treatment options, and\nimprove healthcare standards. Among these, kidney disease stands out as a\nparticularly severe condition affecting men and women worldwide. Nonetheless,\nthere is a pressing need for continued research into innovative, early\ndiagnostic methods to develop more effective treatments for such diseases.\nRecently, automatic diagnosis of Kidney Cancer has become an important\nchallenge especially when using deep learning (DL) due to the importance of\ntraining medical datasets, which in most cases are difficult and expensive to\nobtain. Furthermore, in most cases, algorithms require data from the same\ndomain and a powerful computer with efficient storage capacity. To overcome\nthis issue, a new type of learning known as transfer learning (TL) has been\nproposed that can produce impressive results based on other different\npre-trained data. This paper presents, to the best of the authors' knowledge,\nthe first comprehensive survey of DL-based TL frameworks for kidney cancer\ndiagnosis. This is a strong contribution to help researchers understand the\ncurrent challenges and perspectives of this topic. Hence, the main limitations\nand advantages of each framework are identified and detailed critical analyses\nare provided. Looking ahead, the article identifies promising directions for\nfuture research. Moving on, the discussion is concluded by reflecting on the\npivotal role of TL in the development of precision medicine and its effects on\nclinical practice and research in oncology.\n","authors":["Yassine Habchi","Hamza Kheddar","Yassine Himeur","Abdelkrim Boukabou","Shadi Atalla","Wathiq Mansoor","Hussain Al-Ahmad"],"pdf_url":"https://arxiv.org/pdf/2408.04318v1.pdf","comment":"32 pages, 8 figures and 8 tables"},{"id":"http://arxiv.org/abs/2403.11868v7","updated":"2024-08-08T08:45:08Z","published":"2024-03-18T15:22:09Z","title":"View-Consistent 3D Editing with Gaussian Splatting","summary":"  The advent of 3D Gaussian Splatting (3DGS) has revolutionized 3D editing,\noffering efficient, high-fidelity rendering and enabling precise local\nmanipulations. Currently, diffusion-based 2D editing models are harnessed to\nmodify multi-view rendered images, which then guide the editing of 3DGS models.\nHowever, this approach faces a critical issue of multi-view inconsistency,\nwhere the guidance images exhibit significant discrepancies across views,\nleading to mode collapse and visual artifacts of 3DGS. To this end, we\nintroduce View-consistent Editing (VcEdit), a novel framework that seamlessly\nincorporates 3DGS into image editing processes, ensuring multi-view consistency\nin edited guidance images and effectively mitigating mode collapse issues.\nVcEdit employs two innovative consistency modules: the Cross-attention\nConsistency Module and the Editing Consistency Module, both designed to reduce\ninconsistencies in edited images. By incorporating these consistency modules\ninto an iterative pattern, VcEdit proficiently resolves the issue of multi-view\ninconsistency, facilitating high-quality 3DGS editing across a diverse range of\nscenes. Further video results are shown in http://vcedit.github.io.\n","authors":["Yuxuan Wang","Xuanyu Yi","Zike Wu","Na Zhao","Long Chen","Hanwang Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.11868v7.pdf","comment":"accepted to ECCV 2024"},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2401.10110v4","updated":"2024-08-08T08:42:31Z","published":"2024-01-18T16:27:09Z","title":"SVIPTR: Fast and Efficient Scene Text Recognition with Vision Permutable\n  Extractor","summary":"  Scene Text Recognition (STR) is an important and challenging upstream task\nfor building structured information databases, that involves recognizing text\nwithin images of natural scenes. Although current state-of-the-art (SOTA)\nmodels for STR exhibit high performance, they typically suffer from low\ninference efficiency due to their reliance on hybrid architectures comprised of\nvisual encoders and sequence decoders. In this work, we propose a VIsion\nPermutable extractor for fast and efficient Scene Text Recognition (SVIPTR),\nwhich achieves an impressive balance between high performance and rapid\ninference speeds in the domain of STR. Specifically, SVIPTR leverages a\nvisual-semantic extractor with a pyramid structure, characterized by the\nPermutation and combination of local and global self-attention layers. This\ndesign results in a lightweight and efficient model and its inference is\ninsensitive to input length. Extensive experimental results on various standard\ndatasets for both Chinese and English scene text recognition validate the\nsuperiority of SVIPTR. Notably, the SVIPTR-T (Tiny) variant delivers highly\ncompetitive accuracy on par with other lightweight models and achieves SOTA\ninference speeds. Meanwhile, the SVIPTR-L (Large) attains SOTA accuracy in\nsingle-encoder-type models, while maintaining a low parameter count and\nfavorable inference speed. Our proposed method provides a compelling solution\nfor the STR challenge, which greatly benefits real-world applications requiring\nfast and efficient STR. The code is publicly available at\nhttps://github.com/cxfyxl/VIPTR.\n","authors":["Xianfu Cheng","Weixiao Zhou","Xiang Li","Jian Yang","Hang Zhang","Tao Sun","Wei Zhang","Yuying Mai","Tongliang Li","Xiaoming Chen","Zhoujun Li"],"pdf_url":"https://arxiv.org/pdf/2401.10110v4.pdf","comment":"10 pages, 4 figures, 6 tables"},{"id":"http://arxiv.org/abs/2408.04300v1","updated":"2024-08-08T08:35:21Z","published":"2024-08-08T08:35:21Z","title":"An Explainable Non-local Network for COVID-19 Diagnosis","summary":"  The CNN has achieved excellent results in the automatic classification of\nmedical images. In this study, we propose a novel deep residual 3D attention\nnon-local network (NL-RAN) to classify CT images included COVID-19, common\npneumonia, and normal to perform rapid and explainable COVID-19 diagnosis. We\nbuilt a deep residual 3D attention non-local network that could achieve\nend-to-end training. The network is embedded with a nonlocal module to capture\nglobal information, while a 3D attention module is embedded to focus on the\ndetails of the lesion so that it can directly analyze the 3D lung CT and output\nthe classification results. The output of the attention module can be used as a\nheat map to increase the interpretability of the model. 4079 3D CT scans were\nincluded in this study. Each scan had a unique label (novel coronavirus\npneumonia, common pneumonia, and normal). The CT scans cohort was randomly\nsplit into a training set of 3263 scans, a validation set of 408 scans, and a\ntesting set of 408 scans. And compare with existing mainstream classification\nmethods, such as CovNet, CBAM, ResNet, etc. Simultaneously compare the\nvisualization results with visualization methods such as CAM. Model performance\nwas evaluated using the Area Under the ROC Curve(AUC), precision, and F1-score.\nThe NL-RAN achieved the AUC of 0.9903, the precision of 0.9473, and the\nF1-score of 0.9462, surpass all the classification methods compared. The heat\nmap output by the attention module is also clearer than the heat map output by\nCAM. Our experimental results indicate that our proposed method performs\nsignificantly better than existing methods. In addition, the first attention\nmodule outputs a heat map containing detailed outline information to increase\nthe interpretability of the model. Our experiments indicate that the inference\nof our model is fast. It can provide real-time assistance with diagnosis.\n","authors":["Jingfu Yang","Peng Huang","Jing Hu","Shu Hu","Siwei Lyu","Xin Wang","Jun Guo","Xi Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04300v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04299v1","updated":"2024-08-08T08:25:38Z","published":"2024-08-08T08:25:38Z","title":"Respiratory Subtraction for Pulmonary Microwave Ablation Evaluation","summary":"  Currently, lung cancer is a leading cause of global cancer mortality, often\nnecessitating minimally invasive interventions. Microwave ablation (MWA) is\nextensively utilized for both primary and secondary lung tumors. Although\nnumerous clinical guidelines and standards for MWA have been established, the\nclinical evaluation of ablation surgery remains challenging and requires\nlong-term patient follow-up for confirmation. In this paper, we propose a\nmethod termed respiratory subtraction to evaluate lung tumor ablation therapy\nperformance based on pre- and post-operative image guidance. Initially,\npreoperative images undergo coarse rigid registration to their corresponding\npostoperative positions, followed by further non-rigid registration.\nSubsequently, subtraction images are generated by subtracting the registered\npreoperative images from the postoperative ones. Furthermore, to enhance the\nclinical assessment of MWA treatment performance, we devise a quantitative\nanalysis metric to evaluate ablation efficacy by comparing differences between\ntumor areas and treatment areas. To the best of our knowledge, this is the\npioneering work in the field to facilitate the assessment of MWA surgery\nperformance on pulmonary tumors. Extensive experiments involving 35 clinical\ncases further validate the efficacy of the respiratory subtraction method. The\nexperimental results confirm the effectiveness of the respiratory subtraction\nmethod and the proposed quantitative evaluation metric in assessing lung tumor\ntreatment.\n","authors":["Wan Li","Xinyun Zhong","Wei Li","Song Zhang","Moheng Rong","Yan Xi","Peng Yuan","Zechen Wang","Xiaolei Jiang","Rongxi Yi","Hui Tang","Yang Chen","Chaohui Tong","Zhan Wu","Feng Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04299v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04294v1","updated":"2024-08-08T08:17:50Z","published":"2024-08-08T08:17:50Z","title":"Dual-branch PolSAR Image Classification Based on GraphMAE and Local\n  Feature Extraction","summary":"  The annotation of polarimetric synthetic aperture radar (PolSAR) images is a\nlabor-intensive and time-consuming process. Therefore, classifying PolSAR\nimages with limited labels is a challenging task in remote sensing domain. In\nrecent years, self-supervised learning approaches have proven effective in\nPolSAR image classification with sparse labels. However, we observe a lack of\nresearch on generative selfsupervised learning in the studied task. Motivated\nby this, we propose a dual-branch classification model based on generative\nself-supervised learning in this paper. The first branch is a\nsuperpixel-branch, which learns superpixel-level polarimetric representations\nusing a generative self-supervised graph masked autoencoder. To acquire finer\nclassification results, a convolutional neural networks-based pixel-branch is\nfurther incorporated to learn pixel-level features. Classification with fused\ndual-branch features is finally performed to obtain the predictions.\nExperimental results on the benchmark Flevoland dataset demonstrate that our\napproach yields promising classification results.\n","authors":["Yuchen Wang","Ziyi Guo","Haixia Bi","Danfeng Hong","Chen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04290v1","updated":"2024-08-08T08:06:42Z","published":"2024-08-08T08:06:42Z","title":"Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale\n  Transformer Approach","summary":"  Pneumonia, a severe respiratory disease, poses significant diagnostic\nchallenges, especially in underdeveloped regions. Traditional diagnostic\nmethods, such as chest X-rays, suffer from variability in interpretation among\nradiologists, necessitating reliable automated tools. In this study, we propose\na novel approach combining deep learning and transformer-based attention\nmechanisms to enhance pneumonia detection from chest X-rays. Our method begins\nwith lung segmentation using a TransUNet model that integrates our specialized\ntransformer module, which has fewer parameters compared to common transformers\nwhile maintaining performance. This model is trained on the \"Chest Xray Masks\nand Labels\" dataset and then applied to the Kermany and Cohen datasets to\nisolate lung regions, enhancing subsequent classification tasks. For\nclassification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101)\nto extract multi-scale feature maps, processed through our modified transformer\nmodule. By employing our specialized transformer, we attain superior results\nwith significantly fewer parameters compared to common transformer models. Our\napproach achieves high accuracy rates of 92.79% on the Kermany dataset and\n95.11% on the Cohen dataset, ensuring robust and efficient performance suitable\nfor resource-constrained environments.\n\"https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia\"\n","authors":["Alireza Saber","Pouria Parhami","Alimihammad Siahkarzadeh","Amirreza Fateh"],"pdf_url":"https://arxiv.org/pdf/2408.04290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.04158v2","updated":"2024-08-08T07:51:10Z","published":"2024-06-06T15:18:59Z","title":"Sparse Multi-baseline SAR Cross-modal 3D Reconstruction of Vehicle\n  Targets","summary":"  Multi-baseline SAR 3D imaging faces significant challenges due to data\nsparsity. In recent years, deep learning techniques have achieved notable\nsuccess in enhancing the quality of sparse SAR 3D imaging. However, previous\nwork typically rely on full-aperture high-resolution radar images to supervise\nthe training of deep neural networks (DNNs), utilizing only single-modal\ninformation from radar data. Consequently, imaging performance is limited, and\nacquiring full-aperture data for multi-baseline SAR is costly and sometimes\nimpractical in real-world applications. In this paper, we propose a Cross-Modal\nReconstruction Network (CMR-Net), which integrates differentiable render and\ncross-modal supervision with optical images to reconstruct highly sparse\nmulti-baseline SAR 3D images of vehicle targets into visually structured and\nhigh-resolution images. We meticulously designed the network architecture and\ntraining strategies to enhance network generalization capability. Remarkably,\nCMR-Net, trained solely on simulated data, demonstrates high-resolution\nreconstruction capabilities on both publicly available simulation datasets and\nreal measured datasets, outperforming traditional sparse reconstruction\nalgorithms based on compressed sensing and other learning-based methods.\nAdditionally, using optical images as supervision provides a cost-effective way\nto build training datasets, reducing the difficulty of method dissemination.\nOur work showcases the broad prospects of deep learning in multi-baseline SAR\n3D imaging and offers a novel path for researching radar imaging based on\ncross-modal learning theory.\n","authors":["Da Li","Guoqiang Zhao","Houjun Sun","Jiacheng Bao"],"pdf_url":"https://arxiv.org/pdf/2406.04158v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.09767v2","updated":"2024-08-08T07:44:14Z","published":"2023-12-15T13:15:42Z","title":"DreamTalk: When Emotional Talking Head Generation Meets Diffusion\n  Probabilistic Models","summary":"  Emotional talking head generation has attracted growing attention. Previous\nmethods, which are mainly GAN-based, still struggle to consistently produce\nsatisfactory results across diverse emotions and cannot conveniently specify\npersonalized emotions. In this work, we leverage powerful diffusion models to\naddress the issue and propose DreamTalk, a framework that employs meticulous\ndesign to unlock the potential of diffusion models in generating emotional\ntalking heads. Specifically, DreamTalk consists of three crucial components: a\ndenoising network, a style-aware lip expert, and a style predictor. The\ndiffusion-based denoising network can consistently synthesize high-quality\naudio-driven face motions across diverse emotions. To enhance lip-motion\naccuracy and emotional fullness, we introduce a style-aware lip expert that can\nguide lip-sync while preserving emotion intensity. To more conveniently specify\npersonalized emotions, a diffusion-based style predictor is utilized to predict\nthe personalized emotion directly from the audio, eliminating the need for\nextra emotion reference. By this means, DreamTalk can consistently generate\nvivid talking faces across diverse emotions and conveniently specify\npersonalized emotions. Extensive experiments validate DreamTalk's effectiveness\nand superiority. The code is available at\nhttps://github.com/ali-vilab/dreamtalk.\n","authors":["Yifeng Ma","Shiwei Zhang","Jiayu Wang","Xiang Wang","Yingya Zhang","Zhidong Deng"],"pdf_url":"https://arxiv.org/pdf/2312.09767v2.pdf","comment":"Project Page: https://dreamtalk-project.github.io"},{"id":"http://arxiv.org/abs/2408.04273v1","updated":"2024-08-08T07:14:57Z","published":"2024-08-08T07:14:57Z","title":"SG-JND: Semantic-Guided Just Noticeable Distortion Predictor For Image\n  Compression","summary":"  Just noticeable distortion (JND), representing the threshold of distortion in\nan image that is minimally perceptible to the human visual system (HVS), is\ncrucial for image compression algorithms to achieve a trade-off between\ntransmission bit rate and image quality. However, traditional JND prediction\nmethods only rely on pixel-level or sub-band level features, lacking the\nability to capture the impact of image content on JND. To bridge this gap, we\npropose a Semantic-Guided JND (SG-JND) network to leverage semantic information\nfor JND prediction. In particular, SG-JND consists of three essential modules:\nthe image preprocessing module extracts semantic-level patches from images, the\nfeature extraction module extracts multi-layer features by utilizing the\ncross-scale attention layers, and the JND prediction module regresses the\nextracted features into the final JND value. Experimental results show that\nSG-JND achieves the state-of-the-art performance on two publicly available JND\ndatasets, which demonstrates the effectiveness of SG-JND and highlight the\nsignificance of incorporating semantic information in JND assessment.\n","authors":["Linhan Cao","Wei Sun","Xiongkuo Min","Jun Jia","Zicheng Zhang","Zijian Chen","Yucheng Zhu","Lizhou Liu","Qiubo Chen","Jing Chen","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2408.04273v1.pdf","comment":"Accepted by ICIP 2024"},{"id":"http://arxiv.org/abs/2408.04268v1","updated":"2024-08-08T07:11:57Z","published":"2024-08-08T07:11:57Z","title":"Evaluating Modern Approaches in 3D Scene Reconstruction: NeRF vs\n  Gaussian-Based Methods","summary":"  Exploring the capabilities of Neural Radiance Fields (NeRF) and\nGaussian-based methods in the context of 3D scene reconstruction, this study\ncontrasts these modern approaches with traditional Simultaneous Localization\nand Mapping (SLAM) systems. Utilizing datasets such as Replica and ScanNet, we\nassess performance based on tracking accuracy, mapping fidelity, and view\nsynthesis. Findings reveal that NeRF excels in view synthesis, offering unique\ncapabilities in generating new perspectives from existing data, albeit at\nslower processing speeds. Conversely, Gaussian-based methods provide rapid\nprocessing and significant expressiveness but lack comprehensive scene\ncompletion. Enhanced by global optimization and loop closure techniques, newer\nmethods like NICE-SLAM and SplaTAM not only surpass older frameworks such as\nORB-SLAM2 in terms of robustness but also demonstrate superior performance in\ndynamic and complex environments. This comparative analysis bridges theoretical\nresearch with practical implications, shedding light on future developments in\nrobust 3D scene reconstruction across various real-world applications.\n","authors":["Yiming Zhou","Zixuan Zeng","Andi Chen","Xiaofan Zhou","Haowei Ni","Shiyao Zhang","Panfeng Li","Liangxi Liu","Mengyao Zheng","Xupeng Chen"],"pdf_url":"https://arxiv.org/pdf/2408.04268v1.pdf","comment":"Accepted by 2024 6th International Conference on Data-driven\n  Optimization of Complex Systems"},{"id":"http://arxiv.org/abs/2401.10373v2","updated":"2024-08-08T07:06:40Z","published":"2024-01-18T20:43:43Z","title":"Harmonized Spatial and Spectral Learning for Robust and Generalized\n  Medical Image Segmentation","summary":"  Deep learning has demonstrated remarkable achievements in medical image\nsegmentation. However, prevailing deep learning models struggle with poor\ngeneralization due to (i) intra-class variations, where the same class appears\ndifferently in different samples, and (ii) inter-class independence, resulting\nin difficulties capturing intricate relationships between distinct objects,\nleading to higher false negative cases. This paper presents a novel approach\nthat synergies spatial and spectral representations to enhance\ndomain-generalized medical image segmentation. We introduce the innovative\nSpectral Correlation Coefficient objective to improve the model's capacity to\ncapture middle-order features and contextual long-range dependencies. This\nobjective complements traditional spatial objectives by incorporating valuable\nspectral information. Extensive experiments reveal that optimizing this\nobjective with existing architectures like UNet and TransUNet significantly\nenhances generalization, interpretability, and noise robustness, producing more\nconfident predictions. For instance, in cardiac segmentation, we observe a 0.81\npp and 1.63 pp (pp = percentage point) improvement in DSC over UNet and\nTransUNet, respectively. Our interpretability study demonstrates that, in most\ntasks, objectives optimized with UNet outperform even TransUNet by introducing\nglobal contextual information alongside local details. These findings\nunderscore the versatility and effectiveness of our proposed method across\ndiverse imaging modalities and medical domains.\n","authors":["Vandan Gorade","Sparsh Mittal","Debesh Jha","Rekha Singhal","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2401.10373v2.pdf","comment":"Early Accepted at ICPR-2024 for Oral Presentation"},{"id":"http://arxiv.org/abs/2408.04262v1","updated":"2024-08-08T06:59:32Z","published":"2024-08-08T06:59:32Z","title":"CoBooM: Codebook Guided Bootstrapping for Medical Image Representation\n  Learning","summary":"  Self-supervised learning (SSL) has emerged as a promising paradigm for\nmedical image analysis by harnessing unannotated data. Despite their potential,\nthe existing SSL approaches overlook the high anatomical similarity inherent in\nmedical images. This makes it challenging for SSL methods to capture diverse\nsemantic content in medical images consistently. This work introduces a novel\nand generalized solution that implicitly exploits anatomical similarities by\nintegrating codebooks in SSL. The codebook serves as a concise and informative\ndictionary of visual patterns, which not only aids in capturing nuanced\nanatomical details but also facilitates the creation of robust and generalized\nfeature representations. In this context, we propose CoBooM, a novel framework\nfor self-supervised medical image learning by integrating continuous and\ndiscrete representations. The continuous component ensures the preservation of\nfine-grained details, while the discrete aspect facilitates coarse-grained\nfeature extraction through the structured embedding space. To understand the\neffectiveness of CoBooM, we conduct a comprehensive evaluation of various\nmedical datasets encompassing chest X-rays and fundus images. The experimental\nresults reveal a significant performance gain in classification and\nsegmentation tasks.\n","authors":["Azad Singh","Deepak Mishra"],"pdf_url":"https://arxiv.org/pdf/2408.04262v1.pdf","comment":"Accepted in MICCAI 2024"},{"id":"http://arxiv.org/abs/2408.04261v1","updated":"2024-08-08T06:58:48Z","published":"2024-08-08T06:58:48Z","title":"Unveiling Hidden Visual Information: A Reconstruction Attack Against\n  Adversarial Visual Information Hiding","summary":"  This paper investigates the security vulnerabilities of\nadversarial-example-based image encryption by executing data reconstruction\n(DR) attacks on encrypted images. A representative image encryption method is\nthe adversarial visual information hiding (AVIH), which uses type-I adversarial\nexample training to protect gallery datasets used in image recognition tasks.\nIn the AVIH method, the type-I adversarial example approach creates images that\nappear completely different but are still recognized by machines as the\noriginal ones. Additionally, the AVIH method can restore encrypted images to\ntheir original forms using a predefined private key generative model. For the\nbest security, assigning a unique key to each image is recommended; however,\nstorage limitations may necessitate some images sharing the same key model.\nThis raises a crucial security question for AVIH: How many images can safely\nshare the same key model without being compromised by a DR attack? To address\nthis question, we introduce a dual-strategy DR attack against the AVIH\nencryption method by incorporating (1) generative-adversarial loss and (2)\naugmented identity loss, which prevent DR from overfitting -- an issue akin to\nthat in machine learning. Our numerical results validate this approach through\nimage recognition and re-identification benchmarks, demonstrating that our\nstrategy can significantly enhance the quality of reconstructed images, thereby\nrequiring fewer key-sharing encrypted images. Our source code to reproduce our\nresults will be available soon.\n","authors":["Jonggyu Jang","Hyeonsu Lyu","Seongjin Hwang","Hyun Jong Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04261v1.pdf","comment":"12 pages"},{"id":"http://arxiv.org/abs/2408.04258v1","updated":"2024-08-08T06:56:33Z","published":"2024-08-08T06:56:33Z","title":"UHNet: An Ultra-Lightweight and High-Speed Edge Detection Network","summary":"  Edge detection is crucial in medical image processing, enabling precise\nextraction of structural information to support lesion identification and image\nanalysis. Traditional edge detection models typically rely on complex\nConvolutional Neural Networks and Vision Transformer architectures. Due to\ntheir numerous parameters and high computational demands, these models are\nlimited in their application on resource-constrained devices. This paper\npresents an ultra-lightweight edge detection model (UHNet), characterized by\nits minimal parameter count, rapid computation speed, negligible of\npre-training costs, and commendable performance. UHNet boasts impressive\nperformance metrics with 42.3k parameters, 166 FPS, and 0.79G FLOPs. By\nemploying an innovative feature extraction module and optimized residual\nconnection method, UHNet significantly reduces model complexity and\ncomputational requirements. Additionally, a lightweight feature fusion strategy\nis explored, enhancing detection accuracy. Experimental results on the BSDS500,\nNYUD, and BIPED datasets validate that UHNet achieves remarkable edge detection\nperformance while maintaining high efficiency. This work not only provides new\ninsights into the design of lightweight edge detection models but also\ndemonstrates the potential and application prospects of the UHNet model in\nengineering applications such as medical image processing. The codes are\navailable at https://github.com/stoneLi20cv/UHNet\n","authors":["Fuzhang Li","Chuan Lin"],"pdf_url":"https://arxiv.org/pdf/2408.04258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03030v2","updated":"2024-08-08T06:32:30Z","published":"2024-08-06T08:24:47Z","title":"Nighttime Pedestrian Detection Based on Fore-Background Contrast\n  Learning","summary":"  The significance of background information is frequently overlooked in\ncontemporary research concerning channel attention mechanisms. This study\naddresses the issue of suboptimal single-spectral nighttime pedestrian\ndetection performance under low-light conditions by incorporating background\ninformation into the channel attention mechanism. Despite numerous studies\nfocusing on the development of efficient channel attention mechanisms, the\nrelevance of background information has been largely disregarded. By adopting a\ncontrast learning approach, we reexamine channel attention with regard to\npedestrian objects and background information for nighttime pedestrian\ndetection, resulting in the proposed Fore-Background Contrast Attention (FBCA).\nFBCA possesses two primary attributes: (1) channel descriptors form remote\ndependencies with global spatial feature information; (2) the integration of\nbackground information enhances the distinction between channels concentrating\non low-light pedestrian features and those focusing on background information.\nConsequently, the acquired channel descriptors exhibit a higher semantic level\nand spatial accuracy. Experimental outcomes demonstrate that FBCA significantly\noutperforms existing methods in single-spectral nighttime pedestrian detection,\nachieving state-of-the-art results on the NightOwls and TJU-DHD-pedestrian\ndatasets. Furthermore, this methodology also yields performance improvements\nfor the multispectral LLVIP dataset. These findings indicate that integrating\nbackground information into the channel attention mechanism effectively\nmitigates detector performance degradation caused by illumination factors in\nnighttime scenarios.\n","authors":["He Yao","Yongjun Zhang","Huachun Jian","Li Zhang","Ruzhong Cheng"],"pdf_url":"https://arxiv.org/pdf/2408.03030v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04249v1","updated":"2024-08-08T06:29:32Z","published":"2024-08-08T06:29:32Z","title":"InstantStyleGaussian: Efficient Art Style Transfer with 3D Gaussian\n  Splatting","summary":"  We present InstantStyleGaussian, an innovative 3D style transfer method based\non the 3D Gaussian Splatting (3DGS) scene representation. By inputting a target\nstyle image, it quickly generates new 3D GS scenes. Our approach operates on\npre-reconstructed GS scenes, combining diffusion models with an improved\niterative dataset update strategy. It utilizes diffusion models to generate\ntarget style images, adds these new images to the training dataset, and uses\nthis dataset to iteratively update and optimize the GS scenes. Extensive\nexperimental results demonstrate that our method ensures high-quality stylized\nscenes while offering significant advantages in style transfer speed and\nconsistency.\n","authors":["Xin-Yi Yu","Jun-Xin Yu","Li-Bo Zhou","Yan Wei","Lin-Lin Ou"],"pdf_url":"https://arxiv.org/pdf/2408.04249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04243v1","updated":"2024-08-08T06:16:00Z","published":"2024-08-08T06:16:00Z","title":"MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning","summary":"  With the exponential growth of multimedia data, leveraging multimodal sensors\npresents a promising approach for improving accuracy in human activity\nrecognition. Nevertheless, accurately identifying these activities using both\nvideo data and wearable sensor data presents challenges due to the\nlabor-intensive data annotation, and reliance on external pretrained models or\nadditional data. To address these challenges, we introduce Multimodal Masked\nAutoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal\nmasked autoencoder with a synchronized masking strategy tailored for wearable\nsensors. This masking strategy compels the networks to capture more meaningful\nspatiotemporal features, which enables effective self-supervised pretraining\nwithout the need for external data. Furthermore, Mu-MAE leverages the\nrepresentation extracted from multimodal masked autoencoders as prior\ninformation input to a cross-attention multimodal fusion layer. This fusion\nlayer emphasizes spatiotemporal features requiring attention across different\nmodalities while highlighting differences from other classes, aiding in the\nclassification of various classes in metric-based one-shot learning.\nComprehensive evaluations on MMAct one-shot classification show that Mu-MAE\noutperforms all the evaluated approaches, achieving up to an 80.17% accuracy\nfor five-way one-shot multimodal classification, without the use of additional\ndata.\n","authors":["Rex Liu","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04243v1.pdf","comment":"IEEE MIPR 2024"},{"id":"http://arxiv.org/abs/2408.04235v1","updated":"2024-08-08T05:41:09Z","published":"2024-08-08T05:41:09Z","title":"LLDif: Diffusion Models for Low-light Emotion Recognition","summary":"  This paper introduces LLDif, a novel diffusion-based facial expression\nrecognition (FER) framework tailored for extremely low-light (LL) environments.\nImages captured under such conditions often suffer from low brightness and\nsignificantly reduced contrast, presenting challenges to conventional methods.\nThese challenges include poor image quality that can significantly reduce the\naccuracy of emotion recognition. LLDif addresses these issues with a novel\ntwo-stage training process that combines a Label-aware CLIP (LA-CLIP), an\nembedding prior network (PNET), and a transformer-based network adept at\nhandling the noise of low-light images. The first stage involves LA-CLIP\ngenerating a joint embedding prior distribution (EPD) to guide the LLformer in\nlabel recovery. In the second stage, the diffusion model (DM) refines the EPD\ninference, ultilising the compactness of EPD for precise predictions.\nExperimental evaluations on various LL-FER datasets have shown that LLDif\nachieves competitive performance, underscoring its potential to enhance FER\napplications in challenging lighting conditions.\n","authors":["Zhifeng Wang","Kaihao Zhang","Ramesh Sankaranarayana"],"pdf_url":"https://arxiv.org/pdf/2408.04235v1.pdf","comment":"Accepted by ICPR2024"},{"id":"http://arxiv.org/abs/2408.04227v1","updated":"2024-08-08T05:30:59Z","published":"2024-08-08T05:30:59Z","title":"Physical prior guided cooperative learning framework for joint\n  turbulence degradation estimation and infrared video restoration","summary":"  Infrared imaging and turbulence strength measurements are in widespread\ndemand in many fields. This paper introduces a Physical Prior Guided\nCooperative Learning (P2GCL) framework to jointly enhance atmospheric\nturbulence strength estimation and infrared image restoration. P2GCL involves a\ncyclic collaboration between two models, i.e., a TMNet measures turbulence\nstrength and outputs the refractive index structure constant (Cn2) as a\nphysical prior, a TRNet conducts infrared image sequence restoration based on\nCn2 and feeds the restored images back to the TMNet to boost the measurement\naccuracy. A novel Cn2-guided frequency loss function and a physical constraint\nloss are introduced to align the training process with physical theories.\nExperiments demonstrate P2GCL achieves the best performance for both turbulence\nstrength estimation (improving Cn2 MAE by 0.0156, enhancing R2 by 0.1065) and\nimage restoration (enhancing PSNR by 0.2775 dB), validating the significant\nimpact of physical prior guided cooperative learning.\n","authors":["Ziran Zhang","Yuhang Tang","Zhigang Wang","Yueting Chen","Bin Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.04227v1.pdf","comment":"21"},{"id":"http://arxiv.org/abs/2408.04224v1","updated":"2024-08-08T05:17:27Z","published":"2024-08-08T05:17:27Z","title":"Cross-View Meets Diffusion: Aerial Image Synthesis with Geometry and\n  Text Guidance","summary":"  Aerial imagery analysis is critical for many research fields. However,\nobtaining frequent high-quality aerial images is not always accessible due to\nits high effort and cost requirements. One solution is to use the\nGround-to-Aerial (G2A) technique to synthesize aerial images from easily\ncollectible ground images. However, G2A is rarely studied, because of its\nchallenges, including but not limited to, the drastic view changes, occlusion,\nand range of visibility. In this paper, we present a novel Geometric Preserving\nGround-to-Aerial (G2A) image synthesis (GPG2A) model that can generate\nrealistic aerial images from ground images. GPG2A consists of two stages. The\nfirst stage predicts the Bird's Eye View (BEV) segmentation (referred to as the\nBEV layout map) from the ground image. The second stage synthesizes the aerial\nimage from the predicted BEV layout map and text descriptions of the ground\nimage. To train our model, we present a new multi-modal cross-view dataset,\nnamely VIGORv2 which is built upon VIGOR with newly collected aerial images,\nmaps, and text descriptions. Our extensive experiments illustrate that GPG2A\nsynthesizes better geometry-preserved aerial images than existing models. We\nalso present two applications, data augmentation for cross-view\ngeo-localization and sketch-based region search, to further verify the\neffectiveness of our GPG2A. The code and data will be publicly available.\n","authors":["Ahmad Arrabi","Xiaohan Zhang","Waqas Sultan","Chen Chen","Safwan Wshah"],"pdf_url":"https://arxiv.org/pdf/2408.04224v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.15143v2","updated":"2024-08-08T05:15:07Z","published":"2024-07-21T12:32:00Z","title":"Rethinking Feature Backbone Fine-tuning for Remote Sensing Object\n  Detection","summary":"  Recently, numerous methods have achieved impressive performance in remote\nsensing object detection, relying on convolution or transformer architectures.\nSuch detectors typically have a feature backbone to extract useful features\nfrom raw input images. For the remote sensing domain, a common practice among\ncurrent detectors is to initialize the backbone with pre-training on ImageNet\nconsisting of natural scenes. Fine-tuning the backbone is then typically\nrequired to generate features suitable for remote-sensing images. However, this\ncould hinder the extraction of basic visual features in long-term training,\nthus restricting performance improvement. To mitigate this issue, we propose a\nnovel method named DBF (Dynamic Backbone Freezing) for feature backbone\nfine-tuning on remote sensing object detection. Our method aims to handle the\ndilemma of whether the backbone should extract low-level generic features or\npossess specific knowledge of the remote sensing domain, by introducing a\nmodule called 'Freezing Scheduler' to dynamically manage the update of backbone\nfeatures during training. Extensive experiments on DOTA and DIOR-R show that\nour approach enables more accurate model learning while substantially reducing\ncomputational costs. Our method can be seamlessly adopted without additional\neffort due to its straightforward design.\n","authors":["Yechan Kim","JongHyun Park","SooYeon Kim","Moongu Jeon"],"pdf_url":"https://arxiv.org/pdf/2407.15143v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2408.04223v1","updated":"2024-08-08T05:14:07Z","published":"2024-08-08T05:14:07Z","title":"VideoQA in the Era of LLMs: An Empirical Study","summary":"  Video Large Language Models (Video-LLMs) are flourishing and has advanced\nmany video-language tasks. As a golden testbed, Video Question Answering\n(VideoQA) plays pivotal role in Video-LLM developing. This work conducts a\ntimely and comprehensive study of Video-LLMs' behavior in VideoQA, aiming to\nelucidate their success and failure modes, and provide insights towards more\nhuman-like video understanding and question answering. Our analyses demonstrate\nthat Video-LLMs excel in VideoQA; they can correlate contextual cues and\ngenerate plausible responses to questions about varied video contents. However,\nmodels falter in handling video temporality, both in reasoning about temporal\ncontent ordering and grounding QA-relevant temporal moments. Moreover, the\nmodels behave unintuitively - they are unresponsive to adversarial video\nperturbations while being sensitive to simple variations of candidate answers\nand questions. Also, they do not necessarily generalize better. The findings\ndemonstrate Video-LLMs' QA capability in standard condition yet highlight their\nsevere deficiency in robustness and interpretability, suggesting the urgent\nneed on rationales in Video-LLM developing.\n","authors":["Junbin Xiao","Nanxin Huang","Hangyu Qin","Dongyang Li","Yicong Li","Fengbin Zhu","Zhulin Tao","Jianxing Yu","Liang Lin","Tat-Seng Chua","Angela Yao"],"pdf_url":"https://arxiv.org/pdf/2408.04223v1.pdf","comment":"Preprint. Under Review"},{"id":"http://arxiv.org/abs/2408.04221v1","updated":"2024-08-08T05:09:02Z","published":"2024-08-08T05:09:02Z","title":"Connective Viewpoints of Signal-to-Noise Diffusion Models","summary":"  Diffusion models (DM) have become fundamental components of generative\nmodels, excelling across various domains such as image creation, audio\ngeneration, and complex data interpolation. Signal-to-Noise diffusion models\nconstitute a diverse family covering most state-of-the-art diffusion models.\nWhile there have been several attempts to study Signal-to-Noise (S2N) diffusion\nmodels from various perspectives, there remains a need for a comprehensive\nstudy connecting different viewpoints and exploring new perspectives. In this\nstudy, we offer a comprehensive perspective on noise schedulers, examining\ntheir role through the lens of the signal-to-noise ratio (SNR) and its\nconnections to information theory. Building upon this framework, we have\ndeveloped a generalized backward equation to enhance the performance of the\ninference process.\n","authors":["Khanh Doan","Long Tung Vuong","Tuan Nguyen","Anh Tuan Bui","Quyen Tran","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2408.04221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02369v2","updated":"2024-08-08T04:54:47Z","published":"2024-08-05T10:38:50Z","title":"The NPU-ASLP System Description for Visual Speech Recognition in CNVSRC\n  2024","summary":"  This paper delineates the visual speech recognition (VSR) system introduced\nby the NPU-ASLP (Team 237) in the second Chinese Continuous Visual Speech\nRecognition Challenge (CNVSRC 2024), engaging in all four tracks, including the\nfixed and open tracks of Single-Speaker VSR Task and Multi-Speaker VSR Task. In\nterms of data processing, we leverage the lip motion extractor from the\nbaseline1 to produce multiscale video data. Besides, various augmentation\ntechniques are applied during training, encompassing speed perturbation, random\nrotation, horizontal flipping, and color transformation. The VSR model adopts\nan end-to-end architecture with joint CTC/attention loss, introducing Enhanced\nResNet3D visual frontend, E-Branchformer encoder, and Bi-directional\nTransformer decoder. Our approach yields a 30.47% CER for the Single-Speaker\nTask and 34.30% CER for the Multi-Speaker Task, securing second place in the\nopen track of the Single-Speaker Task and first place in the other three\ntracks.\n","authors":["He Wang","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2408.02369v2.pdf","comment":"2 pages, 2 figures, CNVSRC 2024 System Report"},{"id":"http://arxiv.org/abs/2312.02934v4","updated":"2024-08-08T04:42:52Z","published":"2023-12-05T18:05:14Z","title":"WoVoGen: World Volume-aware Diffusion for Controllable Multi-camera\n  Driving Scene Generation","summary":"  Generating multi-camera street-view videos is critical for augmenting\nautonomous driving datasets, addressing the urgent demand for extensive and\nvaried data. Due to the limitations in diversity and challenges in handling\nlighting conditions, traditional rendering-based methods are increasingly being\nsupplanted by diffusion-based methods. However, a significant challenge in\ndiffusion-based methods is ensuring that the generated sensor data preserve\nboth intra-world consistency and inter-sensor coherence. To address these\nchallenges, we combine an additional explicit world volume and propose the\nWorld Volume-aware Multi-camera Driving Scene Generator (WoVoGen). This system\nis specifically designed to leverage 4D world volume as a foundational element\nfor video generation. Our model operates in two distinct phases: (i)\nenvisioning the future 4D temporal world volume based on vehicle control\nsequences, and (ii) generating multi-camera videos, informed by this envisioned\n4D temporal world volume and sensor interconnectivity. The incorporation of the\n4D world volume empowers WoVoGen not only to generate high-quality street-view\nvideos in response to vehicle control inputs but also to facilitate scene\nediting tasks.\n","authors":["Jiachen Lu","Ze Huang","Zeyu Yang","Jiahui Zhang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2312.02934v4.pdf","comment":"ECCV 2024"},{"id":"http://arxiv.org/abs/2408.04212v1","updated":"2024-08-08T04:34:29Z","published":"2024-08-08T04:34:29Z","title":"Is SAM 2 Better than SAM in Medical Image Segmentation?","summary":"  Segment Anything Model (SAM) demonstrated impressive performance in zero-shot\npromptable segmentation on natural images. The recently released Segment\nAnything Model 2 (SAM 2) model claims to have better performance than SAM on\nimages while extending the model's capabilities to video segmentation. It is\nimportant to evaluate the recent model's ability in medical image segmentation\nin a zero-shot promptable manner. In this work, we performed extensive studies\nwith multiple datasets from different imaging modalities to compare the\nperformance between SAM and SAM 2. We used two point prompt strategies: (i)\nsingle positive prompt near the centroid of the target structure and (ii)\nadditional positive prompts placed randomly within the target structure. The\nevaluation included 21 unique organ-modality combinations including abdominal\nstructures, cardiac structures, and fetal head images acquired from publicly\navailable MRI, CT, and Ultrasound datasets. The preliminary results, based on\n2D images, indicate that while SAM 2 may perform slightly better in a few\ncases, but it does not in general surpass SAM for medical image segmentation.\nEspecially when the contrast is lower like in CT, Ultrasound images, SAM 2\nperforms poorly than SAM. For MRI images, SAM 2 performs at par or better than\nSAM. Similar to SAM, SAM 2 also suffers from over-segmentation issue especially\nwhen the boundaries of the to-be-segmented organ is fuzzy in nature.\n","authors":["Sourya Sengupta","Satrajit Chakrabarty","Ravi Soni"],"pdf_url":"https://arxiv.org/pdf/2408.04212v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2106.11528v3","updated":"2024-08-08T04:18:34Z","published":"2021-06-22T03:44:03Z","title":"Recent Deep Semi-supervised Learning Approaches and Related Works","summary":"  This work proposes an overview of the recent semi-supervised learning\napproaches and related works. Despite the remarkable success of neural networks\nin various applications, there exist a few formidable constraints, including\nthe need for a large amount of labeled data. Therefore, semi-supervised\nlearning, which is a learning scheme in which scarce labels and a larger amount\nof unlabeled data are utilized to train models (e.g., deep neural networks), is\ngetting more important. Based on the key assumptions of semi-supervised\nlearning, which are the manifold assumption, cluster assumption, and continuity\nassumption, the work reviews the recent semi-supervised learning approaches. In\nparticular, the methods in regard to using deep neural networks in a\nsemi-supervised learning setting are primarily discussed. In addition, the\nexisting works are first classified based on the underlying idea and explained,\nthen the holistic approaches that unify the aforementioned ideas are detailed.\n","authors":["Gyeongho Kim"],"pdf_url":"https://arxiv.org/pdf/2106.11528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.11106v2","updated":"2024-08-08T04:03:27Z","published":"2023-10-17T09:44:30Z","title":"3D Structure-guided Network for Tooth Alignment in 2D Photograph","summary":"  Orthodontics focuses on rectifying misaligned teeth (i.e., malocclusions),\naffecting both masticatory function and aesthetics. However, orthodontic\ntreatment often involves complex, lengthy procedures. As such, generating a 2D\nphotograph depicting aligned teeth prior to orthodontic treatment is crucial\nfor effective dentist-patient communication and, more importantly, for\nencouraging patients to accept orthodontic intervention. In this paper, we\npropose a 3D structure-guided tooth alignment network that takes 2D photographs\nas input (e.g., photos captured by smartphones) and aligns the teeth within the\n2D image space to generate an orthodontic comparison photograph featuring\naesthetically pleasing, aligned teeth. Notably, while the process operates\nwithin a 2D image space, our method employs 3D intra-oral scanning models\ncollected in clinics to learn about orthodontic treatment, i.e., projecting the\npre- and post-orthodontic 3D tooth structures onto 2D tooth contours, followed\nby a diffusion model to learn the mapping relationship. Ultimately, the aligned\ntooth contours are leveraged to guide the generation of a 2D photograph with\naesthetically pleasing, aligned teeth and realistic textures. We evaluate our\nnetwork on various facial photographs, demonstrating its exceptional\nperformance and strong applicability within the orthodontic industry.\n","authors":["Yulong Dou","Lanzhuju Mei","Dinggang Shen","Zhiming Cui"],"pdf_url":"https://arxiv.org/pdf/2310.11106v2.pdf","comment":"Accepted by The 34th British Machine Vision Conference (BMVC 2023)\n  Our BMVC webpage is https://proceedings.bmvc2023.org/322/"},{"id":"http://arxiv.org/abs/2407.19451v3","updated":"2024-08-08T04:01:03Z","published":"2024-07-28T10:05:11Z","title":"Perm: A Parametric Representation for Multi-Style 3D Hair Modeling","summary":"  We present Perm, a learned parametric model of human 3D hair designed to\nfacilitate various hair-related applications. Unlike previous work that jointly\nmodels the global hair shape and local strand details, we propose to\ndisentangle them using a PCA-based strand representation in the frequency\ndomain, thereby allowing more precise editing and output control. Specifically,\nwe leverage our strand representation to fit and decompose hair geometry\ntextures into low- to high-frequency hair structures. These decomposed textures\nare later parameterized with different generative models, emulating common\nstages in the hair modeling process. We conduct extensive experiments to\nvalidate the architecture design of \\textsc{Perm}, and finally deploy the\ntrained model as a generic prior to solve task-agnostic problems, further\nshowcasing its flexibility and superiority in tasks such as 3D hair\nparameterization, hairstyle interpolation, single-view hair reconstruction, and\nhair-conditioned image generation. Our code, data, and supplemental can be\nfound at our project page: https://cs.yale.edu/homes/che/projects/perm/\n","authors":["Chengan He","Xin Sun","Zhixin Shu","Fujun Luan","Sren Pirk","Jorge Alejandro Amador Herrera","Dominik L. Michels","Tuanfeng Y. Wang","Meng Zhang","Holly Rushmeier","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2407.19451v3.pdf","comment":"Project page: https://cs.yale.edu/homes/che/projects/perm/"},{"id":"http://arxiv.org/abs/2402.17485v3","updated":"2024-08-08T03:48:38Z","published":"2024-02-27T13:10:11Z","title":"EMO: Emote Portrait Alive -- Generating Expressive Portrait Videos with\n  Audio2Video Diffusion Model under Weak Conditions","summary":"  In this work, we tackle the challenge of enhancing the realism and\nexpressiveness in talking head video generation by focusing on the dynamic and\nnuanced relationship between audio cues and facial movements. We identify the\nlimitations of traditional techniques that often fail to capture the full\nspectrum of human expressions and the uniqueness of individual facial styles.\nTo address these issues, we propose EMO, a novel framework that utilizes a\ndirect audio-to-video synthesis approach, bypassing the need for intermediate\n3D models or facial landmarks. Our method ensures seamless frame transitions\nand consistent identity preservation throughout the video, resulting in highly\nexpressive and lifelike animations. Experimental results demonsrate that EMO is\nable to produce not only convincing speaking videos but also singing videos in\nvarious styles, significantly outperforming existing state-of-the-art\nmethodologies in terms of expressiveness and realism.\n","authors":["Linrui Tian","Qi Wang","Bang Zhang","Liefeng Bo"],"pdf_url":"https://arxiv.org/pdf/2402.17485v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.06342v3","updated":"2024-08-08T03:25:02Z","published":"2024-05-10T09:18:17Z","title":"Compression-Realized Deep Structural Network for Video Quality\n  Enhancement","summary":"  This paper focuses on the task of quality enhancement for compressed videos.\nAlthough deep network-based video restorers achieve impressive progress, most\nof the existing methods lack a structured design to optimally leverage the\npriors within compression codecs. Since the quality degradation of the video is\nprimarily induced by the compression algorithm, a new paradigm is urgently\nneeded for a more ``conscious'' process of quality enhancement. As a result, we\npropose the Compression-Realized Deep Structural Network (CRDS), introducing\nthree inductive biases aligned with the three primary processes in the classic\ncompression codec, merging the strengths of classical encoder architecture with\ndeep network capabilities. Inspired by the residual extraction and domain\ntransformation process in the codec, a pre-trained Latent Degradation Residual\nAuto-Encoder is proposed to transform video frames into a latent feature space,\nand the mutual neighborhood attention mechanism is integrated for precise\nmotion estimation and residual extraction. Furthermore, drawing inspiration\nfrom the quantization noise distribution of the codec, CRDS proposes a novel\nProgressive Denoising framework with intermediate supervision that decomposes\nthe quality enhancement into a series of simpler denoising sub-tasks.\nExperimental results on datasets like LDV 2.0 and MFQE 2.0 indicate our\napproach surpasses state-of-the-art models.\n","authors":["Hanchi Sun","Xiaohong Liu","Xinyang Jiang","Yifei Shen","Dongsheng Li","Xiongkuo Min","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2405.06342v3.pdf","comment":"Accepted by ACM MM'24"},{"id":"http://arxiv.org/abs/2301.01156v3","updated":"2024-08-08T03:22:43Z","published":"2023-01-03T15:33:48Z","title":"Reference Twice: A Simple and Unified Baseline for Few-Shot Instance\n  Segmentation","summary":"  Few-Shot Instance Segmentation (FSIS) requires detecting and segmenting novel\nclasses with limited support examples. Existing methods based on Region\nProposal Networks (RPNs) face two issues: 1) Overfitting suppresses novel class\nobjects; 2) Dual-branch models require complex spatial correlation strategies\nto prevent spatial information loss when generating class prototypes. We\nintroduce a unified framework, Reference Twice (RefT), to exploit the\nrelationship between support and query features for FSIS and related tasks. Our\nthree main contributions are: 1) A novel transformer-based baseline that avoids\noverfitting, offering a new direction for FSIS; 2) Demonstrating that support\nobject queries encode key factors after base training, allowing query features\nto be enhanced twice at both feature and query levels using simple\ncross-attention, thus avoiding complex spatial correlation interaction; 3)\nIntroducing a class-enhanced base knowledge distillation loss to address the\nissue of DETR-like models struggling with incremental settings due to the input\nprojection layer, enabling easy extension to incremental FSIS. Extensive\nexperimental evaluations on the COCO dataset under three FSIS settings\ndemonstrate that our method performs favorably against existing approaches\nacross different shots, \\eg, $+8.2/+9.4$ performance gain over state-of-the-art\nmethods with 10/30-shots. Source code and models will be available at\nhttps://github.com/hanyue1648/RefT.\n","authors":["Yue Han","Jiangning Zhang","Yabiao Wang","Chengjie Wang","Yong Liu","Lu Qi","Xiangtai Li","Ming-Hsuan Yang"],"pdf_url":"https://arxiv.org/pdf/2301.01156v3.pdf","comment":"Accepted by T-PAMI"},{"id":"http://arxiv.org/abs/2406.18037v2","updated":"2024-08-08T03:16:23Z","published":"2024-06-26T03:10:57Z","title":"Towards Synchronous Memorizability and Generalizability with\n  Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation","summary":"  The ability to learn sequentially from different data sites is crucial for a\ndeep network in solving practical medical image diagnosis problems due to\nprivacy restrictions and storage limitations. However, adapting on incoming\nsite leads to catastrophic forgetting on past sites and decreases\ngeneralizablity on unseen sites. Existing Continual Learning (CL) and Domain\nGeneralization (DG) methods have been proposed to solve these two challenges\nrespectively, but none of them can address both simultaneously. Recognizing\nthis limitation, this paper proposes a novel training paradigm, learning\ntowards Synchronous Memorizability and Generalizability (SMG-Learning). To\nachieve this, we create the orientational gradient alignment to ensure\nmemorizability on previous sites, and arbitrary gradient alignment to enhance\ngeneralizability on unseen sites. This approach is named as Parallel Gradient\nAlignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives\nusing the first-order Taylor expansion to reduce computational cost of aligning\ngradients. Considering that performing gradient alignments, especially for\nprevious sites, is not feasible due to the privacy constraints, we design a\nSite-Modulated Diffusion (SMD) model to generate images with site-specific\nlearnable prompts, replaying images have similar data distributions as previous\nsites. We evaluate our method on two medical image segmentation tasks, where\ndata from different sites arrive sequentially. Experimental results show that\nour method efficiently enhances both memorizability and generalizablity better\nthan other state-of-the-art methods, delivering satisfactory performance across\nall sites. Our code will be available at:\nhttps://github.com/dyxu-cuhkcse/SMG-Learning.\n","authors":["Dunyuan Xu","Xi Wang","Jingyang Zhang","Pheng-Ann Heng"],"pdf_url":"https://arxiv.org/pdf/2406.18037v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.04187v1","updated":"2024-08-08T03:11:12Z","published":"2024-08-08T03:11:12Z","title":"Medical Graph RAG: Towards Safe Medical Large Language Model via Graph\n  Retrieval-Augmented Generation","summary":"  We introduce a novel graph-based Retrieval-Augmented Generation (RAG)\nframework specifically designed for the medical domain, called\n\\textbf{MedGraphRAG}, aimed at enhancing Large Language Model (LLM)\ncapabilities and generating evidence-based results, thereby improving safety\nand reliability when handling private medical data. Our comprehensive pipeline\nbegins with a hybrid static-semantic approach to document chunking,\nsignificantly improving context capture over traditional methods. Extracted\nentities are used to create a three-tier hierarchical graph structure, linking\nentities to foundational medical knowledge sourced from medical papers and\ndictionaries. These entities are then interconnected to form meta-graphs, which\nare merged based on semantic similarities to develop a comprehensive global\ngraph. This structure supports precise information retrieval and response\ngeneration. The retrieval process employs a U-retrieve method to balance global\nawareness and indexing efficiency of the LLM. Our approach is validated through\na comprehensive ablation study comparing various methods for document chunking,\ngraph construction, and information retrieval. The results not only demonstrate\nthat our hierarchical graph construction method consistently outperforms\nstate-of-the-art models on multiple medical Q\\&A benchmarks, but also confirms\nthat the responses generated include source documentation, significantly\nenhancing the reliability of medical LLMs in practical applications. Code will\nbe at: https://github.com/MedicineToken/Medical-Graph-RAG/tree/main\n","authors":["Junde Wu","Jiayuan Zhu","Yunli Qi"],"pdf_url":"https://arxiv.org/pdf/2408.04187v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2202.11703v3","updated":"2024-08-08T03:09:21Z","published":"2022-02-23T18:58:56Z","title":"Paying U-Attention to Textures: Multi-Stage Hourglass Vision Transformer\n  for Universal Texture Synthesis","summary":"  We present a novel U-Attention vision Transformer for universal texture\nsynthesis. We exploit the natural long-range dependencies enabled by the\nattention mechanism to allow our approach to synthesize diverse textures while\npreserving their structures in a single inference. We propose a hierarchical\nhourglass backbone that attends to the global structure and performs patch\nmapping at varying scales in a coarse-to-fine-to-coarse stream. Completed by\nskip connection and convolution designs that propagate and fuse information at\ndifferent scales, our hierarchical U-Attention architecture unifies attention\nto features from macro structures to micro details, and progressively refines\nsynthesis results at successive stages. Our method achieves stronger 2$\\times$\nsynthesis than previous work on both stochastic and structured textures while\ngeneralizing to unseen textures without fine-tuning. Ablation studies\ndemonstrate the effectiveness of each component of our architecture.\n","authors":["Shouchang Guo","Valentin Deschaintre","Douglas Noll","Arthur Roullier"],"pdf_url":"https://arxiv.org/pdf/2202.11703v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04424v2","updated":"2024-08-08T03:01:31Z","published":"2023-12-07T16:49:09Z","title":"Cascade-Zero123: One Image to Highly Consistent 3D with Self-Prompted\n  Nearby Views","summary":"  Synthesizing multi-view 3D from one single image is a significant but\nchallenging task. Zero-1-to-3 methods have achieved great success by lifting a\n2D latent diffusion model to the 3D scope. The target view image is generated\nwith a single-view source image and the camera pose as condition information.\nHowever, due to the high sparsity of the single input image, Zero-1-to-3 tends\nto produce geometry and appearance inconsistency across views, especially for\ncomplex objects. To tackle this issue, we propose to supply more condition\ninformation for the generation model but in a self-prompt way. A cascade\nframework is constructed with two Zero-1-to-3 models, named Cascade-Zero123,\nwhich progressively extract 3D information from the source image. Specifically,\nseveral nearby views are first generated by the first model and then fed into\nthe second-stage model along with the source image as generation conditions.\nWith amplified self-prompted condition images, our Cascade-Zero123 generates\nmore consistent novel-view images than Zero-1-to-3. Experiment results\ndemonstrate remarkable promotion, especially for various complex and\nchallenging scenes, involving insects, humans, transparent objects, and stacked\nmultiple objects etc. More demos and code are available at\nhttps://cascadezero123.github.io.\n","authors":["Yabo Chen","Jiemin Fang","Yuyang Huang","Taoran Yi","Xiaopeng Zhang","Lingxi Xie","Xinggang Wang","Wenrui Dai","Hongkai Xiong","Qi Tian"],"pdf_url":"https://arxiv.org/pdf/2312.04424v2.pdf","comment":"ECCV 2024. Project page: https://cascadezero123.github.io/"},{"id":"http://arxiv.org/abs/2408.03361v2","updated":"2024-08-08T02:43:06Z","published":"2024-08-06T17:59:21Z","title":"GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards\n  General Medical AI","summary":"  Large Vision-Language Models (LVLMs) are capable of handling diverse data\ntypes such as imaging, text, and physiological signals, and can be applied in\nvarious fields. In the medical field, LVLMs have a high potential to offer\nsubstantial assistance for diagnosis and treatment. Before that, it is crucial\nto develop benchmarks to evaluate LVLMs' effectiveness in various medical\napplications. Current benchmarks are often built upon specific academic\nliterature, mainly focusing on a single domain, and lacking varying perceptual\ngranularities. Thus, they face specific challenges, including limited clinical\nrelevance, incomplete evaluations, and insufficient guidance for interactive\nLVLMs. To address these limitations, we developed the GMAI-MMBench, the most\ncomprehensive general medical AI benchmark with well-categorized data structure\nand multi-perceptual granularity to date. It is constructed from 285 datasets\nacross 39 medical image modalities, 18 clinical-related tasks, 18 departments,\nand 4 perceptual granularities in a Visual Question Answering (VQA) format.\nAdditionally, we implemented a lexical tree structure that allows users to\ncustomize evaluation tasks, accommodating various assessment needs and\nsubstantially supporting medical AI research and applications. We evaluated 50\nLVLMs, and the results show that even the advanced GPT-4o only achieves an\naccuracy of 52%, indicating significant room for improvement. Moreover, we\nidentified five key insufficiencies in current cutting-edge LVLMs that need to\nbe addressed to advance the development of better medical applications. We\nbelieve that GMAI-MMBench will stimulate the community to build the next\ngeneration of LVLMs toward GMAI.\n  Project Page: https://uni-medical.github.io/GMAI-MMBench.github.io/\n","authors":["Pengcheng Chen","Jin Ye","Guoan Wang","Yanjun Li","Zhongying Deng","Wei Li","Tianbin Li","Haodong Duan","Ziyan Huang","Yanzhou Su","Benyou Wang","Shaoting Zhang","Bin Fu","Jianfei Cai","Bohan Zhuang","Eric J Seibel","Junjun He","Yu Qiao"],"pdf_url":"https://arxiv.org/pdf/2408.03361v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19674v4","updated":"2024-08-08T02:39:15Z","published":"2024-07-29T03:30:09Z","title":"Advancing Prompt Learning through an External Layer","summary":"  Prompt learning represents a promising method for adapting pre-trained\nvision-language models (VLMs) to various downstream tasks by learning a set of\ntext embeddings. One challenge inherent to these methods is the poor\ngeneralization performance due to the invalidity of the learned text embeddings\nfor unseen tasks. A straightforward approach to bridge this gap is to freeze\nthe text embeddings in prompts, which results in a lack of capacity to adapt\nVLMs for downstream tasks. To address this dilemma, we propose a paradigm\ncalled EnPrompt with a novel External Layer (EnLa). Specifically, we propose a\ntextual external layer and learnable visual embeddings for adapting VLMs to\ndownstream tasks. The learnable external layer is built upon valid embeddings\nof pre-trained CLIP. This design considers the balance of learning capabilities\nbetween the two branches. To align the textual and visual features, we propose\na novel two-pronged approach: i) we introduce the optimal transport as the\ndiscrepancy metric to align the vision and text modalities, and ii) we\nintroduce a novel strengthening feature to enhance the interaction between\nthese two modalities. Four representative experiments (i.e., base-to-novel\ngeneralization, few-shot learning, cross-dataset generalization, domain shifts\ngeneralization) across 15 datasets demonstrate that our method outperforms the\nexisting prompt learning method.\n","authors":["Fangming Cui","Xun Yang","Chao Wu","Liang Xiao","Xinmei Tian"],"pdf_url":"https://arxiv.org/pdf/2407.19674v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04175v1","updated":"2024-08-08T02:38:19Z","published":"2024-08-08T02:38:19Z","title":"pyBregMan: A Python library for Bregman Manifolds","summary":"  A Bregman manifold is a synonym for a dually flat space in information\ngeometry which admits as a canonical divergence a Bregman divergence. Bregman\nmanifolds are induced by smooth strictly convex functions like the cumulant or\npartition functions of regular exponential families, the negative entropy of\nmixture families, or the characteristic functions of regular cones just to list\na few such convex Bregman generators. We describe the design of pyBregMan, a\nlibrary which implements generic operations on Bregman manifolds and\ninstantiate several common Bregman manifolds used in information sciences. At\nthe core of the library is the notion of Legendre-Fenchel duality inducing a\ncanonical pair of dual potential functions and dual Bregman divergences. The\nlibrary also implements the Fisher-Rao manifolds of categorical/multinomial\ndistributions and multivariate normal distributions. To demonstrate the use of\nthe pyBregMan kernel manipulating those Bregman and Fisher-Rao manifolds, the\nlibrary also provides several core algorithms for various applications in\nstatistics, machine learning, information fusion, and so on.\n","authors":["Frank Nielsen","Alexander Soen"],"pdf_url":"https://arxiv.org/pdf/2408.04175v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2406.07822v2","updated":"2024-08-08T02:36:04Z","published":"2024-06-12T02:43:19Z","title":"Tell Me What's Next: Textual Foresight for Generic UI Representations","summary":"  Mobile app user interfaces (UIs) are rich with action, text, structure, and\nimage content that can be utilized to learn generic UI representations for\ntasks like automating user commands, summarizing content, and evaluating the\naccessibility of user interfaces. Prior work has learned strong visual\nrepresentations with local or global captioning losses, but fails to retain\nboth granularities. To combat this, we propose Textual Foresight, a novel\npretraining objective for learning UI screen representations. Textual Foresight\ngenerates global text descriptions of future UI states given a current UI and\nlocal action taken. Our approach requires joint reasoning over elements and\nentire screens, resulting in improved UI features: on generation tasks, UI\nagents trained with Textual Foresight outperform state-of-the-art by 2% with\n28x fewer images. We train with our newly constructed mobile app dataset,\nOpenApp, which results in the first public dataset for app UI representation\nlearning. OpenApp enables new baselines, and we find Textual Foresight improves\naverage task performance over them by 5.7% while having access to 2x less data.\n","authors":["Andrea Burns","Kate Saenko","Bryan A. Plummer"],"pdf_url":"https://arxiv.org/pdf/2406.07822v2.pdf","comment":"Accepted to ACL 2024 Findings. Data and code to be released at\n  https://github.com/aburns4/textualforesight"},{"id":"http://arxiv.org/abs/2408.04172v1","updated":"2024-08-08T02:34:41Z","published":"2024-08-08T02:34:41Z","title":"MultiColor: Image Colorization by Learning from Multiple Color Spaces","summary":"  Deep networks have shown impressive performance in the image restoration\ntasks, such as image colorization. However, we find that previous approaches\nrely on the digital representation from single color model with a specific\nmapping function, a.k.a., color space, during the colorization pipeline. In\nthis paper, we first investigate the modeling of different color spaces, and\nfind each of them exhibiting distinctive characteristics with unique\ndistribution of colors. The complementarity among multiple color spaces leads\nto benefits for the image colorization task.\n  We present MultiColor, a new learning-based approach to automatically\ncolorize grayscale images that combines clues from multiple color spaces.\nSpecifically, we employ a set of dedicated colorization modules for individual\ncolor space. Within each module, a transformer decoder is first employed to\nrefine color query embeddings and then a color mapper produces color channel\nprediction using the embeddings and semantic features. With these predicted\ncolor channels representing various color spaces, a complementary network is\ndesigned to exploit the complementarity and generate pleasing and reasonable\ncolorized images. We conduct extensive experiments on real-world datasets, and\nthe results demonstrate superior performance over the state-of-the-arts.\n","authors":["Xiangcheng Du","Zhao Zhou","Yanlong Wang","Zhuoyao Wang","Yingbin Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2408.04172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04171v1","updated":"2024-08-08T02:32:48Z","published":"2024-08-08T02:32:48Z","title":"Rotation center identification based on geometric relationships for\n  rotary motion deblurring","summary":"  Non-blind rotary motion deblurring (RMD) aims to recover the latent clear\nimage from a rotary motion blurred (RMB) image. The rotation center is a\ncrucial input parameter in non-blind RMD methods. Existing methods directly\nestimate the rotation center from the RMB image. However they always suffer\nsignificant errors, and the performance of RMD is limited. For the assembled\nimaging systems, the position of the rotation center remains fixed. Leveraging\nthis prior knowledge, we propose a geometric-based method for rotation center\nidentification and analyze its error range. Furthermore, we construct a RMB\nimaging system. The experiment demonstrates that our method achieves less than\n1-pixel error along a single axis (x-axis or y-axis). We utilize the\nconstructed imaging system to capture real RMB images, and experimental results\nshow that our method can help existing RMD approaches yield better RMD images.\n","authors":["Jinhui Qin","Yong Ma","Jun Huang","Fan Fan","You Du"],"pdf_url":"https://arxiv.org/pdf/2408.04171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04170v1","updated":"2024-08-08T02:31:04Z","published":"2024-08-08T02:31:04Z","title":"M2EF-NNs: Multimodal Multi-instance Evidence Fusion Neural Networks for\n  Cancer Survival Prediction","summary":"  Accurate cancer survival prediction is crucial for assisting clinical doctors\nin formulating treatment plans. Multimodal data, including histopathological\nimages and genomic data, offer complementary and comprehensive information that\ncan greatly enhance the accuracy of this task. However, the current methods,\ndespite yielding promising results, suffer from two notable limitations: they\ndo not effectively utilize global context and disregard modal uncertainty. In\nthis study, we put forward a neural network model called M2EF-NNs, which\nleverages multimodal and multi-instance evidence fusion techniques for accurate\ncancer survival prediction. Specifically, to capture global information in the\nimages, we use a pre-trained Vision Transformer (ViT) model to obtain patch\nfeature embeddings of histopathological images. Then, we introduce a multimodal\nattention module that uses genomic embeddings as queries and learns the\nco-attention mapping between genomic and histopathological images to achieve an\nearly interaction fusion of multimodal information and better capture their\ncorrelations. Subsequently, we are the first to apply the Dempster-Shafer\nevidence theory (DST) to cancer survival prediction. We parameterize the\ndistribution of class probabilities using the processed multimodal features and\nintroduce subjective logic to estimate the uncertainty associated with\ndifferent modalities. By combining with the Dempster-Shafer theory, we can\ndynamically adjust the weights of class probabilities after multimodal fusion\nto achieve trusted survival prediction. Finally, Experimental validation on the\nTCGA datasets confirms the significant improvements achieved by our proposed\nmethod in cancer survival prediction and enhances the reliability of the model.\n","authors":["Hui Luo","Jiashuang Huang","Hengrong Ju","Tianyi Zhou","Weiping Ding"],"pdf_url":"https://arxiv.org/pdf/2408.04170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08759v2","updated":"2024-08-08T02:28:17Z","published":"2024-06-13T02:41:11Z","title":"GaussianForest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed\n  Scene Modeling","summary":"  The field of novel-view synthesis has recently witnessed the emergence of 3D\nGaussian Splatting, which represents scenes in a point-based manner and renders\nthrough rasterization. This methodology, in contrast to Radiance Fields that\nrely on ray tracing, demonstrates superior rendering quality and speed.\nHowever, the explicit and unstructured nature of 3D Gaussians poses a\nsignificant storage challenge, impeding its broader application. To address\nthis challenge, we introduce the Gaussian-Forest modeling framework, which\nhierarchically represents a scene as a forest of hybrid 3D Gaussians. Each\nhybrid Gaussian retains its unique explicit attributes while sharing implicit\nones with its sibling Gaussians, thus optimizing parameterization with\nsignificantly fewer variables. Moreover, adaptive growth and pruning strategies\nare designed, ensuring detailed representation in complex regions and a notable\nreduction in the number of required Gaussians. Extensive experiments\ndemonstrate that Gaussian-Forest not only maintains comparable speed and\nquality but also achieves a compression rate surpassing 10 times, marking a\nsignificant advancement in efficient scene modeling. Codes will be available at\nhttps://github.com/Xian-Bei/GaussianForest.\n","authors":["Fengyi Zhang","Yadan Luo","Tianjun Zhang","Lin Zhang","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2406.08759v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01976v2","updated":"2024-08-08T02:15:41Z","published":"2024-08-04T09:44:47Z","title":"Single-Point Supervised High-Resolution Dynamic Network for Infrared\n  Small Target Detection","summary":"  Infrared small target detection (IRSTD) tasks are extremely challenging for\ntwo main reasons: 1) it is difficult to obtain accurate labelling information\nthat is critical to existing methods, and 2) infrared (IR) small target\ninformation is easily lost in deep networks. To address these issues, we\npropose a single-point supervised high-resolution dynamic network (SSHD-Net).\nIn contrast to existing methods, we achieve state-of-the-art (SOTA) detection\nperformance using only single-point supervision. Specifically, we first design\na high-resolution cross-feature extraction module (HCEM), that achieves\nbi-directional feature interaction through stepped feature cascade channels\n(SFCC). It balances network depth and feature resolution to maintain deep IR\nsmall-target information. Secondly, the effective integration of global and\nlocal features is achieved through the dynamic coordinate fusion module (DCFM),\nwhich enhances the anti-interference ability in complex backgrounds. In\naddition, we introduce the high-resolution multilevel residual module (HMRM) to\nenhance the semantic information extraction capability. Finally, we design the\nadaptive target localization detection head (ATLDH) to improve detection\naccuracy. Experiments on the publicly available datasets NUDT-SIRST and\nIRSTD-1k demonstrate the effectiveness of our method. Compared to other SOTA\nmethods, our method can achieve better detection performance with only a single\npoint of supervision.\n","authors":["Jing Wu","Rixiang Ni","Feng Huang","Zhaobing Qiu","Liqiong Chen","Changhai Luo","Yunxiang Li","Youli Li"],"pdf_url":"https://arxiv.org/pdf/2408.01976v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04158v1","updated":"2024-08-08T02:03:10Z","published":"2024-08-08T02:03:10Z","title":"Efficient Single Image Super-Resolution with Entropy Attention and\n  Receptive Field Augmentation","summary":"  Transformer-based deep models for single image super-resolution (SISR) have\ngreatly improved the performance of lightweight SISR tasks in recent years.\nHowever, they often suffer from heavy computational burden and slow inference\ndue to the complex calculation of multi-head self-attention (MSA), seriously\nhindering their practical application and deployment. In this work, we present\nan efficient SR model to mitigate the dilemma between model efficiency and SR\nperformance, which is dubbed Entropy Attention and Receptive Field Augmentation\nnetwork (EARFA), and composed of a novel entropy attention (EA) and a shifting\nlarge kernel attention (SLKA). From the perspective of information theory, EA\nincreases the entropy of intermediate features conditioned on a Gaussian\ndistribution, providing more informative input for subsequent reasoning. On the\nother hand, SLKA extends the receptive field of SR models with the assistance\nof channel shifting, which also favors to boost the diversity of hierarchical\nfeatures. Since the implementation of EA and SLKA does not involve complex\ncomputations (such as extensive matrix multiplications), the proposed method\ncan achieve faster nonlinear inference than Transformer-based SR models while\nmaintaining better SR performance. Extensive experiments show that the proposed\nmodel can significantly reduce the delay of model inference while achieving the\nSR performance comparable with other advanced models.\n","authors":["Xiaole Zhao","Linze Li","Chengxing Xie","Xiaoming Zhang","Ting Jiang","Wenjie Lin","Shuaicheng Liu","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2408.04158v1.pdf","comment":"Accepted to ACM MM 2024"},{"id":"http://arxiv.org/abs/2406.01916v2","updated":"2024-08-08T01:50:52Z","published":"2024-06-04T02:57:09Z","title":"FastLGS: Speeding up Language Embedded Gaussians with Feature Grid\n  Mapping","summary":"  The semantically interactive radiance field has always been an appealing task\nfor its potential to facilitate user-friendly and automated real-world 3D scene\nunderstanding applications. However, it is a challenging task to achieve high\nquality, efficiency and zero-shot ability at the same time with semantics in\nradiance fields. In this work, we present FastLGS, an approach that supports\nreal-time open-vocabulary query within 3D Gaussian Splatting (3DGS) under high\nresolution. We propose the semantic feature grid to save multi-view CLIP\nfeatures which are extracted based on Segment Anything Model (SAM) masks, and\nmap the grids to low dimensional features for semantic field training through\n3DGS. Once trained, we can restore pixel-aligned CLIP embeddings through\nfeature grids from rendered features for open-vocabulary queries. Comparisons\nwith other state-of-the-art methods prove that FastLGS can achieve the first\nplace performance concerning both speed and accuracy, where FastLGS is 98x\nfaster than LERF and 4x faster than LangSplat. Meanwhile, experiments show that\nFastLGS is adaptive and compatible with many downstream tasks, such as 3D\nsegmentation and 3D object inpainting, which can be easily applied to other 3D\nmanipulation systems.\n","authors":["Yuzhou Ji","He Zhu","Junshu Tang","Wuyi Liu","Zhizhong Zhang","Yuan Xie","Xin Tan"],"pdf_url":"https://arxiv.org/pdf/2406.01916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04150v1","updated":"2024-08-08T01:31:38Z","published":"2024-08-08T01:31:38Z","title":"Decorrelating Structure via Adapters Makes Ensemble Learning Practical\n  for Semi-supervised Learning","summary":"  In computer vision, traditional ensemble learning methods exhibit either a\nlow training efficiency or the limited performance to enhance the reliability\nof deep neural networks. In this paper, we propose a lightweight,\nloss-function-free, and architecture-agnostic ensemble learning by the\nDecorrelating Structure via Adapters (DSA) for various visual tasks.\nConcretely, the proposed DSA leverages the structure-diverse adapters to\ndecorrelate multiple prediction heads without any tailed regularization or\nloss. This allows DSA to be easily extensible to architecture-agnostic networks\nfor a range of computer vision tasks. Importantly, the theoretically analysis\nshows that the proposed DSA has a lower bias and variance than that of the\nsingle head based method (which is adopted by most of the state of art\napproaches). Consequently, the DSA makes deep networks reliable and robust for\nthe various real-world challenges, \\textit{e.g.}, data corruption, and label\nnoises. Extensive experiments combining the proposed method with FreeMatch\nachieved the accuracy improvements of 5.35% on CIFAR-10 dataset with 40 labeled\ndata and 0.71% on CIFAR-100 dataset with 400 labeled data. Besides, combining\nthe proposed method with DualPose achieved the improvements in the Percentage\nof Correct Keypoints (PCK) by 2.08% on the Sniffing dataset with 100 data (30\nlabeled data), 5.2% on the FLIC dataset with 100 data (including 50 labeled\ndata), and 2.35% on the LSP dataset with 200 data (100 labeled data).\n","authors":["Jiaqi Wu","Junbiao Pang","Qingming Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04145v1","updated":"2024-08-08T01:12:21Z","published":"2024-08-08T01:12:21Z","title":"ComKD-CLIP: Comprehensive Knowledge Distillation for Contrastive\n  Language-Image Pre-traning Model","summary":"  Contrastive Language-Image Pre-training (CLIP) excels in integrating semantic\ninformation between images and text through contrastive learning techniques. It\nhas achieved remarkable performance in various multimodal tasks. However, the\ndeployment of large CLIP models is hindered in resource-limited environments,\nwhile smaller models frequently fall short of meeting performance benchmarks\nnecessary for practical applications. In this paper, we propose a novel\napproach, coined as ComKD-CLIP: Comprehensive Knowledge Distillation for\nContrastive Language-Image Pre-traning Model, which aims to comprehensively\ndistill the knowledge from a large teacher CLIP model into a smaller student\nmodel, ensuring comparable performance with significantly reduced parameters.\nComKD-CLIP is composed of two key mechanisms: Image Feature Alignment (IFAlign)\nand Educational Attention (EduAttention). IFAlign makes the image features\nextracted by the student model closely match those extracted by the teacher\nmodel, enabling the student to learn teacher's knowledge of extracting image\nfeatures. EduAttention explores the cross-relationships between text features\nextracted by the teacher model and image features extracted by the student\nmodel, enabling the student model to learn how the teacher model integrates\ntext-image features. In addition, ComKD-CLIP can refine the knowledge distilled\nfrom IFAlign and EduAttention leveraging the results of text-image feature\nfusion by the teacher model, ensuring student model accurately absorbs the\nknowledge of teacher model. Extensive experiments conducted on 11 datasets have\ndemonstrated the superiority of the proposed method.\n","authors":["Yifan Chen","Xiaozhen Qiao","Zhe Sun","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2408.04145v1.pdf","comment":"first submit"},{"id":"http://arxiv.org/abs/2408.04144v1","updated":"2024-08-08T01:07:28Z","published":"2024-08-08T01:07:28Z","title":"Integrated Dynamic Phenological Feature for Remote Sensing Image Land\n  Cover Change Detection","summary":"  Remote sensing image change detection (CD) is essential for analyzing land\nsurface changes over time, with a significant challenge being the\ndifferentiation of actual changes from complex scenes while filtering out\npseudo-changes. A primary contributor to this challenge is the intra-class\ndynamic changes due to phenological characteristics in natural areas. To\novercome this, we introduce the InPhea model, which integrates phenological\nfeatures into a remote sensing image CD framework. The model features a\ndetector with a differential attention module for improved feature\nrepresentation of change information, coupled with high-resolution feature\nextraction and spatial pyramid blocks to enhance performance. Additionally, a\nconstrainer with four constraint modules and a multi-stage contrastive learning\napproach is employed to aid in the model's understanding of phenological\ncharacteristics. Experiments on the HRSCD, SECD, and PSCD-Wuhan datasets reveal\nthat InPhea outperforms other models, confirming its effectiveness in\naddressing phenological pseudo-changes and its overall model superiority.\n","authors":["Yi Liu","Chenhao Sun","Hao Ye","Xiangying Liu","Weilong Ju"],"pdf_url":"https://arxiv.org/pdf/2408.04144v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16043v2","updated":"2024-08-08T00:54:09Z","published":"2023-11-27T18:07:58Z","title":"Relightable 3D Gaussians: Realistic Point Cloud Relighting with BRDF\n  Decomposition and Ray Tracing","summary":"  In this paper, we present a novel differentiable point-based rendering\nframework to achieve photo-realistic relighting. To make the reconstructed\nscene relightable, we enhance vanilla 3D Gaussians by associating extra\nproperties, including normal vectors, BRDF parameters, and incident lighting\nfrom various directions. From a collection of multi-view images, the 3D scene\nis optimized through 3D Gaussian Splatting while BRDF and lighting are\ndecomposed by physically based differentiable rendering. To produce plausible\nshadow effects in photo-realistic relighting, we introduce an innovative\npoint-based ray tracing with the bounding volume hierarchies for efficient\nvisibility pre-computation. Extensive experiments demonstrate our improved BRDF\nestimation, novel view synthesis and relighting results compared to\nstate-of-the-art approaches. The proposed framework showcases the potential to\nrevolutionize the mesh-based graphics pipeline with a point-based pipeline\nenabling editing, tracing, and relighting.\n","authors":["Jian Gao","Chun Gu","Youtian Lin","Zhihao Li","Hao Zhu","Xun Cao","Li Zhang","Yao Yao"],"pdf_url":"https://arxiv.org/pdf/2311.16043v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09340v4","updated":"2024-08-08T00:30:35Z","published":"2023-03-16T14:21:45Z","title":"Improving Automated Hemorrhage Detection in Sparse-view Computed\n  Tomography via Deep Convolutional Neural Network based Artifact Reduction","summary":"  This is a preprint. The latest version has been published here:\nhttps://pubs.rsna.org/doi/10.1148/ryai.230275\n  Purpose: Sparse-view computed tomography (CT) is an effective way to reduce\ndose by lowering the total number of views acquired, albeit at the expense of\nimage quality, which, in turn, can impact the ability to detect diseases. We\nexplore deep learning-based artifact reduction in sparse-view cranial CT scans\nand its impact on automated hemorrhage detection. Methods: We trained a U-Net\nfor artefact reduction on simulated sparse-view cranial CT scans from 3000\npatients obtained from a public dataset and reconstructed with varying levels\nof sub-sampling. Additionally, we trained a convolutional neural network on\nfully sampled CT data from 17,545 patients for automated hemorrhage detection.\nWe evaluated the classification performance using the area under the receiver\noperator characteristic curves (AUC-ROCs) with corresponding 95% confidence\nintervals (CIs) and the DeLong test, along with confusion matrices. The\nperformance of the U-Net was compared to an analytical approach based on total\nvariation (TV). Results: The U-Net performed superior compared to unprocessed\nand TV-processed images with respect to image quality and automated hemorrhage\ndiagnosis. With U-Net post-processing, the number of views can be reduced from\n4096 (AUC-ROC: 0.974; 95% CI: 0.972-0.976) views to 512 views (0.973;\n0.971-0.975) with minimal decrease in hemorrhage detection (P<.001) and to 256\nviews (0.967; 0.964-0.969) with a slight performance decrease (P<.001).\nConclusion: The results suggest that U-Net based artifact reduction\nsubstantially enhances automated hemorrhage detection in sparse-view cranial\nCTs. Our findings highlight that appropriate post-processing is crucial for\noptimal image quality and diagnostic accuracy while minimizing radiation dose.\n","authors":["Johannes Thalhammer","Manuel Schultheiss","Tina Dorosti","Tobias Lasser","Franz Pfeiffer","Daniela Pfeiffer","Florian Schaff"],"pdf_url":"https://arxiv.org/pdf/2303.09340v4.pdf","comment":"11 pages, 6 figures, 1 table"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2407.21024v2","updated":"2024-08-08T15:32:43Z","published":"2024-07-13T14:23:57Z","title":"An Autonomous GIS Agent Framework for Geospatial Data Retrieval","summary":"  Powered by the emerging large language models (LLMs), autonomous geographic\ninformation systems (GIS) agents have the potential to accomplish spatial\nanalyses and cartographic tasks. However, a research gap exists to support\nfully autonomous GIS agents: how to enable agents to discover and download the\nnecessary data for geospatial analyses. This study proposes an autonomous GIS\nagent framework capable of retrieving required geospatial data by generating,\nexecuting, and debugging programs. The framework utilizes the LLM as the\ndecision-maker, selects the appropriate data source (s) from a pre-defined\nsource list, and fetches the data from the chosen source. Each data source has\na handbook that records the metadata and technical details for data retrieval.\nThe proposed framework is designed in a plug-and-play style to ensure\nflexibility and extensibility. Human users or autonomous data scrawlers can add\nnew data sources by adding new handbooks. We developed a prototype agent based\non the framework, released as a QGIS plugin (GeoData Retrieve Agent) and a\nPython program. Experiment results demonstrate its capability of retrieving\ndata from various sources including OpenStreetMap, administrative boundaries\nand demographic data from the US Census Bureau, satellite basemaps from ESRI\nWorld Imagery, global digital elevation model (DEM) from OpenTopography.org,\nweather data from a commercial provider, the COVID-19 cases from the NYTimes\nGitHub. Our study is among the first attempts to develop an autonomous\ngeospatial data retrieval agent.\n","authors":["Huan Ning","Zhenlong Li","Temitope Akinboyewa","M. Naser Lessani"],"pdf_url":"https://arxiv.org/pdf/2407.21024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2010.13442v4","updated":"2024-08-08T13:42:45Z","published":"2020-10-26T09:27:39Z","title":"Refl-Spanners: A Purely Regular Approach to Non-Regular Core Spanners","summary":"  The regular spanners (characterised by vset-automata) are closed under the\nalgebraic operations of union, join and projection, and have desirable\nalgorithmic properties. The core spanners (introduced by Fagin, Kimelfeld,\nReiss, and Vansummeren (PODS 2013, JACM 2015) as a formalisation of the core\nfunctionality of the query language AQL used in IBM's SystemT) additionally\nneed string-equality selections and it has been shown by Freydenberger and\nHolldack (ICDT 2016, Theory of Computing Systems 2018) that this leads to high\ncomplexity and even undecidability of the typical problems in static analysis\nand query evaluation. We propose an alternative approach to core spanners: by\nincorporating the string-equality selections directly into the regular language\nthat represents the underlying regular spanner (instead of treating it as an\nalgebraic operation on the table extracted by the regular spanner), we obtain a\nfragment of core spanners that, while having slightly weaker expressive power\nthan the full class of core spanners, arguably still covers the intuitive\napplications of string-equality selections for information extraction and has\nmuch better upper complexity bounds of the typical problems in static analysis\nand query evaluation.\n","authors":["Markus L. Schmid","Nicole Schweikardt"],"pdf_url":"https://arxiv.org/pdf/2010.13442v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04443v1","updated":"2024-08-08T13:14:39Z","published":"2024-08-08T13:14:39Z","title":"Pairing Clustered Inverted Indexes with kNN Graphs for Fast Approximate\n  Retrieval over Learned Sparse Representations","summary":"  Learned sparse representations form an effective and interpretable class of\nembeddings for text retrieval. While exact top-k retrieval over such embeddings\nfaces efficiency challenges, a recent algorithm called Seismic has enabled\nremarkably fast, highly-accurate approximate retrieval. Seismic statically\nprunes inverted lists, organizes each list into geometrically-cohesive blocks,\nand augments each block with a summary vector. At query time, each inverted\nlist associated with a query term is traversed one block at a time in an\narbitrary order, with the inner product between the query and summaries\ndetermining if a block must be evaluated. When a block is deemed promising, its\ndocuments are fully evaluated with a forward index. Seismic is one to two\norders of magnitude faster than state-of-the-art inverted index-based solutions\nand significantly outperforms the winning graph-based submissions to the BigANN\n2023 Challenge. In this work, we speed up Seismic further by introducing two\ninnovations to its query processing subroutine. First, we traverse blocks in\norder of importance, rather than arbitrarily. Second, we take the list of\ndocuments retrieved by Seismic and expand it to include the neighbors of each\ndocument using an offline k-regular nearest neighbor graph; the expanded list\nis then ranked to produce the final top-k set. Experiments on two public\ndatasets show that our extension, named SeismicWave, can reach almost-exact\naccuracy levels and is up to 2.2x faster than Seismic.\n","authors":["Sebastian Bruch","Franco Maria Nardini","Cosimo Rulli","Rossano Venturini"],"pdf_url":"https://arxiv.org/pdf/2408.04443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.11517v2","updated":"2024-08-08T12:52:43Z","published":"2024-05-19T11:12:10Z","title":"On the Convergence of No-Regret Dynamics in Information Retrieval Games\n  with Proportional Ranking Functions","summary":"  Publishers who publish their content on the web act strategically, in a\nbehavior that can be modeled within the online learning framework. Regret, a\ncentral concept in machine learning, serves as a canonical measure for\nassessing the performance of learning agents within this framework. We prove\nthat any proportional content ranking function with a concave activation\nfunction induces games in which no-regret learning dynamics converge. Moreover,\nfor proportional ranking functions, we prove the equivalence of the concavity\nof the activation function, the social concavity of the induced games and the\nconcavity of the induced games. We also study the empirical trade-offs between\npublishers' and users' welfare, under different choices of the activation\nfunction, using a state-of-the-art no-regret dynamics algorithm. Furthermore,\nwe demonstrate how the choice of the ranking function and changes in the\necosystem structure affect these welfare measures, as well as the dynamics'\nconvergence rate.\n","authors":["Omer Madmon","Idan Pipano","Itamar Reinman","Moshe Tennenholtz"],"pdf_url":"https://arxiv.org/pdf/2405.11517v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04395v1","updated":"2024-08-08T11:58:41Z","published":"2024-08-08T11:58:41Z","title":"Enhanced Semantic Graph Based Approach With Sentiment Analysis For User\n  Interest Retrieval From Social Sites","summary":"  Blogs and social networking sites serve as a platform to the users for\nexpressing their interests, ideas and thoughts. Targeted marketing uses the\nrecommendation systems for suggesting their services and products to the users\nor clients. So the method used by target marketing is extraction of keywords\nand main topics from the user generated texts. Most of conventional methods\ninvolve identifying the personal interests just on the basis of surveys and\nrating systems. But the proposed research differs in manner that it aim at\nusing the user generated text as a source medium for identifying and analyzing\nthe personal interest as a knowledge base area of users. Semantic graph based\napproach is proposed research work that identifies the references of clients\nand users by analyzing their own texts such as tweets. The keywords need to be\nextracted from the text generated by the user on the social networking sites.\nThis can be made possible by using several algorithms that extracts the\nkeywords automatically from the available content provided by the user. Based\non frequency and degree it ranks the extracted keywords. Furthermore, semantic\ngraph based model assists in providing useful suggestions just by extracting\nthe interests of users by analyzing their contents from social media. In this\napproach graph comprises of nodes and edges where nodes represents the keywords\nextracted by the algorithm and edges shows the semantic connection between the\nnodes. The method does not require internet related user activities like\nsurveys or ratings to gather user interest related information.\n","authors":["Usama Ahmed Jamal"],"pdf_url":"https://arxiv.org/pdf/2408.04395v1.pdf","comment":"This research was conducted as part of Master Thesis in Computer\n  Science by the first author at HITEC University Taxila"},{"id":"http://arxiv.org/abs/2408.04388v1","updated":"2024-08-08T11:44:57Z","published":"2024-08-08T11:44:57Z","title":"MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with\n  Large Language Models","summary":"  We study an emerging and intriguing problem of multimodal temporal event\nforecasting with large language models. Compared to using text or graph\nmodalities, the investigation of utilizing images for temporal event\nforecasting has not been fully explored, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we are particularly interested in\ntwo key questions of: 1) why images will help in temporal event forecasting,\nand 2) how to integrate images into the LLM-based forecasting framework. To\nanswer these research questions, we propose to identify two essential functions\nthat images play in the scenario of temporal event forecasting, i.e.,\nhighlighting and complementary. Then, we develop a novel framework, named\nMM-Forecast. It employs an Image Function Identification module to recognize\nthese functions as verbal descriptions using multimodal large language models\n(MLLMs), and subsequently incorporates these function descriptions into\nLLM-based forecasting models. To evaluate our approach, we construct a new\nmultimodal dataset, MidEast-TE-mm, by extending an existing event dataset\nMidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast\ncan correctly identify the image functions, and further more, incorporating\nthese verbal function descriptions significantly improves the forecasting\nperformance. The dataset, code, and prompts are available at\nhttps://github.com/LuminosityX/MM-Forecast.\n","authors":["Haoxuan Li","Zhengmao Yang","Yunshan Ma","Yi Bin","Yang Yang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2408.04388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04382v1","updated":"2024-08-08T11:37:32Z","published":"2024-08-08T11:37:32Z","title":"Judgment2vec: Apply Graph Analytics to Searching and Recommendation of\n  Similar Judgments","summary":"  In court practice, legal professionals rely on their training to provide\nopinions that resolve cases, one of the most crucial aspects being the ability\nto identify similar judgments from previous courts efficiently. However,\nfinding a similar case is challenging and often depends on experience, legal\ndomain knowledge, and extensive labor hours, making veteran lawyers or judges\nindispensable. This research aims to automate the analysis of judgment text\nsimilarity. We utilized a judgment dataset labeled as the \"golden standard\" by\nexperts, which includes human-verified features that can be converted into an\n\"expert similarity score.\" We then constructed a knowledge graph based on\n\"case-article\" relationships, ranking each case using natural language\nprocessing to derive a \"Node2vec similarity score.\" By evaluating these two\nsimilarity scores, we identified their discrepancies and relationships. The\nresults can significantly reduce the labor hours required for legal searches\nand recommendations, with potential applications extending to various fields of\ninformation retrieval.\n","authors":["Hsuan-Lei Shao"],"pdf_url":"https://arxiv.org/pdf/2408.04382v1.pdf","comment":"5 pages, 7 figures, 2 tables"},{"id":"http://arxiv.org/abs/2408.04381v1","updated":"2024-08-08T11:35:52Z","published":"2024-08-08T11:35:52Z","title":"Understanding and Modeling Job Marketplace with Pretrained Language\n  Models","summary":"  Job marketplace is a heterogeneous graph composed of interactions among\nmembers (job-seekers), companies, and jobs. Understanding and modeling job\nmarketplace can benefit both job seekers and employers, ultimately contributing\nto the greater good of the society. However, existing graph neural network\n(GNN)-based methods have shallow understandings of the associated textual\nfeatures and heterogeneous relations. To address the above challenges, we\npropose PLM4Job, a job marketplace foundation model that tightly couples\npretrained language models (PLM) with job market graph, aiming to fully utilize\nthe pretrained knowledge and reasoning ability to model member/job textual\nfeatures as well as various member-job relations simultaneously. In the\npretraining phase, we propose a heterogeneous ego-graph-based prompting\nstrategy to model and aggregate member/job textual features based on the\ntopological structure around the target member/job node, where entity type\nembeddings and graph positional embeddings are introduced accordingly to model\ndifferent entities and their heterogeneous relations. Meanwhile, a\nproximity-aware attention alignment strategy is designed to dynamically adjust\nthe attention of the PLM on ego-graph node tokens in the prompt, such that the\nattention can be better aligned with job marketplace semantics. Extensive\nexperiments at LinkedIn demonstrate the effectiveness of PLM4Job.\n","authors":["Yaochen Zhu","Liang Wu","Binchi Zhang","Song Wang","Qi Guo","Liangjie Hong","Luke Simon","Jundong Li"],"pdf_url":"https://arxiv.org/pdf/2408.04381v1.pdf","comment":"accepted by CIKM'24 applied research track"},{"id":"http://arxiv.org/abs/2408.04332v1","updated":"2024-08-08T09:35:01Z","published":"2024-08-08T09:35:01Z","title":"Mitigating Exposure Bias in Online Learning to Rank Recommendation: A\n  Novel Reward Model for Cascading Bandits","summary":"  Exposure bias is a well-known issue in recommender systems where items and\nsuppliers are not equally represented in the recommendation results. This bias\nbecomes particularly problematic over time as a few items are repeatedly\nover-represented in recommendation lists, leading to a feedback loop that\nfurther amplifies this bias. Although extensive research has addressed this\nissue in model-based or neighborhood-based recommendation algorithms, less\nattention has been paid to online recommendation models, such as those based on\ntop-K contextual bandits, where recommendation models are dynamically updated\nwith ongoing user feedback. In this paper, we study exposure bias in a class of\nwell-known contextual bandit algorithms known as Linear Cascading Bandits. We\nanalyze these algorithms in their ability to handle exposure bias and provide a\nfair representation of items in the recommendation results. Our analysis\nreveals that these algorithms fail to mitigate exposure bias in the long run\nduring the course of ongoing user interactions. We propose an Exposure-Aware\nreward model that updates the model parameters based on two factors: 1)\nimplicit user feedback and 2) the position of the item in the recommendation\nlist. The proposed model mitigates exposure bias by controlling the utility\nassigned to the items based on their exposure in the recommendation list. Our\nexperiments with two real-world datasets show that our proposed reward model\nimproves the exposure fairness of the linear cascading bandits over time while\nmaintaining the recommendation accuracy. It also outperforms the current\nbaselines. Finally, we prove a high probability upper regret bound for our\nproposed model, providing theoretical guarantees for its performance.\n","authors":["Masoud Mansoury","Bamshad Mobasher","Herke van Hoof"],"pdf_url":"https://arxiv.org/pdf/2408.04332v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04245v1","updated":"2024-08-08T06:17:13Z","published":"2024-08-08T06:17:13Z","title":"Scalable Transformer for High Dimensional Multivariate Time Series\n  Forecasting","summary":"  Deep models for Multivariate Time Series (MTS) forecasting have recently\ndemonstrated significant success. Channel-dependent models capture complex\ndependencies that channel-independent models cannot capture. However, the\nnumber of channels in real-world applications outpaces the capabilities of\nexisting channel-dependent models, and contrary to common expectations, some\nmodels underperform the channel-independent models in handling high-dimensional\ndata, which raises questions about the performance of channel-dependent models.\nTo address this, our study first investigates the reasons behind the suboptimal\nperformance of these channel-dependent models on high-dimensional MTS data. Our\nanalysis reveals that two primary issues lie in the introduced noise from\nunrelated series that increases the difficulty of capturing the crucial\ninter-channel dependencies, and challenges in training strategies due to\nhigh-dimensional data. To address these issues, we propose STHD, the Scalable\nTransformer for High-Dimensional Multivariate Time Series Forecasting. STHD has\nthree components: a) Relation Matrix Sparsity that limits the noise introduced\nand alleviates the memory issue; b) ReIndex applied as a training strategy to\nenable a more flexible batch size setting and increase the diversity of\ntraining data; and c) Transformer that handles 2-D inputs and captures channel\ndependencies. These components jointly enable STHD to manage the\nhigh-dimensional MTS while maintaining computational feasibility. Furthermore,\nexperimental results show STHD's considerable improvement on three\nhigh-dimensional datasets: Crime-Chicago, Wiki-People, and Traffic. The source\ncode and dataset are publicly available\nhttps://github.com/xinzzzhou/ScalableTransformer4HighDimensionMTSF.git.\n","authors":["Xin Zhou","Weiqing Wang","Wray Buntine","Shilin Qu","Abishek Sriramulu","Weicong Tan","Christoph Bergmeir"],"pdf_url":"https://arxiv.org/pdf/2408.04245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04232v1","updated":"2024-08-08T05:37:17Z","published":"2024-08-08T05:37:17Z","title":"Enhanced Traffic Flow Prediction with Multi-Segment Fusion Tensor Graph\n  Convolutional Networks","summary":"  Accurate traffic Flow Prediction can assist in traffic management, route\nplanning, and congestion mitigation, which holds significant importance in\nenhancing the efficiency and reliability of intelligent transportation systems\n(ITS). However, existing traffic flow prediction models suffer from limitations\nin capturing the complex spatial-temporal dependencies within traffic networks.\nIn order to address this issue, this study proposes a multi-segment fusion\ntensor graph convolutional network (MS-FTGCN) for traffic flow prediction with\nthe following three-fold ideas: a) building a unified spatial-temporal graph\nconvolutional framework based on Tensor M-product, which capture the\nspatial-temporal patterns simultaneously; b) incorporating hourly, daily, and\nweekly components to model multi temporal properties of traffic flows,\nrespectively; c) fusing the outputs of the three components by attention\nmechanism to obtain the final traffic flow prediction results. The results of\nexperiments conducted on two traffic flow datasets demonstrate that the\nproposed MS-FTGCN outperforms the state-of-the-art models.\n","authors":["Wei Zhang","Peng Tang"],"pdf_url":"https://arxiv.org/pdf/2408.04232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04211v1","updated":"2024-08-08T04:31:29Z","published":"2024-08-08T04:31:29Z","title":"MMREC: LLM Based Multi-Modal Recommender System","summary":"  The importance of recommender systems is growing rapidly due to the\nexponential increase in the volume of content generated daily. This surge in\ncontent presents unique challenges for designing effective recommender systems.\nKey among these challenges is the need to effectively leverage the vast amounts\nof natural language data and images that represent user preferences. This paper\npresents a novel approach to enhancing recommender systems by leveraging Large\nLanguage Models (LLMs) and deep learning techniques. The proposed framework\naims to improve the accuracy and relevance of recommendations by incorporating\nmulti-modal information processing and by the use of unified latent space\nrepresentation. The study explores the potential of LLMs to better understand\nand utilize natural language data in recommendation contexts, addressing the\nlimitations of previous methods. The framework efficiently extracts and\nintegrates text and image information through LLMs, unifying diverse modalities\nin a latent space to simplify the learning process for the ranking model.\nExperimental results demonstrate the enhanced discriminative power of the model\nwhen utilizing multi-modal information. This research contributes to the\nevolving field of recommender systems by showcasing the potential of LLMs and\nmulti-modal data integration to create more personalized and contextually\nrelevant recommendations.\n","authors":["Jiahao Tian","Jinman Zhao","Zhenkai Wang","Zhicheng Ding"],"pdf_url":"https://arxiv.org/pdf/2408.04211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04197v1","updated":"2024-08-08T03:35:35Z","published":"2024-08-08T03:35:35Z","title":"Pairwise Judgment Formulation for Semantic Embedding Model in Web Search","summary":"  Semantic Embedding Model (SEM), a neural network-based Siamese architecture,\nis gaining momentum in information retrieval and natural language processing.\nIn order to train SEM in a supervised fashion for Web search, the search engine\nquery log is typically utilized to automatically formulate pairwise judgments\nas training data. Despite the growing application of semantic embeddings in the\nsearch engine industry, little work has been done on formulating effective\npairwise judgments for training SEM. In this paper, we make the first in-depth\ninvestigation of a wide range of strategies for generating pairwise judgments\nfor SEM. An interesting (perhaps surprising) discovery reveals that the\nconventional pairwise judgment formulation strategy wildly used in the field of\npairwise Learning-to-Rank (LTR) is not necessarily effective for training SEM.\nThrough a large-scale empirical study based on query logs and click-through\nactivities from a major commercial search engine, we demonstrate the effective\nstrategies for SEM and highlight the advantages of a hybrid heuristic (i.e.,\nClicked > Non-Clicked) in comparison to the atomic heuristics (e.g., Clicked >\nSkipped) in LTR. We conclude with best practices for training SEM and offer\npromising insights for future research.\n","authors":["Mengze Hong","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04197v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04174v1","updated":"2024-08-08T02:36:04Z","published":"2024-08-08T02:36:04Z","title":"wav2graph: A Framework for Supervised Learning Knowledge Graph from\n  Speech","summary":"  Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.\n","authors":["Khai Le-Duc","Quy-Anh Dang","Tan-Hanh Pham","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2408.04174v1.pdf","comment":"Preprint, 32 pages"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.04619v1","updated":"2024-08-08T17:49:07Z","published":"2024-08-08T17:49:07Z","title":"Transformer Explainer: Interactive Learning of Text-Generative Models","summary":"  Transformers have revolutionized machine learning, yet their inner workings\nremain opaque to many. We present Transformer Explainer, an interactive\nvisualization tool designed for non-experts to learn about Transformers through\nthe GPT-2 model. Our tool helps users understand complex Transformer concepts\nby integrating a model overview and enabling smooth transitions across\nabstraction levels of mathematical operations and model structures. It runs a\nlive GPT-2 instance locally in the user's browser, empowering users to\nexperiment with their own input and observe in real-time how the internal\ncomponents and parameters of the Transformer work together to predict the next\ntokens. Our tool requires no installation or special hardware, broadening the\npublic's education access to modern generative AI techniques. Our open-sourced\ntool is available at https://poloclub.github.io/transformer-explainer/. A video\ndemo is available at https://youtu.be/ECR4oAwocjs.\n","authors":["Aeree Cho","Grace C. Kim","Alexander Karpekov","Alec Helbling","Zijie J. Wang","Seongmin Lee","Benjamin Hoover","Duen Horng Chau"],"pdf_url":"https://arxiv.org/pdf/2408.04619v1.pdf","comment":"To be presented at IEEE VIS 2024"},{"id":"http://arxiv.org/abs/2408.04614v1","updated":"2024-08-08T17:42:32Z","published":"2024-08-08T17:42:32Z","title":"Better Alignment with Instruction Back-and-Forth Translation","summary":"  We propose a new method, instruction back-and-forth translation, to construct\nhigh-quality synthetic data grounded in world knowledge for aligning large\nlanguage models (LLMs). Given documents from a web corpus, we generate and\ncurate synthetic instructions using the backtranslation approach proposed by Li\net al.(2023a), and rewrite the responses to improve their quality further based\non the initial documents. Fine-tuning with the resulting (backtranslated\ninstruction, rewritten response) pairs yields higher win rates on AlpacaEval\nthan using other common instruction datasets such as Humpback, ShareGPT, Open\nOrca, Alpaca-GPT4 and Self-instruct. We also demonstrate that rewriting the\nresponses with an LLM outperforms direct distillation, and the two generated\ntext distributions exhibit significant distinction in embedding space. Further\nanalysis shows that our backtranslated instructions are of higher quality than\nother sources of synthetic instructions, while our responses are more diverse\nand complex than those obtained from distillation. Overall we find that\ninstruction back-and-forth translation combines the best of both worlds --\nmaking use of the information diversity and quantity found on the web, while\nensuring the quality of the responses which is necessary for effective\nalignment.\n","authors":["Thao Nguyen","Jeffrey Li","Sewoong Oh","Ludwig Schmidt","Jason Weston","Luke Zettlemoyer","Xian Li"],"pdf_url":"https://arxiv.org/pdf/2408.04614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04607v1","updated":"2024-08-08T17:27:29Z","published":"2024-08-08T17:27:29Z","title":"Risk and cross validation in ridge regression with correlated samples","summary":"  Recent years have seen substantial advances in our understanding of\nhigh-dimensional ridge regression, but existing theories assume that training\nexamples are independent. By leveraging recent techniques from random matrix\ntheory and free probability, we provide sharp asymptotics for the in- and\nout-of-sample risks of ridge regression when the data points have arbitrary\ncorrelations. We demonstrate that in this setting, the generalized cross\nvalidation estimator (GCV) fails to correctly predict the out-of-sample risk.\nHowever, in the case where the noise residuals have the same correlations as\nthe data points, one can modify the GCV to yield an efficiently-computable\nunbiased estimator that concentrates in the high-dimensional limit, which we\ndub CorrGCV. We further extend our asymptotic analysis to the case where the\ntest point has nontrivial correlations with the training set, a setting often\nencountered in time series forecasting. Assuming knowledge of the correlation\nstructure of the time series, this again yields an extension of the GCV\nestimator, and sharply characterizes the degree to which such test points yield\nan overly optimistic prediction of long-time risk. We validate the predictions\nof our theory across a variety of high dimensional data.\n","authors":["Alexander Atanasov","Jacob A. Zavatone-Veth","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2408.04607v1.pdf","comment":"44 pages, 18 figures"},{"id":"http://arxiv.org/abs/2408.04595v1","updated":"2024-08-08T17:11:36Z","published":"2024-08-08T17:11:36Z","title":"Inference with the Upper Confidence Bound Algorithm","summary":"  In this paper, we discuss the asymptotic behavior of the Upper Confidence\nBound (UCB) algorithm in the context of multiarmed bandit problems and discuss\nits implication in downstream inferential tasks. While inferential tasks become\nchallenging when data is collected in a sequential manner, we argue that this\nproblem can be alleviated when the sequential algorithm at hand satisfies\ncertain stability property. This notion of stability is motivated from the\nseminal work of Lai and Wei (1982). Our first main result shows that such a\nstability property is always satisfied for the UCB algorithm, and as a result\nthe sample means for each arm are asymptotically normal. Next, we examine the\nstability properties of the UCB algorithm when the number of arms $K$ is\nallowed to grow with the number of arm pulls $T$. We show that in such a case\nthe arms are stable when $\\frac{\\log K}{\\log T} \\rightarrow 0$, and the number\nof near-optimal arms are large.\n","authors":["Koulik Khamaru","Cun-Hui Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.04595v1.pdf","comment":"17 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.04590v1","updated":"2024-08-08T17:01:26Z","published":"2024-08-08T17:01:26Z","title":"Learn To Learn More Precisely","summary":"  Meta-learning has been extensively applied in the domains of few-shot\nlearning and fast adaptation, achieving remarkable performance. While\nMeta-learning methods like Model-Agnostic Meta-Learning (MAML) and its variants\nprovide a good set of initial parameters for the model, the model still tends\nto learn shortcut features, which leads to poor generalization. In this paper,\nwe propose the formal conception of \"learn to learn more precisely\", which aims\nto make the model learn precise target knowledge from data and reduce the\neffect of noisy knowledge, such as background and noise. To achieve this\ntarget, we proposed a simple and effective meta-learning framework named Meta\nSelf-Distillation(MSD) to maximize the consistency of learned knowledge,\nenhancing the models' ability to learn precise target knowledge. In the inner\nloop, MSD uses different augmented views of the same support data to update the\nmodel respectively. Then in the outer loop, MSD utilizes the same query data to\noptimize the consistency of learned knowledge, enhancing the model's ability to\nlearn more precisely. Our experiment demonstrates that MSD exhibits remarkable\nperformance in few-shot classification tasks in both standard and augmented\nscenarios, effectively boosting the accuracy and consistency of knowledge\nlearned by the model.\n","authors":["Runxi Cheng","Yongxian Wei","Xianglong He","Wanyun Zhu","Songsong Huang","Fei Richard Yu","Fei Ma","Chun Yuan"],"pdf_url":"https://arxiv.org/pdf/2408.04590v1.pdf","comment":"10pages,4 figures, meta learning"},{"id":"http://arxiv.org/abs/2401.12731v2","updated":"2024-08-08T16:56:42Z","published":"2024-01-23T13:04:02Z","title":"The Distributional Uncertainty of the SHAP score in Explainable Machine\n  Learning","summary":"  Attribution scores reflect how important the feature values in an input\nentity are for the output of a machine learning model. One of the most popular\nattribution scores is the SHAP score, which is an instantiation of the general\nShapley value used in coalition game theory. The definition of this score\nrelies on a probability distribution on the entity population. Since the exact\ndistribution is generally unknown, it needs to be assigned subjectively or be\nestimated from data, which may lead to misleading feature scores. In this\npaper, we propose a principled framework for reasoning on SHAP scores under\nunknown entity population distributions. In our framework, we consider an\nuncertainty region that contains the potential distributions, and the SHAP\nscore of a feature becomes a function defined over this region. We study the\nbasic problems of finding maxima and minima of this function, which allows us\nto determine tight ranges for the SHAP scores of all features. In particular,\nwe pinpoint the complexity of these problems, and other related ones, showing\nthem to be NP-complete. Finally, we present experiments on a real-world\ndataset, showing that our framework may contribute to a more robust feature\nscoring.\n","authors":["Santiago Cifuentes","Leopoldo Bertossi","Nina Pardal","Sergio Abriola","Maria Vanina Martinez","Miguel Romero"],"pdf_url":"https://arxiv.org/pdf/2401.12731v2.pdf","comment":"In ECAI 2024 proceedings"},{"id":"http://arxiv.org/abs/2408.04586v1","updated":"2024-08-08T16:56:03Z","published":"2024-08-08T16:56:03Z","title":"Sampling for View Synthesis: From Local Light Field Fusion to Neural\n  Radiance Fields and Beyond","summary":"  Capturing and rendering novel views of complex real-world scenes is a\nlong-standing problem in computer graphics and vision, with applications in\naugmented and virtual reality, immersive experiences and 3D photography. The\nadvent of deep learning has enabled revolutionary advances in this area,\nclassically known as image-based rendering. However, previous approaches\nrequire intractably dense view sampling or provide little or no guidance for\nhow users should sample views of a scene to reliably render high-quality novel\nviews. Local light field fusion proposes an algorithm for practical view\nsynthesis from an irregular grid of sampled views that first expands each\nsampled view into a local light field via a multiplane image scene\nrepresentation, then renders novel views by blending adjacent local light\nfields. Crucially, we extend traditional plenoptic sampling theory to derive a\nbound that specifies precisely how densely users should sample views of a given\nscene when using our algorithm. We achieve the perceptual quality of Nyquist\nrate view sampling while using up to 4000x fewer views. Subsequent developments\nhave led to new scene representations for deep learning with view synthesis,\nnotably neural radiance fields, but the problem of sparse view synthesis from a\nsmall number of images has only grown in importance. We reprise some of the\nrecent results on sparse and even single image view synthesis, while posing the\nquestion of whether prescriptive sampling guidelines are feasible for the new\ngeneration of image-based rendering algorithms.\n","authors":["Ravi Ramamoorthi"],"pdf_url":"https://arxiv.org/pdf/2408.04586v1.pdf","comment":"Article written for Frontiers of Science Award, International\n  Congress on Basic Science, 2024"},{"id":"http://arxiv.org/abs/2408.04583v1","updated":"2024-08-08T16:48:33Z","published":"2024-08-08T16:48:33Z","title":"Unveiling the Power of Sparse Neural Networks for Feature Selection","summary":"  Sparse Neural Networks (SNNs) have emerged as powerful tools for efficient\nfeature selection. Leveraging the dynamic sparse training (DST) algorithms\nwithin SNNs has demonstrated promising feature selection capabilities while\ndrastically reducing computational overheads. Despite these advancements,\nseveral critical aspects remain insufficiently explored for feature selection.\nQuestions persist regarding the choice of the DST algorithm for network\ntraining, the choice of metric for ranking features/neurons, and the\ncomparative performance of these methods across diverse datasets when compared\nto dense networks. This paper addresses these gaps by presenting a\ncomprehensive systematic analysis of feature selection with sparse neural\nnetworks. Moreover, we introduce a novel metric considering sparse neural\nnetwork characteristics, which is designed to quantify feature importance\nwithin the context of SNNs. Our findings show that feature selection with SNNs\ntrained with DST algorithms can achieve, on average, more than $50\\%$ memory\nand $55\\%$ FLOPs reduction compared to the dense networks, while outperforming\nthem in terms of the quality of the selected features. Our code and the\nsupplementary material are available on GitHub\n(\\url{https://github.com/zahraatashgahi/Neuron-Attribution}).\n","authors":["Zahra Atashgahi","Tennison Liu","Mykola Pechenizkiy","Raymond Veldhuis","Decebal Constantin Mocanu","Mihaela van der Schaar"],"pdf_url":"https://arxiv.org/pdf/2408.04583v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.00161v2","updated":"2024-08-08T16:31:05Z","published":"2024-07-31T21:12:21Z","title":"Automatic Generation of Behavioral Test Cases For Natural Language\n  Processing Using Clustering and Prompting","summary":"  Recent work in behavioral testing for natural language processing (NLP)\nmodels, such as Checklist, is inspired by related paradigms in software\nengineering testing. They allow evaluation of general linguistic capabilities\nand domain understanding, hence can help evaluate conceptual soundness and\nidentify model weaknesses. However, a major challenge is the creation of test\ncases. The current packages rely on semi-automated approach using manual\ndevelopment which requires domain expertise and can be time consuming. This\npaper introduces an automated approach to develop test cases by exploiting the\npower of large language models and statistical techniques. It clusters the text\nrepresentations to carefully construct meaningful groups and then apply\nprompting techniques to automatically generate Minimal Functionality Tests\n(MFT). The well-known Amazon Reviews corpus is used to demonstrate our\napproach. We analyze the behavioral test profiles across four different\nclassification algorithms and discuss the limitations and strengths of those\nmodels.\n","authors":["Ying Li","Rahul Singh","Tarun Joshi","Agus Sudjianto"],"pdf_url":"https://arxiv.org/pdf/2408.00161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04570v1","updated":"2024-08-08T16:29:09Z","published":"2024-08-08T16:29:09Z","title":"Mathematical Programming For Adaptive Experiments","summary":"  Adaptive experimentation can significantly improve statistical power, but\nstandard algorithms overlook important practical issues including batched and\ndelayed feedback, personalization, non-stationarity, multiple objectives, and\nconstraints. To address these issues, the current algorithm design paradigm\ncrafts tailored methods for each problem instance. Since it is infeasible to\ndevise novel algorithms for every real-world instance, practitioners often have\nto resort to suboptimal approximations that do not address all of their\nchallenges. Moving away from developing bespoke algorithms for each setting, we\npresent a mathematical programming view of adaptive experimentation that can\nflexibly incorporate a wide range of objectives, constraints, and statistical\nprocedures. By formulating a dynamic program in the batched limit, our modeling\nframework enables the use of scalable optimization methods (e.g., SGD and\nauto-differentiation) to solve for treatment allocations. We evaluate our\nframework on benchmarks modeled after practical challenges such as\nnon-stationarity, personalization, multi-objectives, and constraints. Unlike\nbespoke algorithms such as modified variants of Thomson sampling, our\nmathematical programming approach provides remarkably robust performance across\ninstances.\n","authors":["Ethan Che","Daniel R. Jiang","Hongseok Namkoong","Jimmy Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04569v1","updated":"2024-08-08T16:28:56Z","published":"2024-08-08T16:28:56Z","title":"Activation thresholds and expressiveness of polynomial neural networks","summary":"  Polynomial neural networks have been implemented in a range of applications\nand present an advantageous framework for theoretical machine learning. A\npolynomial neural network of fixed architecture and activation degree gives an\nalgebraic map from the network's weights to a set of polynomials. The image of\nthis map is the space of functions representable by the network. Its Zariski\nclosure is an affine variety known as a neurovariety. The dimension of a\npolynomial neural network's neurovariety provides a measure of its\nexpressivity. In this work, we introduce the notion of the activation threshold\nof a network architecture which expresses when the dimension of a neurovariety\nachieves its theoretical maximum. In addition, we prove expressiveness results\nfor polynomial neural networks with equi-width~architectures.\n","authors":["Bella Finkel","Jose Israel Rodriguez","Chenxi Wu","Thomas Yahl"],"pdf_url":"https://arxiv.org/pdf/2408.04569v1.pdf","comment":"13 pages"},{"id":"http://arxiv.org/abs/2307.02694v3","updated":"2024-08-08T16:24:52Z","published":"2023-07-05T23:53:55Z","title":"Loss Functions and Metrics in Deep Learning","summary":"  When training or evaluating deep learning models, two essential parts are\npicking the proper loss function and deciding on performance metrics. In this\npaper, we provide a comprehensive overview of the most common loss functions\nand metrics used across many different types of deep learning tasks, from\ngeneral tasks such as regression and classification to more specific tasks in\nComputer Vision and Natural Language Processing. We introduce the formula for\neach loss and metric, discuss their strengths and limitations, and describe how\nthese methods can be applied to various problems within deep learning. We hope\nthis work serves as a reference for researchers and practitioners in the field,\nhelping them make informed decisions when selecting the most appropriate loss\nfunction and performance metrics for their deep learning projects.\n","authors":["Juan Terven","Diana M. Cordova-Esparza","Alfonso Ramirez-Pedraza","Edgar A. Chavez-Urbiola","Julio A. Romero-Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2307.02694v3.pdf","comment":"76 pages, 4 figures, 13 tables, 127 equations"},{"id":"http://arxiv.org/abs/2408.04556v1","updated":"2024-08-08T16:13:26Z","published":"2024-08-08T16:13:26Z","title":"Bias-Aware Low-Rank Adaptation: Mitigating Catastrophic Inheritance of\n  Large Language Models","summary":"  Large language models (LLMs) have exhibited remarkable proficiency across a\ndiverse array of natural language processing (NLP) tasks. However, adapting\nLLMs to downstream applications typically necessitates computationally\nintensive and memory-demanding fine-tuning procedures. To mitigate these\nburdens, parameter-efficient fine-tuning (PEFT) techniques have emerged as a\npromising approach to tailor LLMs with minimal computational overhead. While\nPEFT methods offer substantial advantages, they do not fully address the\npervasive issue of bias propagation from pre-training data. In this work, we\nintroduce Bias-Aware Low-Rank Adaptation (BA-LoRA), a novel PEFT method\ndesigned to counteract bias inheritance. BA-LoRA incorporates three distinct\nregularization terms: (1) consistency regularizer, (2) diversity regularizer,\nand (3) singular vector decomposition regularizer. These regularizers\ncollectively aim to improve the generative models' consistency, diversity, and\ngeneralization capabilities during the fine-tuning process. Through extensive\nexperiments on a variety of natural language understanding (NLU) and natural\nlanguage generation (NLG) tasks, employing prominent LLMs such as LLaMA,\nMistral, and Gemma, we demonstrate that BA-LoRA surpasses the performance of\nLoRA and its state-of-the-art variants. Moreover, our method effectively\nmitigates the deleterious effects of pre-training bias, leading to more\nreliable and robust model outputs. The code is available at\nhttps://github.com/cyp-jlu-ai/BA-LoRA.\n","authors":["Yupeng Chang","Yi Chang","Yuan Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04556v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2405.01462v2","updated":"2024-08-08T16:11:33Z","published":"2024-05-02T16:50:47Z","title":"Uncertainty for Active Learning on Graphs","summary":"  Uncertainty Sampling is an Active Learning strategy that aims to improve the\ndata efficiency of machine learning models by iteratively acquiring labels of\ndata points with the highest uncertainty. While it has proven effective for\nindependent data its applicability to graphs remains under-explored. We propose\nthe first extensive study of Uncertainty Sampling for node classification: (1)\nWe benchmark Uncertainty Sampling beyond predictive uncertainty and highlight a\nsignificant performance gap to other Active Learning strategies. (2) We develop\nground-truth Bayesian uncertainty estimates in terms of the data generating\nprocess and prove their effectiveness in guiding Uncertainty Sampling toward\noptimal queries. We confirm our results on synthetic data and design an\napproximate approach that consistently outperforms other uncertainty estimators\non real datasets. (3) Based on this analysis, we relate pitfalls in modeling\nuncertainty to existing methods. Our analysis enables and informs the\ndevelopment of principled uncertainty estimation on graphs.\n","authors":["Dominik Fuchsgruber","Tom Wollschlger","Bertrand Charpentier","Antonio Oroz","Stephan Gnnemann"],"pdf_url":"https://arxiv.org/pdf/2405.01462v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.01561v3","updated":"2024-08-08T16:00:01Z","published":"2024-06-03T17:44:11Z","title":"Long and Short Guidance in Score identity Distillation for One-Step\n  Text-to-Image Generation","summary":"  Diffusion-based text-to-image generation models trained on extensive\ntext-image pairs have shown the capacity to generate photorealistic images\nconsistent with textual descriptions. However, a significant limitation of\nthese models is their slow sample generation, which requires iterative\nrefinement through the same network. In this paper, we enhance Score identity\nDistillation (SiD) by developing long and short classifier-free guidance (LSG)\nto efficiently distill pretrained Stable Diffusion models without using real\ntraining data. SiD aims to optimize a model-based explicit score matching loss,\nutilizing a score-identity-based approximation alongside the proposed LSG for\npractical computation. By training exclusively with fake images synthesized\nwith its one-step generator, SiD equipped with LSG rapidly improves FID and\nCLIP scores, achieving state-of-the-art FID performance while maintaining a\ncompetitive CLIP score. Specifically, its data-free distillation of Stable\nDiffusion 1.5 achieves a record low FID of 8.15 on the COCO-2014 validation\nset, with a CLIP score of 0.304 at an LSG scale of 1.5, and an FID of 9.56 with\na CLIP score of 0.313 at an LSG scale of 2. Our code and distilled one-step\ntext-to-image generators are available at\nhttps://github.com/mingyuanzhou/SiD-LSG.\n","authors":["Mingyuan Zhou","Zhendong Wang","Huangjie Zheng","Hai Huang"],"pdf_url":"https://arxiv.org/pdf/2406.01561v3.pdf","comment":"Code and model checkpoints available at\n  https://github.com/mingyuanzhou/SiD-LSG"},{"id":"http://arxiv.org/abs/2408.04543v1","updated":"2024-08-08T15:50:03Z","published":"2024-08-08T15:50:03Z","title":"Quantum Machine Learning: Performance and Security Implications in\n  Real-World Applications","summary":"  Quantum computing has garnered significant attention in recent years from\nboth academia and industry due to its potential to achieve a \"quantum\nadvantage\" over classical computers. The advent of quantum computing introduces\nnew challenges for security and privacy. This poster explores the performance\nand security implications of quantum computing through a case study of machine\nlearning in a real-world application. We compare the performance of quantum\nmachine learning (QML) algorithms to their classical counterparts using the\nAlzheimer's disease dataset. Our results indicate that QML algorithms show\npromising potential while they still have not surpassed classical algorithms in\nterms of learning capability and convergence difficulty, and running quantum\nalgorithms through simulations on classical computers requires significantly\nlarge memory space and CPU time. Our study also indicates that QMLs have\ninherited vulnerabilities from classical machine learning algorithms while also\nintroduce new attack vectors.\n","authors":["Zhengping Jay Luo","Tyler Stewart","Mourya Narasareddygari","Rui Duan","Shangqing Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.04543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.03508v2","updated":"2024-08-08T15:37:19Z","published":"2024-08-07T02:19:17Z","title":"Autonomous, Self-driving Multi-Step Growth of Semiconductor\n  Heterostructures Guided by Machine Learning","summary":"  The semiconductor industry has prioritized automating repetitive tasks by\nclosed-loop, autonomous experimentation which enables accelerated optimization\nof complex multi-step processes. The emergence of machine learning (ML) has\nushered in automated process with minimal human intervention. In this work, we\ndevelop SemiEpi, a self-driving automation platform capable of executing\nmolecular beam epitaxy (MBE) growth with multi-steps, continuous in-situ\nmonitoring, and on-the-fly feedback control. By integrating standard hardware,\nhomemade software, curve fitting, and multiple ML models, SemiEpi operates\nautonomously, eliminating the need for extensive expertise in MBE processes to\nachieve optimal outcomes. The platform actively learns from previous\nexperimental results, identifying favorable conditions and proposing new\nexperiments to achieve the desired results. We standardize and optimize growth\nfor InAs/GaAs quantum dots (QDs) heterostructures to showcase the power of\nML-guided multi-step growth. A temperature calibration was implemented to get\nthe initial growth condition, and fine control of the process was executed\nusing ML. Leveraging RHEED movies acquired during the growth, SemiEpi\nsuccessfully identified and optimized a novel route for multi-step\nheterostructure growth. This work demonstrates the capabilities of closed-loop,\nML-guided systems in addressing challenges in multi-step growth for any device.\nOur method is critical to achieve repeatable materials growth using\ncommercially scalable tools. Our strategy facilitates the development of a\nhardware-independent process and enhancing process repeatability and stability,\neven without exhaustive knowledge of growth parameters.\n","authors":["Chao Shen","Wenkang Zhan","Hongyu Sun","Kaiyao Xin","Bo Xu","Zhanguo Wang","Chao Zhao"],"pdf_url":"https://arxiv.org/pdf/2408.03508v2.pdf","comment":"5 figures"},{"id":"http://arxiv.org/abs/2408.04532v1","updated":"2024-08-08T15:33:02Z","published":"2024-08-08T15:33:02Z","title":"How Transformers Utilize Multi-Head Attention in In-Context Learning? A\n  Case Study on Sparse Linear Regression","summary":"  Despite the remarkable success of transformer-based models in various\nreal-world tasks, their underlying mechanisms remain poorly understood. Recent\nstudies have suggested that transformers can implement gradient descent as an\nin-context learner for linear regression problems and have developed various\ntheoretical analyses accordingly. However, these works mostly focus on the\nexpressive power of transformers by designing specific parameter constructions,\nlacking a comprehensive understanding of their inherent working mechanisms\npost-training. In this study, we consider a sparse linear regression problem\nand investigate how a trained multi-head transformer performs in-context\nlearning. We experimentally discover that the utilization of multi-heads\nexhibits different patterns across layers: multiple heads are utilized and\nessential in the first layer, while usually only a single head is sufficient\nfor subsequent layers. We provide a theoretical explanation for this\nobservation: the first layer preprocesses the context data, and the following\nlayers execute simple optimization steps based on the preprocessed context.\nMoreover, we demonstrate that such a preprocess-then-optimize algorithm can\nsignificantly outperform naive gradient descent and ridge regression\nalgorithms. Further experimental results support our explanations. Our findings\noffer insights into the benefits of multi-head attention and contribute to\nunderstanding the more intricate mechanisms hidden within trained transformers.\n","authors":["Xingwu Chen","Lei Zhao","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2408.04532v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04531v1","updated":"2024-08-08T15:32:12Z","published":"2024-08-08T15:32:12Z","title":"AExGym: Benchmarks and Environments for Adaptive Experimentation","summary":"  Innovations across science and industry are evaluated using randomized trials\n(a.k.a. A/B tests). While simple and robust, such static designs are\ninefficient or infeasible for testing many hypotheses. Adaptive designs can\ngreatly improve statistical power in theory, but they have seen limited\nadoption due to their fragility in practice. We present a benchmark for\nadaptive experimentation based on real-world datasets, highlighting prominent\npractical challenges to operationalizing adaptivity: non-stationarity,\nbatched/delayed feedback, multiple outcomes and objectives, and external\nvalidity. Our benchmark aims to spur methodological development that puts\npractical performance (e.g., robustness) as a central concern, rather than\nmathematical guarantees on contrived instances. We release an open source\nlibrary, AExGym, which is designed with modularity and extensibility in mind to\nallow experimentation practitioners to develop custom environments and\nalgorithms.\n","authors":["Jimmy Wang","Ethan Che","Daniel R. Jiang","Hongseok Namkoong"],"pdf_url":"https://arxiv.org/pdf/2408.04531v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04526v1","updated":"2024-08-08T15:26:18Z","published":"2024-08-08T15:26:18Z","title":"Hybrid Reinforcement Learning Breaks Sample Size Barriers in Linear MDPs","summary":"  Hybrid Reinforcement Learning (RL), where an agent learns from both an\noffline dataset and online explorations in an unknown environment, has garnered\nsignificant recent interest. A crucial question posed by Xie et al. (2022) is\nwhether hybrid RL can improve upon the existing lower bounds established in\npurely offline and purely online RL without relying on the single-policy\nconcentrability assumption. While Li et al. (2023) provided an affirmative\nanswer to this question in the tabular PAC RL case, the question remains\nunsettled for both the regret-minimizing RL case and the non-tabular case.\n  In this work, building upon recent advancements in offline RL and\nreward-agnostic exploration, we develop computationally efficient algorithms\nfor both PAC and regret-minimizing RL with linear function approximation,\nwithout single-policy concentrability. We demonstrate that these algorithms\nachieve sharper error or regret bounds that are no worse than, and can improve\non, the optimal sample complexity in offline RL (the first algorithm, for PAC\nRL) and online RL (the second algorithm, for regret-minimizing RL) in linear\nMarkov decision processes (MDPs), regardless of the quality of the behavior\npolicy. To our knowledge, this work establishes the tightest theoretical\nguarantees currently available for hybrid RL in linear MDPs.\n","authors":["Kevin Tan","Wei Fan","Yuting Wei"],"pdf_url":"https://arxiv.org/pdf/2408.04526v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04520v1","updated":"2024-08-08T15:21:07Z","published":"2024-08-08T15:21:07Z","title":"Advancing Molecular Machine (Learned) Representations with\n  Stereoelectronics-Infused Molecular Graphs","summary":"  Molecular representation is a foundational element in our understanding of\nthe physical world. Its importance ranges from the fundamentals of chemical\nreactions to the design of new therapies and materials. Previous molecular\nmachine learning models have employed strings, fingerprints, global features,\nand simple molecular graphs that are inherently information-sparse\nrepresentations. However, as the complexity of prediction tasks increases, the\nmolecular representation needs to encode higher fidelity information. This work\nintroduces a novel approach to infusing quantum-chemical-rich information into\nmolecular graphs via stereoelectronic effects. We show that the explicit\naddition of stereoelectronic interactions significantly improves the\nperformance of molecular machine learning models. Furthermore,\nstereoelectronics-infused representations can be learned and deployed with a\ntailored double graph neural network workflow, enabling its application to any\ndownstream molecular machine learning task. Finally, we show that the learned\nrepresentations allow for facile stereoelectronic evaluation of previously\nintractable systems, such as entire proteins, opening new avenues of molecular\ndesign.\n","authors":["Daniil A. Boiko","Thiago Reschtzegger","Benjamin Sanchez-Lengeling","Samuel M. Blau","Gabe Gomes"],"pdf_url":"https://arxiv.org/pdf/2408.04520v1.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2403.16677v2","updated":"2024-08-08T15:02:13Z","published":"2024-03-25T12:14:48Z","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with\n  Neural Feature Compression","summary":"  Nanosatellite constellations equipped with sensors capturing large geographic\nregions provide unprecedented opportunities for Earth observation. As\nconstellation sizes increase, network contention poses a downlink bottleneck.\nOrbital Edge Computing (OEC) leverages limited onboard compute resources to\nreduce transfer costs by processing the raw captures at the source. However,\ncurrent solutions have limited practicability due to reliance on crude\nfiltering methods or over-prioritizing particular downstream tasks.\n  This work presents FOOL, an OEC-native and task-agnostic feature compression\nmethod that preserves prediction performance. FOOL partitions high-resolution\nsatellite imagery to maximize throughput. Further, it embeds context and\nleverages inter-tile dependencies to lower transfer costs with negligible\noverhead. While FOOL is a feature compressor, it can recover images with\ncompetitive scores on quality measures at lower bitrates. We extensively\nevaluate transfer cost reduction by including the peculiarity of intermittently\navailable network connections in low earth orbit. Lastly, we test the\nfeasibility of our system for standardized nanosatellite form factors. We\ndemonstrate that FOOL permits downlinking over 100x the data volume without\nrelying on prior information on the downstream tasks.\n","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"pdf_url":"https://arxiv.org/pdf/2403.16677v2.pdf","comment":"18 pages, double column, 19 figures, 7 tables, Revision 1"},{"id":"http://arxiv.org/abs/2408.04499v1","updated":"2024-08-08T14:50:48Z","published":"2024-08-08T14:50:48Z","title":"Knowledge-Aided Semantic Communication Leveraging Probabilistic\n  Graphical Modeling","summary":"  In this paper, we propose a semantic communication approach based on\nprobabilistic graphical model (PGM). The proposed approach involves\nconstructing a PGM from a training dataset, which is then shared as common\nknowledge between the transmitter and receiver. We evaluate the importance of\nvarious semantic features and present a PGM-based compression algorithm\ndesigned to eliminate predictable portions of semantic information.\nFurthermore, we introduce a technique to reconstruct the discarded semantic\ninformation at the receiver end, generating approximate results based on the\nPGM. Simulation results indicate a significant improvement in transmission\nefficiency over existing methods, while maintaining the quality of the\ntransmitted images.\n","authors":["Haowen Wan","Qianqian Yang","Jiancheng Tang","Zhiguo shi"],"pdf_url":"https://arxiv.org/pdf/2408.04499v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04498v1","updated":"2024-08-08T14:46:01Z","published":"2024-08-08T14:46:01Z","title":"Model-Based Transfer Learning for Contextual Reinforcement Learning","summary":"  Deep reinforcement learning is a powerful approach to complex decision\nmaking. However, one issue that limits its practical application is its\nbrittleness, sometimes failing to train in the presence of small changes in the\nenvironment. This work is motivated by the empirical observation that directly\napplying an already trained model to a related task often works remarkably\nwell, also called zero-shot transfer. We take this practical trick one step\nfurther to consider how to systematically select good tasks to train,\nmaximizing overall performance across a range of tasks. Given the high cost of\ntraining, it is critical to choose a small set of training tasks. The key idea\nbehind our approach is to explicitly model the performance loss (generalization\ngap) incurred by transferring a trained model. We hence introduce Model-Based\nTransfer Learning (MBTL) for solving contextual RL problems. In this work, we\nmodel the performance loss as a simple linear function of task context\nsimilarity. Furthermore, we leverage Bayesian optimization techniques to\nefficiently model and estimate the unknown training performance of the task\nspace. We theoretically show that the method exhibits regret that is sublinear\nin the number of training tasks and discuss conditions to further tighten\nregret bounds. We experimentally validate our methods using urban traffic and\nstandard control benchmarks. Despite the conceptual simplicity, the\nexperimental results suggest that MBTL can achieve greater performance than\nstrong baselines, including exhaustive training on all tasks, multi-task\ntraining, and random selection of training tasks. This work lays the\nfoundations for investigating explicit modeling of generalization, thereby\nenabling principled yet effective methods for contextual RL.\n","authors":["Jung-Hoon Cho","Vindula Jayawardana","Sirui Li","Cathy Wu"],"pdf_url":"https://arxiv.org/pdf/2408.04498v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.13458v3","updated":"2024-08-08T14:26:23Z","published":"2023-03-23T17:26:12Z","title":"Optimization Dynamics of Equivariant and Augmented Neural Networks","summary":"  We investigate the optimization of neural networks on symmetric data, and\ncompare the strategy of constraining the architecture to be equivariant to that\nof using data augmentation. Our analysis reveals that that the relative\ngeometry of the admissible and the equivariant layers, respectively, plays a\nkey role. Under natural assumptions on the data, network, loss, and group of\nsymmetries, we show that compatibility of the spaces of admissible layers and\nequivariant layers, in the sense that the corresponding orthogonal projections\ncommute, implies that the sets of equivariant stationary points are identical\nfor the two strategies. If the linear layers of the network also are given a\nunitary parametrization, the set of equivariant layers is even invariant under\nthe gradient flow for augmented models. Our analysis however also reveals that\neven in the latter situation, stationary points may be unstable for augmented\ntraining although they are stable for the manifestly equivariant models.\n","authors":["Oskar Nordenfors","Fredrik Ohlsson Axel Flinth"],"pdf_url":"https://arxiv.org/pdf/2303.13458v3.pdf","comment":"v3: Completely revised manuscript: New framework for neural nets, new\n  main result (involving compability condition), new experiments, new author.\n  v2: Revised manuscript. Mostly small edits, apart from new experiments (see\n  Appendix E)"},{"id":"http://arxiv.org/abs/2408.04482v1","updated":"2024-08-08T14:19:11Z","published":"2024-08-08T14:19:11Z","title":"SegXAL: Explainable Active Learning for Semantic Segmentation in Driving\n  Scene Scenarios","summary":"  Most of the sophisticated AI models utilize huge amounts of annotated data\nand heavy training to achieve high-end performance. However, there are certain\nchallenges that hinder the deployment of AI models \"in-the-wild\" scenarios,\ni.e., inefficient use of unlabeled data, lack of incorporation of human\nexpertise, and lack of interpretation of the results. To mitigate these\nchallenges, we propose a novel Explainable Active Learning (XAL) model,\nXAL-based semantic segmentation model \"SegXAL\", that can (i) effectively\nutilize the unlabeled data, (ii) facilitate the \"Human-in-the-loop\" paradigm,\nand (iii) augment the model decisions in an interpretable way. In particular,\nwe investigate the application of the SegXAL model for semantic segmentation in\ndriving scene scenarios. The SegXAL model proposes the image regions that\nrequire labeling assistance from Oracle by dint of explainable AI (XAI) and\nuncertainty measures in a weakly-supervised manner. Specifically, we propose a\nnovel Proximity-aware Explainable-AI (PAE) module and Entropy-based Uncertainty\n(EBU) module to get an Explainable Error Mask, which enables the machine\nteachers/human experts to provide intuitive reasoning behind the results and to\nsolicit feedback to the AI system via an active learning strategy. Such a\nmechanism bridges the semantic gap between man and machine through\ncollaborative intelligence, where humans and AI actively enhance each other's\ncomplementary strengths. A novel high-confidence sample selection technique\nbased on the DICE similarity coefficient is also presented within the SegXAL\nframework. Extensive quantitative and qualitative analyses are carried out in\nthe benchmarking Cityscape dataset. Results show the outperformance of our\nproposed SegXAL against other state-of-the-art models.\n","authors":["Sriram Mandalika","Athira Nambiar"],"pdf_url":"https://arxiv.org/pdf/2408.04482v1.pdf","comment":"17 pages, 7 figures. To appear in the proceedings of the 27th\n  International Conference on Pattern Recognition (ICPR), 01-05 December, 2024,\n  Kolkata, India"},{"id":"http://arxiv.org/abs/2408.04478v1","updated":"2024-08-08T14:08:39Z","published":"2024-08-08T14:08:39Z","title":"NFDI4Health workflow and service for synthetic data generation,\n  assessment and risk management","summary":"  Individual health data is crucial for scientific advancements, particularly\nin developing Artificial Intelligence (AI); however, sharing real patient\ninformation is often restricted due to privacy concerns. A promising solution\nto this challenge is synthetic data generation. This technique creates entirely\nnew datasets that mimic the statistical properties of real data, while\npreserving confidential patient information. In this paper, we present the\nworkflow and different services developed in the context of Germany's National\nData Infrastructure project NFDI4Health. First, two state-of-the-art AI tools\n(namely, VAMBN and MultiNODEs) for generating synthetic health data are\noutlined. Further, we introduce SYNDAT (a public web-based tool) which allows\nusers to visualize and assess the quality and risk of synthetic data provided\nby desired generative models. Additionally, the utility of the proposed methods\nand the web-based tool is showcased using data from Alzheimer's Disease\nNeuroimaging Initiative (ADNI) and the Center for Cancer Registry Data of the\nRobert Koch Institute (RKI).\n","authors":["Sobhan Moazemi","Tim Adams","Hwei Geok NG","Lisa Khnel","Julian Schneider","Anatol-Fiete Nher","Juliane Fluck","Holger Frhlich"],"pdf_url":"https://arxiv.org/pdf/2408.04478v1.pdf","comment":"9 pages, 4 figures, accepted for publication in the proceedings of\n  the 69th Annual Conference of the Society for Medical Informatics, Biometry\n  and Epidemiology (GMDS)"},{"id":"http://arxiv.org/abs/2408.03685v2","updated":"2024-08-08T13:52:44Z","published":"2024-08-07T10:53:07Z","title":"RL-ADN: A High-Performance Deep Reinforcement Learning Environment for\n  Optimal Energy Storage Systems Dispatch in Active Distribution Networks","summary":"  Deep Reinforcement Learning (DRL) presents a promising avenue for optimizing\nEnergy Storage Systems (ESSs) dispatch in distribution networks. This paper\nintroduces RL-ADN, an innovative open-source library specifically designed for\nsolving the optimal ESSs dispatch in active distribution networks. RL-ADN\noffers unparalleled flexibility in modeling distribution networks, and ESSs,\naccommodating a wide range of research goals. A standout feature of RL-ADN is\nits data augmentation module, based on Gaussian Mixture Model and Copula (GMC)\nfunctions, which elevates the performance ceiling of DRL agents. Additionally,\nRL-ADN incorporates the Laurent power flow solver, significantly reducing the\ncomputational burden of power flow calculations during training without\nsacrificing accuracy. The effectiveness of RL-ADN is demonstrated using in\ndifferent sizes of distribution networks, showing marked performance\nimprovements in the adaptability of DRL algorithms for ESS dispatch tasks. This\nenhancement is particularly beneficial from the increased diversity of training\nscenarios. Furthermore, RL-ADN achieves a tenfold increase in computational\nefficiency during training, making it highly suitable for large-scale network\napplications. The library sets a new benchmark in DRL-based ESSs dispatch in\ndistribution networks and it is poised to advance DRL applications in\ndistribution network operations significantly. RL-ADN is available at:\nhttps://github.com/ShengrenHou/RL-ADN and\nhttps://github.com/distributionnetworksTUDelft/RL-ADN.\n","authors":["Shengren Hou","Shuyi Gao","Weijie Xia","Edgar Mauricio Salazar Duque","Peter Palensky","Pedro P. Vergara"],"pdf_url":"https://arxiv.org/pdf/2408.03685v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04461v1","updated":"2024-08-08T13:42:18Z","published":"2024-08-08T13:42:18Z","title":"Random Walk Diffusion for Efficient Large-Scale Graph Generation","summary":"  Graph generation addresses the problem of generating new graphs that have a\ndata distribution similar to real-world graphs. While previous diffusion-based\ngraph generation methods have shown promising results, they often struggle to\nscale to large graphs. In this work, we propose ARROW-Diff (AutoRegressive\nRandOm Walk Diffusion), a novel random walk-based diffusion approach for\nefficient large-scale graph generation. Our method encompasses two components\nin an iterative process of random walk sampling and graph pruning. We\ndemonstrate that ARROW-Diff can scale to large graphs efficiently, surpassing\nother baseline methods in terms of both generation time and multiple graph\nstatistics, reflecting the high quality of the generated graphs.\n","authors":["Tobias Bernecker","Ghalia Rehawi","Francesco Paolo Casale","Janine Knauer-Arloth","Annalisa Marsico"],"pdf_url":"https://arxiv.org/pdf/2408.04461v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04460v1","updated":"2024-08-08T13:39:09Z","published":"2024-08-08T13:39:09Z","title":"An experimental comparative study of backpropagation and alternatives\n  for training binary neural networks for image classification","summary":"  Current artificial neural networks are trained with parameters encoded as\nfloating point numbers that occupy lots of memory space at inference time. Due\nto the increase in the size of deep learning models, it is becoming very\ndifficult to consider training and using artificial neural networks on edge\ndevices. Binary neural networks promise to reduce the size of deep neural\nnetwork models, as well as to increase inference speed while decreasing energy\nconsumption. Thus, they may allow the deployment of more powerful models on\nedge devices. However, binary neural networks are still proven to be difficult\nto train using the backpropagation-based gradient descent scheme. This paper\nextends the work of \\cite{crulis2023alternatives}, which proposed adapting to\nbinary neural networks two promising alternatives to backpropagation originally\ndesigned for continuous neural networks, and experimented with them on simple\nimage classification datasets. This paper proposes new experiments on the\nImageNette dataset, compares three different model architectures for image\nclassification, and adds two additional alternatives to backpropagation.\n","authors":["Ben Crulis","Barthelemy Serres","Cyril de Runz","Gilles Venturini"],"pdf_url":"https://arxiv.org/pdf/2408.04460v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.13000v2","updated":"2024-08-08T13:33:12Z","published":"2024-03-12T16:25:38Z","title":"Duwak: Dual Watermarks in Large Language Models","summary":"  As large language models (LLM) are increasingly used for text generation\ntasks, it is critical to audit their usages, govern their applications, and\nmitigate their potential harms. Existing watermark techniques are shown\neffective in embedding single human-imperceptible and machine-detectable\npatterns without significantly affecting generated text quality and semantics.\nHowever, the efficiency in detecting watermarks, i.e., the minimum number of\ntokens required to assert detection with significance and robustness against\npost-editing, is still debatable. In this paper, we propose, Duwak, to\nfundamentally enhance the efficiency and quality of watermarking by embedding\ndual secret patterns in both token probability distribution and sampling\nschemes. To mitigate expression degradation caused by biasing toward certain\ntokens, we design a contrastive search to watermark the sampling scheme, which\nminimizes the token repetition and enhances the diversity. We theoretically\nexplain the interdependency of the two watermarks within Duwak. We evaluate\nDuwak extensively on Llama2 under various post-editing attacks, against four\nstate-of-the-art watermarking techniques and combinations of them. Our results\nshow that Duwak marked text achieves the highest watermarked text quality at\nthe lowest required token count for detection, up to 70% tokens less than\nexisting approaches, especially under post paraphrasing.\n","authors":["Chaoyi Zhu","Jeroen Galjaard","Pin-Yu Chen","Lydia Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2403.13000v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16449v2","updated":"2024-08-08T13:32:10Z","published":"2024-05-26T06:33:11Z","title":"Reinforcement Learning for Jump-Diffusions, with Financial Applications","summary":"  We study continuous-time reinforcement learning (RL) for stochastic control\nin which system dynamics are governed by jump-diffusion processes. We formulate\nan entropy-regularized exploratory control problem with stochastic policies to\ncapture the exploration--exploitation balance essential for RL. Unlike the pure\ndiffusion case initially studied by Wang et al. (2020), the derivation of the\nexploratory dynamics under jump-diffusions calls for a careful formulation of\nthe jump part. Through a theoretical analysis, we find that one can simply use\nthe same policy evaluation and $q$-learning algorithms in Jia and Zhou (2022a,\n2023), originally developed for controlled diffusions, without needing to check\na priori whether the underlying data come from a pure diffusion or a\njump-diffusion. However, we show that the presence of jumps ought to affect\nparameterizations of actors and critics in general. We investigate as an\napplication the mean--variance portfolio selection problem with stock price\nmodelled as a jump-diffusion, and show that both RL algorithms and\nparameterizations are invariant with respect to jumps. Finally, we present a\ndetailed study on applying the general theory to option hedging.\n","authors":["Xuefeng Gao","Lingfei Li","Xun Yu Zhou"],"pdf_url":"https://arxiv.org/pdf/2405.16449v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04442v1","updated":"2024-08-08T13:14:19Z","published":"2024-08-08T13:14:19Z","title":"FedAD-Bench: A Unified Benchmark for Federated Unsupervised Anomaly\n  Detection in Tabular Data","summary":"  The emergence of federated learning (FL) presents a promising approach to\nleverage decentralized data while preserving privacy. Furthermore, the\ncombination of FL and anomaly detection is particularly compelling because it\nallows for detecting rare and critical anomalies (usually also rare in locally\ngathered data) in sensitive data from multiple sources, such as cybersecurity\nand healthcare. However, benchmarking the performance of anomaly detection\nmethods in FL environments remains an underexplored area. This paper introduces\nFedAD-Bench, a unified benchmark for evaluating unsupervised anomaly detection\nalgorithms within the context of FL. We systematically analyze and compare the\nperformance of recent deep learning anomaly detection models under federated\nsettings, which were typically assessed solely in centralized settings.\nFedAD-Bench encompasses diverse datasets and metrics to provide a holistic\nevaluation. Through extensive experiments, we identify key challenges such as\nmodel aggregation inefficiencies and metric unreliability. We present insights\ninto FL's regularization effects, revealing scenarios in which it outperforms\ncentralized approaches due to its inherent ability to mitigate overfitting. Our\nwork aims to establish a standardized benchmark to guide future research and\ndevelopment in federated anomaly detection, promoting reproducibility and fair\ncomparison across studies.\n","authors":["Ahmed Anwar","Brian Moser","Dayananda Herurkar","Federico Raue","Vinit Hegiste","Tatjana Legler","Andreas Dengel"],"pdf_url":"https://arxiv.org/pdf/2408.04442v1.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2408.04439v1","updated":"2024-08-08T13:10:03Z","published":"2024-08-08T13:10:03Z","title":"Deep Learning for identifying systolic complexes in SCG traces: a\n  cross-dataset analysis","summary":"  The seismocardiographic signal is a promising alternative to the traditional\nECG in the analysis of the cardiac activity. In particular, the systolic\ncomplex is known to be the most informative part of the seismocardiogram, thus\nrequiring further analysis. State-of-art solutions to detect the systolic\ncomplex are based on Deep Learning models, which have been proven effective in\npioneering studies. However, these solutions have only been tested in a\ncontrolled scenario considering only clean signals acquired from users\nmaintained still in supine position. On top of that, all these studies consider\ndata coming from a single dataset, ignoring the benefits and challenges related\nto a cross-dataset scenario. In this work, a cross-dataset experimental\nanalysis was performed considering also data from a real-world scenario. Our\nfindings prove the effectiveness of a deep learning solution, while showing the\nimportance of a personalization step to contrast the domain shift, namely a\nchange in data distribution between training and testing data. Finally, we\ndemonstrate the benefits of a multi-channels approach, leveraging the\ninformation extracted from both accelerometers and gyroscopes data.\n","authors":["Michele Craighero","Sarah Solbiati","Federica Mozzini","Enrico Caiani","Giacomo Boracchi"],"pdf_url":"https://arxiv.org/pdf/2408.04439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2212.08648v3","updated":"2024-08-08T13:09:51Z","published":"2022-12-16T18:48:54Z","title":"Connecting Permutation Equivariant Neural Networks and Partition\n  Diagrams","summary":"  Permutation equivariant neural networks are often constructed using tensor\npowers of $\\mathbb{R}^{n}$ as their layer spaces. We show that all of the\nweight matrices that appear in these neural networks can be obtained from\nSchur-Weyl duality between the symmetric group and the partition algebra. In\nparticular, we adapt Schur-Weyl duality to derive a simple, diagrammatic method\nfor calculating the weight matrices themselves.\n","authors":["Edward Pearce-Crump"],"pdf_url":"https://arxiv.org/pdf/2212.08648v3.pdf","comment":"ECAI 2024; 16 pages"},{"id":"http://arxiv.org/abs/2310.20609v2","updated":"2024-08-08T12:51:23Z","published":"2023-10-31T16:44:26Z","title":"Graph Matching via convex relaxation to the simplex","summary":"  This paper addresses the Graph Matching problem, which consists of finding\nthe best possible alignment between two input graphs, and has many applications\nin computer vision, network deanonymization and protein alignment. A common\napproach to tackle this problem is through convex relaxations of the NP-hard\n\\emph{Quadratic Assignment Problem} (QAP).\n  Here, we introduce a new convex relaxation onto the unit simplex and develop\nan efficient mirror descent scheme with closed-form iterations for solving this\nproblem. Under the correlated Gaussian Wigner model, we show that the simplex\nrelaxation admits a unique solution with high probability. In the noiseless\ncase, this is shown to imply exact recovery of the ground truth permutation.\nAdditionally, we establish a novel sufficiency condition for the input matrix\nin standard greedy rounding methods, which is less restrictive than the\ncommonly used `diagonal dominance' condition. We use this condition to show\nexact one-step recovery of the ground truth (holding almost surely) via the\nmirror descent scheme, in the noiseless setting. We also use this condition to\nobtain significantly improved conditions for the GRAMPA algorithm [Fan et al.\n2019] in the noiseless setting.\n","authors":["Ernesto Araya Valdivia","Hemant Tyagi"],"pdf_url":"https://arxiv.org/pdf/2310.20609v2.pdf","comment":"We fixed some typos and added Lemma 4. Reference to the published\n  version below"},{"id":"http://arxiv.org/abs/2408.04424v1","updated":"2024-08-08T12:48:54Z","published":"2024-08-08T12:48:54Z","title":"Detection of Animal Movement from Weather Radar using Self-Supervised\n  Learning","summary":"  Detecting flying animals (e.g., birds, bats, and insects) using weather radar\nhelps gain insights into animal movement and migration patterns, aids in\nmanagement efforts (such as biosecurity) and enhances our understanding of the\necosystem.The conventional approach to detecting animals in weather radar\ninvolves thresholding: defining and applying thresholds for the radar\nvariables, based on expert opinion. More recently, Deep Learning approaches\nhave been shown to provide improved performance in detection. However,\nobtaining sufficient labelled weather radar data for flying animals to build\nlearning-based models is time-consuming and labor-intensive. To address the\nchallenge of data labelling, we propose a self-supervised learning method for\ndetecting animal movement. In our proposed method, we pre-train our model on a\nlarge dataset with noisy labels produced by a threshold approach. The key\nadvantage is that the pre-trained dataset size is limited only by the number of\nradar images available. We then fine-tune the model on a small human-labelled\ndataset. Our experiments on Australian weather radar data for waterbird\nsegmentation show that the proposed method outperforms the current state-of-the\nart approach by 43.53% in the dice co-efficient statistic.\n","authors":["Mubin Ul Haque","Joel Janek Dabrowski","Rebecca M. Rogers","Hazel Parry"],"pdf_url":"https://arxiv.org/pdf/2408.04424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04413v1","updated":"2024-08-08T12:40:27Z","published":"2024-08-08T12:40:27Z","title":"Deeploy: Enabling Energy-Efficient Deployment of Small Language Models\n  On Heterogeneous Microcontrollers","summary":"  With the rise of Embodied Foundation Models (EFMs), most notably Small\nLanguage Models (SLMs), adapting Transformers for edge applications has become\na very active field of research. However, achieving end-to-end deployment of\nSLMs on microcontroller (MCU)-class chips without high-bandwidth off-chip main\nmemory access is still an open challenge. In this paper, we demonstrate\nhigh-efficiency end-to-end SLM deployment on a multicore RISC-V (RV32) MCU\naugmented with ML instruction extensions and a hardware neural processing unit\n(NPU). To automate the exploration of the constrained, multi-dimensional memory\nvs. computation tradeoffs involved in aggressive SLM deployment on\nheterogeneous (multicore+NPU) resources, we introduce Deeploy, a novel Deep\nNeural Network (DNN) compiler, which generates highly-optimized C code\nrequiring minimal runtime support. We demonstrate that Deeploy generates\nend-to-end code for executing SLMs, fully exploiting the RV32 cores'\ninstruction extensions and the NPU: We achieve leading-edge energy and\nthroughput of \\SI{490}{\\micro\\joule \\per Token}, at \\SI{340}{Token \\per\n\\second} for an SLM trained on the TinyStories dataset, running for the first\ntime on an MCU-class device without external memory.\n","authors":["Moritz Scherer","Luka Macan","Victor Jung","Philip Wiese","Luca Bompani","Alessio Burrello","Francesco Conti","Luca Benini"],"pdf_url":"https://arxiv.org/pdf/2408.04413v1.pdf","comment":"Accepted for publication at ESWEEK - CASES 2024"},{"id":"http://arxiv.org/abs/2406.01661v2","updated":"2024-08-08T12:17:56Z","published":"2024-06-03T17:55:02Z","title":"A Diffusion Model Framework for Unsupervised Neural Combinatorial\n  Optimization","summary":"  Learning to sample from intractable distributions over discrete sets without\nrelying on corresponding training data is a central problem in a wide range of\nfields, including Combinatorial Optimization. Currently, popular deep\nlearning-based approaches rely primarily on generative models that yield exact\nsample likelihoods. This work introduces a method that lifts this restriction\nand opens the possibility to employ highly expressive latent variable models\nlike diffusion models. Our approach is conceptually based on a loss that upper\nbounds the reverse Kullback-Leibler divergence and evades the requirement of\nexact sample likelihoods. We experimentally validate our approach in data-free\nCombinatorial Optimization and demonstrate that our method achieves a new\nstate-of-the-art on a wide range of benchmark problems.\n","authors":["Sebastian Sanokowski","Sepp Hochreiter","Sebastian Lehner"],"pdf_url":"https://arxiv.org/pdf/2406.01661v2.pdf","comment":"Accepted at ICML 2024"},{"id":"http://arxiv.org/abs/2408.04407v1","updated":"2024-08-08T12:16:14Z","published":"2024-08-08T12:16:14Z","title":"Clutter Classification Using Deep Learning in Multiple Stages","summary":"  Path loss prediction for wireless communications is highly dependent on the\nlocal environment. Propagation models including clutter information have been\nshown to significantly increase model accuracy. This paper explores the\napplication of deep learning to satellite imagery to identify environmental\nclutter types automatically. Recognizing these clutter types has numerous uses,\nbut our main application is to use clutter information to enhance propagation\nprediction models. Knowing the type of obstruction (tree, building, and further\nclassifications) can improve the prediction accuracy of key propagation metrics\nsuch as path loss.\n","authors":["Ryan Dempsey","Jonathan Ethier"],"pdf_url":"https://arxiv.org/pdf/2408.04407v1.pdf","comment":"SoutheastCon 2024"},{"id":"http://arxiv.org/abs/2408.04406v1","updated":"2024-08-08T12:15:45Z","published":"2024-08-08T12:15:45Z","title":"Finite sample learning of moving targets","summary":"  We consider a moving target that we seek to learn from samples. Our results\nextend randomized techniques developed in control and optimization for a\nconstant target to the case where the target is changing. We derive a novel\nbound on the number of samples that are required to construct a probably\napproximately correct (PAC) estimate of the target. Furthermore, when the\nmoving target is a convex polytope, we provide a constructive method of\ngenerating the PAC estimate using a mixed integer linear program (MILP). The\nproposed method is demonstrated on an application to autonomous emergency\nbraking.\n","authors":["Nikolaus Vertovec","Kostas Margellos","Maria Prandini"],"pdf_url":"https://arxiv.org/pdf/2408.04406v1.pdf","comment":"13 pages, 7 figures"},{"id":"http://arxiv.org/abs/2310.18247v3","updated":"2024-08-08T12:15:18Z","published":"2023-10-27T16:34:00Z","title":"Guided Data Augmentation for Offline Reinforcement Learning and\n  Imitation Learning","summary":"  In offline reinforcement learning (RL), an RL agent learns to solve a task\nusing only a fixed dataset of previously collected data. While offline RL has\nbeen successful in learning real-world robot control policies, it typically\nrequires large amounts of expert-quality data to learn effective policies that\ngeneralize to out-of-distribution states. Unfortunately, such data is often\ndifficult and expensive to acquire in real-world tasks. Several recent works\nhave leveraged data augmentation (DA) to inexpensively generate additional\ndata, but most DA works apply augmentations in a random fashion and ultimately\nproduce highly suboptimal augmented experience. In this work, we propose Guided\nData Augmentation (GuDA), a human-guided DA framework that generates\nexpert-quality augmented data. The key insight behind GuDA is that while it may\nbe difficult to demonstrate the sequence of actions required to produce expert\ndata, a user can often easily characterize when an augmented trajectory segment\nrepresents progress toward task completion. Thus, a user can restrict the space\nof possible augmentations to automatically reject suboptimal augmented data. To\nextract a policy from GuDA, we use off-the-shelf offline reinforcement learning\nand behavior cloning algorithms. We evaluate GuDA on a physical robot soccer\ntask as well as simulated D4RL navigation tasks, a simulated autonomous driving\ntask, and a simulated soccer task. Empirically, GuDA enables learning given a\nsmall initial dataset of potentially suboptimal experience and outperforms a\nrandom DA strategy as well as a model-based DA strategy.\n","authors":["Nicholas E. Corrado","Yuxiao Qu","John U. Balis","Adam Labiosa","Josiah P. Hanna"],"pdf_url":"https://arxiv.org/pdf/2310.18247v3.pdf","comment":"RLC 2024"},{"id":"http://arxiv.org/abs/2408.04405v1","updated":"2024-08-08T12:14:17Z","published":"2024-08-08T12:14:17Z","title":"Probabilistic energy forecasting through quantile regression in\n  reproducing kernel Hilbert spaces","summary":"  Accurate energy demand forecasting is crucial for sustainable and resilient\nenergy development. To meet the Net Zero Representative Concentration Pathways\n(RCP) $4.5$ scenario in the DACH countries, increased renewable energy\nproduction, energy storage, and reduced commercial building consumption are\nneeded. This scenario's success depends on hydroelectric capacity and climatic\nfactors. Informed decisions require quantifying uncertainty in forecasts. This\nstudy explores a non-parametric method based on \\emph{reproducing kernel\nHilbert spaces (RKHS)}, known as kernel quantile regression, for energy\nprediction. Our experiments demonstrate its reliability and sharpness, and we\nbenchmark it against state-of-the-art methods in load and price forecasting for\nthe DACH region. We offer our implementation in conjunction with additional\nscripts to ensure the reproducibility of our research.\n","authors":["Luca Pernigo","Rohan Sen","Davide Baroli"],"pdf_url":"https://arxiv.org/pdf/2408.04405v1.pdf","comment":"12 pages, {Owner/Author | ACM} {2024}. This is the author's version\n  of the work. It is posted here for your personal use. Not for redistribution.\n  The definitive Version of Record will published in https://energy.acm.org/eir"},{"id":"http://arxiv.org/abs/2408.04400v1","updated":"2024-08-08T12:08:55Z","published":"2024-08-08T12:08:55Z","title":"DIVE: Subgraph Disagreement for Graph Out-of-Distribution Generalization","summary":"  This paper addresses the challenge of out-of-distribution (OOD)\ngeneralization in graph machine learning, a field rapidly advancing yet\ngrappling with the discrepancy between source and target data distributions.\nTraditional graph learning algorithms, based on the assumption of uniform\ndistribution between training and test data, falter in real-world scenarios\nwhere this assumption fails, resulting in suboptimal performance. A principal\nfactor contributing to this suboptimal performance is the inherent simplicity\nbias of neural networks trained through Stochastic Gradient Descent (SGD),\nwhich prefer simpler features over more complex yet equally or more predictive\nones. This bias leads to a reliance on spurious correlations, adversely\naffecting OOD performance in various tasks such as image recognition, natural\nlanguage understanding, and graph classification. Current methodologies,\nincluding subgraph-mixup and information bottleneck approaches, have achieved\npartial success but struggle to overcome simplicity bias, often reinforcing\nspurious correlations. To tackle this, we propose DIVE, training a collection\nof models to focus on all label-predictive subgraphs by encouraging the models\nto foster divergence on the subgraph mask, which circumvents the limitation of\na model solely focusing on the subgraph corresponding to simple structural\npatterns. Specifically, we employs a regularizer to punish overlap in extracted\nsubgraphs across models, thereby encouraging different models to concentrate on\ndistinct structural patterns. Model selection for robust OOD performance is\nachieved through validation accuracy. Tested across four datasets from GOOD\nbenchmark and one dataset from DrugOOD benchmark, our approach demonstrates\nsignificant improvement over existing methods, effectively addressing the\nsimplicity bias and enhancing generalization in graph machine learning.\n","authors":["Xin Sun","Liang Wang","Qiang Liu","Shu Wu","Zilei Wang","Liang Wang"],"pdf_url":"https://arxiv.org/pdf/2408.04400v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04396v1","updated":"2024-08-08T12:03:03Z","published":"2024-08-08T12:03:03Z","title":"Evaluating the Impact of Pulse Oximetry Bias in Machine Learning under\n  Counterfactual Thinking","summary":"  Algorithmic bias in healthcare mirrors existing data biases. However, the\nfactors driving unfairness are not always known. Medical devices capture\nsignificant amounts of data but are prone to errors; for instance, pulse\noximeters overestimate the arterial oxygen saturation of darker-skinned\nindividuals, leading to worse outcomes. The impact of this bias in machine\nlearning (ML) models remains unclear. This study addresses the technical\nchallenges of quantifying the impact of medical device bias in downstream ML.\nOur experiments compare a \"perfect world\", without pulse oximetry bias, using\nSaO2 (blood-gas), to the \"actual world\", with biased measurements, using SpO2\n(pulse oximetry). Under this counterfactual design, two models are trained with\nidentical data, features, and settings, except for the method of measuring\noxygen saturation: models using SaO2 are a \"control\" and models using SpO2 a\n\"treatment\". The blood-gas oximetry linked dataset was a suitable test-bed,\ncontaining 163,396 nearly-simultaneous SpO2 - SaO2 paired measurements, aligned\nwith a wide array of clinical features and outcomes. We studied three\nclassification tasks: in-hospital mortality, respiratory SOFA score in the next\n24 hours, and SOFA score increase by two points. Models using SaO2 instead of\nSpO2 generally showed better performance. Patients with overestimation of O2 by\npulse oximetry of > 3% had significant decreases in mortality prediction\nrecall, from 0.63 to 0.59, P < 0.001. This mirrors clinical processes where\nbiased pulse oximetry readings provide clinicians with false reassurance of\npatients' oxygen levels. A similar degradation happened in ML models, with\npulse oximetry biases leading to more false negatives in predicting adverse\noutcomes.\n","authors":["Ins Martins","Joo Matos","Tiago Gonalves","Leo A. Celi","A. Ian Wong","Jaime S. Cardoso"],"pdf_url":"https://arxiv.org/pdf/2408.04396v1.pdf","comment":"10 pages; accepted at MICCAI's Third Workshop on Applications of\n  Medical AI (2024)"},{"id":"http://arxiv.org/abs/2408.04391v1","updated":"2024-08-08T11:51:34Z","published":"2024-08-08T11:51:34Z","title":"Robustness investigation of quality measures for the assessment of\n  machine learning models","summary":"  In this paper the accuracy and robustness of quality measures for the\nassessment of machine learning models are investigated. The prediction quality\nof a machine learning model is evaluated model-independent based on a\ncross-validation approach, where the approximation error is estimated for\nunknown data. The presented measures quantify the amount of explained variation\nin the model prediction. The reliability of these measures is assessed by means\nof several numerical examples, where an additional data set for the\nverification of the estimated prediction error is available. Furthermore, the\nconfidence bounds of the presented quality measures are estimated and local\nquality measures are derived from the prediction residuals obtained by the\ncross-validation approach.\n","authors":["Thomas Most","Lars Grning","Sebastian Wolff"],"pdf_url":"https://arxiv.org/pdf/2408.04391v1.pdf","comment":"under review, submitted to the journal Engineering Modelling,\n  Analysis & Simulation (EMAS)"},{"id":"http://arxiv.org/abs/2408.04380v1","updated":"2024-08-08T11:34:31Z","published":"2024-08-08T11:34:31Z","title":"Deep Generative Models in Robotics: A Survey on Learning from Multimodal\n  Demonstrations","summary":"  Learning from Demonstrations, the field that proposes to learn robot behavior\nmodels from data, is gaining popularity with the emergence of deep generative\nmodels. Although the problem has been studied for years under names such as\nImitation Learning, Behavioral Cloning, or Inverse Reinforcement Learning,\nclassical methods have relied on models that don't capture complex data\ndistributions well or don't scale well to large numbers of demonstrations. In\nrecent years, the robot learning community has shown increasing interest in\nusing deep generative models to capture the complexity of large datasets. In\nthis survey, we aim to provide a unified and comprehensive review of the last\nyear's progress in the use of deep generative models in robotics. We present\nthe different types of models that the community has explored, such as\nenergy-based models, diffusion models, action value maps, or generative\nadversarial networks. We also present the different types of applications in\nwhich deep generative models have been used, from grasp generation to\ntrajectory generation or cost learning. One of the most important elements of\ngenerative models is the generalization out of distributions. In our survey, we\nreview the different decisions the community has made to improve the\ngeneralization of the learned models. Finally, we highlight the research\nchallenges and propose a number of future directions for learning deep\ngenerative models in robotics.\n","authors":["Julen Urain","Ajay Mandlekar","Yilun Du","Mahi Shafiullah","Danfei Xu","Katerina Fragkiadaki","Georgia Chalvatzaki","Jan Peters"],"pdf_url":"https://arxiv.org/pdf/2408.04380v1.pdf","comment":"20 pages, 11 figures, submitted to TRO"},{"id":"http://arxiv.org/abs/2210.04797v3","updated":"2024-08-08T11:26:01Z","published":"2022-09-23T16:13:47Z","title":"DeepVol: Volatility Forecasting from High-Frequency Data with Dilated\n  Causal Convolutions","summary":"  Volatility forecasts play a central role among equity risk measures. Besides\ntraditional statistical models, modern forecasting techniques based on machine\nlearning can be employed when treating volatility as a univariate, daily\ntime-series. Moreover, econometric studies have shown that increasing the\nnumber of daily observations with high-frequency intraday data helps to improve\nvolatility predictions. In this work, we propose DeepVol, a model based on\nDilated Causal Convolutions that uses high-frequency data to forecast day-ahead\nvolatility. Our empirical findings demonstrate that dilated convolutional\nfilters are highly effective at extracting relevant information from intraday\nfinancial time-series, proving that this architecture can effectively leverage\npredictive information present in high-frequency data that would otherwise be\nlost if realised measures were precomputed. Simultaneously, dilated\nconvolutional filters trained with intraday high-frequency data help us avoid\nthe limitations of models that use daily data, such as model misspecification\nor manually designed handcrafted features, whose devise involves optimising the\ntrade-off between accuracy and computational efficiency and makes models prone\nto lack of adaptation into changing circumstances. In our analysis, we use two\nyears of intraday data from NASDAQ-100 to evaluate the performance of DeepVol.\nOur empirical results suggest that the proposed deep learning-based approach\neffectively learns global features from high-frequency data, resulting in more\naccurate predictions compared to traditional methodologies and producing more\naccurate risk measures.\n","authors":["Fernando Moreno-Pino","Stefan Zohren"],"pdf_url":"https://arxiv.org/pdf/2210.04797v3.pdf","comment":"Updated version"},{"id":"http://arxiv.org/abs/2408.04377v1","updated":"2024-08-08T11:22:52Z","published":"2024-08-08T11:22:52Z","title":"Anomaly Prediction: A Novel Approach with Explicit Delay and Horizon","summary":"  Detecting anomalies in time series data is a critical challenge across\nvarious domains. Traditional methods typically focus on identifying anomalies\nin immediate subsequent steps, often underestimating the significance of\ntemporal dynamics such as delay time and horizons of anomalies, which generally\nrequire extensive post-analysis. This paper introduces a novel approach for\ntime series anomaly prediction, incorporating temporal information directly\ninto the prediction results. We propose a new dataset specifically designed to\nevaluate this approach and conduct comprehensive experiments using several\nstate-of-the-art methods. results demonstrate the efficacy of our approach in\nproviding timely and accurate anomaly predictions, setting a new benchmark for\nfuture research in this field.\n","authors":["Jiang You","Arben Cela","Ren Natowicz","Jacob Ouanounou","Patrick Siarry"],"pdf_url":"https://arxiv.org/pdf/2408.04377v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04376v1","updated":"2024-08-08T11:18:40Z","published":"2024-08-08T11:18:40Z","title":"Deep Reinforcement Learning for the Design of Metamaterial Mechanisms\n  with Functional Compliance Control","summary":"  Metamaterial mechanisms are micro-architectured compliant structures that\noperate through the elastic deformation of specially designed flexible members.\nThis study develops an efficient design methodology for compliant mechanisms\nusing deep reinforcement learning (RL). For this purpose, design domains are\ndigitized into finite cells with various hinge connections, and finite element\nanalyses (FEAs) are conducted to evaluate the deformation behaviors of the\ncompliance mechanism with different cell combinations. The FEA data are learned\nthrough the RL method to obtain optimal compliant mechanisms for desired\nfunctional requirements. The RL algorithm is applied to the design of a\ncompliant door-latch mechanism, exploring the effect of human guidance and\ntiling direction. The optimal result is achieved with minimal human guidance\nand inward tiling, resulting in a threefold increase in the predefined reward\ncompared to human-designed mechanisms. The proposed approach is extended to the\ndesign of a soft gripper mechanism, where the effect of hinge connections is\nadditionally considered. The optimal design under hinge penalization reveals\nremarkably enhanced compliance, and its performance is validated by\nexperimental tests using an additively manufactured gripper. These findings\ndemonstrate that RL-optimized designs outperform those developed with human\ninsight, providing an efficient design methodology for cell-based compliant\nmechanisms in practical applications.\n","authors":["Yejun Choi","Yeoneung Kim","Keun Park"],"pdf_url":"https://arxiv.org/pdf/2408.04376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04369v1","updated":"2024-08-08T10:58:33Z","published":"2024-08-08T10:58:33Z","title":"Analyzing Consumer Reviews for Understanding Drivers of Hotels Ratings:\n  An Indian Perspective","summary":"  In the internet era, almost every business entity is trying to have its\ndigital footprint in digital media and other social media platforms. For these\nentities, word of mouse is also very important. Particularly, this is quite\ncrucial for the hospitality sector dealing with hotels, restaurants etc.\nConsumers do read other consumers reviews before making final decisions. This\nis where it becomes very important to understand which aspects are affecting\nmost in the minds of the consumers while giving their ratings. The current\nstudy focuses on the consumer reviews of Indian hotels to extract aspects\nimportant for final ratings. The study involves gathering data using web\nscraping methods, analyzing the texts using Latent Dirichlet Allocation for\ntopic extraction and sentiment analysis for aspect-specific sentiment mapping.\nFinally, it incorporates Random Forest to understand the importance of the\naspects in predicting the final rating of a user.\n","authors":["Subhasis Dasgupta","Soumya Roy","Jaydip Sen"],"pdf_url":"https://arxiv.org/pdf/2408.04369v1.pdf","comment":"This is the pre-print of the paper that was accepted for oral\n  presentation and publication in the proceedings of IEEE ICCCNT 2024 which was\n  organized as IIT Mandi, India from June 24 to 28, 2024. The paper is 5 pages\n  long and it contains 4 figures and 6 tables. The is not the final version of\n  the paper"},{"id":"http://arxiv.org/abs/2408.04360v1","updated":"2024-08-08T10:47:02Z","published":"2024-08-08T10:47:02Z","title":"Detecting Car Speed using Object Detection and Depth Estimation: A Deep\n  Learning Framework","summary":"  Road accidents are quite common in almost every part of the world, and, in\nmajority, fatal accidents are attributed to over speeding of vehicles. The\ntendency to over speeding is usually tried to be controlled using check points\nat various parts of the road but not all traffic police have the device to\ncheck speed with existing speed estimating devices such as LIDAR based, or\nRadar based guns. The current project tries to address the issue of vehicle\nspeed estimation with handheld devices such as mobile phones or wearable\ncameras with network connection to estimate the speed using deep learning\nframeworks.\n","authors":["Subhasis Dasgupta","Arshi Naaz","Jayeeta Choudhury","Nancy Lahiri"],"pdf_url":"https://arxiv.org/pdf/2408.04360v1.pdf","comment":"This is the pre-print of the paper which was accepted for oral\n  presentation and publication in the proceedings of IEEE CONIT 2024, organized\n  at Pune from June 21 to 23, 2024. The paper is 6 pages long and it contains\n  11 figures and 1 table. This is not the final version of the paper"},{"id":"http://arxiv.org/abs/2407.07454v3","updated":"2024-08-08T10:40:43Z","published":"2024-07-10T08:16:13Z","title":"CM-DQN: A Value-Based Deep Reinforcement Learning Model to Simulate\n  Confirmation Bias","summary":"  In human decision-making tasks, individuals learn through trials and\nprediction errors. When individuals learn the task, some are more influenced by\ngood outcomes, while others weigh bad outcomes more heavily. Such confirmation\nbias can lead to different learning effects. In this study, we propose a new\nalgorithm in Deep Reinforcement Learning, CM-DQN, which applies the idea of\ndifferent update strategies for positive or negative prediction errors, to\nsimulate the human decision-making process when the task's states are\ncontinuous while the actions are discrete. We test in Lunar Lander environment\nwith confirmatory, disconfirmatory bias and non-biased to observe the learning\neffects. Moreover, we apply the confirmation model in a multi-armed bandit\nproblem (environment in discrete states and discrete actions), which utilizes\nthe same idea as our proposed algorithm, as a contrast experiment to\nalgorithmically simulate the impact of different confirmation bias in\ndecision-making process. In both experiments, confirmatory bias indicates a\nbetter learning effect.\n","authors":["Jiacheng Shen","Lihan Feng"],"pdf_url":"https://arxiv.org/pdf/2407.07454v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.16093v3","updated":"2024-08-08T10:20:02Z","published":"2023-11-27T18:58:34Z","title":"Visual cognition in multimodal large language models","summary":"  A chief goal of artificial intelligence is to build machines that think like\npeople. Yet it has been argued that deep neural network architectures fail to\naccomplish this. Researchers have asserted these models' limitations in the\ndomains of causal reasoning, intuitive physics, and intuitive psychology. Yet\nrecent advancements, namely the rise of large language models, particularly\nthose designed for visual processing, have rekindled interest in the potential\nto emulate human-like cognitive abilities. This paper evaluates the current\nstate of vision-based large language models in the domains of intuitive\nphysics, causal reasoning, and intuitive psychology. Through a series of\ncontrolled experiments, we investigate the extent to which these modern models\ngrasp complex physical interactions, causal relationships, and intuitive\nunderstanding of others' preferences. Our findings reveal that, while some of\nthese models demonstrate a notable proficiency in processing and interpreting\nvisual data, they still fall short of human capabilities in these areas. Our\nresults emphasize the need for integrating more robust mechanisms for\nunderstanding causality, physical dynamics, and social cognition into\nmodern-day, vision-based language models, and point out the importance of\ncognitively-inspired benchmarks.\n","authors":["Luca M. Schulze Buschoff","Elif Akata","Matthias Bethge","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2311.16093v3.pdf","comment":"Updated manuscript"},{"id":"http://arxiv.org/abs/2408.04339v1","updated":"2024-08-08T09:49:26Z","published":"2024-08-08T09:49:26Z","title":"Self-Supervised Contrastive Graph Clustering Network via Structural\n  Information Fusion","summary":"  Graph clustering, a classical task in graph learning, involves partitioning\nthe nodes of a graph into distinct clusters. This task has applications in\nvarious real-world scenarios, such as anomaly detection, social network\nanalysis, and community discovery. Current graph clustering methods commonly\nrely on module pre-training to obtain a reliable prior distribution for the\nmodel, which is then used as the optimization objective. However, these methods\noften overlook deeper supervised signals, leading to sub-optimal reliability of\nthe prior distribution. To address this issue, we propose a novel deep graph\nclustering method called CGCN. Our approach introduces contrastive signals and\ndeep structural information into the pre-training process. Specifically, CGCN\nutilizes a contrastive learning mechanism to foster information\ninteroperability among multiple modules and allows the model to adaptively\nadjust the degree of information aggregation for different order structures.\nOur CGCN method has been experimentally validated on multiple real-world graph\ndatasets, showcasing its ability to boost the dependability of prior clustering\ndistributions acquired through pre-training. As a result, we observed notable\nenhancements in the performance of the model.\n","authors":["Xiaoyang Ji","Yuchen Zhou","Haofu Yang","Shiyue Xu","Jiahao Li"],"pdf_url":"https://arxiv.org/pdf/2408.04339v1.pdf","comment":"6 pages, 3 figures"},{"id":"http://arxiv.org/abs/2407.06704v2","updated":"2024-08-08T09:41:40Z","published":"2024-07-09T09:31:15Z","title":"Self-supervised visual learning from interactions with objects","summary":"  Self-supervised learning (SSL) has revolutionized visual representation\nlearning, but has not achieved the robustness of human vision. A reason for\nthis could be that SSL does not leverage all the data available to humans\nduring learning. When learning about an object, humans often purposefully turn\nor move around objects and research suggests that these interactions can\nsubstantially enhance their learning. Here we explore whether such\nobject-related actions can boost SSL. For this, we extract the actions\nperformed to change from one ego-centric view of an object to another in four\nvideo datasets. We then introduce a new loss function to learn visual and\naction embeddings by aligning the performed action with the representations of\ntwo images extracted from the same clip. This permits the performed actions to\nstructure the latent visual representation. Our experiments show that our\nmethod consistently outperforms previous methods on downstream category\nrecognition. In our analysis, we find that the observed improvement is\nassociated with a better viewpoint-wise alignment of different objects from the\nsame category. Overall, our work demonstrates that embodied interactions with\nobjects can improve SSL of object categories.\n","authors":["Arthur Aubret","Cline Teulire","Jochen Triesch"],"pdf_url":"https://arxiv.org/pdf/2407.06704v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.16010v2","updated":"2024-08-08T09:12:13Z","published":"2024-07-22T19:33:12Z","title":"AIDE: Antithetical, Intent-based, and Diverse Example-Based Explanations","summary":"  For many use-cases, it is often important to explain the prediction of a\nblack-box model by identifying the most influential training data samples.\nExisting approaches lack customization for user intent and often provide a\nhomogeneous set of explanation samples, failing to reveal the model's reasoning\nfrom different angles.\n  In this paper, we propose AIDE, an approach for providing antithetical (i.e.,\ncontrastive), intent-based, diverse explanations for opaque and complex models.\nAIDE distinguishes three types of explainability intents: interpreting a\ncorrect, investigating a wrong, and clarifying an ambiguous prediction. For\neach intent, AIDE selects an appropriate set of influential training samples\nthat support or oppose the prediction either directly or by contrast. To\nprovide a succinct summary, AIDE uses diversity-aware sampling to avoid\nredundancy and increase coverage of the training data.\n  We demonstrate the effectiveness of AIDE on image and text classification\ntasks, in three ways: quantitatively, assessing correctness and continuity;\nqualitatively, comparing anecdotal evidence from AIDE and other example-based\napproaches; and via a user study, evaluating multiple aspects of AIDE. The\nresults show that AIDE addresses the limitations of existing methods and\nexhibits desirable traits for an explainability method.\n","authors":["Ikhtiyor Nematov","Dimitris Sacharidis","Tomer Sagi","Katja Hose"],"pdf_url":"https://arxiv.org/pdf/2407.16010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02407v2","updated":"2024-08-08T09:06:50Z","published":"2024-08-05T12:01:42Z","title":"Terracorder: Sense Long and Prosper","summary":"  In-situ sensing devices need to be deployed in remote environments for long\nperiods of time; minimizing their power consumption is vital for maximising\nboth their operational lifetime and coverage. We introduce Terracorder -- a\nversatile multi-sensor device -- and showcase its exceptionally low power\nconsumption using an on-device reinforcement learning scheduler. We prototype a\nunique device setup for biodiversity monitoring and compare its battery life\nusing our scheduler against a number of fixed schedules; the scheduler captures\nmore than 80% of events at less than 50% of the number of activations of the\nbest-performing fixed schedule. We then explore how a collaborative scheduler\ncan maximise the useful operation of a network of devices, improving overall\nnetwork power consumption and robustness.\n","authors":["Josh Millar","Sarab Sethi","Hamed Haddadi","Anil Madhavapeddy"],"pdf_url":"https://arxiv.org/pdf/2408.02407v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2404.03710v2","updated":"2024-08-08T09:03:51Z","published":"2024-04-04T13:43:17Z","title":"Self-organized free-flight arrival for urban air mobility","summary":"  Urban air mobility is an innovative mode of transportation in which electric\nvertical takeoff and landing (eVTOL) vehicles operate between nodes called\nvertiports. We outline a self-organized vertiport arrival system based on deep\nreinforcement learning. The airspace around the vertiport is assumed to be\ncircular, and the vehicles can freely operate inside. Each aircraft is\nconsidered an individual agent and follows a shared policy, resulting in\ndecentralized actions that are based on local information. We investigate the\ndevelopment of the reinforcement learning policy during training and illustrate\nhow the algorithm moves from suboptimal local holding patterns to a safe and\nefficient final policy. The latter is validated in simulation-based scenarios,\nincluding robustness analyses against sensor noise and a changing distribution\nof inbound traffic. Lastly, we deploy the final policy on small-scale unmanned\naerial vehicles to showcase its real-world usability.\n","authors":["Martin Waltz","Ostap Okhrin","Michael Schultz"],"pdf_url":"https://arxiv.org/pdf/2404.03710v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04318v1","updated":"2024-08-08T08:52:29Z","published":"2024-08-08T08:52:29Z","title":"Deep Transfer Learning for Kidney Cancer Diagnosis","summary":"  Many incurable diseases prevalent across global societies stem from various\ninfluences, including lifestyle choices, economic conditions, social factors,\nand genetics. Research predominantly focuses on these diseases due to their\nwidespread nature, aiming to decrease mortality, enhance treatment options, and\nimprove healthcare standards. Among these, kidney disease stands out as a\nparticularly severe condition affecting men and women worldwide. Nonetheless,\nthere is a pressing need for continued research into innovative, early\ndiagnostic methods to develop more effective treatments for such diseases.\nRecently, automatic diagnosis of Kidney Cancer has become an important\nchallenge especially when using deep learning (DL) due to the importance of\ntraining medical datasets, which in most cases are difficult and expensive to\nobtain. Furthermore, in most cases, algorithms require data from the same\ndomain and a powerful computer with efficient storage capacity. To overcome\nthis issue, a new type of learning known as transfer learning (TL) has been\nproposed that can produce impressive results based on other different\npre-trained data. This paper presents, to the best of the authors' knowledge,\nthe first comprehensive survey of DL-based TL frameworks for kidney cancer\ndiagnosis. This is a strong contribution to help researchers understand the\ncurrent challenges and perspectives of this topic. Hence, the main limitations\nand advantages of each framework are identified and detailed critical analyses\nare provided. Looking ahead, the article identifies promising directions for\nfuture research. Moving on, the discussion is concluded by reflecting on the\npivotal role of TL in the development of precision medicine and its effects on\nclinical practice and research in oncology.\n","authors":["Yassine Habchi","Hamza Kheddar","Yassine Himeur","Abdelkrim Boukabou","Shadi Atalla","Wathiq Mansoor","Hussain Al-Ahmad"],"pdf_url":"https://arxiv.org/pdf/2408.04318v1.pdf","comment":"32 pages, 8 figures and 8 tables"},{"id":"http://arxiv.org/abs/2408.04315v1","updated":"2024-08-08T08:48:54Z","published":"2024-08-08T08:48:54Z","title":"Federated Cubic Regularized Newton Learning with\n  Sparsification-amplified Differential Privacy","summary":"  This paper investigates the use of the cubic-regularized Newton method within\na federated learning framework while addressing two major concerns that\ncommonly arise in federated learning: privacy leakage and communication\nbottleneck. We introduce a federated learning algorithm called Differentially\nPrivate Federated Cubic Regularized Newton (DP-FCRN). By leveraging\nsecond-order techniques, our algorithm achieves lower iteration complexity\ncompared to first-order methods. We also incorporate noise perturbation during\nlocal computations to ensure privacy. Furthermore, we employ sparsification in\nuplink transmission, which not only reduces the communication costs but also\namplifies the privacy guarantee. Specifically, this approach reduces the\nnecessary noise intensity without compromising privacy protection. We analyze\nthe convergence properties of our algorithm and establish the privacy\nguarantee. Finally, we validate the effectiveness of the proposed algorithm\nthrough experiments on a benchmark dataset.\n","authors":["Wei Huo","Changxin Liu","Kemi Ding","Karl Henrik Johansson","Ling Shi"],"pdf_url":"https://arxiv.org/pdf/2408.04315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04313v1","updated":"2024-08-08T08:47:20Z","published":"2024-08-08T08:47:20Z","title":"Better Locally Private Sparse Estimation Given Multiple Samples Per User","summary":"  Previous studies yielded discouraging results for item-level locally\ndifferentially private linear regression with $s^*$-sparsity assumption, where\nthe minimax rate for $nm$ samples is $\\mathcal{O}(s^{*}d / nm\\varepsilon^2)$.\nThis can be challenging for high-dimensional data, where the dimension $d$ is\nextremely large. In this work, we investigate user-level locally differentially\nprivate sparse linear regression. We show that with $n$ users each contributing\n$m$ samples, the linear dependency of dimension $d$ can be eliminated, yielding\nan error upper bound of $\\mathcal{O}(s^{*2} / nm\\varepsilon^2)$. We propose a\nframework that first selects candidate variables and then conducts estimation\nin the narrowed low-dimensional space, which is extendable to general sparse\nestimation problems with tight error bounds. Experiments on both synthetic and\nreal datasets demonstrate the superiority of the proposed methods. Both the\ntheoretical and empirical results suggest that, with the same number of\nsamples, locally private sparse estimation is better conducted when multiple\nsamples per user are available.\n","authors":["Yuheng Ma","Ke Jia","Hanfang Yang"],"pdf_url":"https://arxiv.org/pdf/2408.04313v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.11337v3","updated":"2024-08-08T08:44:52Z","published":"2023-12-18T16:41:30Z","title":"Challenges for Reinforcement Learning in Quantum Circuit Design","summary":"  Quantum computing (QC) in the current NISQ era is still limited in size and\nprecision. Hybrid applications mitigating those shortcomings are prevalent to\ngain early insight and advantages. Hybrid quantum machine learning (QML)\ncomprises both the application of QC to improve machine learning (ML) and ML to\nimprove QC architectures. This work considers the latter, leveraging\nreinforcement learning (RL) to improve quantum circuit design (QCD), which we\nformalize by a set of generic objectives. Furthermore, we propose qcd-gym, a\nconcrete framework formalized as a Markov decision process, to enable learning\npolicies capable of controlling a universal set of continuously parameterized\nquantum gates. Finally, we provide benchmark comparisons to assess the\nshortcomings and strengths of current state-of-the-art RL algorithms.\n","authors":["Philipp Altmann","Jonas Stein","Michael Klle","Adelina Brligea","Thomas Gabor","Thomy Phan","Sebastian Feld","Claudia Linnhoff-Popien"],"pdf_url":"https://arxiv.org/pdf/2312.11337v3.pdf","comment":"11 pages, 4 figures, accepted for publication at the 2024 IEEE\n  International Conference on Quantum Computing and Engineering (QCE)"},{"id":"http://arxiv.org/abs/2407.11652v6","updated":"2024-08-08T08:44:29Z","published":"2024-07-16T12:18:20Z","title":"CCVA-FL: Cross-Client Variations Adaptive Federated Learning for Medical\n  Imaging","summary":"  Federated Learning (FL) offers a privacy-preserving approach to train models\non decentralized data. Its potential in healthcare is significant, but\nchallenges arise due to cross-client variations in medical image data,\nexacerbated by limited annotations. This paper introduces Cross-Client\nVariations Adaptive Federated Learning (CCVA-FL) to address these issues.\nCCVA-FL aims to minimize cross-client variations by transforming images into a\ncommon feature space. It involves expert annotation of a subset of images from\neach client, followed by the selection of a client with the least data\ncomplexity as the target. Synthetic medical images are then generated using\nScalable Diffusion Models with Transformers (DiT) based on the target client's\nannotated images. These synthetic images, capturing diversity and representing\nthe original data, are shared with other clients. Each client then translates\nits local images into the target image space using image-to-image translation.\nThe translated images are subsequently used in a federated learning setting to\ndevelop a server model. Our results demonstrate that CCVA-FL outperforms\nVanilla Federated Averaging by effectively addressing data distribution\ndifferences across clients without compromising privacy.\n","authors":["Sunny Gupta","Amit Sethi"],"pdf_url":"https://arxiv.org/pdf/2407.11652v6.pdf","comment":"I found critical errors in the manuscript affecting its validity. I\n  need to correct these before resubmitting. Major changes to methodology and\n  results are underway, significantly altering the content. I will resubmit the\n  revised version"},{"id":"http://arxiv.org/abs/2408.04310v1","updated":"2024-08-08T08:42:47Z","published":"2024-08-08T08:42:47Z","title":"Constructing Adversarial Examples for Vertical Federated Learning:\n  Optimal Client Corruption through Multi-Armed Bandit","summary":"  Vertical federated learning (VFL), where each participating client holds a\nsubset of data features, has found numerous applications in finance,\nhealthcare, and IoT systems. However, adversarial attacks, particularly through\nthe injection of adversarial examples (AEs), pose serious challenges to the\nsecurity of VFL models. In this paper, we investigate such vulnerabilities\nthrough developing a novel attack to disrupt the VFL inference process, under a\npractical scenario where the adversary is able to adaptively corrupt a subset\nof clients. We formulate the problem of finding optimal attack strategies as an\nonline optimization problem, which is decomposed into an inner problem of\nadversarial example generation (AEG) and an outer problem of corruption pattern\nselection (CPS). Specifically, we establish the equivalence between the\nformulated CPS problem and a multi-armed bandit (MAB) problem, and propose the\nThompson sampling with Empirical maximum reward (E-TS) algorithm for the\nadversary to efficiently identify the optimal subset of clients for corruption.\nThe key idea of E-TS is to introduce an estimation of the expected maximum\nreward for each arm, which helps to specify a small set of competitive arms, on\nwhich the exploration for the optimal arm is performed. This significantly\nreduces the exploration space, which otherwise can quickly become prohibitively\nlarge as the number of clients increases. We analytically characterize the\nregret bound of E-TS, and empirically demonstrate its capability of efficiently\nrevealing the optimal corruption pattern with the highest attack success rate,\nunder various datasets of popular VFL tasks.\n","authors":["Duanyi Yao","Songze Li","Ye Xue","Jin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04310v1.pdf","comment":"Published on ICLR2024"},{"id":"http://arxiv.org/abs/2408.04309v1","updated":"2024-08-08T08:42:30Z","published":"2024-08-08T08:42:30Z","title":"TheGlueNote: Learned Representations for Robust and Flexible Note\n  Alignment","summary":"  Note alignment refers to the task of matching individual notes of two\nversions of the same symbolically encoded piece. Methods addressing this task\ncommonly rely on sequence alignment algorithms such as Hidden Markov Models or\nDynamic Time Warping (DTW) applied directly to note or onset sequences. While\nsuccessful in many cases, such methods struggle with large mismatches between\nthe versions. In this work, we learn note-wise representations from data\naugmented with various complex mismatch cases, e.g. repeats, skips, block\ninsertions, and long trills. At the heart of our approach lies a transformer\nencoder network - TheGlueNote - which predicts pairwise note similarities for\ntwo 512 note subsequences. We postprocess the predicted similarities using\nflavors of weightedDTW and pitch-separated onsetDTW to retrieve note matches\nfor two sequences of arbitrary length. Our approach performs on par with the\nstate of the art in terms of note alignment accuracy, is considerably more\nrobust to version mismatches, and works directly on any pair of MIDI files.\n","authors":["Silvan David Peter","Gerhard Widmer"],"pdf_url":"https://arxiv.org/pdf/2408.04309v1.pdf","comment":"to be published in Proceedings of the 25th International Society for\n  Music Information Retrieval Conference (ISMIR), 2024"},{"id":"http://arxiv.org/abs/2408.04307v1","updated":"2024-08-08T08:40:15Z","published":"2024-08-08T08:40:15Z","title":"Partial Experts Checkpoint: Efficient Fault Tolerance for Sparse\n  Mixture-of-Experts Model Training","summary":"  As large language models continue to scale up, the imperative for fault\ntolerance in distributed deep learning systems intensifies, becoming a focal\narea of AI infrastructure research. Checkpoint has emerged as the predominant\nfault tolerance strategy, with extensive studies dedicated to optimizing its\nefficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model\npresents new challenges for traditional checkpoint techniques due to the\nsubstantial increase in model size, despite comparable computational demands to\ndense models. Breaking new ground in the realm of efficient fault tolerance for\nMoE model training, we introduce a novel Partial Experts Checkpoint (PEC)\nmechanism alongside a corresponding PEC fault-tolerant system. Our approach\nstrategically checkpoints a selected subset of experts, thereby significantly\nreducing the checkpoint size for MoE models to a level comparable with that of\ndense models. The empirical analysis on our 8-expert GPT-MoE model demonstrates\nthat the proposed PEC approach facilitates a substantial 54.2% decrease in the\nsize of non-redundant checkpoint (no data-parallel duplication), without\ncompromising the final model quality. Moreover, our PEC fault-tolerant system\nachieves a 76.9% reduction in checkpoint workload per data-parallel distributed\nrank, thereby correspondingly diminishing the checkpointing time and\nfacilitating complete overlap with the training process.\n","authors":["Weilin Cai","Le Qin","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2408.04307v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04303v1","updated":"2024-08-08T08:37:28Z","published":"2024-08-08T08:37:28Z","title":"Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language\n  Adaptation of LLMs for Low-Resource NLP","summary":"  The development of monolingual language models for low and mid-resource\nlanguages continues to be hindered by the difficulty in sourcing high-quality\ntraining data. In this study, we present a novel cross-lingual vocabulary\ntransfer strategy, trans-tokenization, designed to tackle this challenge and\nenable more efficient language adaptation. Our approach focuses on adapting a\nhigh-resource monolingual LLM to an unseen target language by initializing the\ntoken embeddings of the target language using a weighted average of\nsemantically similar token embeddings from the source language. For this, we\nleverage a translation resource covering both the source and target languages.\nWe validate our method with the Tweeties, a series of trans-tokenized LLMs, and\ndemonstrate their competitive performance on various downstream tasks across a\nsmall but diverse set of languages. Additionally, we introduce Hydra LLMs,\nmodels with multiple swappable language modeling heads and embedding tables,\nwhich further extend the capabilities of our trans-tokenization strategy. By\ndesigning a Hydra LLM based on the multilingual model TowerInstruct, we\ndeveloped a state-of-the-art machine translation model for Tatar, in a\nzero-shot manner, completely bypassing the need for high-quality parallel data.\nThis breakthrough is particularly significant for low-resource languages like\nTatar, where high-quality parallel data is hard to come by. By lowering the\ndata and time requirements for training high-quality models, our\ntrans-tokenization strategy allows for the development of LLMs for a wider\nrange of languages, especially those with limited resources. We hope that our\nwork will inspire further research and collaboration in the field of\ncross-lingual vocabulary transfer and contribute to the empowerment of\nlanguages on a global scale.\n","authors":["Franois Remy","Pieter Delobelle","Hayastan Avetisyan","Alfiya Khabibullina","Miryam de Lhoneux","Thomas Demeester"],"pdf_url":"https://arxiv.org/pdf/2408.04303v1.pdf","comment":"Accepted at COLM 2024"},{"id":"http://arxiv.org/abs/2402.12062v3","updated":"2024-08-08T08:36:03Z","published":"2024-02-19T11:30:00Z","title":"Causal Equal Protection as Algorithmic Fairness","summary":"  By combining the philosophical literature on statistical evidence and the\ninterdisciplinary literature on algorithmic fairness, we revisit recent\nobjections against classification parity in light of causal analyses of\nalgorithmic fairness and the distinction between predictive and diagnostic\nevidence. We focus on trial proceedings as a black-box classification algorithm\nin which defendants are sorted into two groups by convicting or acquitting\nthem. We defend a novel principle, causal equal protection, that combines\nclassification parity with the causal approach. In the do-calculus, causal\nequal protection requires that individuals should not be subject to uneven\nrisks of classification error because of their protected or socially salient\ncharacteristics. The explicit use of protected characteristics, however, may be\nrequired if it equalizes these risks.\n","authors":["Marcello Di Bello","Nicol Cangiotti","Michele Loi"],"pdf_url":"https://arxiv.org/pdf/2402.12062v3.pdf","comment":"16 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04301v1","updated":"2024-08-08T08:35:32Z","published":"2024-08-08T08:35:32Z","title":"Tackling Noisy Clients in Federated Learning with End-to-end Label\n  Correction","summary":"  Recently, federated learning (FL) has achieved wide successes for diverse\nprivacy-sensitive applications without sacrificing the sensitive private\ninformation of clients. However, the data quality of client datasets can not be\nguaranteed since corresponding annotations of different clients often contain\ncomplex label noise of varying degrees, which inevitably causes the performance\ndegradation. Intuitively, the performance degradation is dominated by clients\nwith higher noise rates since their trained models contain more misinformation\nfrom data, thus it is necessary to devise an effective optimization scheme to\nmitigate the negative impacts of these noisy clients. In this work, we propose\na two-stage framework FedELC to tackle this complicated label noise issue. The\nfirst stage aims to guide the detection of noisy clients with higher label\nnoise, while the second stage aims to correct the labels of noisy clients' data\nvia an end-to-end label correction framework which is achieved by learning\npossible ground-truth labels of noisy clients' datasets via back propagation.\nWe implement sixteen related methods and evaluate five datasets with three\ntypes of complicated label noise scenarios for a comprehensive comparison.\nExtensive experimental results demonstrate our proposed framework achieves\nsuperior performance than its counterparts for different scenarios.\nAdditionally, we effectively improve the data quality of detected noisy\nclients' local datasets with our label correction framework. The code is\navailable at https://github.com/Sprinter1999/FedELC.\n","authors":["Xuefeng Jiang","Sheng Sun","Jia Li","Jingjing Xue","Runhan Li","Zhiyuan Wu","Gang Xu","Yuwei Wang","Min Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04301v1.pdf","comment":"To appear in ACM CIKM'24 full research paper track"},{"id":"http://arxiv.org/abs/2408.04295v1","updated":"2024-08-08T08:18:05Z","published":"2024-08-08T08:18:05Z","title":"Assigning Credit with Partial Reward Decoupling in Multi-Agent Proximal\n  Policy Optimization","summary":"  Multi-agent proximal policy optimization (MAPPO) has recently demonstrated\nstate-of-the-art performance on challenging multi-agent reinforcement learning\ntasks. However, MAPPO still struggles with the credit assignment problem,\nwherein the sheer difficulty in ascribing credit to individual agents' actions\nscales poorly with team size. In this paper, we propose a multi-agent\nreinforcement learning algorithm that adapts recent developments in credit\nassignment to improve upon MAPPO. Our approach leverages partial reward\ndecoupling (PRD), which uses a learned attention mechanism to estimate which of\na particular agent's teammates are relevant to its learning updates. We use\nthis estimate to dynamically decompose large groups of agents into smaller,\nmore manageable subgroups. We empirically demonstrate that our approach,\nPRD-MAPPO, decouples agents from teammates that do not influence their expected\nfuture reward, thereby streamlining credit assignment. We additionally show\nthat PRD-MAPPO yields significantly higher data efficiency and asymptotic\nperformance compared to both MAPPO and other state-of-the-art methods across\nseveral multi-agent tasks, including StarCraft II. Finally, we propose a\nversion of PRD-MAPPO that is applicable to \\textit{shared} reward settings,\nwhere PRD was previously not applicable, and empirically show that this also\nleads to performance improvements over MAPPO.\n","authors":["Aditya Kapoor","Benjamin Freed","Howie Choset","Jeff Schneider"],"pdf_url":"https://arxiv.org/pdf/2408.04295v1.pdf","comment":"20 pages, 5 figures, 12 tables, Reinforcement Learning Journal and\n  Reinforcement Learning Conference 2024"},{"id":"http://arxiv.org/abs/2408.04294v1","updated":"2024-08-08T08:17:50Z","published":"2024-08-08T08:17:50Z","title":"Dual-branch PolSAR Image Classification Based on GraphMAE and Local\n  Feature Extraction","summary":"  The annotation of polarimetric synthetic aperture radar (PolSAR) images is a\nlabor-intensive and time-consuming process. Therefore, classifying PolSAR\nimages with limited labels is a challenging task in remote sensing domain. In\nrecent years, self-supervised learning approaches have proven effective in\nPolSAR image classification with sparse labels. However, we observe a lack of\nresearch on generative selfsupervised learning in the studied task. Motivated\nby this, we propose a dual-branch classification model based on generative\nself-supervised learning in this paper. The first branch is a\nsuperpixel-branch, which learns superpixel-level polarimetric representations\nusing a generative self-supervised graph masked autoencoder. To acquire finer\nclassification results, a convolutional neural networks-based pixel-branch is\nfurther incorporated to learn pixel-level features. Classification with fused\ndual-branch features is finally performed to obtain the predictions.\nExperimental results on the benchmark Flevoland dataset demonstrate that our\napproach yields promising classification results.\n","authors":["Yuchen Wang","Ziyi Guo","Haixia Bi","Danfeng Hong","Chen Xu"],"pdf_url":"https://arxiv.org/pdf/2408.04294v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04290v1","updated":"2024-08-08T08:06:42Z","published":"2024-08-08T08:06:42Z","title":"Efficient and Accurate Pneumonia Detection Using a Novel Multi-Scale\n  Transformer Approach","summary":"  Pneumonia, a severe respiratory disease, poses significant diagnostic\nchallenges, especially in underdeveloped regions. Traditional diagnostic\nmethods, such as chest X-rays, suffer from variability in interpretation among\nradiologists, necessitating reliable automated tools. In this study, we propose\na novel approach combining deep learning and transformer-based attention\nmechanisms to enhance pneumonia detection from chest X-rays. Our method begins\nwith lung segmentation using a TransUNet model that integrates our specialized\ntransformer module, which has fewer parameters compared to common transformers\nwhile maintaining performance. This model is trained on the \"Chest Xray Masks\nand Labels\" dataset and then applied to the Kermany and Cohen datasets to\nisolate lung regions, enhancing subsequent classification tasks. For\nclassification, we employ pre-trained ResNet models (ResNet-50 and ResNet-101)\nto extract multi-scale feature maps, processed through our modified transformer\nmodule. By employing our specialized transformer, we attain superior results\nwith significantly fewer parameters compared to common transformer models. Our\napproach achieves high accuracy rates of 92.79% on the Kermany dataset and\n95.11% on the Cohen dataset, ensuring robust and efficient performance suitable\nfor resource-constrained environments.\n\"https://github.com/amirrezafateh/Multi-Scale-Transformer-Pneumonia\"\n","authors":["Alireza Saber","Pouria Parhami","Alimihammad Siahkarzadeh","Amirreza Fateh"],"pdf_url":"https://arxiv.org/pdf/2408.04290v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21670v2","updated":"2024-08-08T07:59:50Z","published":"2024-07-31T15:13:39Z","title":"Universal Approximation Theory: Foundations for Parallelism in Neural\n  Networks","summary":"  Neural networks are increasingly evolving towards training large models with\nbig data, a method that has demonstrated superior performance across many\ntasks. However, this approach introduces an urgent problem: current deep\nlearning models are predominantly serial, meaning that as the number of network\nlayers increases, so do the training and inference times. This is unacceptable\nif deep learning is to continue advancing. Therefore, this paper proposes a\ndeep learning parallelization strategy based on the Universal Approximation\nTheorem (UAT). From this foundation, we designed a parallel network called\nPara-Former to test our theory. Unlike traditional serial models, the inference\ntime of Para-Former does not increase with the number of layers, significantly\naccelerating the inference speed of multi-layer networks. Experimental results\nvalidate the effectiveness of this network.\n","authors":["Wei Wang","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2407.21670v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.19429v3","updated":"2024-08-08T07:56:48Z","published":"2024-07-28T08:39:28Z","title":"FTF-ER: Feature-Topology Fusion-Based Experience Replay Method for\n  Continual Graph Learning","summary":"  Continual graph learning (CGL) is an important and challenging task that aims\nto extend static GNNs to dynamic task flow scenarios. As one of the mainstream\nCGL methods, the experience replay (ER) method receives widespread attention\ndue to its superior performance. However, existing ER methods focus on\nidentifying samples by feature significance or topological relevance, which\nlimits their utilization of comprehensive graph data. In addition, the\ntopology-based ER methods only consider local topological information and add\nneighboring nodes to the buffer, which ignores the global topological\ninformation and increases memory overhead. To bridge these gaps, we propose a\nnovel method called Feature-Topology Fusion-based Experience Replay (FTF-ER) to\neffectively mitigate the catastrophic forgetting issue with enhanced\nefficiency. Specifically, from an overall perspective to maximize the\nutilization of the entire graph data, we propose a highly complementary\napproach including both feature and global topological information, which can\nsignificantly improve the effectiveness of the sampled nodes. Moreover, to\nfurther utilize global topological information, we propose Hodge Potential\nScore (HPS) as a novel module to calculate the topological importance of nodes.\nHPS derives a global node ranking via Hodge decomposition on graphs, providing\nmore accurate global topological information compared to neighbor sampling. By\nexcluding neighbor sampling, HPS significantly reduces buffer storage costs for\nacquiring topological information and simultaneously decreases training time.\nCompared with state-of-the-art methods, FTF-ER achieves a significant\nimprovement of 3.6% in AA and 7.1% in AF on the OGB-Arxiv dataset,\ndemonstrating its superior performance in the class-incremental learning\nsetting.\n","authors":["Jinhui Pang","Changqing Lin","Xiaoshuai Hao","Rong Yin","Zixuan Wang","Zhihui Zhang","Jinglin He","Huang Tai Sheng"],"pdf_url":"https://arxiv.org/pdf/2407.19429v3.pdf","comment":"Accepted by ACM Multimedia 2024"},{"id":"http://arxiv.org/abs/2408.04283v1","updated":"2024-08-08T07:41:16Z","published":"2024-08-08T07:41:16Z","title":"Prompt-Assisted Semantic Interference Cancellation on Moderate\n  Interference Channels","summary":"  The performance of conventional interference management strategies degrades\nwhen interference power is comparable to signal power. We consider a new\nperspective on interference management using semantic communication.\nSpecifically, a multi-user semantic communication system is considered on\nmoderate interference channels (ICs), for which a novel framework of deep\nlearning-based prompt-assisted semantic interference cancellation (DeepPASIC)\nis proposed. Each transmitted signal is partitioned into common and private\nparts. The common parts of different users are transmitted simultaneously in a\nshared medium, resulting in superposition. The private part, on the other hand,\nserves as a prompt to assist in canceling the interference suffered by the\ncommon part at the semantic level. Simulation results demonstrate that the\nproposed DeepPASIC outperforms conventional interference management strategies\nunder moderate interference conditions.\n","authors":["Zian Meng","Qiang Li","Ashish Pandharipande","Xiaohu Ge"],"pdf_url":"https://arxiv.org/pdf/2408.04283v1.pdf","comment":"5 pages, 5 figures"},{"id":"http://arxiv.org/abs/2408.04277v1","updated":"2024-08-08T07:31:22Z","published":"2024-08-08T07:31:22Z","title":"Stability Analysis of Equivariant Convolutional Representations Through\n  The Lens of Equivariant Multi-layered CKNs","summary":"  In this paper we construct and theoretically analyse group equivariant\nconvolutional kernel networks (CKNs) which are useful in understanding the\ngeometry of (equivariant) CNNs through the lens of reproducing kernel Hilbert\nspaces (RKHSs). We then proceed to study the stability analysis of such\nequiv-CKNs under the action of diffeomorphism and draw a connection with\nequiv-CNNs, where the goal is to analyse the geometry of inductive biases of\nequiv-CNNs through the lens of reproducing kernel Hilbert spaces (RKHSs).\nTraditional deep learning architectures, including CNNs, trained with\nsophisticated optimization algorithms is vulnerable to perturbations, including\n`adversarial examples'. Understanding the RKHS norm of such models through CKNs\nis useful in designing the appropriate architecture and can be useful in\ndesigning robust equivariant representation learning models.\n","authors":["Soutrik Roy Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2408.04277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04276v1","updated":"2024-08-08T07:24:28Z","published":"2024-08-08T07:24:28Z","title":"Early Risk Assessment Model for ICA Timing Strategy in Unstable Angina\n  Patients Using Multi-Modal Machine Learning","summary":"  Background: Invasive coronary arteriography (ICA) is recognized as the gold\nstandard for diagnosing cardiovascular diseases, including unstable angina\n(UA). The challenge lies in determining the optimal timing for ICA in UA\npatients, balancing the need for revascularization in high-risk patients\nagainst the potential complications in low-risk ones. Unlike myocardial\ninfarction, UA does not have specific indicators like ST-segment deviation or\ncardiac enzymes, making risk assessment complex. Objectives: Our study aims to\nenhance the early risk assessment for UA patients by utilizing machine learning\nalgorithms. These algorithms can potentially identify patients who would\nbenefit most from ICA by analyzing less specific yet related indicators that\nare challenging for human physicians to interpret. Methods: We collected data\nfrom 640 UA patients at Shanghai General Hospital, including medical history\nand electrocardiograms (ECG). Machine learning algorithms were trained using\nmulti-modal demographic characteristics including clinical risk factors,\nsymptoms, biomarker levels, and ECG features extracted by pre-trained neural\nnetworks. The goal was to stratify patients based on their revascularization\nrisk. Additionally, we translated our models into applicable and explainable\nlook-up tables through discretization for practical clinical use. Results: The\nstudy achieved an Area Under the Curve (AUC) of $0.719 \\pm 0.065$ in risk\nstratification, significantly surpassing the widely adopted GRACE score's AUC\nof $0.579 \\pm 0.044$. Conclusions: The results suggest that machine learning\ncan provide superior risk stratification for UA patients. This improved\nstratification could help in balancing the risks, costs, and complications\nassociated with ICA, indicating a potential shift in clinical assessment\npractices for unstable angina.\n","authors":["Candi Zheng","Kun Liu","Yang Wang","Shiyi Chen","Hongli Li"],"pdf_url":"https://arxiv.org/pdf/2408.04276v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11427v2","updated":"2024-08-08T07:23:01Z","published":"2024-07-16T06:45:27Z","title":"Semi-Supervised Generative Models for Disease Trajectories: A Case Study\n  on Systemic Sclerosis","summary":"  We propose a deep generative approach using latent temporal processes for\nmodeling and holistically analyzing complex disease trajectories, with a\nparticular focus on Systemic Sclerosis (SSc). We aim to learn temporal latent\nrepresentations of the underlying generative process that explain the observed\npatient disease trajectories in an interpretable and comprehensive way. To\nenhance the interpretability of these latent temporal processes, we develop a\nsemi-supervised approach for disentangling the latent space using established\nmedical knowledge. By combining the generative approach with medical\ndefinitions of different characteristics of SSc, we facilitate the discovery of\nnew aspects of the disease. We show that the learned temporal latent processes\ncan be utilized for further data analysis and clinical hypothesis testing,\nincluding finding similar patients and clustering SSc patient trajectories into\nnovel sub-types. Moreover, our method enables personalized online monitoring\nand prediction of multivariate time series with uncertainty quantification.\n","authors":["Ccile Trottet","Manuel Schrch","Ahmed Allam","Imon Barua","Liubov Petelytska","David Launay","Paolo Air","Radim Bev","Christopher Denton","Mislav Radic","Oliver Distler","Anna-Maria Hoffmann-Vold","Michael Krauthammer","the EUSTAR collaborators"],"pdf_url":"https://arxiv.org/pdf/2407.11427v2.pdf","comment":"Accepted at Machine Learning for Healthcare 2024. arXiv admin note:\n  substantial text overlap with arXiv:2311.08149"},{"id":"http://arxiv.org/abs/2407.06204v2","updated":"2024-08-08T07:13:37Z","published":"2024-06-26T16:34:33Z","title":"A Survey on Mixture of Experts","summary":"  Large language models (LLMs) have garnered unprecedented advancements across\ndiverse fields, ranging from natural language processing to computer vision and\nbeyond. The prowess of LLMs is underpinned by their substantial model size,\nextensive and diverse datasets, and the vast computational power harnessed\nduring training, all of which contribute to the emergent abilities of LLMs\n(e.g., in-context learning) that are not present in small models. Within this\ncontext, the mixture of experts (MoE) has emerged as an effective method for\nsubstantially scaling up model capacity with minimal computation overhead,\ngaining significant attention from academia and industry. Despite its growing\nprevalence, there lacks a systematic and comprehensive review of the literature\non MoE. This survey seeks to bridge that gap, serving as an essential resource\nfor researchers delving into the intricacies of MoE. We first briefly introduce\nthe structure of the MoE layer, followed by proposing a new taxonomy of MoE.\nNext, we overview the core designs for various MoE models including both\nalgorithmic and systemic aspects, alongside collections of available\nopen-source implementations, hyperparameter configurations and empirical\nevaluations. Furthermore, we delineate the multifaceted applications of MoE in\npractice, and outline some potential directions for future research. To\nfacilitate ongoing updates and the sharing of cutting-edge developments in MoE\nresearch, we have established a resource repository accessible at\nhttps://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts.\n","authors":["Weilin Cai","Juyong Jiang","Fan Wang","Jing Tang","Sunghun Kim","Jiayi Huang"],"pdf_url":"https://arxiv.org/pdf/2407.06204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.10373v2","updated":"2024-08-08T07:06:40Z","published":"2024-01-18T20:43:43Z","title":"Harmonized Spatial and Spectral Learning for Robust and Generalized\n  Medical Image Segmentation","summary":"  Deep learning has demonstrated remarkable achievements in medical image\nsegmentation. However, prevailing deep learning models struggle with poor\ngeneralization due to (i) intra-class variations, where the same class appears\ndifferently in different samples, and (ii) inter-class independence, resulting\nin difficulties capturing intricate relationships between distinct objects,\nleading to higher false negative cases. This paper presents a novel approach\nthat synergies spatial and spectral representations to enhance\ndomain-generalized medical image segmentation. We introduce the innovative\nSpectral Correlation Coefficient objective to improve the model's capacity to\ncapture middle-order features and contextual long-range dependencies. This\nobjective complements traditional spatial objectives by incorporating valuable\nspectral information. Extensive experiments reveal that optimizing this\nobjective with existing architectures like UNet and TransUNet significantly\nenhances generalization, interpretability, and noise robustness, producing more\nconfident predictions. For instance, in cardiac segmentation, we observe a 0.81\npp and 1.63 pp (pp = percentage point) improvement in DSC over UNet and\nTransUNet, respectively. Our interpretability study demonstrates that, in most\ntasks, objectives optimized with UNet outperform even TransUNet by introducing\nglobal contextual information alongside local details. These findings\nunderscore the versatility and effectiveness of our proposed method across\ndiverse imaging modalities and medical domains.\n","authors":["Vandan Gorade","Sparsh Mittal","Debesh Jha","Rekha Singhal","Ulas Bagci"],"pdf_url":"https://arxiv.org/pdf/2401.10373v2.pdf","comment":"Early Accepted at ICPR-2024 for Oral Presentation"},{"id":"http://arxiv.org/abs/2408.04254v1","updated":"2024-08-08T06:47:21Z","published":"2024-08-08T06:47:21Z","title":"Generating Fine-Grained Causality in Climate Time Series Data for\n  Forecasting and Anomaly Detection","summary":"  Understanding the causal interaction of time series variables can contribute\nto time series data analysis for many real-world applications, such as climate\nforecasting and extreme weather alerts. However, causal relationships are\ndifficult to be fully observed in real-world complex settings, such as\nspatial-temporal data from deployed sensor networks. Therefore, to capture\nfine-grained causal relations among spatial-temporal variables for further a\nmore accurate and reliable time series analysis, we first design a conceptual\nfine-grained causal model named TBN Granger Causality, which adds\ntime-respecting Bayesian Networks to the previous time-lagged Neural Granger\nCausality to offset the instantaneous effects. Second, we propose an end-to-end\ndeep generative model called TacSas, which discovers TBN Granger Causality in a\ngenerative manner to help forecast time series data and detect possible\nanomalies during the forecast. For evaluations, besides the causality discovery\nbenchmark Lorenz-96, we also test TacSas on climate benchmark ERA5 for climate\nforecasting and the extreme weather benchmark of NOAA for extreme weather\nalerts.\n","authors":["Dongqi Fu","Yada Zhu","Hanghang Tong","Kommy Weldemariam","Onkar Bhardwaj","Jingrui He"],"pdf_url":"https://arxiv.org/pdf/2408.04254v1.pdf","comment":"ICML 2024 AI for Science Workshop"},{"id":"http://arxiv.org/abs/2312.10308v4","updated":"2024-08-08T06:40:12Z","published":"2023-12-16T03:50:24Z","title":"Event-Based Contrastive Learning for Medical Time Series","summary":"  In clinical practice, one often needs to identify whether a patient is at\nhigh risk of adverse outcomes after some key medical event. For example,\nquantifying the risk of adverse outcomes after an acute cardiovascular event\nhelps healthcare providers identify those patients at the highest risk of poor\noutcomes; i.e., patients who benefit from invasive therapies that can lower\ntheir risk. Assessing the risk of adverse outcomes, however, is challenging due\nto the complexity, variability, and heterogeneity of longitudinal medical data,\nespecially for individuals suffering from chronic diseases like heart failure.\nIn this paper, we introduce Event-Based Contrastive Learning (EBCL) - a method\nfor learning embeddings of heterogeneous patient data that preserves temporal\ninformation before and after key index events. We demonstrate that EBCL can be\nused to construct models that yield improved performance on important\ndownstream tasks relative to other pretraining methods. We develop and test the\nmethod using a cohort of heart failure patients obtained from a large hospital\nnetwork and the publicly available MIMIC-IV dataset consisting of patients in\nan intensive care unit at a large tertiary care center. On both cohorts, EBCL\npretraining yields models that are performant with respect to a number of\ndownstream tasks, including mortality, hospital readmission, and length of\nstay. In addition, unsupervised EBCL embeddings effectively cluster heart\nfailure patients into subgroups with distinct outcomes, thereby providing\ninformation that helps identify new heart failure phenotypes. The contrastive\nframework around the index event can be adapted to a wide array of time-series\ndatasets and provides information that can be used to guide personalized care.\n","authors":["Hyewon Jeong","Nassim Oufattole","Matthew Mcdermott","Aparna Balagopalan","Bryan Jangeesingh","Marzyeh Ghassemi","Collin Stultz"],"pdf_url":"https://arxiv.org/pdf/2312.10308v4.pdf","comment":"Accepted at Unifying Representations in Neural Models Workshop in\n  NeurIPS 2023, MLHC 2024"},{"id":"http://arxiv.org/abs/2403.19913v2","updated":"2024-08-08T06:38:31Z","published":"2024-03-29T01:53:24Z","title":"MANGO: A Benchmark for Evaluating Mapping and Navigation Abilities of\n  Large Language Models","summary":"  Large language models such as ChatGPT and GPT-4 have recently achieved\nastonishing performance on a variety of natural language processing tasks. In\nthis paper, we propose MANGO, a benchmark to evaluate their capabilities to\nperform text-based mapping and navigation. Our benchmark includes 53 mazes\ntaken from a suite of textgames: each maze is paired with a walkthrough that\nvisits every location but does not cover all possible paths. The task is\nquestion-answering: for each maze, a large language model reads the walkthrough\nand answers hundreds of mapping and navigation questions such as \"How should\nyou go to Attic from West of House?\" and \"Where are we if we go north and east\nfrom Cellar?\". Although these questions are easy to humans, it turns out that\neven GPT-4, the best-to-date language model, performs poorly at answering them.\nFurther, our experiments suggest that a strong mapping and navigation ability\nwould benefit large language models in performing relevant downstream tasks,\nsuch as playing textgames. Our MANGO benchmark will facilitate future research\non methods that improve the mapping and navigation capabilities of language\nmodels. We host our leaderboard, data, code, and evaluation program at\nhttps://mango.ttic.edu and https://github.com/oaklight/mango/.\n","authors":["Peng Ding","Jiading Fang","Peng Li","Kangrui Wang","Xiaochen Zhou","Mo Yu","Jing Li","Matthew R. Walter","Hongyuan Mei"],"pdf_url":"https://arxiv.org/pdf/2403.19913v2.pdf","comment":"COLM 2024 camera-ready"},{"id":"http://arxiv.org/abs/2408.04251v1","updated":"2024-08-08T06:36:56Z","published":"2024-08-08T06:36:56Z","title":"Cooperative Multi-Agent Deep Reinforcement Learning in Content Ranking\n  Optimization","summary":"  In a typical e-commerce setting, Content Ranking Optimization (CRO)\nmechanisms are employed to surface content on the search page to fulfill\ncustomers' shopping missions. CRO commonly utilizes models such as contextual\ndeep bandits model to independently rank content at different positions, e.g.,\none optimizer dedicated to organic search results and another to sponsored\nresults. However, this regional optimization approach does not necessarily\ntranslate to whole page optimization, e.g., maximizing revenue at the top of\nthe page may inadvertently diminish the revenue of lower positions. In this\npaper, we propose a reinforcement learning based method for whole page ranking\nto jointly optimize across all positions by: 1) shifting from position level\noptimization to whole page level optimization to achieve an overall optimized\nranking; 2) applying reinforcement learning to optimize for the cumulative\nrewards instead of the instant reward. We formulate page level CRO as a\ncooperative Multi-agent Markov Decision Process , and address it with the novel\nMulti-Agent Deep Deterministic Policy Gradient (MADDPG) model. MADDPG supports\na flexible and scalable joint optimization framework by adopting a \"centralized\ntraining and decentralized execution\" approach. Extensive experiments\ndemonstrate that MADDPG scales to a 2.5 billion action space in the public\nMujoco environment, and outperforms the deep bandits modeling by 25.7% on the\noffline CRO data set from a leading e-commerce company. We foresee that this\nnovel multi-agent optimization is applicable to similar joint optimization\nproblems in the field of information retrieval.\n","authors":["Zhou Qin","Kai Yuan","Pratik Lahiri","Wenyang Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04251v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2408.04245v1","updated":"2024-08-08T06:17:13Z","published":"2024-08-08T06:17:13Z","title":"Scalable Transformer for High Dimensional Multivariate Time Series\n  Forecasting","summary":"  Deep models for Multivariate Time Series (MTS) forecasting have recently\ndemonstrated significant success. Channel-dependent models capture complex\ndependencies that channel-independent models cannot capture. However, the\nnumber of channels in real-world applications outpaces the capabilities of\nexisting channel-dependent models, and contrary to common expectations, some\nmodels underperform the channel-independent models in handling high-dimensional\ndata, which raises questions about the performance of channel-dependent models.\nTo address this, our study first investigates the reasons behind the suboptimal\nperformance of these channel-dependent models on high-dimensional MTS data. Our\nanalysis reveals that two primary issues lie in the introduced noise from\nunrelated series that increases the difficulty of capturing the crucial\ninter-channel dependencies, and challenges in training strategies due to\nhigh-dimensional data. To address these issues, we propose STHD, the Scalable\nTransformer for High-Dimensional Multivariate Time Series Forecasting. STHD has\nthree components: a) Relation Matrix Sparsity that limits the noise introduced\nand alleviates the memory issue; b) ReIndex applied as a training strategy to\nenable a more flexible batch size setting and increase the diversity of\ntraining data; and c) Transformer that handles 2-D inputs and captures channel\ndependencies. These components jointly enable STHD to manage the\nhigh-dimensional MTS while maintaining computational feasibility. Furthermore,\nexperimental results show STHD's considerable improvement on three\nhigh-dimensional datasets: Crime-Chicago, Wiki-People, and Traffic. The source\ncode and dataset are publicly available\nhttps://github.com/xinzzzhou/ScalableTransformer4HighDimensionMTSF.git.\n","authors":["Xin Zhou","Weiqing Wang","Wray Buntine","Shilin Qu","Abishek Sriramulu","Weicong Tan","Christoph Bergmeir"],"pdf_url":"https://arxiv.org/pdf/2408.04245v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04242v1","updated":"2024-08-08T06:08:04Z","published":"2024-08-08T06:08:04Z","title":"The Ungrounded Alignment Problem","summary":"  Modern machine learning systems have demonstrated substantial abilities with\nmethods that either embrace or ignore human-provided knowledge, but combining\nbenefits of both styles remains a challenge. One particular challenge involves\ndesigning learning systems that exhibit built-in responses to specific abstract\nstimulus patterns, yet are still plastic enough to be agnostic about the\nmodality and exact form of their inputs. In this paper, we investigate what we\ncall The Ungrounded Alignment Problem, which asks How can we build in\npredefined knowledge in a system where we don't know how a given stimulus will\nbe grounded? This paper examines a simplified version of the general problem,\nwhere an unsupervised learner is presented with a sequence of images for the\ncharacters in a text corpus, and this learner is later evaluated on its ability\nto recognize specific (possibly rare) sequential patterns. Importantly, the\nlearner is given no labels during learning or evaluation, but must map images\nfrom an unknown font or permutation to its correct class label. That is, at no\npoint is our learner given labeled images, where an image vector is explicitly\nassociated with a class label. Despite ample work in unsupervised and\nself-supervised loss functions, all current methods require a labeled\nfine-tuning phase to map the learned representations to correct classes.\nFinding this mapping in the absence of labels may seem a fool's errand, but our\nmain result resolves this seeming paradox. We show that leveraging only letter\nbigram frequencies is sufficient for an unsupervised learner both to reliably\nassociate images to class labels and to reliably identify trigger words in the\nsequence of inputs. More generally, this method suggests an approach for\nencoding specific desired innate behaviour in modality-agnostic models.\n","authors":["Marc Pickett","Aakash Kumar Nain","Joseph Modayil","Llion Jones"],"pdf_url":"https://arxiv.org/pdf/2408.04242v1.pdf","comment":"7 pages, plus references and appendix"},{"id":"http://arxiv.org/abs/2408.04236v1","updated":"2024-08-08T05:43:20Z","published":"2024-08-08T05:43:20Z","title":"Cluster-Wide Task Slowdown Detection in Cloud System","summary":"  Slow task detection is a critical problem in cloud operation and maintenance\nsince it is highly related to user experience and can bring substantial\nliquidated damages. Most anomaly detection methods detect it from a single-task\naspect. However, considering millions of concurrent tasks in large-scale cloud\ncomputing clusters, it becomes impractical and inefficient. Moreover,\nsingle-task slowdowns are very common and do not necessarily indicate a\nmalfunction of a cluster due to its violent fluctuation nature in a virtual\nenvironment. Thus, we shift our attention to cluster-wide task slowdowns by\nutilizing the duration time distribution of tasks across a cluster, so that the\ncomputation complexity is not relevant to the number of tasks.\n  The task duration time distribution often exhibits compound periodicity and\nlocal exceptional fluctuations over time. Though transformer-based methods are\none of the most powerful methods to capture these time series normal variation\npatterns, we empirically find and theoretically explain the flaw of the\nstandard attention mechanism in reconstructing subperiods with low amplitude\nwhen dealing with compound periodicity.\n  To tackle these challenges, we propose SORN (i.e., Skimming Off subperiods in\ndescending amplitude order and Reconstructing Non-slowing fluctuation), which\nconsists of a Skimming Attention mechanism to reconstruct the compound\nperiodicity and a Neural Optimal Transport module to distinguish cluster-wide\nslowdowns from other exceptional fluctuations. Furthermore, since anomalies in\nthe training set are inevitable in a practical scenario, we propose a picky\nloss function, which adaptively assigns higher weights to reliable time slots\nin the training set. Extensive experiments demonstrate that SORN outperforms\nstate-of-the-art methods on multiple real-world industrial datasets.\n","authors":["Feiyi Chen","Yingying Zhang","Lunting Fan","Yuxuan Liang","Guansong Pang","Qingsong Wen","Shuiguang Deng"],"pdf_url":"https://arxiv.org/pdf/2408.04236v1.pdf","comment":"This paper has been accepted by KDD2024"},{"id":"http://arxiv.org/abs/2408.04232v1","updated":"2024-08-08T05:37:17Z","published":"2024-08-08T05:37:17Z","title":"Enhanced Traffic Flow Prediction with Multi-Segment Fusion Tensor Graph\n  Convolutional Networks","summary":"  Accurate traffic Flow Prediction can assist in traffic management, route\nplanning, and congestion mitigation, which holds significant importance in\nenhancing the efficiency and reliability of intelligent transportation systems\n(ITS). However, existing traffic flow prediction models suffer from limitations\nin capturing the complex spatial-temporal dependencies within traffic networks.\nIn order to address this issue, this study proposes a multi-segment fusion\ntensor graph convolutional network (MS-FTGCN) for traffic flow prediction with\nthe following three-fold ideas: a) building a unified spatial-temporal graph\nconvolutional framework based on Tensor M-product, which capture the\nspatial-temporal patterns simultaneously; b) incorporating hourly, daily, and\nweekly components to model multi temporal properties of traffic flows,\nrespectively; c) fusing the outputs of the three components by attention\nmechanism to obtain the final traffic flow prediction results. The results of\nexperiments conducted on two traffic flow datasets demonstrate that the\nproposed MS-FTGCN outperforms the state-of-the-art models.\n","authors":["Wei Zhang","Peng Tang"],"pdf_url":"https://arxiv.org/pdf/2408.04232v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04229v1","updated":"2024-08-08T05:33:21Z","published":"2024-08-08T05:33:21Z","title":"Probabilistic Circuits for Cumulative Distribution Functions","summary":"  A probabilistic circuit (PC) succinctly expresses a function that represents\na multivariate probability distribution and, given sufficient structural\nproperties of the circuit, supports efficient probabilistic inference.\nTypically a PC computes the probability mass (or density) function (PMF or PDF)\nof the distribution. We consider PCs instead computing the cumulative\ndistribution function (CDF). We show that for distributions over binary random\nvariables these representations (PMF and CDF) are essentially equivalent, in\nthe sense that one can be transformed to the other in polynomial time. We then\nshow how a similar equivalence holds for distributions over finite discrete\nvariables using a modification of the standard encoding with binary variables\nthat aligns with the CDF semantics. Finally we show that for continuous\nvariables, smooth, decomposable PCs computing PDFs and CDFs can be efficiently\ntransformed to each other by modifying only the leaves of the circuit.\n","authors":["Oliver Broadrick","William Cao","Benjie Wang","Martin Trapp","Guy Van den Broeck"],"pdf_url":"https://arxiv.org/pdf/2408.04229v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04221v1","updated":"2024-08-08T05:09:02Z","published":"2024-08-08T05:09:02Z","title":"Connective Viewpoints of Signal-to-Noise Diffusion Models","summary":"  Diffusion models (DM) have become fundamental components of generative\nmodels, excelling across various domains such as image creation, audio\ngeneration, and complex data interpolation. Signal-to-Noise diffusion models\nconstitute a diverse family covering most state-of-the-art diffusion models.\nWhile there have been several attempts to study Signal-to-Noise (S2N) diffusion\nmodels from various perspectives, there remains a need for a comprehensive\nstudy connecting different viewpoints and exploring new perspectives. In this\nstudy, we offer a comprehensive perspective on noise schedulers, examining\ntheir role through the lens of the signal-to-noise ratio (SNR) and its\nconnections to information theory. Building upon this framework, we have\ndeveloped a generalized backward equation to enhance the performance of the\ninference process.\n","authors":["Khanh Doan","Long Tung Vuong","Tuan Nguyen","Anh Tuan Bui","Quyen Tran","Thanh-Toan Do","Dinh Phung","Trung Le"],"pdf_url":"https://arxiv.org/pdf/2408.04221v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04220v1","updated":"2024-08-08T05:06:22Z","published":"2024-08-08T05:06:22Z","title":"Diffusion Guided Language Modeling","summary":"  Current language models demonstrate remarkable proficiency in text\ngeneration. However, for many applications it is desirable to control\nattributes, such as sentiment, or toxicity, of the generated language --\nideally tailored towards each specific use case and target audience. For\nauto-regressive language models, existing guidance methods are prone to\ndecoding errors that cascade during generation and degrade performance. In\ncontrast, text diffusion models can easily be guided with, for example, a\nsimple linear sentiment classifier -- however they do suffer from significantly\nhigher perplexity than auto-regressive alternatives. In this paper we use a\nguided diffusion model to produce a latent proposal that steers an\nauto-regressive language model to generate text with desired properties. Our\nmodel inherits the unmatched fluency of the auto-regressive approach and the\nplug-and-play flexibility of diffusion. We show that it outperforms previous\nplug-and-play guidance methods across a wide range of benchmark data sets.\nFurther, controlling a new attribute in our framework is reduced to training a\nsingle logistic regression classifier.\n","authors":["Justin Lovelace","Varsha Kishore","Yiwei Chen","Kilian Q. Weinberger"],"pdf_url":"https://arxiv.org/pdf/2408.04220v1.pdf","comment":"ACL Findings 2024"},{"id":"http://arxiv.org/abs/2106.11528v3","updated":"2024-08-08T04:18:34Z","published":"2021-06-22T03:44:03Z","title":"Recent Deep Semi-supervised Learning Approaches and Related Works","summary":"  This work proposes an overview of the recent semi-supervised learning\napproaches and related works. Despite the remarkable success of neural networks\nin various applications, there exist a few formidable constraints, including\nthe need for a large amount of labeled data. Therefore, semi-supervised\nlearning, which is a learning scheme in which scarce labels and a larger amount\nof unlabeled data are utilized to train models (e.g., deep neural networks), is\ngetting more important. Based on the key assumptions of semi-supervised\nlearning, which are the manifold assumption, cluster assumption, and continuity\nassumption, the work reviews the recent semi-supervised learning approaches. In\nparticular, the methods in regard to using deep neural networks in a\nsemi-supervised learning setting are primarily discussed. In addition, the\nexisting works are first classified based on the underlying idea and explained,\nthen the holistic approaches that unify the aforementioned ideas are detailed.\n","authors":["Gyeongho Kim"],"pdf_url":"https://arxiv.org/pdf/2106.11528v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04353v5","updated":"2024-08-08T04:15:31Z","published":"2023-10-06T16:21:22Z","title":"An In-Context Learning Agent for Formal Theorem-Proving","summary":"  We present an in-context learning agent for formal theorem-proving in\nenvironments like Lean and Coq. Current state-of-the-art models for the problem\nare finetuned on environment-specific proof data. By contrast, our approach,\ncalled COPRA, repeatedly asks a high-capacity, general-purpose large language\nmodel (GPT-4) to propose tactic applications from within a stateful\nbacktracking search. Proposed tactics are executed in the underlying proof\nenvironment. Feedback from the execution is used to build the prompt for the\nnext model query, along with selected information from the search history and\nlemmas retrieved from an external database. We evaluate our implementation of\nCOPRA on the miniF2F benchmark for Lean and a set of Coq tasks from the\nCompCert project. On these benchmarks, COPRA significantly outperforms few-shot\ninvocations of GPT-4. It also compares favorably against finetuning-based\napproaches, outperforming ReProver, a state-of-the-art finetuned approach for\nLean, in terms of the pass@1 metric. Our code and data are available at\nhttps://github.com/trishullab/copra.\n","authors":["Amitayush Thakur","George Tsoukalas","Yeming Wen","Jimmy Xin","Swarat Chaudhuri"],"pdf_url":"https://arxiv.org/pdf/2310.04353v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04206v1","updated":"2024-08-08T04:05:50Z","published":"2024-08-08T04:05:50Z","title":"DC Algorithm for Estimation of Sparse Gaussian Graphical Models","summary":"  Sparse estimation for Gaussian graphical models is a crucial technique for\nmaking the relationships among numerous observed variables more interpretable\nand quantifiable. Various methods have been proposed, including graphical\nlasso, which utilizes the $\\ell_1$ norm as a regularization term, as well as\nmethods employing non-convex regularization terms. However, most of these\nmethods approximate the $\\ell_0$ norm with convex functions. To estimate more\naccurate solutions, it is desirable to treat the $\\ell_0$ norm directly as a\nregularization term. In this study, we formulate the sparse estimation problem\nfor Gaussian graphical models using the $\\ell_0$ norm and propose a method to\nsolve this problem using the Difference of Convex functions Algorithm (DCA).\nSpecifically, we convert the $\\ell_0$ norm constraint into an equivalent\nlargest-$K$ norm constraint, reformulate the constrained problem into a\npenalized form, and solve it using the DC algorithm (DCA). Furthermore, we\ndesigned an algorithm that efficiently computes using graphical lasso.\nExperimental results with synthetic data show that our method yields results\nthat are equivalent to or better than existing methods. Comparisons of model\nlearning through cross-validation confirm that our method is particularly\nadvantageous in selecting true edges.\n","authors":["Tomokaze Shiratori","Yuichi Takano"],"pdf_url":"https://arxiv.org/pdf/2408.04206v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07887v5","updated":"2024-08-08T03:49:59Z","published":"2023-12-13T04:14:22Z","title":"Learn or Recall? Revisiting Incremental Learning with Pre-trained\n  Language Models","summary":"  Incremental Learning (IL) has been a long-standing problem in both vision and\nNatural Language Processing (NLP) communities. In recent years, as Pre-trained\nLanguage Models (PLMs) have achieved remarkable progress in various NLP\ndownstream tasks, utilizing PLMs as backbones has become a common practice in\nrecent research of IL in NLP. Most assume that catastrophic forgetting is the\nbiggest obstacle to achieving superior IL performance and propose various\ntechniques to overcome this issue. However, we find that this assumption is\nproblematic. Specifically, we revisit more than 20 methods on four\nclassification tasks (Text Classification, Intent Classification, Relation\nExtraction, and Named Entity Recognition) under the two most popular IL\nsettings (Class-Incremental and Task-Incremental) and reveal that most of them\nseverely underestimate the inherent anti-forgetting ability of PLMs. Based on\nthe observation, we propose a frustratingly easy method called SEQ* for IL with\nPLMs. The results show that SEQ* has competitive or superior performance\ncompared to state-of-the-art (SOTA) IL methods and requires considerably less\ntrainable parameters and training time. These findings urge us to revisit the\nIL with PLMs and encourage future studies to have a fundamental understanding\nof the catastrophic forgetting in PLMs. The data, code and scripts are publicly\navailable at\nhttps://github.com/zzz47zzz/codebase-for-incremental-learning-with-llm.\n","authors":["Junhao Zheng","Shengjie Qiu","Qianli Ma"],"pdf_url":"https://arxiv.org/pdf/2312.07887v5.pdf","comment":"ACL 2024 main conference (Oral)"},{"id":"http://arxiv.org/abs/2312.10385v4","updated":"2024-08-08T03:44:21Z","published":"2023-12-16T08:48:46Z","title":"Imitate the Good and Avoid the Bad: An Incremental Approach to Safe\n  Reinforcement Learning","summary":"  A popular framework for enforcing safe actions in Reinforcement Learning (RL)\nis Constrained RL, where trajectory based constraints on expected cost (or\nother cost measures) are employed to enforce safety and more importantly these\nconstraints are enforced while maximizing expected reward. Most recent\napproaches for solving Constrained RL convert the trajectory based cost\nconstraint into a surrogate problem that can be solved using minor\nmodifications to RL methods. A key drawback with such approaches is an over or\nunderestimation of the cost constraint at each state. Therefore, we provide an\napproach that does not modify the trajectory based cost constraint and instead\nimitates ``good'' trajectories and avoids ``bad'' trajectories generated from\nincrementally improving policies. We employ an oracle that utilizes a reward\nthreshold (which is varied with learning) and the overall cost constraint to\nlabel trajectories as ``good'' or ``bad''. A key advantage of our approach is\nthat we are able to work from any starting policy or set of trajectories and\nimprove on it. In an exhaustive set of experiments, we demonstrate that our\napproach is able to outperform top benchmark approaches for solving Constrained\nRL problems, with respect to expected cost, CVaR cost, or even unknown cost\nconstraints.\n","authors":["Huy Hoang","Tien Mai","Pradeep Varakantham"],"pdf_url":"https://arxiv.org/pdf/2312.10385v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04193v1","updated":"2024-08-08T03:25:41Z","published":"2024-08-08T03:25:41Z","title":"Uncertainty-Aware Crime Prediction With Spatial Temporal Multivariate\n  Graph Neural Networks","summary":"  Crime forecasting is a critical component of urban analysis and essential for\nstabilizing society today. Unlike other time series forecasting problems, crime\nincidents are sparse, particularly in small regions and within specific time\nperiods. Traditional spatial-temporal deep learning models often struggle with\nthis sparsity, as they typically cannot effectively handle the non-Gaussian\nnature of crime data, which is characterized by numerous zeros and\nover-dispersed patterns. To address these challenges, we introduce a novel\napproach termed Spatial Temporal Multivariate Zero-Inflated Negative Binomial\nGraph Neural Networks (STMGNN-ZINB). This framework leverages diffusion and\nconvolution networks to analyze spatial, temporal, and multivariate\ncorrelations, enabling the parameterization of probabilistic distributions of\ncrime incidents. By incorporating a Zero-Inflated Negative Binomial model,\nSTMGNN-ZINB effectively manages the sparse nature of crime data, enhancing\nprediction accuracy and the precision of confidence intervals. Our evaluation\non real-world datasets confirms that STMGNN-ZINB outperforms existing models,\nproviding a more reliable tool for predicting and understanding crime dynamics.\n","authors":["Zepu Wang","Xiaobo Ma","Huajie Yang","Weimin Lvu","Peng Sun","Sharath Chandra Guntuku"],"pdf_url":"https://arxiv.org/pdf/2408.04193v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.12539v3","updated":"2024-08-08T03:20:17Z","published":"2023-08-24T03:53:55Z","title":"CALM : A Multi-task Benchmark for Comprehensive Assessment of Language\n  Model Bias","summary":"  As language models (LMs) become increasingly powerful and widely used, it is\nimportant to quantify them for sociodemographic bias with potential for harm.\nPrior measures of bias are sensitive to perturbations in the templates designed\nto compare performance across social groups, due to factors such as low\ndiversity or limited number of templates. Also, most previous work considers\nonly one NLP task. We introduce Comprehensive Assessment of Language Models\n(CALM) for robust measurement of two types of universally relevant\nsociodemographic bias, gender and race. CALM integrates sixteen datasets for\nquestion-answering, sentiment analysis and natural language inference. Examples\nfrom each dataset are filtered to produce 224 templates with high diversity\n(e.g., length, vocabulary). We assemble 50 highly frequent person names for\neach of seven distinct demographic groups to generate 78,400 prompts covering\nthe three NLP tasks. Our empirical evaluation shows that CALM bias scores are\nmore robust and far less sensitive than previous bias measurements to\nperturbations in the templates, such as synonym substitution, or to random\nsubset selection of templates. We apply CALM to 20 large language models, and\nfind that for 2 language model series, larger parameter models tend to be more\nbiased than smaller ones. The T0 series is the least biased model families, of\nthe 20 LLMs investigated here. The code is available at\nhttps://github.com/vipulgupta1011/CALM.\n","authors":["Vipul Gupta","Pranav Narayanan Venkit","Hugo Laurenon","Shomir Wilson","Rebecca J. Passonneau"],"pdf_url":"https://arxiv.org/pdf/2308.12539v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04190v1","updated":"2024-08-08T03:18:42Z","published":"2024-08-08T03:18:42Z","title":"Listwise Reward Estimation for Offline Preference-based Reinforcement\n  Learning","summary":"  In Reinforcement Learning (RL), designing precise reward functions remains to\nbe a challenge, particularly when aligning with human intent. Preference-based\nRL (PbRL) was introduced to address this problem by learning reward models from\nhuman feedback. However, existing PbRL methods have limitations as they often\noverlook the second-order preference that indicates the relative strength of\npreference. In this paper, we propose Listwise Reward Estimation (LiRE), a\nnovel approach for offline PbRL that leverages second-order preference\ninformation by constructing a Ranked List of Trajectories (RLT), which can be\nefficiently built by using the same ternary feedback type as traditional\nmethods. To validate the effectiveness of LiRE, we propose a new offline PbRL\ndataset that objectively reflects the effect of the estimated rewards. Our\nextensive experiments on the dataset demonstrate the superiority of LiRE, i.e.,\noutperforming state-of-the-art baselines even with modest feedback budgets and\nenjoying robustness with respect to the number of feedbacks and feedback noise.\nOur code is available at https://github.com/chwoong/LiRE\n","authors":["Heewoong Choi","Sangwon Jung","Hongjoon Ahn","Taesup Moon"],"pdf_url":"https://arxiv.org/pdf/2408.04190v1.pdf","comment":"21 pages, ICML 2024"},{"id":"http://arxiv.org/abs/2308.09296v4","updated":"2024-08-08T03:06:37Z","published":"2023-08-18T04:45:56Z","title":"CARLA: Self-supervised Contrastive Representation Learning for Time\n  Series Anomaly Detection","summary":"  One main challenge in time series anomaly detection (TSAD) is the lack of\nlabelled data in many real-life scenarios. Most of the existing anomaly\ndetection methods focus on learning the normal behaviour of unlabelled time\nseries in an unsupervised manner. The normal boundary is often defined tightly,\nresulting in slight deviations being classified as anomalies, consequently\nleading to a high false positive rate and a limited ability to generalise\nnormal patterns. To address this, we introduce a novel end-to-end\nself-supervised ContrAstive Representation Learning approach for time series\nAnomaly detection (CARLA). While existing contrastive learning methods assume\nthat augmented time series windows are positive samples and temporally distant\nwindows are negative samples, we argue that these assumptions are limited as\naugmentation of time series can transform them to negative samples, and a\ntemporally distant window can represent a positive sample. Our contrastive\napproach leverages existing generic knowledge about time series anomalies and\ninjects various types of anomalies as negative samples. Therefore, CARLA not\nonly learns normal behaviour but also learns deviations indicating anomalies.\nIt creates similar representations for temporally closed windows and distinct\nones for anomalies. Additionally, it leverages the information about\nrepresentations' neighbours through a self-supervised approach to classify\nwindows based on their nearest/furthest neighbours to further enhance the\nperformance of anomaly detection. In extensive tests on seven major real-world\ntime series anomaly detection datasets, CARLA shows superior performance over\nstate-of-the-art self-supervised and unsupervised TSAD methods. Our research\nshows the potential of contrastive representation learning to advance time\nseries anomaly detection.\n","authors":["Zahra Zamanzadeh Darban","Geoffrey I. Webb","Shirui Pan","Charu C. Aggarwal","Mahsa Salehi"],"pdf_url":"https://arxiv.org/pdf/2308.09296v4.pdf","comment":"36 pages, 9 figures, 9 tables"},{"id":"http://arxiv.org/abs/2408.04179v1","updated":"2024-08-08T02:53:09Z","published":"2024-08-08T02:53:09Z","title":"An Upper Confidence Bound Approach to Estimating the Maximum Mean","summary":"  Estimating the maximum mean finds a variety of applications in practice. In\nthis paper, we study estimation of the maximum mean using an upper confidence\nbound (UCB) approach where the sampling budget is adaptively allocated to one\nof the systems. We study in depth the existing grand average (GA) estimator,\nand propose a new largest-size average (LSA) estimator. Specifically, we\nestablish statistical guarantees, including strong consistency, asymptotic mean\nsquared errors, and central limit theorems (CLTs) for both estimators, which\nare new to the literature. We show that LSA is preferable over GA, as the bias\nof the former decays at a rate much faster than that of the latter when sample\nsize increases. By using the CLTs, we further construct asymptotically valid\nconfidence intervals for the maximum mean, and propose a single hypothesis test\nfor a multiple comparison problem with application to clinical trials.\nStatistical efficiency of the resulting point and interval estimates and the\nproposed single hypothesis test is demonstrated via numerical examples.\n","authors":["Zhang Kun","Liu Guangwu","Shi Wen"],"pdf_url":"https://arxiv.org/pdf/2408.04179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.12808v2","updated":"2024-08-08T02:44:47Z","published":"2024-02-20T08:27:50Z","title":"Learning Generalization and Regularization of Nonhomogeneous Temporal\n  Poisson Processes","summary":"  The Poisson process, especially the nonhomogeneous Poisson process (NHPP), is\nan essentially important counting process with numerous real-world\napplications. Up to date, almost all works in the literature have been on the\nestimation of NHPPs with infinite data using non-data driven binning methods.\nIn this paper, we formulate the problem of estimation of NHPPs from finite and\nlimited data as a learning generalization problem. We mathematically show that\nwhile binning methods are essential for the estimation of NHPPs, they pose a\nthreat of overfitting when the amount of data is limited. We propose a\nframework for regularized learning of NHPPs with two new adaptive and\ndata-driven binning methods that help to remove the ad-hoc tuning of binning\nparameters. Our methods are experimentally tested on synthetic and real-world\ndatasets and the results show their effectiveness.\n","authors":["Son Nguyen Van","Hoai Nguyen Xuan"],"pdf_url":"https://arxiv.org/pdf/2402.12808v2.pdf","comment":"This work has been submitted to the IEEE for possible publication.\n  Copyright may be transferred without notice, after which this version may no\n  longer be accessible"},{"id":"http://arxiv.org/abs/2408.04175v1","updated":"2024-08-08T02:38:19Z","published":"2024-08-08T02:38:19Z","title":"pyBregMan: A Python library for Bregman Manifolds","summary":"  A Bregman manifold is a synonym for a dually flat space in information\ngeometry which admits as a canonical divergence a Bregman divergence. Bregman\nmanifolds are induced by smooth strictly convex functions like the cumulant or\npartition functions of regular exponential families, the negative entropy of\nmixture families, or the characteristic functions of regular cones just to list\na few such convex Bregman generators. We describe the design of pyBregMan, a\nlibrary which implements generic operations on Bregman manifolds and\ninstantiate several common Bregman manifolds used in information sciences. At\nthe core of the library is the notion of Legendre-Fenchel duality inducing a\ncanonical pair of dual potential functions and dual Bregman divergences. The\nlibrary also implements the Fisher-Rao manifolds of categorical/multinomial\ndistributions and multivariate normal distributions. To demonstrate the use of\nthe pyBregMan kernel manipulating those Bregman and Fisher-Rao manifolds, the\nlibrary also provides several core algorithms for various applications in\nstatistics, machine learning, information fusion, and so on.\n","authors":["Frank Nielsen","Alexander Soen"],"pdf_url":"https://arxiv.org/pdf/2408.04175v1.pdf","comment":"28 pages"},{"id":"http://arxiv.org/abs/2408.04174v1","updated":"2024-08-08T02:36:04Z","published":"2024-08-08T02:36:04Z","title":"wav2graph: A Framework for Supervised Learning Knowledge Graph from\n  Speech","summary":"  Knowledge graphs (KGs) enhance the performance of large language models\n(LLMs) and search engines by providing structured, interconnected data that\nimproves reasoning and context-awareness. However, KGs only focus on text data,\nthereby neglecting other modalities such as speech. In this work, we introduce\nwav2graph, the first framework for supervised learning knowledge graph from\nspeech data. Our pipeline are straightforward: (1) constructing a KG based on\ntranscribed spoken utterances and a named entity database, (2) converting KG\ninto embedding vectors, and (3) training graph neural networks (GNNs) for node\nclassification and link prediction tasks. Through extensive experiments\nconducted in inductive and transductive learning contexts using\nstate-of-the-art GNN models, we provide baseline results and error analysis for\nnode classification and link prediction tasks on human transcripts and\nautomatic speech recognition (ASR) transcripts, including evaluations using\nboth encoder-based and decoder-based node embeddings, as well as monolingual\nand multilingual acoustic pre-trained models. All related code, data, and\nmodels are published online.\n","authors":["Khai Le-Duc","Quy-Anh Dang","Tan-Hanh Pham","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2408.04174v1.pdf","comment":"Preprint, 32 pages"},{"id":"http://arxiv.org/abs/2207.05377v2","updated":"2024-08-08T02:10:45Z","published":"2022-07-12T08:20:41Z","title":"On the Generalization for Transfer Learning: An Information-Theoretic\n  Analysis","summary":"  Transfer learning, or domain adaptation, is concerned with machine learning\nproblems in which training and testing data come from possibly different\nprobability distributions. In this work, we give an information-theoretic\nanalysis of the generalization error and excess risk of transfer learning\nalgorithms. Our results suggest, perhaps as expected, that the Kullback-Leibler\n(KL) divergence $D(\\mu\\|\\mu')$ plays an important role in the characterizations\nwhere $\\mu$ and $\\mu'$ denote the distribution of the training data and the\ntesting data, respectively. Specifically, we provide generalization error and\nexcess risk upper bounds for learning algorithms where data from both\ndistributions are available in the training phase. Recognizing that the bounds\ncould be sub-optimal in general, we provide improved excess risk upper bounds\nfor a certain class of algorithms, including the empirical risk minimization\n(ERM) algorithm, by making stronger assumptions through the \\textit{central\ncondition}. To demonstrate the usefulness of the bounds, we further extend the\nanalysis to the Gibbs algorithm and the noisy stochastic gradient descent\nmethod. We then generalize the mutual information bound with other divergences\nsuch as $\\phi$-divergence and Wasserstein distance, which may lead to tighter\nbounds and can handle the case when $\\mu$ is not absolutely continuous with\nrespect to $\\mu'$. Several numerical results are provided to demonstrate our\ntheoretical findings. Lastly, to address the problem that the bounds are often\nnot directly applicable in practice due to the absence of the distributional\nknowledge of the data, we develop an algorithm (called InfoBoost) that\ndynamically adjusts the importance weights for both source and target data\nbased on certain information measures. The empirical results show the\neffectiveness of the proposed algorithm.\n","authors":["Xuetong Wu","Jonathan H. Manton","Uwe Aickelin","Jingge Zhu"],"pdf_url":"https://arxiv.org/pdf/2207.05377v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.05879v2","updated":"2024-08-08T01:54:30Z","published":"2024-04-08T21:26:04Z","title":"Rapid and Precise Topological Comparison with Merge Tree Neural Networks","summary":"  Merge trees are a valuable tool in the scientific visualization of scalar\nfields; however, current methods for merge tree comparisons are computationally\nexpensive, primarily due to the exhaustive matching between tree nodes. To\naddress this challenge, we introduce the Merge Tree Neural Network (MTNN), a\nlearned neural network model designed for merge tree comparison. The MTNN\nenables rapid and high-quality similarity computation. We first demonstrate how\nto train graph neural networks, which emerged as effective encoders for graphs,\nin order to produce embeddings of merge trees in vector spaces for efficient\nsimilarity comparison. Next, we formulate the novel MTNN model that further\nimproves the similarity comparisons by integrating the tree and node embeddings\nwith a new topological attention mechanism. We demonstrate the effectiveness of\nour model on real-world data in different domains and examine our model's\ngeneralizability across various datasets. Our experimental analysis\ndemonstrates our approach's superiority in accuracy and efficiency. In\nparticular, we speed up the prior state-of-the-art by more than $100\\times$ on\nthe benchmark datasets while maintaining an error rate below $0.1\\%$.\n","authors":["Yu Qin","Brittany Terese Fasy","Carola Wenk","Brian Summa"],"pdf_url":"https://arxiv.org/pdf/2404.05879v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2309.00736v4","updated":"2024-08-08T01:49:10Z","published":"2023-09-01T21:16:02Z","title":"Prediction Error Estimation in Random Forests","summary":"  In this paper, error estimates of classification Random Forests are\nquantitatively assessed. Based on the initial theoretical framework built by\nBates et al. (2023), the true error rate and expected error rate are\ntheoretically and empirically investigated in the context of a variety of error\nestimation methods common to Random Forests. We show that in the classification\ncase, Random Forests' estimates of prediction error is closer on average to the\ntrue error rate instead of the average prediction error. This is opposite the\nfindings of Bates et al. (2023) which are given for logistic regression. We\nfurther show that our result holds across different error estimation strategies\nsuch as cross-validation, bagging, and data splitting.\n","authors":["Ian Krupkin","Johanna Hardin"],"pdf_url":"https://arxiv.org/pdf/2309.00736v4.pdf","comment":"As we were working on revisions, we found a fatal flaw in the\n  procedure. All of the results are problematic / wrong"},{"id":"http://arxiv.org/abs/2311.08817v2","updated":"2024-08-08T01:46:19Z","published":"2023-11-15T09:38:53Z","title":"MAP's not dead yet: Uncovering true language model modes by conditioning\n  away degeneracy","summary":"  It has been widely observed that exact or approximate MAP (mode-seeking)\ndecoding from natural language generation (NLG) models consistently leads to\ndegenerate outputs (Holtzman et al., 2019; Stahlberg and Byrne, 2019). Prior\nwork has attributed this behavior to either a fundamental and unavoidable\ninadequacy of modes in probabilistic models or weaknesses in language modeling.\nContrastingly, we argue that degenerate modes can even occur in the absence of\nany modeling error, due to contamination of the training data. Specifically, we\nargue that mixing even a tiny amount of low-entropy noise with a population\ntext distribution can cause the data distribution's mode to become degenerate.\nWe therefore propose to apply MAP decoding to the model's true conditional\ndistribution where the conditioning variable explicitly avoids specific\ndegenerate behavior. Using exact search, we empirically verify that the\nlength-conditional modes of machine translation models and language models are\nindeed more fluent and topical than their unconditional modes. For the first\ntime, we also share many examples of exact modal sequences from these models,\nand from several variants of the LLaMA-7B model. Notably, we observe that\nvarious kinds of degenerate modes persist, even at the scale of LLaMA-7B.\nAlthough we cannot tractably address these degeneracies with exact search, we\nperform a classifier-based approximate search on LLaMA-7B, a model which was\nnot trained for instruction following, and find that we are able to elicit\nreasonable outputs without any finetuning.\n","authors":["Davis Yoshida","Kartik Goyal","Kevin Gimpel"],"pdf_url":"https://arxiv.org/pdf/2311.08817v2.pdf","comment":"52 pages, 5 figures, ACL version"},{"id":"http://arxiv.org/abs/2408.04154v1","updated":"2024-08-08T01:42:31Z","published":"2024-08-08T01:42:31Z","title":"The Data Addition Dilemma","summary":"  In many machine learning for healthcare tasks, standard datasets are\nconstructed by amassing data across many, often fundamentally dissimilar,\nsources. But when does adding more data help, and when does it hinder progress\non desired model outcomes in real-world settings? We identify this situation as\nthe \\textit{Data Addition Dilemma}, demonstrating that adding training data in\nthis multi-source scaling context can at times result in reduced overall\naccuracy, uncertain fairness outcomes, and reduced worst-subgroup performance.\nWe find that this possibly arises from an empirically observed trade-off\nbetween model performance improvements due to data scaling and model\ndeterioration from distribution shift. We thus establish baseline strategies\nfor navigating this dilemma, introducing distribution shift heuristics to guide\ndecision-making on which data sources to add in data scaling, in order to yield\nthe expected model performance improvements. We conclude with a discussion of\nthe required considerations for data collection and suggestions for studying\ndata composition and scale in the age of increasingly larger models.\n","authors":["Judy Hanwen Shen","Inioluwa Deborah Raji","Irene Y. Chen"],"pdf_url":"https://arxiv.org/pdf/2408.04154v1.pdf","comment":"Machine Learning For Health Care 2024 (MLHC)"},{"id":"http://arxiv.org/abs/2408.00992v3","updated":"2024-08-08T01:23:11Z","published":"2024-08-02T03:44:14Z","title":"Fairness in Large Language Models in Three Hours","summary":"  Large Language Models (LLMs) have demonstrated remarkable success across\nvarious domains but often lack fairness considerations, potentially leading to\ndiscriminatory outcomes against marginalized populations. Unlike fairness in\ntraditional machine learning, fairness in LLMs involves unique backgrounds,\ntaxonomies, and fulfillment techniques. This tutorial provides a systematic\noverview of recent advances in the literature concerning fair LLMs, beginning\nwith real-world case studies to introduce LLMs, followed by an analysis of bias\ncauses therein. The concept of fairness in LLMs is then explored, summarizing\nthe strategies for evaluating bias and the algorithms designed to promote\nfairness. Additionally, resources for assessing bias in LLMs, including\ntoolkits and datasets, are compiled, and current research challenges and open\nquestions in the field are discussed. The repository is available at\n\\url{https://github.com/LavinWong/Fairness-in-Large-Language-Models}.\n","authors":["Thang Doan Viet","Zichong Wang","Minh Nhat Nguyen","Wenbin Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.00992v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.09340v4","updated":"2024-08-08T00:30:35Z","published":"2023-03-16T14:21:45Z","title":"Improving Automated Hemorrhage Detection in Sparse-view Computed\n  Tomography via Deep Convolutional Neural Network based Artifact Reduction","summary":"  This is a preprint. The latest version has been published here:\nhttps://pubs.rsna.org/doi/10.1148/ryai.230275\n  Purpose: Sparse-view computed tomography (CT) is an effective way to reduce\ndose by lowering the total number of views acquired, albeit at the expense of\nimage quality, which, in turn, can impact the ability to detect diseases. We\nexplore deep learning-based artifact reduction in sparse-view cranial CT scans\nand its impact on automated hemorrhage detection. Methods: We trained a U-Net\nfor artefact reduction on simulated sparse-view cranial CT scans from 3000\npatients obtained from a public dataset and reconstructed with varying levels\nof sub-sampling. Additionally, we trained a convolutional neural network on\nfully sampled CT data from 17,545 patients for automated hemorrhage detection.\nWe evaluated the classification performance using the area under the receiver\noperator characteristic curves (AUC-ROCs) with corresponding 95% confidence\nintervals (CIs) and the DeLong test, along with confusion matrices. The\nperformance of the U-Net was compared to an analytical approach based on total\nvariation (TV). Results: The U-Net performed superior compared to unprocessed\nand TV-processed images with respect to image quality and automated hemorrhage\ndiagnosis. With U-Net post-processing, the number of views can be reduced from\n4096 (AUC-ROC: 0.974; 95% CI: 0.972-0.976) views to 512 views (0.973;\n0.971-0.975) with minimal decrease in hemorrhage detection (P<.001) and to 256\nviews (0.967; 0.964-0.969) with a slight performance decrease (P<.001).\nConclusion: The results suggest that U-Net based artifact reduction\nsubstantially enhances automated hemorrhage detection in sparse-view cranial\nCTs. Our findings highlight that appropriate post-processing is crucial for\noptimal image quality and diagnostic accuracy while minimizing radiation dose.\n","authors":["Johannes Thalhammer","Manuel Schultheiss","Tina Dorosti","Tobias Lasser","Franz Pfeiffer","Daniela Pfeiffer","Florian Schaff"],"pdf_url":"https://arxiv.org/pdf/2303.09340v4.pdf","comment":"11 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2406.15786v4","updated":"2024-08-08T00:30:20Z","published":"2024-06-22T08:41:48Z","title":"What Matters in Transformers? Not All Attention is Needed","summary":"  Scaling Transformer-based large language models (LLMs) has demonstrated\npromising performance across various tasks. However, it also introduces\nredundant structures, posing challenges for real-world deployment. Despite some\nrecognition of redundancy in LLMs, the variability of redundancy across\ndifferent modules, such as MLP and Attention layers, is under-explored. In this\nwork, we investigate the varying redundancy across different modules within\nTransformers, including Blocks, MLP, and Attention layers, using a\nsimilarity-based metric. This metric operates on the premise that redundant\nstructures produce outputs highly similar to their inputs. Surprisingly, while\nattention layers are essential for transformers and distinguish them from other\nmainstream architectures, we found that a large proportion of attention layers\nexhibit excessively high similarity and can be safely pruned without degrading\nperformance, leading to reduced memory and computation costs. Additionally, we\nfurther propose a method that jointly drops Attention and MLP layers, achieving\nimproved performance and dropping ratios. Extensive experiments demonstrate the\neffectiveness of our methods, e.g., Llama-3-70B maintains comparable\nperformance even after pruning half of the attention layers. Our findings\nprovide valuable insights for future network architecture design. The code is\nreleased at: \\url{https://github.com/Shwai-He/LLM-Drop}.\n","authors":["Shwai He","Guoheng Sun","Zheyu Shen","Ang Li"],"pdf_url":"https://arxiv.org/pdf/2406.15786v4.pdf","comment":"15 pages, 13 figures, 6 tables"}],"Multimedia":[{"id":"http://arxiv.org/abs/2408.04547v1","updated":"2024-08-08T15:55:35Z","published":"2024-08-08T15:55:35Z","title":"Emotional Cues Extraction and Fusion for Multi-modal Emotion Prediction\n  and Recognition in Conversation","summary":"  Emotion Prediction in Conversation (EPC) aims to forecast the emotions of\nforthcoming utterances by utilizing preceding dialogues. Previous EPC\napproaches relied on simple context modeling for emotion extraction,\noverlooking fine-grained emotion cues at the word level. Additionally, prior\nworks failed to account for the intrinsic differences between modalities,\nresulting in redundant information. To overcome these limitations, we propose\nan emotional cues extraction and fusion network, which consists of two stages:\na modality-specific learning stage that utilizes word-level labels and prosody\nlearning to construct emotion embedding spaces for each modality, and a\ntwo-step fusion stage for integrating multi-modal features. Moreover, the\nemotion features extracted by our model are also applicable to the Emotion\nRecognition in Conversation (ERC) task. Experimental results validate the\nefficacy of the proposed method, demonstrating superior performance on both\nIEMOCAP and MELD datasets.\n","authors":["Haoxiang Shi","Ziqi Liang","Jun Yu"],"pdf_url":"https://arxiv.org/pdf/2408.04547v1.pdf","comment":"Accepted by INTERSPEECH 2024"},{"id":"http://arxiv.org/abs/2408.04388v1","updated":"2024-08-08T11:44:57Z","published":"2024-08-08T11:44:57Z","title":"MM-Forecast: A Multimodal Approach to Temporal Event Forecasting with\n  Large Language Models","summary":"  We study an emerging and intriguing problem of multimodal temporal event\nforecasting with large language models. Compared to using text or graph\nmodalities, the investigation of utilizing images for temporal event\nforecasting has not been fully explored, especially in the era of large\nlanguage models (LLMs). To bridge this gap, we are particularly interested in\ntwo key questions of: 1) why images will help in temporal event forecasting,\nand 2) how to integrate images into the LLM-based forecasting framework. To\nanswer these research questions, we propose to identify two essential functions\nthat images play in the scenario of temporal event forecasting, i.e.,\nhighlighting and complementary. Then, we develop a novel framework, named\nMM-Forecast. It employs an Image Function Identification module to recognize\nthese functions as verbal descriptions using multimodal large language models\n(MLLMs), and subsequently incorporates these function descriptions into\nLLM-based forecasting models. To evaluate our approach, we construct a new\nmultimodal dataset, MidEast-TE-mm, by extending an existing event dataset\nMidEast-TE-mini with images. Empirical studies demonstrate that our MM-Forecast\ncan correctly identify the image functions, and further more, incorporating\nthese verbal function descriptions significantly improves the forecasting\nperformance. The dataset, code, and prompts are available at\nhttps://github.com/LuminosityX/MM-Forecast.\n","authors":["Haoxuan Li","Zhengmao Yang","Yunshan Ma","Yi Bin","Yang Yang","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2408.04388v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04383v1","updated":"2024-08-08T11:38:27Z","published":"2024-08-08T11:38:27Z","title":"The algorithmic nature of song-sequencing: statistical regularities in\n  music albums","summary":"  Based on a review of anecdotal beliefs, we explored patterns of\ntrack-sequencing within professional music albums. We found that songs with\nhigh levels of valence, energy and loudness are more likely to be positioned at\nthe beginning of each album. We also found that transitions between consecutive\ntracks tend to alternate between increases and decreases of valence and energy.\nThese findings were used to build a system which automates the process of\nalbum-sequencing. Our results and hypothesis have both practical and\ntheoretical applications. Practically, sequencing regularities can be used to\ninform playlist generation systems. Theoretically, we show weak to moderate\nsupport for the idea that music is perceived in both global and local contexts.\n","authors":["Pedro Neto","Martin Hartmann","Geoff Luck","Petri Toiviainen"],"pdf_url":"https://arxiv.org/pdf/2408.04383v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01669v3","updated":"2024-08-08T11:19:37Z","published":"2024-08-03T05:35:13Z","title":"SynopGround: A Large-Scale Dataset for Multi-Paragraph Video Grounding\n  from TV Dramas and Synopses","summary":"  Video grounding is a fundamental problem in multimodal content understanding,\naiming to localize specific natural language queries in an untrimmed video.\nHowever, current video grounding datasets merely focus on simple events and are\neither limited to shorter videos or brief sentences, which hinders the model\nfrom evolving toward stronger multimodal understanding capabilities. To address\nthese limitations, we present a large-scale video grounding dataset named\nSynopGround, in which more than 2800 hours of videos are sourced from popular\nTV dramas and are paired with accurately localized human-written synopses. Each\nparagraph in the synopsis serves as a language query and is manually annotated\nwith precise temporal boundaries in the long video. These paragraph queries are\ntightly correlated to each other and contain a wealth of abstract expressions\nsummarizing video storylines and specific descriptions portraying event\ndetails, which enables the model to learn multimodal perception on more\nintricate concepts over longer context dependencies. Based on the dataset, we\nfurther introduce a more complex setting of video grounding dubbed\nMulti-Paragraph Video Grounding (MPVG), which takes as input multiple\nparagraphs and a long video for grounding each paragraph query to its temporal\ninterval. In addition, we propose a novel Local-Global Multimodal Reasoner\n(LGMR) to explicitly model the local-global structures of long-term multimodal\ninputs for MPVG. Our method provides an effective baseline solution to the\nmulti-paragraph video grounding problem. Extensive experiments verify the\nproposed model's effectiveness as well as its superiority in long-term\nmulti-paragraph video grounding over prior state-of-the-arts. Dataset and code\nare publicly available. Project page: https://synopground.github.io/.\n","authors":["Chaolei Tan","Zihang Lin","Junfu Pu","Zhongang Qi","Wei-Yi Pei","Zhi Qu","Yexin Wang","Ying Shan","Wei-Shi Zheng","Jian-Fang Hu"],"pdf_url":"https://arxiv.org/pdf/2408.01669v3.pdf","comment":"Accepted to ACM MM 2024. Project page: https://synopground.github.io/"},{"id":"http://arxiv.org/abs/2305.08389v3","updated":"2024-08-08T09:28:22Z","published":"2023-05-15T07:12:19Z","title":"Edit As You Wish: Video Caption Editing with Multi-grained User Control","summary":"  Automatically narrating videos in natural language complying with user\nrequests, i.e. Controllable Video Captioning task, can help people manage\nmassive videos with desired intentions. However, existing works suffer from two\nshortcomings: 1) the control signal is single-grained which can not satisfy\ndiverse user intentions; 2) the video description is generated in a single\nround which can not be further edited to meet dynamic needs. In this paper, we\npropose a novel \\textbf{V}ideo \\textbf{C}aption \\textbf{E}diting \\textbf{(VCE)}\ntask to automatically revise an existing video description guided by\nmulti-grained user requests. Inspired by human writing-revision habits, we\ndesign the user command as a pivotal triplet \\{\\textit{operation, position,\nattribute}\\} to cover diverse user needs from coarse-grained to fine-grained.\nTo facilitate the VCE task, we \\textit{automatically} construct an open-domain\nbenchmark dataset named VATEX-EDIT and \\textit{manually} collect an e-commerce\ndataset called EMMAD-EDIT. We further propose a specialized small-scale model\n(i.e., OPA) compared with two generalist Large Multi-modal Models to perform an\nexhaustive analysis of the novel task. For evaluation, we adopt comprehensive\nmetrics considering caption fluency, command-caption consistency, and\nvideo-caption alignment. Experiments reveal the task challenges of fine-grained\nmulti-modal semantics understanding and processing. Our datasets, codes, and\nevaluation tools are available at https://github.com/yaolinli/VCE.\n","authors":["Linli Yao","Yuanmeng Zhang","Ziheng Wang","Xinglin Hou","Tiezheng Ge","Yuning Jiang","Xu Sun","Qin Jin"],"pdf_url":"https://arxiv.org/pdf/2305.08389v3.pdf","comment":"Accepted by ACM MM 2024"},{"id":"http://arxiv.org/abs/2408.04326v1","updated":"2024-08-08T09:09:37Z","published":"2024-08-08T09:09:37Z","title":"Multi-Scale and Detail-Enhanced Segment Anything Model for Salient\n  Object Detection","summary":"  Salient Object Detection (SOD) aims to identify and segment the most\nprominent objects in images. Advanced SOD methods often utilize various\nConvolutional Neural Networks (CNN) or Transformers for deep feature\nextraction. However, these methods still deliver low performance and poor\ngeneralization in complex cases. Recently, Segment Anything Model (SAM) has\nbeen proposed as a visual fundamental model, which gives strong segmentation\nand generalization capabilities. Nonetheless, SAM requires accurate prompts of\ntarget objects, which are unavailable in SOD. Additionally, SAM lacks the\nutilization of multi-scale and multi-level information, as well as the\nincorporation of fine-grained details. To address these shortcomings, we\npropose a Multi-scale and Detail-enhanced SAM (MDSAM) for SOD. Specifically, we\nfirst introduce a Lightweight Multi-Scale Adapter (LMSA), which allows SAM to\nlearn multi-scale information with very few trainable parameters. Then, we\npropose a Multi-Level Fusion Module (MLFM) to comprehensively utilize the\nmulti-level information from the SAM's encoder. Finally, we propose a Detail\nEnhancement Module (DEM) to incorporate SAM with fine-grained details.\nExperimental results demonstrate the superior performance of our model on\nmultiple SOD datasets and its strong generalization on other segmentation\ntasks. The source code is released at https://github.com/BellyBeauty/MDSAM.\n","authors":["Shixuan Gao","Pingping Zhang","Tianyu Yan","Huchuan Lu"],"pdf_url":"https://arxiv.org/pdf/2408.04326v1.pdf","comment":"This work is accepted by ACM MM2024"},{"id":"http://arxiv.org/abs/2406.07430v2","updated":"2024-08-08T07:34:50Z","published":"2024-06-11T16:34:02Z","title":"Learning Domain-Invariant Features for Out-of-Context News Detection","summary":"  Out-of-context news is a common type of misinformation on online media\nplatforms. This involves posting a caption, alongside a mismatched news image.\nExisting out-of-context news detection models only consider the scenario where\npre-labeled data is available for each domain, failing to address the\nout-of-context news detection on unlabeled domains (e.g. news topics or\nagencies). In this work, we therefore focus on domain adaptive out-of-context\nnews detection. In order to effectively adapt the detection model to unlabeled\nnews topics or agencies, we propose ConDA-TTA (Contrastive Domain Adaptation\nwith Test-Time Adaptation) which applies contrastive learning and maximum mean\ndiscrepancy (MMD) to learn domain-invariant features. In addition, we leverage\ntest-time target domain statistics to further assist domain adaptation.\nExperimental results show that our approach outperforms baselines in most\ndomain adaptation settings on two public datasets, by as much as 2.93% in F1\nand 2.08% in accuracy.\n","authors":["Yimeng Gu","Mengqi Zhang","Ignacio Castro","Shu Wu","Gareth Tyson"],"pdf_url":"https://arxiv.org/pdf/2406.07430v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04243v1","updated":"2024-08-08T06:16:00Z","published":"2024-08-08T06:16:00Z","title":"MU-MAE: Multimodal Masked Autoencoders-Based One-Shot Learning","summary":"  With the exponential growth of multimedia data, leveraging multimodal sensors\npresents a promising approach for improving accuracy in human activity\nrecognition. Nevertheless, accurately identifying these activities using both\nvideo data and wearable sensor data presents challenges due to the\nlabor-intensive data annotation, and reliance on external pretrained models or\nadditional data. To address these challenges, we introduce Multimodal Masked\nAutoencoders-Based One-Shot Learning (Mu-MAE). Mu-MAE integrates a multimodal\nmasked autoencoder with a synchronized masking strategy tailored for wearable\nsensors. This masking strategy compels the networks to capture more meaningful\nspatiotemporal features, which enables effective self-supervised pretraining\nwithout the need for external data. Furthermore, Mu-MAE leverages the\nrepresentation extracted from multimodal masked autoencoders as prior\ninformation input to a cross-attention multimodal fusion layer. This fusion\nlayer emphasizes spatiotemporal features requiring attention across different\nmodalities while highlighting differences from other classes, aiding in the\nclassification of various classes in metric-based one-shot learning.\nComprehensive evaluations on MMAct one-shot classification show that Mu-MAE\noutperforms all the evaluated approaches, achieving up to an 80.17% accuracy\nfor five-way one-shot multimodal classification, without the use of additional\ndata.\n","authors":["Rex Liu","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.04243v1.pdf","comment":"IEEE MIPR 2024"},{"id":"http://arxiv.org/abs/2408.04172v1","updated":"2024-08-08T02:34:41Z","published":"2024-08-08T02:34:41Z","title":"MultiColor: Image Colorization by Learning from Multiple Color Spaces","summary":"  Deep networks have shown impressive performance in the image restoration\ntasks, such as image colorization. However, we find that previous approaches\nrely on the digital representation from single color model with a specific\nmapping function, a.k.a., color space, during the colorization pipeline. In\nthis paper, we first investigate the modeling of different color spaces, and\nfind each of them exhibiting distinctive characteristics with unique\ndistribution of colors. The complementarity among multiple color spaces leads\nto benefits for the image colorization task.\n  We present MultiColor, a new learning-based approach to automatically\ncolorize grayscale images that combines clues from multiple color spaces.\nSpecifically, we employ a set of dedicated colorization modules for individual\ncolor space. Within each module, a transformer decoder is first employed to\nrefine color query embeddings and then a color mapper produces color channel\nprediction using the embeddings and semantic features. With these predicted\ncolor channels representing various color spaces, a complementary network is\ndesigned to exploit the complementarity and generate pleasing and reasonable\ncolorized images. We conduct extensive experiments on real-world datasets, and\nthe results demonstrate superior performance over the state-of-the-arts.\n","authors":["Xiangcheng Du","Zhao Zhou","Yanlong Wang","Zhuoyao Wang","Yingbin Zheng","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2408.04172v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.08759v2","updated":"2024-08-08T02:28:17Z","published":"2024-06-13T02:41:11Z","title":"GaussianForest: Hierarchical-Hybrid 3D Gaussian Splatting for Compressed\n  Scene Modeling","summary":"  The field of novel-view synthesis has recently witnessed the emergence of 3D\nGaussian Splatting, which represents scenes in a point-based manner and renders\nthrough rasterization. This methodology, in contrast to Radiance Fields that\nrely on ray tracing, demonstrates superior rendering quality and speed.\nHowever, the explicit and unstructured nature of 3D Gaussians poses a\nsignificant storage challenge, impeding its broader application. To address\nthis challenge, we introduce the Gaussian-Forest modeling framework, which\nhierarchically represents a scene as a forest of hybrid 3D Gaussians. Each\nhybrid Gaussian retains its unique explicit attributes while sharing implicit\nones with its sibling Gaussians, thus optimizing parameterization with\nsignificantly fewer variables. Moreover, adaptive growth and pruning strategies\nare designed, ensuring detailed representation in complex regions and a notable\nreduction in the number of required Gaussians. Extensive experiments\ndemonstrate that Gaussian-Forest not only maintains comparable speed and\nquality but also achieves a compression rate surpassing 10 times, marking a\nsignificant advancement in efficient scene modeling. Codes will be available at\nhttps://github.com/Xian-Bei/GaussianForest.\n","authors":["Fengyi Zhang","Yadan Luo","Tianjun Zhang","Lin Zhang","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2406.08759v2.pdf","comment":null}]}}